{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"coppafish","text":"<p>coppafish is a data analysis pipeline for decoding coppaFISH (combinatorial padlock-probe-amplified fluorescence  in situ hybridization) datasets. coppaFISH  is a method for in situ transcriptomics which produces a series of images arranged in terms of tiles, rounds and  channels. coppafish then determines the distribution of genes via image processing,  spot detection, registration and  gene calling. The distribution of genes (like that shown above - each marker  refers to a different gene) can then be used to determine the location and type of cells using  pciSeq.</p>"},{"location":"#installation","title":"Installation","text":"<p>coppafish supports python 3.8 and above. It can be installed using pip:</p> <pre><code>pip install coppafish\n</code></pre> <p>To use the napari Viewer and matplotlib diagnostics,  the plotting version must be installed:</p> <pre><code>pip install coppafish[plotting]\n</code></pre> <p>To use the optimised code, which is recommended for running the  find spots and OMP sections of the pipeline (otherwise they are very slow), the optimised version must be installed:</p> <pre><code>pip install coppafish[optimised]\n</code></pre> <p>Installing on Windows</p> <p>The optimised code requires jax which is not supported on Windows, thus the optimised version  of coppafish cannot be used on Windows.</p> <p>The optimised and plotting features can both be installed by running:</p> <pre><code>pip install coppafish[optimised,plotting]\n</code></pre>"},{"location":"config/","title":"Default Config Settings","text":""},{"location":"config/#file_names","title":"file_names","text":"<p>The file_names section specifies the files that will be used throughout the pipeline. Variables in this section can be changed at any point in the pipeline, and the notebook created using it can still be loaded in.</p> <ul> <li> <p>notebook_name: str.</p> <p>Name of notebook file in output directory will be notebook_name.npz </p> <p>Default: <code>notebook</code></p> </li> <li> <p>input_dir: str.</p> <p>Directory where the raw .nd2 files or .npy stacks are </p> <p>Default: <code>MUST BE SPECIFIED</code></p> </li> <li> <p>output_dir: str.</p> <p>Directory where notebook is saved </p> <p>Default: <code>MUST BE SPECIFIED</code></p> </li> <li> <p>tile_dir: str.</p> <p>Directory where tile .npy files saved </p> <p>Default: <code>MUST BE SPECIFIED</code></p> </li> <li> <p>round: maybe_list_str.</p> <p>Names of .nd2 files for the imaging rounds. Leave empty if only using anchor. </p> <p>Default: <code>None</code></p> </li> <li> <p>anchor: maybe_str.</p> <p>Name of the file for the anchor round. Leave empty if not using anchor. </p> <p>Default: <code>None</code></p> </li> <li> <p>raw_extension: str.</p> <p>.nd2 or .npy indicating the data type of the raw data. </p> <p>Default: <code>.nd2</code></p> </li> <li> <p>raw_metadata: maybe_str.</p> <p>If .npy raw_extension, this is the name of the .json file in input_dir which contains the metadata required extracted from the initial .nd2 files. I.e. it contains the output of coppafish/utils/nd2/save_metadata: </p> <ul> <li> <p><code>xy_pos</code> - <code>List [n_tiles x 2]</code>. xy position of tiles in pixels. </p> </li> <li> <p><code>pixel_microns</code> - <code>float</code>. xy pixel size in microns. </p> </li> <li> <p><code>pixel_microns_z</code> - <code>float</code>. z pixel size in microns. </p> </li> <li> <p><code>sizes</code> - dict with fov (<code>t</code>), channels (<code>c</code>), y, x, z-planes (<code>z</code>) dimensions. </p> </li> </ul> <p>Default: <code>None</code></p> </li> <li> <p>dye_camera_laser: maybe_file.</p> <p>csv file giving the approximate raw intensity for each dye with each camera/laser combination. If not set, the file coppafish/setup/dye_camera_laser_raw_intensity.csv file will be used. </p> <p>Default: <code>None</code></p> </li> <li> <p>code_book: str.</p> <p>Text file which contains the codes indicating which dye to expect on each round for each gene. </p> <p>Default: <code>MUST BE SPECIFIED</code></p> </li> <li> <p>scale: str.</p> <p>Text file saved in tile_dir containing <code>extract['scale']</code> and <code>extract['scale_anchor']</code> values used to create the tile .npy files in the tile_dir. If the second value is 0, it means <code>extract['scale_anchor']</code> has not been calculated yet. </p> <p>If the extract step of the pipeline is re-run with <code>extract['scale']</code> or <code>extract['scale_anchor']</code> different to values saved here, an error will be raised. </p> <p>Default: <code>scale</code></p> </li> <li> <p>psf: str.</p> <p>npy file in output directory indicating average spot shape. If deconvolution required and file does not exist, will be computed automatically in extract step. (this is psf before tapering and scaled to fill uint16 range). </p> <p>Default: <code>psf</code></p> </li> <li> <p>omp_spot_shape: str.</p> <p>npy file in output_dir indicating average shape in omp coefficient image. It only indicates the sign of the coefficient i.e. only contains -1, 0, 1. If file does not exist, it is computed from the coefficient images of all genes of the central tile. </p> <p>Default: <code>omp_spot_shape</code></p> </li> <li> <p>omp_spot_info: str.</p> <p>npy file in output_dir containing information about spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. </p> <p>Default: <code>omp_spot_info</code></p> </li> <li> <p>omp_spot_coef: str.</p> <p>npz file in output_dir containing gene coefficients for all spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. </p> <p>Default: <code>omp_spot_coef</code></p> </li> <li> <p>big_dapi_image: maybe_str.</p> <p>npz file in output_dir where stitched DAPI image is saved. If it does not exist, it will be saved if <code>basic_info['dapi_channel']</code> is not <code>None</code>. Leave blank to not save stitched anchor </p> <p>Default: <code>dapi_image</code></p> </li> <li> <p>big_anchor_image: maybe_str.</p> <p>npz file in output_dir where stitched image of <code>ref_round</code>/<code>ref_channel</code> is saved. If it does not exist, it will be saved. Leave blank to not save stitched anchor </p> <p>Default: <code>anchor_image</code></p> </li> <li> <p>pciseq: list_str.</p> <p>csv files in output_dir where plotting information for pciSeq will be saved. First file is name where omp method output will be saved. Second file is name where ref_spots method output will be saved. If files don't exist, they will be created when the function coppafish/export_to_pciseq is run. </p> <p>Default: <code>pciseq_omp, pciseq_anchor</code></p> </li> </ul>"},{"location":"config/#basic_info","title":"basic_info","text":"<p>The basic_info section indicates information required throughout the pipeline.</p> <ul> <li> <p>is_3d: bool.</p> <p>Whether to use the 3d pipeline. </p> <p>Default: <code>MUST BE SPECIFIED</code></p> </li> <li> <p>anchor_channel: maybe_int.</p> <p>Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. Leave blank if anchor not used. </p> <p>Default: <code>None</code></p> </li> <li> <p>dapi_channel: maybe_int.</p> <p>Channel in anchor round that contains DAPI images. This does not have to be in <code>use_channels</code> as anchor round is dealt with separately. Leave blank if no DAPI. </p> <p>Default: <code>None</code></p> </li> <li> <p>ref_round: maybe_int.</p> <p>Round to align all imaging rounds to. Will be set to <code>anchor_round</code> if <code>anchor_channel</code> and <code>file_names['anchor']</code> specified. </p> <p>Default: <code>None</code></p> </li> <li> <p>ref_channel: maybe_int.</p> <p>Channel in <code>ref_round</code> used as reference and to build coordinate system on. Usually channel with most spots. Will be set to <code>anchor_channel</code> if <code>anchor_channel</code> and <code>file_names['anchor']</code> specified. </p> <p>Default: <code>None</code></p> </li> <li> <p>use_channels: maybe_list_int.</p> <p>Channels in imaging rounds to use throughout pipeline. Leave blank to use all. </p> <p>Default: <code>None</code></p> </li> <li> <p>use_rounds: maybe_list_int.</p> <p>Imaging rounds to use throughout pipeline. Leave blank to use all. </p> <p>Default: <code>None</code></p> </li> <li> <p>use_z: maybe_list_int.</p> <p>z planes used to make tile .npy files. Leave blank to use all. If 2 values provided, all z-planes between and including the values given will be used. </p> <p>Default: <code>None</code></p> </li> <li> <p>use_tiles: maybe_list_int.</p> <p>Tiles used throughout pipeline. Leave blank to use all. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: </p> <p>| 2  | 1  | 0  | </p> <p>| 5  | 4  | 3  | </p> <p>| 8  | 7  | 6  | </p> <p>| 11 | 10 | 9  | </p> <p>Default: <code>None</code></p> </li> <li> <p>ignore_tiles: maybe_list_int.</p> <p>It is often easier to select tiles to remove than to use. All tiles listed here will be ignored. Leave blank to use all. </p> <p>Default: <code>None</code></p> </li> <li> <p>use_dyes: maybe_list_int.</p> <p>Dyes to use when when assigning spots to genes. Leave blank to use all. </p> <p>Default: <code>None</code></p> </li> <li> <p>dye_names: maybe_list_str.</p> <p>Name of dyes used in correct order. So for gene with code <code>360...</code>, gene appears with <code>dye_names[3]</code> in round 0, <code>dye_names[6]</code> in round 1, <code>dye_names[0]</code> in round 2 etc. If left blank, then assumes each channel corresponds to a different dye i.e. code 0 in code_book = channel 0. For quad_cam data, this needs to be specified. </p> <p>Default: <code>None</code></p> </li> <li> <p>channel_camera: maybe_list_int.</p> <p><code>channel_camera[i]</code> is the wavelength in nm of the camera used for channel <code>i</code>. Only need to be provided if <code>dye_names</code> provided to help estimate dye intensity in each channel. </p> <p>Default: <code>None</code></p> </li> <li> <p>channel_laser: maybe_list_int.</p> <p><code>channel_laser[i]</code> is the wavelengths in nm of the camera/laser used for channel <code>i</code>. Only need to be provided if <code>dye_names</code> provided to help estimate dye intensity in each channel. </p> <p>Default: <code>None</code></p> </li> <li> <p>tile_pixel_value_shift: int.</p> <p>This is added onto every tile (except DAPI) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to .npy as uint16. </p> <p>Default: <code>15000</code></p> </li> <li> <p>ignore_first_z_plane: bool.</p> <p>Previously had cases where first z plane in .nd2 file was in wrong place and caused focus stacking to be weird or identify lots of spots on first plane. Hence it is safest to not load first plane and this is done if <code>ignore_first_z_plane = True</code>. </p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"config/#extract","title":"extract","text":"<p>The extract section contains parameters which specify how to filter the raw microscope images to produce the .npy files saved to <code>file_names['tile_dir']</code>.</p> <ul> <li> <p>wait_time: int.</p> <p>Time to wait in seconds for raw data to come in before crashing. Assumes first round is already in the <code>file_names['input_dir']</code> Want this to be large so can run pipeline while collecting data. </p> <p>Default: <code>21600</code></p> </li> <li> <p>r1: maybe_int.</p> <p>Filtering is done with a 2D difference of hanning filter with inner radius <code>r1</code> within which it is positive and outer radius <code>r2</code> so annulus between <code>r1</code> and <code>r2</code> is negative. Should be approx radius of spot. Typical = 3. </p> <p>For <code>r1 = 3</code> and <code>r2 = 6</code>, a <code>2048 x 2048 x 50</code> image took 4.1s. For <code>2 &lt;= r1 &lt;= 5</code> and <code>r2</code> double this, the time taken seemed to be constant. </p> <p>Leave blank to auto detect using <code>r1_auto_microns micron</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>r2: maybe_int.</p> <p>Filtering is done with a 2D difference of hanning filter with inner radius <code>r1</code> within which it is positive and outer radius <code>r2</code> so annulus between <code>r1</code> and <code>r2</code> is negative. Should be approx radius of spot. Typical = 6. Leave blank to set to twice <code>r1</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>r_dapi: maybe_int.</p> <p>Filtering for DAPI images is a tophat with r_dapi radius. Should be approx radius of object of interest. Typical = 48. Leave blank to auto detect using <code>r_dapi_auto_microns</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>r1_auto_microns: number.</p> <p>If <code>r1</code> not specified, will convert to units of pixels from this micron value. </p> <p>Default: <code>0.5</code></p> </li> <li> <p>r_dapi_auto_microns: maybe_number.</p> <p>If <code>r_dapi</code> not specified. Will convert to units of pixels from this micron value. Typical = 8.0. If both this and <code>r_dapi</code> left blank, DAPI image will not be filtered and no .npy file saved. Instead DAPI will be loaded directly from raw data and then stitched. </p> <p>Default: <code>None</code></p> </li> <li> <p>scale: maybe_number.</p> <p>Each filtered image is multiplied by scale. This is because the image is saved as uint16 so to gain information from the decimal points, should multiply image so max pixel number is in the 10,000s (less than 65,536). Leave empty to auto-detect using <code>scale_norm</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>scale_norm: maybe_int.</p> <p>If <code>scale</code> not given, <code>scale = scale_norm/max(scale_image)</code>. Where <code>scale_image</code> is the <code>n_channels x n_y x n_x x n_z</code> image belonging to the central tile (saved as <code>nb.extract_debug.scale_tile</code>) of round 0 after filtering and smoothing. </p> <p>Must be less than <code>np.iinfo(np.uint16).max - config['basic_info']['tile_pixel_value_shift']</code> which is typically \\(65535 - 15000 = 50535\\). </p> <p>Default: <code>35000</code></p> </li> <li> <p>scale_anchor: maybe_number.</p> <p>Analogous to <code>scale</code> but have different normalisation for anchor round/anchor channel as not used in final spot_colors. Leave empty to auto-detect using <code>scale_norm</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>auto_thresh_multiplier: number.</p> <p><code>nb.extract.auto_thresh[t,r,c]</code> is default threshold to find spots on tile t, round r, channel c. Value is set to <code>auto_thresh_multiplier * median(abs(image))</code> where <code>image</code> is the image produced for tile t, round r, channel c in the extract step of the pipeline and saved to <code>file_names['tile_dir']</code>. </p> <p>Default: <code>10</code></p> </li> <li> <p>deconvolve: bool.</p> <p>For 3D pipeline, whether to perform wiener deconvolution before hanning filtering. </p> <p>Default: <code>False</code></p> </li> <li> <p>psf_detect_radius_xy: int.</p> <p>Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if <code>deconvolve == True</code>. To detect spot, pixel needs to be above dilation with this radius in xy plane. </p> <p>Default: <code>2</code></p> </li> <li> <p>psf_detect_radius_z: int.</p> <p>Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if <code>deconvolve == True</code>. To detect spot, pixel needs to be above dilation with this radius in z direction. </p> <p>Default: <code>2</code></p> </li> <li> <p>psf_intensity_thresh: maybe_number.</p> <p>Spots contribute to <code>psf</code> if they are above this intensity. If not given, will be computed the same as <code>auto_thresh</code> i.e. <code>median(image) + auto_thresh_multiplier*median(abs(image-median(image)))</code>. Note that for raw data, <code>median(image)</code> is not zero hence the difference. </p> <p>Default: <code>None</code></p> </li> <li> <p>psf_isolation_dist: number.</p> <p>Spots contribute to <code>psf</code> if more than <code>psf_isolation_dist</code> from nearest spot. </p> <p>Default: <code>20</code></p> </li> <li> <p>psf_min_spots: int.</p> <p>Need this many isolated spots to determine <code>psf</code>. </p> <p>Default: <code>300</code></p> </li> <li> <p>psf_shape: list_int.</p> <p>Diameter of psf in y, x, z direction (in units of [xy_pixels, xy_pixels, z_pixels]). </p> <p>Default: <code>181, 181, 19</code></p> </li> <li> <p>psf_annulus_width: number.</p> <p><code>psf</code> is assumed to be radially symmetric within each z-plane so assume all values within annulus of this size (in xy_pixels) to be the same. </p> <p>Default: <code>1.4</code></p> </li> <li> <p>wiener_constant: number.</p> <p>Constant used to compute wiener filter from <code>psf</code>. </p> <p>Default: <code>50000</code></p> </li> <li> <p>wiener_pad_shape: list_int.</p> <p>When applying the wiener filter, we pad the raw image to median value linearly with this many pixels at end of each dimension. </p> <p>Default: <code>20, 20, 3</code></p> </li> <li> <p>r_smooth: maybe_list_int.</p> <p>Radius of averaging filter to do smoothing of filtered image. Provide two numbers to do 2D smoothing and three numbers to do 3D smoothing. Typical 2D: <code>2, 2</code>. Typical 3D: <code>1, 1, 2</code>. Recommended use is in 3D only as it incorporates information between z-planes which filtering with difference of hanning kernels does not. </p> <p>Size of <code>r_smooth</code> has big influence on time taken for smoothing. For a <code>2048 x 2048 x 50</code> image: </p> <ul> <li> <p><code>r_smooth = 1, 1, 2</code>: 2.8 seconds </p> </li> <li> <p><code>r_smooth = 2, 2, 2</code>: 8.5 seconds </p> </li> </ul> <p>Leave empty to do no smoothing. </p> <p>Default: <code>None</code></p> </li> <li> <p>n_clip_warn: int.</p> <p>If the number of pixels that are clipped when saving as uint16 is more than <code>n_clip_warn</code>, a warning message will occur. </p> <p>Default: <code>1000</code></p> </li> <li> <p>n_clip_error: maybe_int.</p> <p>If the number of pixels that are clipped when saving as uint16 is more than <code>n_clip_error</code> for <code>n_clip_error_images_thresh</code> images, the extract and filter step will be halted. If left blank, n_clip_error will be set to 1% of pixels of a single z-plane. </p> <p>Default: <code>None</code></p> </li> <li> <p>n_clip_error_images_thresh: int.</p> <p>If the number of pixels that are clipped when saving as uint16 is more than <code>n_clip_error</code> for <code>n_clip_error_images_thresh</code> images, the extract and filter step will be halted. </p> <p>Default: <code>3</code></p> </li> </ul>"},{"location":"config/#find_spots","title":"find_spots","text":"<p>The find_spots section contains parameters which specify how to convert the images produced in the extract section to point clouds.</p> <ul> <li> <p>radius_xy: int.</p> <p>To be detected as a spot, a pixel needs to be above dilation with structuring element which is a square (<code>np.ones</code>) of width <code>2*radius_xy-1</code> in the xy plane. </p> <p>Default: <code>2</code></p> </li> <li> <p>radius_z: int.</p> <p>To be detected as a spot, a pixel needs to be above dilation with structuring element which is cuboid (<code>np.ones</code>) with width <code>2*radius_z-1</code> in z direction. Must be more than 1 to be 3D. </p> <p>Default: <code>2</code></p> </li> <li> <p>max_spots_2d: int.</p> <p>If number of spots detected on particular z-plane of an imaging round is greater than this, then will only select the <code>max_spots_2d</code> most intense spots on that z-plane. I.e. PCR works better if trying to fit fewer more intense spots. This only applies to imaging rounds and not ref_round/ref_channel as need lots of spots then. In 2D, allow more spots as only 1 z-plane </p> <p>Default: <code>1500</code></p> </li> <li> <p>max_spots_3d: int.</p> <p>Same as <code>max_spots_2d</code> for the 3D pipeline. In 3D, need to allow less spots on a z-plane as have many z-planes. </p> <p>Default: <code>500</code></p> </li> <li> <p>isolation_radius_inner: number.</p> <p>To determine if spots are isolated, filter image with annulus between <code>isolation_radius_inner</code> and <code>isolation_radius</code>. <code>isolation_radius_inner</code> should be approx the radius where intensity of spot crosses from positive to negative. It is in units of xy-pixels. This filtering will only be applied to spots detected in the ref_round/ref_channel. </p> <p>Default: <code>4</code></p> </li> <li> <p>isolation_radius_xy: number.</p> <p>Outer radius of annulus filtering kernel in xy direction in units of xy-pixels. </p> <p>Default: <code>14</code></p> </li> <li> <p>isolation_radius_z: number.</p> <p>Outer radius of annulus filtering kernel in z direction in units of z-pixels. </p> <p>Default: <code>1</code></p> </li> <li> <p>isolation_thresh: maybe_number.</p> <p>Spot is isolated if value of annular filtered image at spot location is below the <code>isolation_thresh</code> value. Leave blank to automatically determine value using <code>auto_isolation_thresh_multiplier</code>. multiplied by the threshold used to detect the spots i.e. the extract_auto_thresh value. </p> <p>Default: <code>None</code></p> </li> <li> <p>auto_isolation_thresh_multiplier: number.</p> <p>If <code>isolation_thresh</code> left blank, it will be set to <code>isolation_thresh = auto_isolation_thresh_multiplier * nb.extract.auto_thresh[:, r, c]</code>. </p> <p>Default: <code>-0.2</code></p> </li> <li> <p>n_spots_warn_fraction: number.</p> <p>Used in coppafish/find_spots/base/check_n_spots </p> <p>A warning will be raised if for any tile, round, channel the number of spots detected is less than: </p> <p><code>n_spots_warn = n_spots_warn_fraction * max_spots * nb.basic_info.nz</code> </p> <p>where <code>max_spots</code> is <code>max_spots_2d</code> if 2D and <code>max_spots_3d</code> if 3D. </p> <p>Default: <code>0.1</code></p> </li> <li> <p>n_spots_error_fraction: number.</p> <p>Used in coppafish/find_spots/base/check_n_spots. An error is raised if any of the following are satisfied: </p> <ul> <li> <p>For any given channel, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of tiles/rounds. </p> </li> <li> <p>For any given tile, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of rounds/channels. </p> </li> <li> <p>For any given round, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of tiles/channels. </p> </li> </ul> <p>Default: <code>0.5</code></p> </li> </ul>"},{"location":"config/#stitch","title":"stitch","text":"<p>The stitch section contains parameters which specify how the overlaps between neighbouring tiles are found. Note that references to south in this section should really be north and west should be east.</p> <ul> <li> <p>expected_overlap: number.</p> <p>Expected fractional overlap between tiles. Used to get initial shift search if not provided. </p> <p>Default: <code>0.1</code></p> </li> <li> <p>auto_n_shifts: list_int.</p> <p>If <code>shift_south_min/max</code> and/or <code>shift_west_min/max</code> not given, the initial shift search will have <code>auto_n_shifts</code> either side of the expected shift given the <code>expected_overlap</code> with step given by <code>shift_step</code>. First value gives \\(n_{shifts}\\) in direction of overlap (y for south, x for west). Second value gives \\(n_{shifts}\\) in other direction (x for south, y for west). Third value gives \\(n_{shifts}\\) in z. </p> <p>Default: <code>20, 20, 1</code></p> </li> <li> <p>shift_south_min: maybe_list_int.</p> <p>Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: <code>-1900, -100, -2</code> </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_south_max: maybe_list_int.</p> <p>Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: <code>-1700, 100, 2</code> </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_west_min: maybe_list_int.</p> <p>Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: <code>-100, -1900, -2</code> </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_west_max: maybe_list_int.</p> <p>Can manually specify initial shifts. Shift range will run between min to max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: <code>100, -1700, 2</code> </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_step: list_int.</p> <p>Step size to use in y, x, z when finding shift between tiles. </p> <p>Default: <code>5, 5, 3</code></p> </li> <li> <p>shift_widen: list_int.</p> <p>If shift in initial search range has score which does not exceed <code>shift_score_thresh</code>, then range will be extrapolated with same step by <code>shift_widen</code> values in y, x, z direction. </p> <p>Default: <code>10, 10, 1</code></p> </li> <li> <p>shift_max_range: list_int.</p> <p>The range of shifts searched over will continue to be increased according to <code>shift_widen</code> until the shift range in the y, x, z direction reaches <code>shift_max_range</code>. If a good shift is still not found, a warning will be printed. </p> <p>Default: <code>300, 300, 10</code></p> </li> <li> <p>neighb_dist_thresh: number.</p> <p>Basically the distance in yx pixels below which neighbours are a good match. </p> <p>Default: <code>2</code></p> </li> <li> <p>shift_score_thresh: maybe_number.</p> <p>A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the <code>shift_score_thresh</code> parameters below using the function coppafish/stitch/shift/get_score_thresh. </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_score_thresh_multiplier: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>2</code></p> </li> <li> <p>shift_score_thresh_min_dist: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>11</code></p> </li> <li> <p>shift_score_thresh_max_dist: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>20</code></p> </li> <li> <p>nz_collapse: int.</p> <p>3D data is converted into <code>np.ceil(nz / nz_collapse)</code> 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. </p> <p>Default: <code>30</code></p> </li> <li> <p>n_shifts_error_fraction: number.</p> <p>Used in coppafish/stitch/check_shifts/check_shifts_stitch If more than this fraction of <code>shifts</code> found between neighbouring tiles have <code>score &lt; score_thresh</code>, an error will be raised. </p> <p>Default: <code>0.5</code></p> </li> <li> <p>save_image_zero_thresh: int.</p> <p>When saving stitched images, all pixels with absolute value less than or equal to <code>save_image_zero_thresh</code> will be set to 0. This helps reduce size of the .npz files and does not lose any important information. </p> <p>Default: <code>20</code></p> </li> </ul>"},{"location":"config/#register_initial","title":"register_initial","text":"<p>The register_initial section contains parameters which specify how the shifts from the ref_round/ref_channel to each imaging round/channel are found. These are then used as the starting point for determining the affine transforms in the register section.</p> <ul> <li> <p>shift_channel: maybe_int.</p> <p>Channel to use to find shifts between rounds to use as starting point for PCR. Leave blank to set to <code>basic_info['ref_channel']</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_min: list_int.</p> <p>Exhaustive search range will include all shifts between min and max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: [-100, -100, -1] </p> <p>Default: <code>-100, -100, -3</code></p> </li> <li> <p>shift_max: list_int.</p> <p>Exhaustive search range will include all shifts between min and max with step given by <code>shift_step</code>. Each entry should be a list of 3 values: [y, x, z]. Typical: [100, 100, 1] </p> <p>Default: <code>100, 100, 3</code></p> </li> <li> <p>shift_step: list_int.</p> <p>Step size to use in y, x, z when performing the exhaustive search to find the shift between tiles. </p> <p>Default: <code>5, 5, 3</code></p> </li> <li> <p>shift_widen: list_int.</p> <p>If shift in initial search range has score which does not exceed <code>shift_score_thresh</code>, then the range will be extrapolated with same step by shift_widen values in y, x, z direction. </p> <p>Default: <code>10, 10, 1</code></p> </li> <li> <p>shift_max_range: list_int.</p> <p>The range of shifts searched over will continue to be increased according to <code>shift_widen</code> until the shift range in the y, x, z direction reaches <code>shift_max_range</code>. If a good shift is still not found, a warning will be printed. </p> <p>Default: <code>500, 500, 10</code></p> </li> <li> <p>neighb_dist_thresh: number.</p> <p>Basically the distance in yx pixels below which neighbours are a good match. </p> <p>Default: <code>2</code></p> </li> <li> <p>shift_score_thresh: maybe_number.</p> <p>A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the <code>shift_score_thresh</code> parameters below using the function coppafish/stitch/shift/get_score_thresh. </p> <p>Default: <code>None</code></p> </li> <li> <p>shift_score_thresh_multiplier: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>1.5</code></p> </li> <li> <p>shift_score_thresh_min_dist: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>11</code></p> </li> <li> <p>shift_score_thresh_max_dist: number.</p> <p><code>shift_score_thresh</code> is set to <code>shift_score_thresh_multiplier</code> multiplied by the mean of scores of shifts a distance between <code>shift_score_thresh_min_dist</code> and <code>shift_score_thresh_max_dist</code> from the best shift. </p> <p>Default: <code>20</code></p> </li> <li> <p>nz_collapse: int.</p> <p>3D data is converted into <code>np.ceil(nz / nz_collapse)</code> 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. </p> <p>Default: <code>30</code></p> </li> <li> <p>n_shifts_error_fraction: number.</p> <p>Used in coppafish/stitch/check_shifts/check_shifts_register If more than this fraction of <code>shifts</code> between the <code>ref_round</code>/<code>ref_channel</code> and each imaging round for each tile have <code>score &lt; score_thresh</code>, an error will be raised. </p> <p>Default: <code>0.5</code></p> </li> </ul>"},{"location":"config/#register","title":"register","text":"<p>The register section contains parameters which specify how the affine transforms from the ref_round/ref_channel to each imaging round/channel are found from the shifts found in the register_initial section.</p> <ul> <li> <p>n_iter: int.</p> <p>Maximum number iterations to run point cloud registration, PCR </p> <p>Default: <code>100</code></p> </li> <li> <p>neighb_dist_thresh_2d: number.</p> <p>Basically the distance in yx pixels below which neighbours are a good match. PCR updates transforms by minimising distances between neighbours which are closer than this. </p> <p>Default: <code>3</code></p> </li> <li> <p>neighb_dist_thresh_3d: number.</p> <p>The same as <code>neighb_dist_thresh_2d</code> but in 3D, we use a larger distance because the size of a z-pixel is greater than a xy pixel. </p> <p>Default: <code>5</code></p> </li> <li> <p>matches_thresh_fract: number.</p> <p>If PCR produces transforms with fewer neighbours (pairs with distance between them less than <code>neighb_dist_thresh</code>) than <code>matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max)</code>, the transform will be re-evaluated with regularization so it is near the average transform. </p> <p>Default: <code>0.25</code></p> </li> <li> <p>matches_thresh_min: int.</p> <p>If PCR produces transforms with fewer neighbours (pairs with distance between them less than <code>neighb_dist_thresh</code>) than <code>matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max)</code>, the transform will be re-evaluated with regularization so it is near the average transform. </p> <p>Default: <code>25</code></p> </li> <li> <p>matches_thresh_max: int.</p> <p>If PCR produces transforms with fewer neighbours (pairs with distance between them less than <code>neighb_dist_thresh</code>) than <code>matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max)</code>, the transform will be re-evaluated with regularization so it is near the average transform. </p> <p>Default: <code>300</code></p> </li> <li> <p>scale_dev_thresh: list_number.</p> <p>If a transform has a chromatic aberration scaling that has an absolute deviation of more than <code>scale_dev_thresh[i]</code> from the median for that colour channel in dimension <code>i</code>, it will be re-evaluated with regularization. There is a threshold for the y, x, z scaling. </p> <p>Default: <code>0.01, 0.01, 0.1</code></p> </li> <li> <p>shift_dev_thresh: list_number.</p> <p>If a transform has a <code>shift[i]</code> that has an absolute deviation of more than <code>shift_dev_thresh[i]</code> from the median for that tile and round in any dimension <code>i</code>, it will be re-evaluated with regularization. There is a threshold for the y, x, z shift. <code>shift_dev_thresh[2]</code> is in z pixels. </p> <p>Default: <code>15, 15, 5</code></p> </li> <li> <p>regularize_constant: int.</p> <p>Constant used when doing regularized least squares. If the number of neighbours are above this, regularization will have little effect. If the number of neighbours is less than this, regularization will have significant effect, and final transform will be similar to transform being regularized towards. </p> <p>Default: <code>500</code></p> </li> <li> <p>regularize_factor: number.</p> <p>The loss function for finding the transform through regularized least squares is: </p> <p>\\(\\sum_s^{n_{neighb}}D_s^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) </p> <p>Where: </p> <ul> <li> <p>\\(D_s^2\\) is the squared distance between the pair of neighbours indicated by \\(s\\). Only neighbours with distance between them on previous iteration les than <code>neighb_dist</code> are considered. </p> </li> <li> <p>\\(\\lambda\\) is <code>regularize_constant</code>. </p> </li> <li> <p>\\(\\mu\\) is <code>regularize_factor</code> such that when \\(n_{neighb} = \\lambda\\) and \\(D_s^2 = D_{shift}^2\\) for all \\(s\\), the two contributions to the loss function are approximately equal i.e. \\(\\mu = D_{shift}^2/D_{scale}^2\\). </p> </li> <li> <p>\\(D_{scale}^2\\) is the squared distance between <code>transform[:3, :]</code> and <code>transform_regularize[:3, :]</code>. I.e. the squared difference of the scaling/rotation part of the transform from the target. </p> </li> <li> <p>\\(D_{shift}^2\\) is the squared distance between <code>transform[3]</code> and <code>transform_regularize[3]</code>. I.e. the squared difference of the shift part of the transform from the target. </p> </li> </ul> <p>So if a typical value of \\(D_{shift}\\) (or \\(D_s\\)) is 2 and a typical value of \\(D_{scale}\\) is 0.009, \\(\\mu = 5\\times10^4\\). </p> <p>Default: <code>5e4</code></p> </li> <li> <p>n_transforms_error_fraction: number.</p> <p>Used in coppafish/register/check_transforms/check_transforms An error is raised if any of the following are satisfied where a failed transform is one with <code>nb.register_debug.n_matches &lt; nb.register_debug.n_matches_thresh</code>. </p> <ul> <li> <p>For any given channel, the fraction of failed transforms was greater than <code>n_transforms_error_fraction</code> of tiles/rounds. </p> </li> <li> <p>For any given tile, the fraction of failed transforms was greater than <code>n_transforms_error_fraction</code> of rounds/channels. </p> </li> <li> <p>For any given round, the fraction of failed transforms was greater than <code>n_transforms_error_fraction</code> of tiles/channels. </p> </li> </ul> <p>Default: <code>0.5</code></p> </li> </ul>"},{"location":"config/#call_spots","title":"call_spots","text":"<p>The call_spots section contains parameters which determine how the <code>bleed_matrix</code> and <code>gene_efficiency</code> are computed, as well as how a gene is assigned to each spot found on the ref_round/ref_channel.</p> <ul> <li> <p>bleed_matrix_method: str.</p> <p><code>bleed_matrix_method</code> can only be <code>single</code> or <code>separate</code>. <code>single</code>: a single bleed matrix is produced for all rounds. <code>separate</code>: a different bleed matrix is made for each round. </p> <p>Default: <code>single</code></p> </li> <li> <p>color_norm_intensities: list_number.</p> <p>Parameter used to get color normalisation factor. <code>color_norm_intensities</code> should be ascending and <code>color_norm_probs</code> should be descending and they should be the same size. The probability of normalised spot color being greater than <code>color_norm_intensities[i]</code> must be less than <code>color_norm_probs[i]</code> for all <code>i</code>. </p> <p>Default: <code>0.5, 1, 5</code></p> </li> <li> <p>color_norm_probs: list_number.</p> <p>Parameter used to get color normalisation factor. <code>color_norm_intensities</code> should be ascending and <code>color_norm_probs</code> should be descending and they should be the same size. The probability of normalised spot color being greater than <code>color_norm_intensities[i]</code> must be less than <code>color_norm_probs[i]</code> for all <code>i</code>. </p> <p>Default: <code>0.01, 5e-4, 1e-5</code></p> </li> <li> <p>bleed_matrix_score_thresh: number.</p> <p>In <code>scaled_k_means</code> part of <code>bleed_matrix</code> calculation, a mean vector for each dye is computed from all spots with a dot product to that mean greater than this. </p> <p>Default: <code>0</code></p> </li> <li> <p>bleed_matrix_min_cluster_size: int.</p> <p>If less than this many vectors are assigned to a dye cluster in the <code>scaled_k_means</code> part of <code>bleed_matrix</code> calculation, the expected code for that dye will be set to 0 for all color channels i.e. bleed matrix computation will have failed. </p> <p>Default: <code>10</code></p> </li> <li> <p>bleed_matrix_n_iter: int.</p> <p>Maximum number of iterations allowed in the <code>scaled_k_means</code> part of <code>bleed_matrix</code> calculation. </p> <p>Default: <code>100</code></p> </li> <li> <p>bleed_matrix_anneal: bool.</p> <p>If <code>True</code>, the <code>scaled_k_means</code> calculation will be performed twice. The second time starting with the output of the first and with <code>score_thresh</code> for cluster <code>i</code> set to the median of the scores assigned to cluster <code>i</code> in the first run. </p> <p>This limits the influence of bad spots to the bleed matrix. </p> <p>Default: <code>True</code></p> </li> <li> <p>background_weight_shift: maybe_number.</p> <p>Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round r for the fitting of the background vector for channel c is <code>1 / (spot_color[r, c] + background_weight_shift)</code> so <code>background_weight_shift</code> ensures this does not go to infinity for small <code>spot_color[r, c]</code>. Typical <code>spot_color[r, c]</code> is 1 for intense spot so <code>background_weight_shift</code> is small fraction of this. Leave blank to set to median absolute intensity of all pixels on the mid z-plane of the central tile. </p> <p>Default: <code>None</code></p> </li> <li> <p>dp_norm_shift: maybe_number.</p> <p>When calculating the <code>dot_product_score</code>, this is the small shift to apply when normalising <code>spot_colors</code> to ensure don't divide by zero. Value is for a single round and is multiplied by <code>sqrt(n_rounds_used)</code> when computing <code>dot_product_score</code>. Expected norm of a spot_color for a single round is 1 so <code>dp_norm_shift</code> is a small fraction of this. Leave blank to set to median L2 norm for a single round of all pixels on the mid z-plane of the central tile. </p> <p>Default: <code>None</code></p> </li> <li> <p>norm_shift_min: number.</p> <p>Minimum possible value of <code>dp_norm_shift</code> and <code>background_weight_shift</code>. </p> <p>Default: <code>0.001</code></p> </li> <li> <p>norm_shift_max: number.</p> <p>Maximum possible value of <code>dp_norm_shift</code> and <code>background_weight_shift</code>. </p> <p>Default: <code>0.5</code></p> </li> <li> <p>norm_shift_precision: number.</p> <p><code>dp_norm_shift</code> and <code>background_weight_shift</code> will be rounded to nearest <code>norm_shift_precision</code>. </p> <p>Default: <code>0.01</code></p> </li> <li> <p>gene_efficiency_min_spots: int.</p> <p>If number of spots assigned to a gene less than or equal to this, <code>gene_efficiency[g]=1</code> for all rounds. </p> <p>Default: <code>25</code></p> </li> <li> <p>gene_efficiency_max: number.</p> <p>Maximum allowed value of <code>gene_efficiency</code> i.e. any one round can be at most this times more important than the median round for every gene. </p> <p>Default: <code>6</code></p> </li> <li> <p>gene_efficiency_min: number.</p> <p>At most <code>ceil(gene_efficiency_min_factor * n_rounds_use)</code> rounds can have <code>gene_efficiency</code> below <code>gene_efficiency_min</code> for any given gene. </p> <p>Default: <code>0.05</code></p> </li> <li> <p>gene_efficiency_min_factor: number.</p> <p>At most <code>ceil(gene_efficiency_min_factor * n_rounds_use)</code> rounds can have <code>gene_efficiency</code> below <code>gene_efficiency_min</code> for any given gene. </p> <p>Default: <code>0.2</code></p> </li> <li> <p>gene_efficiency_n_iter: int.</p> <p><code>gene_efficiency</code> is computed from spots which pass a quality thresholding based on the bled_codes computed with the gene_efficiency of the previous iteration. This process will continue until the <code>gene_effiency</code> converges or <code>gene_efficiency_n_iter</code> iterations are reached. 0 means <code>gene_efficiency</code> will not be used. </p> <p>Default: <code>10</code></p> </li> <li> <p>gene_efficiency_score_thresh: number.</p> <p>Spots used to compute <code>gene_efficiency</code> must have <code>dot_product_score</code> greater than <code>gene_efficiency_score_thresh</code>, difference to second best score greater than <code>gene_efficiency_score_diff_thresh</code> and intensity greater than <code>gene_efficiency_intensity_thresh</code>. </p> <p>Default: <code>0.6</code></p> </li> <li> <p>gene_efficiency_score_diff_thresh: number.</p> <p>Spots used to compute <code>gene_efficiency</code> must have <code>dot_product_score</code> greater than <code>gene_efficiency_score_thresh</code>, difference to second best score greater than <code>gene_efficiency_score_diff_thresh</code> and intensity greater than <code>gene_efficiency_intensity_thresh</code>. </p> <p>Default: <code>0.2</code></p> </li> <li> <p>gene_efficiency_intensity_thresh: maybe_number.</p> <p>Spots used to compute <code>gene_efficiency</code> must have <code>dot_product_score</code> greater than <code>gene_efficiency_score_thresh</code>, difference to second best score greater than <code>gene_efficiency_score_diff_thresh</code> and intensity greater than <code>gene_efficiency_intensity_thresh</code>. Leave blank to determine from <code>gene_efficiency_intensity_thresh_percentile</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>gene_efficiency_intensity_thresh_percentile: int.</p> <p><code>gene_efficiency_intensity_thresh</code> will be set to this percentile of the intensity computed for all pixels on the mid z-plane of the most central tile if not specified. </p> <p>Default: <code>37</code></p> </li> <li> <p>gene_efficiency_intensity_thresh_precision: number.</p> <p><code>gene_efficiency_intensity_thresh</code> will be rounded to nearest <code>gene_efficiency_intensity_thresh_precision</code> if not given. </p> <p>Default: <code>0.001</code></p> </li> <li> <p>gene_efficiency_intensity_thresh_min: number.</p> <p>Min allowed value of <code>gene_efficiency_intensity_thresh</code>. </p> <p>Default: <code>0.001</code></p> </li> <li> <p>gene_efficiency_intensity_thresh_max: number.</p> <p>Max allowed value of <code>gene_efficiency_intensity_thresh</code>. </p> <p>Default: <code>0.2</code></p> </li> <li> <p>alpha: number.</p> <p>When computing the dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\), rounds/channels with background already fit contribute less. The larger \\(\\alpha\\), the lower the contribution. </p> <p>Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. </p> <p>Default: <code>120</code></p> </li> <li> <p>beta: number.</p> <p>Constant used in weighting factor when computing dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\). </p> <p>Default: <code>1</code></p> </li> </ul>"},{"location":"config/#omp","title":"omp","text":"<p>The omp section contains parameters which are use to carry out orthogonal matching pursuit (omp) on every pixel, as well as how to convert the results of this to spot locations.</p> <ul> <li> <p>use_z: maybe_list_int.</p> <p>Can specify z-planes to find spots on If 2 values provided, all z-planes between and including the values given will be used. </p> <p>Default: <code>None</code></p> </li> <li> <p>weight_coef_fit: bool.</p> <p>If <code>False</code>, gene coefficients are found through omp with normal least squares fitting. If <code>True</code>, gene coefficients are found through omp with weighted least squares fitting with rounds/channels which already containing genes contributing less. </p> <p>Default: <code>False</code></p> </li> <li> <p>initial_intensity_thresh: maybe_number.</p> <p>To save time in <code>call_spots_omp</code>, coefficients only found for pixels with intensity of absolute <code>spot_colors</code> greater than <code>initial_intensity_thresh</code>. Leave blank to set to determine using <code>initial_intensity_thresh_auto_param</code> It is also clamped between the <code>initial_intensity_thresh_min</code> and <code>initial_intensity_thresh_max</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>initial_intensity_thresh_percentile: int.</p> <p>If <code>initial_intensity_thresh</code> not given, it will be set to the <code>initial_intensity_thresh_percentile</code> percentile of the absolute intensity of all pixels on the mid z-plane of the central tile. It uses <code>nb.call_spots.abs_intensity_percentile</code> </p> <p>Default: <code>25</code></p> </li> <li> <p>initial_intensity_thresh_min: number.</p> <p>Min allowed value of <code>initial_intensity_thresh</code>. </p> <p>Default: <code>0.001</code></p> </li> <li> <p>initial_intensity_thresh_max: number.</p> <p>Max allowed value of <code>initial_intensity_thresh</code>. </p> <p>Default: <code>0.2</code></p> </li> <li> <p>initial_intensity_precision: number.</p> <p><code>initial_intensity_thresh</code> will be rounded to nearest <code>initial_intensity_precision</code> if not given. </p> <p>Default: <code>0.001</code></p> </li> <li> <p>max_genes: int.</p> <p>The maximum number of genes that can be assigned to each pixel i.e. number of iterations of omp. </p> <p>Default: <code>30</code></p> </li> <li> <p>dp_thresh: number.</p> <p>Pixels only have coefficient found for a gene if that gene has absolute <code>dot_product_score</code> greater than this i.e. this is the stopping criterion for the OMP. </p> <p>Default: <code>0.225</code></p> </li> <li> <p>alpha: number.</p> <p>When computing the dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP, rounds/channels with genes already fit to them, contribute less. The larger \\(\\alpha\\), the lower the contribution. </p> <p>Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. </p> <p>Default: <code>120</code></p> </li> <li> <p>beta: number.</p> <p>Constant used in weighting factor when computing dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP. </p> <p>Default: <code>1</code></p> </li> <li> <p>initial_pos_neighbour_thresh: maybe_int.</p> <p>Only save spots with number of positive coefficient neighbours greater than <code>initial_pos_neighbour_thresh</code>. Leave blank to determine using <code>initial_pos_neighbour_thresh_param</code>. It is also clipped between <code>initial_pos_neighbour_thresh_min</code> and <code>initial_pos_neighbour_thresh_max</code>. </p> <p>Default: <code>None</code></p> </li> <li> <p>initial_pos_neighbour_thresh_param: number.</p> <p>If <code>initial_pos_neighbour_thresh</code> not given, it is set to <code>initial_pos_neighbour_thresh_param</code> multiplied by number of positive values in nb.omp.spot_shape i.e. with <code>initial_pos_neighbour_thresh_param = 0.1</code>, it is set to 10% of the max value. </p> <p>Default: <code>0.1</code></p> </li> <li> <p>initial_pos_neighbour_thresh_min: int.</p> <p>Min allowed value of <code>initial_pos_neighbour_thresh</code>. </p> <p>Default: <code>4</code></p> </li> <li> <p>initial_pos_neighbour_thresh_max: int.</p> <p>Max allowed value of <code>initial_pos_neighbour_thresh</code>. </p> <p>Default: <code>40</code></p> </li> <li> <p>radius_xy: int.</p> <p>To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is a square (<code>np.ones</code>) of width <code>2*radius_xy-1</code> in the xy plane. </p> <p>Default: <code>3</code></p> </li> <li> <p>radius_z: int.</p> <p>To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is cuboid (<code>np.ones</code>) with width <code>2*radius_z-1</code> in z direction. Must be more than 1 to be 3D. </p> <p>Default: <code>2</code></p> </li> <li> <p>shape_max_size: list_int.</p> <p>spot_shape specifies the neighbourhood about each spot in which we count coefficients which contribute to score. It is either given through <code>file_names['omp_spot_shape']</code> or computed using the below parameters with shape prefix. Maximum Y, X, Z size of spot_shape. Will be cropped if there are zeros at the extremities. </p> <p>Default: <code>27, 27, 9</code></p> </li> <li> <p>shape_pos_neighbour_thresh: int.</p> <p>For spot to be used to find <code>spot_shape</code>, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). </p> <p>Default: <code>9</code></p> </li> <li> <p>shape_isolation_dist: number.</p> <p>Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find <code>spot_shape</code>. </p> <p>Default: <code>10</code></p> </li> <li> <p>shape_sign_thresh: number.</p> <p>If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in <code>spot_shape</code> to be 0. Max mean absolute coefficient sign is 1 so must be less than this. </p> <p>Default: <code>0.15</code></p> </li> </ul>"},{"location":"config/#thresholds","title":"thresholds","text":"<p>The thresholds section contains the thresholds used to determine which spots pass a quality thresholding process such that we consider their gene assignments legitimate.</p> <ul> <li> <p>intensity: maybe_number.</p> <p>Final accepted reference and OMP spots both require <code>intensity &gt; thresholds[intensity]</code>. If not given, will be set to same value as <code>nb.call_spots.gene_efficiency_intensity_thresh</code>. intensity for a really intense spot is about 1 so <code>intensity_thresh</code> should be less than this. </p> <p>Default: <code>None</code></p> </li> <li> <p>score_ref: number.</p> <p>Final accepted spots are those which pass quality_threshold which is <code>nb.ref_spots.score &gt; thresholds[score_ref]</code> and <code>nb.ref_spots.intensity &gt; intensity_thresh</code>. quality_threshold requires score computed with coppafish/call_spots/dot_prodduct/dot_product_score to exceed this. Max score is 1 so must be below this. </p> <p>Default: <code>0.25</code></p> </li> <li> <p>score_omp: number.</p> <p>Final accepted OMP spots are those which pass quality_threshold which is: <code>score &gt; thresholds[score_omp]</code> and <code>intensity &gt; thresholds[intensity]</code>. <code>score</code> is given by: <code>score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) /   (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max)</code> Max score is 1 so <code>score_thresh</code> should be less than this. </p> <p>0.15 if more concerned for missed spots than false positives. </p> <p>Default: <code>0.263</code></p> </li> <li> <p>score_omp_multiplier: number.</p> <p>Final accepted OMP spots are those which pass quality_threshold which is: <code>score &gt; thresholds[score_omp]</code> and <code>intensity &gt; thresholds[intensity]</code>. <code>score</code> is given by: <code>score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) /   (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max)</code> </p> <p>0.45 if more concerned for missed spots than false positives. </p> <p>Default: <code>0.95</code></p> </li> </ul>"},{"location":"config_setup/","title":"Setting up the Config File","text":"<p>A config (.ini) file needs to be created for each experiment to run the pipeline.  All parameters not specified in this file will inherit the default values. The parameters with Default = <code>MUST BE SPECIFIED</code> are the bare minimum parameters which need to be set in the experiment config file. If any section, or parameter within a section, is added to the config file  which is not included in the default file, an error will be raised when it is loaded in. Some example config files for typical experiments are listed below.</p>"},{"location":"config_setup/#example-config-files","title":"Example Config Files","text":"3D2D.npy Raw DataNo AnchorQuadCamSeparate Round <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = False\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\nraw_extension = .npy\nraw_metadata = metadata\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nref_round = 2\nref_channel = 4\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6, Exp1_r7, Exp1_r8\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\ndye_names = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425, AF647, AF750\nchannel_camera = 405, 555, 470, 470, 555, 640, 555, 640, 640\nchannel_laser = 405, 405, 445, 470, 520, 520, 555, 640, 730\n</code></pre> <pre><code>[file_names]\nnotebook_name = sep_round_notebook\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nanchor = Exp1_sep_round\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\n</code></pre> Note on Separate Round Config File <p>The Separate Round config file above is used for registering an additional round to an experiment already run to  completion (using a config file like the 3D one indicated above). The pipeline for the Separate Round case  cannot be run further that the stitching section.</p> <p>The <code>run_sep_reg</code> function  (script is here)  runs the pipeline  for the  Separate Round case and then registers the <code>anchor_round</code>/<code>anchor_channel</code> to the <code>anchor_round</code>/<code>anchor_channel</code> of the full experiment.</p> <p>These parameters are explained below and here. </p>"},{"location":"config_setup/#file_names","title":"file_names","text":"<p>If the names of the files change during the pipeline or if they are being accessed from another computer, the file_names section of the configuration file can also be  changed as explained here.</p>"},{"location":"config_setup/#input_dir","title":"input_dir","text":"<p>The input directory is the path to the folder which contains the raw data.  Examples for the two possible cases of <code>raw_extension</code> are given below (i.e. these are respectively what the input  directory looks like for the config files 3D and .npy Raw Data listed above).</p> .nd2.npy <p></p> <p></p> Differences with <code>raw_extension = .npy</code> <p>It is assumed that when <code>raw_extension = .npy</code>, there were initial .nd2 files which contained excess information  (e.g. extra channels). These were then converted to .npy files to get rid of this.</p> <p>For the .npy case, input_dir must also contain a metadata .json file.  This contains the metadata extracted from the initial .nd2 files using the function  save_metadata. An example metadata file is given here for an experiment with  3 tiles and 7 channels.</p> <p>Also, each name listed in the <code>round</code> parameter indicates a folder not a file.  It is assumed these folders were produced using  <code>dask.array.to_npy_stack</code> so the contents of each folder should contain a file named info and a .npy file for each tile, with the name  being the index of the tile in the initial .nd2 file. An example showing the folder for the first round of a  three tile experiment is given below:</p> <p></p>"},{"location":"config_setup/#output_dir","title":"output_dir","text":"<p>The output directory is the path to the folder that you would like the notebook.npz  file containing the experiment  results to be saved.  The image below shows what the output directory typically looks like at the end of the experiment.</p> <p></p> <p>The names of the files produced can be changed by changing the parameters <code>big_anchor_image</code>, <code>big_dapi_image</code>,  <code>notebook_name</code>, <code>omp_spot_coef</code>, <code>omp_spot_info</code> and <code>omp_spot_shape</code> in the config file. </p> <p></p>"},{"location":"config_setup/#tile_dir","title":"tile_dir","text":"<p>The tile directory is the path to the folder that you would like the filtered images for each tile, round and  colour channel to be saved to. </p> <p>If <code>is_3d == True</code>, a .npy file will be produced for each round, tile and channel with the name for round r, tile T, channel C being <code>config['file_names']['round'][r]</code>_tTcC.npy with axis in the  order z-y-x (the name for the anchor round, tile T, channel C will be <code>config['file_names']['anchor']</code>_tTcC.npy). If <code>is_3d == False</code>, a .npy file will be produced for each round and tile called  <code>config['file_names']['round'][r]</code>_tT.npy with axis in the order c-y-x. </p> <p>An example of what the tile directory looks like at the end of the experiment is shown below for a  3D and 2D experiment with 3 tiles and 7 channels:</p> 3D2D <p></p> <p></p>"},{"location":"config_setup/#code_book","title":"code_book","text":"<p>This is the path to the file containing the code for each gene.  The file should be a text file containing two columns, the first being the gene name.  The second is the code specifying which dye that gene should appear in each round.  Thus it is of length <code>n_rounds</code>, containing numbers in the range from <code>0</code> to <code>n_dyes-1</code> inclusive. </p> Example <p>An example is given here so that if</p> <p><code>config['basic_info'][dye_names] = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425</code></p> <p>the gene Sst with the code 6200265 will be expected to appear with the following dyes in each round:</p> <ul> <li>Round 0: <code>ATTO425</code></li> <li>Round 1: <code>AF488</code></li> <li>Round 2: <code>DY405</code></li> <li>Round 3: <code>DY405</code></li> <li>Round 4: <code>AF488</code></li> <li>Round 5: <code>ATTO425</code></li> <li>Round 6: <code>AF594</code></li> </ul>"},{"location":"config_setup/#basic_info","title":"basic_info","text":""},{"location":"config_setup/#anchor_channel","title":"anchor_channel","text":"<p>The <code>anchor_channel</code> is the channel in the <code>anchor_round</code> which contains spots corresponding to all genes.  These spots are used for registration to the imaging rounds and to determine the expected <code>bled_code</code> for each gene.</p>"},{"location":"config_setup/#ref_round","title":"ref_round","text":"<p>If there is no <code>anchor_round</code>, <code>ref_round</code> must be specified instead and it should be a round which contains a lot  of spots in each channel. Spots in <code>ref_round</code> / <code>ref_channel</code> will then be used as reference spots for registration and to determine the expected <code>bled_code</code> for each gene. </p> <p>If the <code>anchor_round</code> is used and <code>ref_round</code> is specified, <code>ref_round</code> will be set to <code>anchor_round</code> (last round)  in the notebook.</p> Problem with not using anchor <p>With no anchor, the registration is likely to be worse because an imaging round is used as a reference. Thus, not all genes will appear in <code>ref_round</code> / <code>ref_channel</code>, but only those which appear with a dye in the <code>ref_round</code> which have high intensity in the <code>ref_channel</code>. Also, we would expect the final spots saved in nb.ref_spots to only correspond to genes appearing in  <code>ref_round</code> / <code>ref_channel</code> and thus lots of genes will be missing.</p> <p>With an anchor though, we expect all genes to show up in <code>anchor_round</code> / <code>anchor_channel</code>.</p>"},{"location":"config_setup/#ref_channel","title":"ref_channel","text":"<p>If there is no <code>anchor_round</code>, <code>ref_channel</code> must be specified instead and it should be the channel in <code>ref_round</code>  which contains the most spots. If the <code>anchor_round</code> is used and both <code>anchor_channel</code> and <code>ref_channel</code> are specified, <code>ref_channel</code> will be set to <code>anchor_channel</code> in the notebook.</p>"},{"location":"config_setup/#dapi_channel","title":"dapi_channel","text":"<p>This is the channel in the <code>anchor_round</code> that contains the DAPI images.  The tiles of this channel will be stitched together and saved in the <code>config['file_names']['output_dir']</code> with a name  <code>config['file_names']['big_dapi_image']</code>. </p> <p><code>dapi_channel</code> does not have to be included in <code>config['basic_info']['use_channels']</code> as the anchor round is dealt with  separately.</p> <p>To tophat filter the raw DAPI images first, either  <code>config['extract']['r_dapi']</code> or <code>config['extract']['r_dapi_auto_microns']</code> must be specified.</p>"},{"location":"config_setup/#specifying-dyes","title":"Specifying Dyes","text":"<p>It is expected that each gene will appear with a single dye in a given round as indicated by  <code>config['file_names']['code_book']</code>. If <code>dye_names</code> is not specified, it is assumed as a starting point for the  <code>bleed_matrix</code> calculation  that the number of dyes is equal to the number of channels and dye 0 will only appear in channel 0, dye 1 will only appear in channel 1 etc. </p> <p>If <code>dye_names</code> is specified, both <code>channel_camera</code> and <code>channel_laser</code> must also be specified. This is so that a starting point for the <code>bleed_matrix</code> calculation can be obtained by  reading off  the expected intensity of each dye in each channel using the file <code>config['file_names']['dye_camera_laser']</code>. </p> <p>The default <code>config['file_names']['dye_camera_laser']</code> is given  here but if a dye, camera or laser not indicated in this file are used in an experiment, a new version must be made.</p>"},{"location":"config_setup/#common-additional-parameters","title":"Common Additional Parameters","text":"<p>There are a few other parameters that may often need to be different to those given in the  default config file.</p>"},{"location":"config_setup/#extractr_smooth","title":"<code>extract[r_smooth]</code>","text":"<p>The parameter <code>r_smooth</code> in the extract section specifies whether to smooth with an averaging kernel after the raw images have been convolved with a  difference of hanning kernel. This will make the extract section of the pipeline slower but will reduce the influence of anomalously high or low intensity pixels. It may be particularly appropriate to 3D data because the difference of hanning convolution is done independently on each z-plane but the smoothing can incorporate information between z-planes.</p> Time for smoothing <p>The size of <code>r_smooth</code> has big influence on time taken for smoothing.  For a <code>2048 x 2048 x 50</code> image:</p> <ul> <li><code>r_smooth = 1, 1, 2</code>: 2.8 seconds</li> <li><code>r_smooth = 2, 2, 2</code>: 8.5 seconds</li> </ul> <p>The convolution with the  difference of hanning kernel takes 4.1 seconds on the same image so smoothing will make the extract section of the pipeline  significantly longer.</p> <p>By default, this is not specified meaning no smoothing is done. If smoothing is needed, typical values are:</p> <ul> <li>2D: <code>r_smooth = 2, 2</code></li> <li>3D: <code>r_smooth = 1, 1, 2</code></li> </ul> <p>The kernel which the image is correlated with is then  <pre><code>np.ones(2 * r_smooth - 1) / np.sum(np.ones(2 * r_smooth - 1))\n</code></pre> so for <code>r_smooth = 2, 2</code> it will be:</p> <pre><code>array([[0.11111111, 0.11111111, 0.11111111],\n       [0.11111111, 0.11111111, 0.11111111],\n       [0.11111111, 0.11111111, 0.11111111]])\n</code></pre> <p>The effect of smoothing can be seen using <code>view_filter</code>.</p>"},{"location":"config_setup/#extractr_dapi","title":"<code>extract[r_dapi]</code>","text":"<p>By default, no filtering will be applied to the dapi_channel image of the anchor_round and thus  no .npy file will be saved to the tile_dir. This can be changed by specifying <code>r_dapi</code> which should be approximately the radius of a feature in the DAPI image (typical <code>r_dapi</code> is 48). In this case, a 2D tophat filtering will be performed using a kernel of radius <code>r_dapi</code>. </p> <p>Alternatively, <code>r_dapi_auto_microns</code> can be specified to be the radius of the kernel in units of microns and <code>r_dapi</code> will be computed automatically by converting this into units of yx-pixels (typical <code>r_dapi_auto_microns</code> is 8).</p> Time for DAPI filtering <p>The size of <code>r_dapi</code> has big influence on time taken for tophat filtering.  For a <code>2048 x 2048 x 50</code> image:</p> <ul> <li><code>r_dapi = 48</code>: 142.4 seconds</li> <li><code>r_smooth = 12</code>: 3.9 seconds</li> </ul> <p>The tophat filtering is only done on one channel for each tile but it is quite slow so it may be best to avoid it, especially for experiments with lots of tiles.</p> <p>The effect of DAPI filtering can be seen using <code>view_filter</code>.</p>"},{"location":"config_setup/#stitchexpected_overlap","title":"<code>stitch[expected_overlap]</code>","text":"<p>This is the expected fractional overlap between neighbouring tiles.  By default, it is 0.1 meaning a 10% overlap is expected.</p>"},{"location":"config_setup/#thresholds","title":"thresholds","text":"<p>The parameters in the thresholds section of the config file contains the thresholds  used to determine which spots pass a quality thresholding process such that we consider their  gene assignments legitimate.</p> <p>The default values are based on an experiment run with ground truth data, but they will likely need adjusting  after investigating the effect of the thresholds using the <code>Viewer</code>.</p>"},{"location":"config_setup/#using-a-subset-of-the-raw-data","title":"Using a subset of the raw data","text":"<p>To run the pipeline with a subset of tiles, imaging rounds, channels or z-planes the following parameters  can be set in the basic_info section of the configuration file:</p> <ul> <li><code>use_tiles</code></li> <li><code>ignore_tiles</code></li> <li><code>use_rounds</code></li> <li><code>use_channels</code></li> <li><code>use_z</code></li> </ul> <p>If midway through the pipeline, it is decided that a particular tile, round or channel is not worth using, it can be removed  without re-running all the steps of the pipeline completed so far.</p>"},{"location":"notebook/","title":"Notebook","text":"<p>The Notebook is a write-once data structure which is saved as a npz file. It stores the output of each stage of the pipeline as a separate NotebookPage.  Each NotebookPage of the Notebook is itself a write-once data structure.  Each NotebookPage may contain many different variables.</p> Times saved to Notebook and NotebookPage <p>Whenever a variable is added to a NotebookPage, in addition to saving the value, it saves the time at which the variable was added (<code>nbp._times</code>).  Likewise, the time at which a NotebookPage is created (<code>nbp._time_created</code>), and the time at which it is added to the Notebook (<code>nb._page_times</code>)  are also recorded automatically. The time the Notebook was created is also recorded (<code>nb._created_time</code>). This both serves as a record of what was done, as well as a source for debugging and optimization.</p> <p>Conceptually, the idea is that a Notebook is like a lab notebook.  In a lab notebook, you write things in a separate section (here, NotebookPage) for each part of the experiment with the appropriate section name.  You only add, you never erase or modify.  Lab notebooks contain intermediate results, as well as the main data collected during the experiment.  All times and labels of all results are written down.</p>"},{"location":"notebook/#create-notebook","title":"Create Notebook","text":"<p>To create a Notebook, pass it the path to the file where the Notebook is to be saved (/Users/user/coppafish/experiment/notebook.npz) and the path to the configuration file (/Users/user/coppafish/experiment/settings.ini):</p> <pre><code>from coppafish import Notebook\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nnb = Notebook(nb_file, ini_file)\n</code></pre> Create just using <code>config_file</code> <p>The Notebook can also be created with just the configuration file through: <pre><code>from coppafish import Notebook\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nnb = Notebook(config_file=ini_file)\n</code></pre> The location where the Notebook is saved (nb._file) will then be set to: <code>config['file_names']['output_dir'] + config['file_names']['notebook_name']</code>.</p> <p>If <code>nb_file</code> already exists, the Notebook located at this path will be loaded.  If not, a new file will be created as soon as the first NotebookPage is added to the Notebook. </p> <p>When the Notebook is created, it will save the contents of the configuration file (<code>nb._config</code>) thus there is no need to pass the <code>config_file</code> argument when re-loading a Notebook. You can just run <code>nb = Notebook('/Users/user/coppafish/experiment/notebook.npz')</code>.</p> Using Notebook outside the coppaFISH pipeline <p>Passing the configuration file to the Notebook allows for several features, however a Notebook can be created without it: <pre><code>from coppafish import Notebook\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\n</code></pre> You can then still add NotebookPages to the Notebook as normal.</p>"},{"location":"notebook/#adding-a-notebookpage","title":"Adding a NotebookPage","text":"<p>To add a NotebookPage called page_name with variable <code>var_1 = 5</code> to the Notebook, you can do the following:</p> <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('page_name')\nnbp.var_1 = 5  \nnb += nbp  # or nb.add_page(nbp) or nb.page_name = nbp\n</code></pre> <p>Whenever a NotebookPage is added to the Notebook, it will trigger the Notebook to be saved (unless  the NotebookPage has a name listed in <code>Notebook._no_save_pages</code>).</p> <p>The variable <code>var_1</code> of the NotebookPage called page_name can then be accessed from the Notebook via <code>nb.page_name.var_1</code>.</p> Adding variables to Notebook <p>Varibles of forms other than NotebookPages can be added directly to the Notebook e.g. <code>nb.var_1 = 5</code>. However, when the Notebook is saved and re-loaded, variables added in this way will no longer be present.</p>"},{"location":"notebook/#deleting-a-notebookpage","title":"Deleting a NotebookPage","text":"<p>To delete a NotebookPage called page_name which is in the Notebook, run <code>del nb.page_name</code>.</p> <p>You may want to do this, for example, to re-run a section of the pipeline with  different parameters in the corresponding section of the configuration file. </p>"},{"location":"notebook/#modifying-a-notebookpage","title":"Modifying a NotebookPage","text":"<p>The NotebookPage is a write-once data structure so once a variable has been added to it, it cannot be  changed (unless it is listed in <code>NotebookPage._NON_RESULT_KEYS</code>). I.e. the following will raise an error:</p> <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('page_name')\nnbp.var_1 = 5  \nnbp.var_1 = 10\n</code></pre> <p>Once a NotebookPage has been added to a Notebook, <code>nbp.finalized</code> will change to <code>True</code> and no more variables will be allowed to be added to the NotebookPage:</p> <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('page_name')\nnbp.var_1 = 5  \nnbp.var_2 = 10  # fine as page not added to notebook yet\nnb += nbp\nnb.page_name.var_3 = 99 # will raise error as page is added to notebook now\n</code></pre> <p>To delete the variable <code>var_1</code> run <code>del nb.var_1</code>. Again, you won't be able to do this once the  NotebookPage has been added to a Notebook.</p>"},{"location":"notebook/#coppafish-specific-notebookpages","title":"coppafish Specific NotebookPages","text":"<p>The names of all the NotebookPages added to the Notebook through the course of the pipeline are given as the headers in the notebook_comments.json file.  Then the bullet points give all the variables that are added to each NotebookPage.</p> <p>When a NotebookPage has one of these names, an error will be raised if you try to assign a variable to it which is not listed in the relevant section of the notebook_comments.json file.  When adding the NotebookPage to the Notebook, an error will be raised unless it contains all  the variables listed in the relevant section of the notebook_comments.json file and no others.</p> <p>Examples of adding a NotebookPage named thresholds to a Notebook are given below:</p> \u2705\u274c Error adding variable to NotebookPage\u274c Error adding NotebookPage to Notebook <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('thresholds')\nnbp.intensity = 0.01\nnbp.score_ref = 0.25\nnbp.score_omp = 0.263\nnbp.score_omp_multiplier = 0.95\nnb += nbp\n</code></pre> <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('thresholds')\nnbp.intensity = 0.01\nnbp.var_1 = 5  # Error here as 'var_1' is not listed in \n               # 'thresholds' section of notebook_comments.json\n</code></pre> <pre><code>from coppafish import NotebookPage\nnbp = NotebookPage('thresholds')\nnbp.intensity = 0.01\nnbp.score_ref = 0.25 = 5  \nnb += nbp  # Error here as 'score_omp' and 'score_omp_multiplier' \n           # are listed in 'thresholds' section of \n           # notebook_comments.json but not added to page.\n</code></pre>"},{"location":"notebook/#describe","title":"Describe","text":"<p>The comments given in the notebook_comments.json file can be accessed from  the NotebookPage by calling the <code>describe</code>  function. An example to print the comment for the variable <code>gene_no</code> in the omp page is given below:</p> CodeOutput <pre><code>nb.omp.describe('gene_no')\n</code></pre> <pre><code>Numpy int16 array [n_spots]\ngene_no[s] is the index of the gene assigned to spot s.\n</code></pre> <p>If <code>describe</code> is called from the Notebook instead, it will loop through all NotebookPages in the Notebook and print the comment for each variable with the correct name that it encounters:</p> CodeOutput <pre><code>nb.describe('gene_no')\n</code></pre> <pre><code>gene_no in ref_spots:\nNumpy int16 array [n_spots]\ngene_no[s] is the index of the gene assigned to spot s.\n\ngene_no in omp:\nNumpy int16 array [n_spots]\ngene_no[s] is the index of the gene assigned to spot s.\n</code></pre> <p>If <code>describe</code> is called from the Notebook  and finds the variable in the configuration file, it will print the section it was found in and its value. E.g. for <code>dp_thresh</code> in the omp section:</p> CodeOutput <pre><code>nb.describe('dp_thresh')\n</code></pre> <pre><code>No variable named dp_thresh in the omp page.\nBut it is in the omp section of the config file and has value:\n0.225\n</code></pre>"},{"location":"notebook/#configuration-file","title":"Configuration File","text":"<p>The configuration file can be returned as a dictionary of dictionaries from the Notebook by using the function <code>get_config</code>:</p> Codeconfig <pre><code>config = nb.get_config()\n</code></pre> <p></p> <p>When the Notebook is re-loaded with a <code>config_file</code> (<code>nb = Notebook(nb_file, config_file)</code>), the configuration file supplied will be compared  to the one saved in the Notebook (<code>nb._config</code>).  If the comparison indicates that the two are different, an error will be raised. Otherwise, when the Notebook is loaded, the saved value of the configuration file  (<code>nb._config</code>) will be changed to the one given by the provided <code>config_file</code>.</p> What is compared? <p>Each NotebookPage added during the coppafish pipeline has a name which is the same as a  section in the configuration file or the same apart from a  _debug suffix.</p> <p>Only sections with a corresponding NotebookPage in the Notebook are compared. The file_names section is also ignored in the comparison as it is included in  <code>Notebook._no_compare_config_sections</code>.</p> <p>So if the pipeline has been run as far as the <code>call_reference_spots</code>  stage, the Notebook will not have the omp page.  In this case, the omp section of the <code>config_file</code> can be changed without causing an error as indicated below:</p> nb._config (saved to Notebook)\u2705 Allowed <code>config_file</code>\u274c <code>config_file</code> giving error <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n\n[omp]\ndp_thresh = 0.3452  ; Allowed because variable is in the omp section\n; and omp page not added to Notebook yet.\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/.../experiment1/raw\noutput_dir = /Users/.../experiment1/output\ntile_dir = /Users/.../experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/.../experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n\n[register]\nn_iter = 52  ; Not allowed because variable is in the register section\n; but register page has been added to Notebook.\n</code></pre>"},{"location":"notebook/#changing-basic_info-mid-pipeline","title":"Changing basic_info mid-pipeline","text":"<p>It is quite common to want to change the basic_info section of the configuration file halfway through the pipeline without re-running the steps of the pipeline which have already been completed.</p> <p>For example, we may have specified the wrong <code>dye_names</code>, but this is not used until the <code>call_reference_spots</code> stage. Or after the find_spots or register sections, we may want to remove some problematic  tiles, rounds or channels (through <code>use_tiles</code>, <code>use_rounds</code> and <code>use_channels</code>).</p> <p>But if the basic_info section of the configuration file is changed,  an error would be raised unless the basic_info NotebookPage is deleted. The code below illustrates how to save a new Notebook with a different basic_info page:</p> CodeOutputnb._config (saved to Notebook)/Users/user/coppafish/experiment/settings_new.ini <pre><code>from coppafish import Notebook\nfrom coppafish.pipeline import set_basic_info\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\n\n# Save new notebook with different name so it does not overwrite old notebook\n# Make sure notebook_name is specified in [file_names] section \n# of settings_new.ini file to be same as name given here.\nnb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz'\nini_file_new = '/Users/user/coppafish/experiment/settings_new.ini'\n\n# config_file not given so will use last one saved to Notebook\nnb = Notebook(nb_file)\nprint('Using config file saved to notebook:')\nprint(f\"use_channels: {nb.basic_info.use_channels}\")\nprint(f\"use_tiles: {nb.basic_info.use_tiles}\")\n\n# Change basic_info\ndel nb.basic_info     # delete old basic info\nnb.save(nb_file_new)  # save Notebook with no basic_info page to new file \n                      # so does not overwrite old Notebook\n# Load in new notebook with new config file with different basic_info\nnb_new = Notebook(nb_file_new, ini_file_new)\n# add new basic_info page to Notebook\nconfig = nb_new.get_config()\nnbp_basic = set_basic_info(config['file_names'], config['basic_info'])\nnb_new += nbp_basic\nprint(f'Using new config file {ini_file_new}:')\nprint(f\"use_channels: {nb_new.basic_info.use_channels}\")\nprint(f\"use_tiles: {nb_new.basic_info.use_tiles}\")\n</code></pre> <pre><code>Using config file saved to notebook:\nuse_channels: [0, 1, 2, 3, 4, 5, 6]\nuse_tiles: [0, 1, 2, 3]\nUsing new config file /Users/user/coppafish/experiment/settings_new.ini:\nuse_channels: [1, 2, 5, 6]\nuse_tiles: [0, 2, 3]\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\nnotebook_name = notebook_new\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\nuse_channels = 1, 2, 5, 6\nuse_tiles = 0, 2, 3\n</code></pre>"},{"location":"notebook/#file_names","title":"file_names","text":"<p>The file_names section of the Notebook is treated differently to deal with the  case where the various file locations have changed e.g. when accessing them from a different computer.</p> <p>The file_names NotebookPage is never saved when the Notebook is saved, and adding a NotebookPage called file_names  does not trigger a save. When the Notebook is loaded in, a file_names NotebookPage will automatically be created and added to the  Notebook if the Notebook contains a basic_info NotebookPage. The file_names NotebookPage will then inherit information from the file_names section of the <code>config_file</code> which was passed to the Notebook  when loading it, in as explained below:</p> CodeOutputnb._config (saved to Notebook)/Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini <pre><code>from coppafish import Notebook\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nini_file = '/Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini'\n\n# config_file not given so will use last one saved to Notebook\nnb_old = Notebook(nb_file)  \nprint('Using config file saved to notebook:')\nprint(nb_old.file_names.output_dir)\nprint(nb_old.file_names.big_anchor_image)\n# tile file path for round 0, tile 0, channel 0\nprint(nb_old.file_names.tile[0][0][0])\n\n# config_file given so will update nb._config\nnb_new = Notebook(nb_file, ini_file)  \nprint(f'Using new config file {ini_file}:')\nprint(nb_new.file_names.output_dir)\nprint(nb_new.file_names.big_anchor_image)\n# tile file path for round 0, tile 0, channel 0\nprint(nb_new.file_names.tile[0][0][0])  \n</code></pre> <pre><code>Using config file saved to notebook:\n/Users/user/coppafish/experiment1/output\n/Users/user/coppafish/experiment1/output/anchor_image.npz\n/Users/user/coppafish/experiment1/tiles/Exp1_r0_t0c0.npy\nUsing new config file /Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini:\n/Users/NEW_USER/coppafish/NEW_EXPERIMENT/output\n/Users/NEW_USER/coppafish/NEW_EXPERIMENT/output/anchor_image.npz\n/Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles/Exp1_r0_t0c0.npy\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/raw\noutput_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output\ntile_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <p>Also, as soon as a NotebookPage named basic_info is added to the Notebook, the file_names NotebookPage will also be  added due to information in the <code>Notebook._no_save_pages</code> dictionary.</p>"},{"location":"notebook_comments/","title":"Notebook Comments","text":""},{"location":"notebook_comments/#file_names","title":"file_names","text":"<p>file_names page contains all files that are used throughout the pipeline. <code>tile</code> is the only variable not included in the config file and is worked out automatically.  Page added to Notebook automatically as soon as basic_info page added.</p> <ul> <li> <p>input_dir: Directory.</p> <p>Where raw nd2 files are</p> </li> <li> <p>output_dir: Directory.</p> <p>Where notebook is saved</p> </li> <li> <p>tile_dir: Directory.</p> <p>Where tile npy files saved</p> </li> <li> <p>round: List [n_rounds].</p> <p>Names of nd2 files for the imaging rounds. If not using, will be an empty list.</p> </li> <li> <p>anchor: String or None.</p> <p>Name of nd2 file for the anchor round. <code>None</code> if anchor not used</p> </li> <li> <p>raw_extension: String.</p> <p>.nd2 or .npy indicating the data type of the raw data.</p> </li> <li> <p>raw_metadata: String or None.</p> <p>If <code>raw_extension = .npy</code>, this is the name of the json file in <code>input_dir</code> which contains the   required metadata extracted from the initial nd2 files. I.e. it is the output of coppafish/utils/nd2/save_metadata</p> </li> <li> <p>dye_camera_laser: File.</p> <p>csv file giving the approximate raw intensity for each dye with each camera/laser combination</p> </li> <li> <p>code_book: File.</p> <p>Text file which contains the codes indicating which dye to expect on each round for each gene</p> </li> <li> <p>scale: File.</p> <p>Text file saved containing the <code>extract['scale']</code> and <code>extract['scale_anchor']</code> values used to create  the tile npy files in the tile_dir. If the second value is 0, it means <code>extract['scale_anchor']</code>  has not been calculated yet. </p> <p>If the extract step of the pipeline is re-run with <code>extract['scale']</code> or  <code>extract['scale_anchor']</code> different to values saved here, an error will be raised.</p> </li> <li> <p>psf: File or None.</p> <p>npy file indicating average spot shape (before padding and scaled to fill uint16 range). Will be <code>None</code> if 2D pipeline used. File won't exist/used if <code>config['extract']['deconvolve'] = False</code>. If 3D, 1st axis in npy file is z.</p> </li> <li> <p>omp_spot_shape: File.</p> <p>npy file indicating average spot shape in OMP coefficient sign images. Saved image is int8 npy with only values being -1, 0, 1.</p> </li> <li> <p>omp_spot_info: File.</p> <p>After each tile is finished in OMP, information about spots found is saved as array to npy file: <code>numpy int16 array [n_spots x 7]</code> containing \\(y\\), \\(x\\), \\(z\\), <code>gene_no</code>, <code>n_pos_neighb</code>, <code>n_neg_neighb</code>, <code>tile</code>. If 3D, 1st axis in npy file is z.</p> </li> <li> <p>omp_spot_coef: File.</p> <p>After each tile is finished in OMP, coefficients for all spots found is saved as sparse  <code>csr_matrix</code> to npz file:  </p> <p><code>CSR_matrix float [n_spots x n_genes]</code> </p> <p>giving coefficient found for each gene for each spot.</p> </li> <li> <p>big_dapi_image: File or None.</p> <p>npz file of stitched DAPI image. None if <code>nb.basic_info.dapi_channel = None</code> If 3D, 1st axis in npz file is z.</p> </li> <li> <p>big_anchor_image: File.</p> <p>npz file of stitched image of <code>ref_round</code>/<code>ref_channel</code>. Will be stitched anchor if anchor used. If 3D, 1st axis in npz file is z.</p> </li> <li> <p>pciseq: List of 2 files.</p> <p>csv files where plotting information for pciSeq is saved. </p> <p><code>pciseq[0]</code> is the path where the OMP method output will be saved. </p> <p><code>pciseq[1]</code> is the path where the ref_spots method output will be saved. </p> <p>If files don't exist, they will be created when the function coppafish/export_to_pciseq is run.</p> </li> <li> <p>tile: List of numpy string arrays [n_tiles][(n_rounds + n_extra_rounds) {x n_channels if 3d}].</p> <p>2D: <code>tile[t][r]</code> is the npy file containing all channels of tile \\(t\\), round \\(r\\). </p> <p>3D: <code>tile[t][r][c]</code> is the npy file containing all z planes for tile \\(t\\), round \\(r\\), channel \\(c\\)</p> </li> </ul>"},{"location":"notebook_comments/#basic_info","title":"basic_info","text":"<p>basic_info page contains information that is used at all stages of the pipeline.  Page added to Notebook in pipeline/basic_info.py</p> <ul> <li> <p>is_3d: Boolean.</p> <p><code>True</code> if 3D pipeline used, <code>False</code> if 2D</p> </li> <li> <p>anchor_round: Integer or None.</p> <p>Index of anchor round (typically the first round after imaging rounds so <code>anchor_round = n_rounds</code>). <code>None</code> if anchor not used.</p> </li> <li> <p>anchor_channel: Integer or None.</p> <p>Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. <code>None</code> if anchor not used.</p> </li> <li> <p>dapi_channel: Integer or None.</p> <p>Channel in anchor round that contains DAPI images. <code>None</code> if no DAPI.</p> </li> <li> <p>ref_round: Integer.</p> <p>Round to align all imaging rounds to. Will be anchor if using.</p> </li> <li> <p>ref_channel: Integer.</p> <p>Channel in reference round used as reference and to build coordinate system on. Usually channel with most spots. Will be <code>anchor_channel</code> if using anchor round</p> </li> <li> <p>use_channels: Integer List [n_use_channels].</p> <p>Channels in imaging rounds to use throughout pipeline.</p> </li> <li> <p>use_rounds: Integer List [n_use_rounds].</p> <p>Imaging rounds to use throughout pipeline.</p> </li> <li> <p>use_z: Integer List [nz].</p> <p>z planes used to make tile npy files</p> </li> <li> <p>use_tiles: Integer List [n_use_tiles].</p> <p>Tiles to use throughout pipeline. For an experiment where the tiles are arranged in a \\(4 \\times 3\\) (\\(n_y \\times n_x\\)) grid,  tile indices are indicated as below: </p> <p>| 2  | 1  | 0  | </p> <p>| 5  | 4  | 3  | </p> <p>| 8  | 7  | 6  | </p> <p>| 11 | 10 | 9  |</p> </li> <li> <p>use_dyes: Integer List [n_use_dyes].</p> <p>Dyes to use when assigning spots to genes.</p> </li> <li> <p>dye_names: String List [n_dyes] or None.</p> <p>Names of all dyes so for gene with code \\(360...\\), gene appears with <code>dye_names[3]</code> in round \\(0\\), <code>dye_names[6]</code> in round \\(1\\), <code>dye_names[0]</code> in round \\(2\\) etc. <code>None</code> if each channel corresponds to a different dye.</p> </li> <li> <p>channel_camera: Integer List [n_channels] or None.</p> <p><code>channel_camera[i]</code> is the wavelength in nm of the camera on channel \\(i\\). <code>None</code> if <code>dye_names = None</code>.</p> </li> <li> <p>channel_laser: Integer List [n_channels] or None.</p> <p><code>channel_laser[i]</code> is the wavelength in nm of the laser on channel \\(i\\). <code>None</code> if <code>dye_names = None</code>.</p> </li> <li> <p>tile_pixel_value_shift: Integer.</p> <p>This is added onto every tile (except DAPI) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to npy as uint16. </p> <p>Typical=15000</p> </li> <li> <p>n_extra_rounds: Integer.</p> <p>Number of non-imaging rounds, typically 1 if using anchor and 0 if not.</p> </li> <li> <p>n_rounds: Integer.</p> <p>Number of imaging rounds in the raw data</p> </li> <li> <p>tile_sz: Integer.</p> <p>\\(yx\\) dimension of tiles in pixels</p> </li> <li> <p>n_tiles: Integer.</p> <p>Number of tiles in the raw data</p> </li> <li> <p>n_channels: Integer.</p> <p>Number of channels in the raw data</p> </li> <li> <p>nz: Integer.</p> <p>Number of z-planes used to make the npy tile images (can be different from number in raw data).</p> </li> <li> <p>n_dyes: Integer.</p> <p>Number of dyes used</p> </li> <li> <p>tile_centre: Numpy float array [3].</p> <p><code>[y, x, z]</code> location of tile centre in units of <code>[yx_pixels, yx_pixels, z_pixels]</code>. For 2D pipeline, <code>tile_centre[2] = 0</code></p> </li> <li> <p>tilepos_yx_nd2: Numpy integer array [n_tiles x 2].</p> <p><code>tilepos_yx_nd2[i, :]</code> is the \\(yx\\) position of tile with fov index \\(i\\) in the nd2 file. </p> <p>Index 0 refers to <code>YX = [0, 0]</code> </p> <p>Index 1 refers to <code>YX = [0, 1]</code> if <code>MaxX &gt; 0</code></p> </li> <li> <p>tilepos_yx: Numpy integer array [n_tiles x 2].</p> <p><code>tilepos_yx[i, :]</code> is the \\(yx\\) position of tile with tile directory (npy files) index \\(i\\). Equally, <code>tilepos_yx[use_tiles[i], :]</code> is \\(yx\\) position of tile <code>use_tiles[i]</code>. </p> <p>Index 0 refers to <code>YX = [MaxY, MaxX]</code> </p> <p>Index 1 refers to <code>YX = [MaxY, MaxX - 1]</code> if <code>MaxX &gt; 0</code></p> </li> <li> <p>pixel_size_xy: Float.</p> <p>\\(yx\\) pixel size in microns</p> </li> <li> <p>pixel_size_z: Float.</p> <p>\\(z\\) pixel size in microns</p> </li> <li> <p>use_anchor: Boolean.</p> <p><code>True</code> if anchor round is used, <code>False</code> if not.</p> </li> </ul>"},{"location":"notebook_comments/#extract","title":"extract","text":"<p>The extract page contains variables from <code>extract_and_filter</code> step which are used later in the  pipeline. <code>auto_thresh</code> is used in <code>find_spots</code> step. <code>hist_values</code> and <code>hist_counts</code> are used for  normalisation between channels in the <code>call_reference_spots</code> step.  Page added to Notebook in pipeline/extract_run.py</p> <ul> <li> <p>auto_thresh: Numpy float array <code>[n_tiles x (n_rounds + n_extra_rounds) x n_channels]</code>.</p> <p><code>auto_thresh[t, r, c]</code> is the threshold spot intensity for tile \\(t\\), round \\(r\\), channel \\(c\\) used for spot detection in the <code>find_spots</code> step of the pipeline.</p> </li> <li> <p>hist_values: Numpy integer array [n_pixel_values].</p> <p>All possible pixel values in saved npy images i.e. length is approx <code>np.iinfo(np.uint16).max</code></p> </li> <li> <p>hist_counts: Numpy integer array <code>[n_pixel_values x n_rounds x n_channels]</code>.</p> <p><code>hist_counts[i, r, c]</code> is the number of pixels across all tiles in round \\(r\\), channel \\(c\\) which had the value <code>hist_values[i]</code>.</p> </li> </ul>"},{"location":"notebook_comments/#extract_debug","title":"extract_debug","text":"<p>extract_debug page stores variables from <code>extract_and_filter</code> step which are not needed later in the pipeline but may be useful for debugging purposes.  Page added to Notebook in pipeline/extract_run.py</p> <ul> <li> <p>n_clip_pixels: Numpy integer array [n_tiles x (n_rounds + n_extra_rounds) x n_channels].</p> <p><code>n_clip_pixels[t, r, c]</code> is the number of pixels in the saved npy tile for tile \\(t\\), round \\(r\\), channel \\(c\\) which had intensity exceeding max uint16 value and so had to be clipped.</p> </li> <li> <p>clip_extract_scale: Numpy float array [n_tiles x (n_rounds + n_extra_rounds) x n_channels].</p> <p><code>clip_extract_scale[t, r, c]</code> is the recommended value for extract_scale such that for tile \\(t\\), round \\(r\\), channel \\(c\\) <code>n_clip_pixels[t, r, c]</code> would be 0. Only computed for images where <code>n_clip_pixels[t, r, c] &gt; 0</code></p> </li> <li> <p>r1: Integer.</p> <p>Filtering is done with a 2D difference of hanning filter with inner radius <code>r1</code> within which it is positive and outer radius <code>r2</code> so annulus between <code>r1</code> and <code>r2</code> is negative. Should be approx radius of spot. </p> <p>By default this is 0.5 micron converted to yx-pixel units which is typically 3.</p> </li> <li> <p>r2: Integer.</p> <p>Filtering is done with a 2D difference of hanning filter with inner radius <code>r1</code> within which it is positive and outer radius <code>r2</code> so annulus between <code>r1</code> and <code>r2</code> is negative. Units are yx-pixels and by default it will be twice r1. </p> <p>Typical: 6</p> </li> <li> <p>r_dapi: Integer or None.</p> <p>Filtering for DAPI images is a tophat with <code>r_dapi</code> radius. Should be approx radius of object of interest. Typically this is 8 micron converted to yx-pixel units which is typically 48. By default, it is <code>None</code> meaning DAPI not filtered at all and npy file not saved.</p> </li> <li> <p>psf: Numpy float array [psf_shape[0] x psf_shape[1] x psf_shape[2]] or None (psf_shape is in config file).</p> <p>Average shape of spot from individual raw spot images normalised so max is 1 and min is 0. <code>None</code> if <code>config['deconvolve'] = False</code>.</p> </li> <li> <p>psf_intensity_thresh: Float.</p> <p>Intensity threshold used to detect spots in raw images which were used to make the psf. None if <code>config['deconvolve'] = False</code> or psf provided without spot detection.</p> </li> <li> <p>psf_tiles_used: Integer list or None.</p> <p>Tiles where spots for psf calculation came from. <code>None</code> if <code>config['deconvolve'] = False</code> or psf provided without spot detection.</p> </li> <li> <p>scale: Float.</p> <p>Multiplier applied to filtered nd2 imaging round images before saving as npy so full uint16 occupied.</p> </li> <li> <p>scale_tile: Integer or None.</p> <p>Tile of image that scale was found from. <code>None</code> if <code>config['extract']['scale']</code> provided.</p> </li> <li> <p>scale_channel: Integer or None.</p> <p>Channel of image that scale was found from. <code>None</code> if <code>config['extract']['scale']</code> provided.</p> </li> <li> <p>scale_z: Integer or None.</p> <p>z plane of image that scale was found from. <code>None</code> if <code>config['extract']['scale']</code> provided.</p> </li> <li> <p>scale_anchor: Float or None.</p> <p>Multiplier applied to filtered nd2 anchor round images before saving as npy so full uint16 occupied. <code>None</code> if <code>use_anchor = False</code>.</p> </li> <li> <p>scale_anchor_tile: Integer or None.</p> <p>Tile of image in anchor round/channel that <code>scale</code> was found from. <code>None</code> if <code>config['extract']['scale']</code> provided or <code>use_anchor = False</code>.</p> </li> <li> <p>scale_anchor_z: Integer or None.</p> <p>z plane of image in anchor round/channel that <code>scale_anchor</code> was found from. <code>None</code> if <code>config['extract']['scale_anchor']</code> provided or <code>use_anchor = False</code>.</p> </li> <li> <p>z_info: Integer.</p> <p>z plane in npy file from which <code>auto_thresh</code> and <code>hist_counts</code> were calculated. By default, this is the mid plane.</p> </li> </ul>"},{"location":"notebook_comments/#find_spots","title":"find_spots","text":"<p>find_spots page contains information about spots found on all tiles, rounds and channels.  Page added to Notebook in pipeline/find_spots.py</p> <ul> <li> <p>isolation_thresh: Numpy float array [n_tiles].</p> <p>Spots found on tile \\(t\\), <code>ref_round</code>, <code>ref_channel</code> are isolated if annular filtered image is below <code>isolation_thresh[t]</code> at spot location. </p> <p>Typical: 0</p> </li> <li> <p>spot_no: Numpy int32 array [n_tiles x (n_rounds + n_extra_rounds) x n_channels].</p> <p><code>spot_no[t, r, c]</code> is the number of spots found on tile \\(t\\), round \\(r\\), channel \\(c\\)</p> </li> <li> <p>spot_details: Numpy int16 array [n_total_spots x 7].</p> <p><code>spot_details[i,:]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> for spot \\(i\\) isolated is 0 for all non reference round/channel spots and is 1 for isolated reference spots. \\(y\\), \\(x\\) gives the local tile coordinates in yx-pixels. \\(z\\) gives local tile coordinate in z-pixels (0 if 2D)</p> </li> </ul>"},{"location":"notebook_comments/#stitch","title":"stitch","text":"<p>stitch page contains information about how tiles were stitched together to give global coordinates. Only <code>tile_origin</code> is used in later stages of the pipeline. Note that references to south in this section should really be north and west should be east.  Page added to Notebook in pipeline/stitch.py</p> <ul> <li> <p>tile_origin: Numpy float array [n_tiles x 3].</p> <p><code>tile_origin[t,:]</code> is the bottom left \\(yxz\\) coordinate of tile \\(t\\). \\(yx\\) coordinates in yx-pixels and z coordinate in z-pixels.</p> </li> <li> <p>south_start_shift_search: Numpy integer array [3 x 3].</p> <p>Initial search range used to find overlap between south neighbouring tiles <code>[i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). <code>[2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>west_start_shift_search: Numpy integer array [3 x 3].</p> <p>Initial search range used to find overlap between west neighbouring tiles <code>[i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). <code>[2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>south_final_shift_search: Numpy integer array [3 x 3].</p> <p>Final search range used to find overlap between south neighbouring tiles <code>[i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). <code>[2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>west_final_shift_search: Numpy integer array [3 x 3].</p> <p>Final search range used to find overlap between west neighbouring tiles <code>[i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). <code>[2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>south_pairs: Numpy integer array [n_south_overlap x 2].</p> <p><code>south_pairs[i, 1]</code> is the tile to the north of <code>south_pairs[i, 0]</code></p> </li> <li> <p>west_pairs: Numpy integer array [n_west_overlap x 2].</p> <p><code>west_pairs[i, 1]</code> is the tile to the east of <code>west_pairs[i, 0]</code></p> </li> <li> <p>south_shifts: Numpy integer array [n_south_overlap x 3].</p> <p><code>south_shifts[i, :]</code> is the \\(yxz\\) shift found that is applied to <code>south_pairs[i, 0]</code> to take it to <code>south_pairs[i, 1]</code> </p> <p>Units: <code>[yx_pixels, yx_pixels, z_pixels]</code>, <code>[:, 2] = 0</code> if 2D.</p> </li> <li> <p>west_shifts: Numpy integer array [n_west_overlap x 3].</p> <p><code>west_shifts[i, :]</code> is the \\(yxz\\) shift found that is applied to <code>west_pairs[i, 0]</code> to take it to <code>west_pairs[i, 1]</code> </p> <p>Units: <code>[yx_pixels, yx_pixels, z_pixels]</code>, <code>[:, 2] = 0</code> if 2D.</p> </li> <li> <p>south_score: Numpy float array [n_south_overlap].</p> <p><code>south_score[i]</code> is approximately the number of matches found for <code>south_shifts[i, :]</code></p> </li> <li> <p>west_score: Numpy float array [n_west_overlap].</p> <p><code>west_score[i]</code> is approximately the number of matches found for <code>west_shifts[i, :]</code></p> </li> <li> <p>south_score_thresh: Numpy float array [n_south_overlap].</p> <p>If <code>south_score[i]</code> is below <code>south_score_thresh[i]</code>, it indicates <code>south_shifts[i]</code> may be incorrect.</p> </li> <li> <p>west_score_thresh: Numpy float array [n_west_overlap].</p> <p>If <code>west_score[i]</code> is below <code>west_score_thresh[i]</code>, it indicates <code>west_shifts[i]</code> found may be incorrect.</p> </li> <li> <p>south_outlier_shifts: Numpy integer array [n_south_overlap x 3].</p> <p>If <code>south_score[i]</code> was below <code>south_score_thresh[i]</code>, <code>south_shifts[i]</code> was found again and old shift recorded as <code>south_outlier_shifts[i]</code>. Will be zero if this did not happen.</p> </li> <li> <p>west_outlier_shifts: Numpy integer array [n_west_overlap x 3].</p> <p>If <code>west_score[i]</code> was below <code>west_score_thresh[i]</code>, <code>west_shifts[i]</code> was found again and old shift recorded as <code>west_outlier_shifts[i]</code>. Will be zero if this did not happen.</p> </li> <li> <p>south_outlier_score: Numpy float array [n_south_overlap].</p> <p>If <code>south_score[i]</code> was below <code>south_score_thresh[i]</code>, <code>south_shifts[i]</code> was found again and old score recorded as <code>south_outlier_score[i]</code>. Will be zero if this did not happen.</p> </li> <li> <p>west_outlier_score: Numpy float array [n_west_overlap].</p> <p>If <code>west_score[i]</code> was below <code>west_score_thresh[i]</code>, <code>west_shifts[i]</code> was found again and old score recorded as <code>west_outlier_score[i]</code>. Will be zero if this did not happen.</p> </li> </ul>"},{"location":"notebook_comments/#register_initial","title":"register_initial","text":"<p>register_initial page contains information about how shift between ref round/channel to each imaging round for each tile was found. These are then used as the starting point for determining the affine transforms. Only <code>shift</code> is used in later stages of the pipeline.  Page added to Notebook in pipeline/register_initial.py</p> <ul> <li> <p>shift: Numpy integer array [n_tiles x n_rounds x 3].</p> <p><code>shift[t, r, :]</code> is the \\(yxz\\) shift found that is applied to tile \\(t\\), <code>ref_round</code> to take it to tile \\(t\\), round \\(r\\). </p> <p>Units: <code>[yx_pixels, yx_pixels, z_pixels]</code>, <code>[:, :, 2] = 0</code> if 2D. Same as <code>initial_shift</code> in register page.</p> </li> <li> <p>shift_channel: Integer.</p> <p>Channel used to find find shifts between rounds to use as starting point for point cloud registration. Typically this is <code>ref_channel</code> or a channel with lots of spots.</p> </li> <li> <p>start_shift_search: Numpy integer array [n_rounds x 3 x 3].</p> <p><code>[r, :, :]</code> is the initial search range used to find shift from reference round to round \\(r\\) for all tiles </p> <p><code>[r, i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). </p> <p><code>[r, 2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>final_shift_search: Numpy integer array [n_rounds x 3 x 3].</p> <p><code>[r, :, :]</code> is the final search range used to find shift from reference round to round \\(r\\) for all tiles </p> <p><code>[r, i, :]</code> is the <code>[min, max, step]</code> of the search in direction \\(i\\) (0 is \\(y\\), 1 is \\(x\\), 2 is \\(z\\)). </p> <p><code>[r, 2,:]</code> is in units of z-pixels and is 0 if 2D.</p> </li> <li> <p>shift_score: Numpy float array [n_tiles x n_rounds].</p> <p><code>shift_score[t, r]</code> is is approximately the number of matches found for <code>shift[t,r]</code></p> </li> <li> <p>shift_score_thresh: Numpy float array [n_tiles x n_rounds].</p> <p>If <code>shift_score[t, r]</code> is below <code>shift_score_thresh[t, r]</code>, it indicates <code>shift[t,r]</code> may be incorrect.</p> </li> <li> <p>shift_outlier: Numpy integer array [n_tiles x n_rounds x 3].</p> <p>If <code>shift_score[t, r]</code> was below <code>shift_score_thresh[t, r]</code>, <code>shift[t, r]</code> was found again and old shift recorded as <code>shift_outlier[t, r]</code>. Will be zero if this did not happen.</p> </li> <li> <p>shift_score_outlier: Numpy float array [n_tiles x n_rounds].</p> <p>If <code>shift_score[t, r]</code> was below <code>shift_score_thresh[t, r]</code>, <code>shift[t, r]</code> was found again and old score recorded as <code>shift_score_outlier[t, r]</code>. Will be zero if this did not happen.</p> </li> </ul>"},{"location":"notebook_comments/#register","title":"register","text":"<p>register page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. Page added to Notebook in pipeline/register.py</p> <ul> <li> <p>initial_shift: Numpy integer array [n_tiles x n_rounds x 3].</p> <p><code>shift[t, r, :]</code> is the \\(yxz\\) shift found that is applied to tile \\(t\\), <code>ref_round</code> to take it to tile \\(t\\), round \\(r\\). Units: <code>[yx_pixels, yx_pixels, z_pixels]</code>, <code>[:, :, 2] = 0</code> if 2D. Same as shift in register_initial page. DON'T KNOW WHY COPIED THIS - PROBABLY SHOULD REMOVE</p> </li> <li> <p>transform: Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3].</p> <p><code>transform[t, r, c]</code> is the affine transform to get from tile \\(t\\), <code>ref_round</code>, <code>ref_channel</code> to tile \\(t\\), round \\(r\\), channel \\(c\\) Before applying to coordinates, they must be centered and z coordinates put into units of yx-pixels. If 2D, z scaling set to 1 while shift and rotation set to 0.</p> </li> </ul>"},{"location":"notebook_comments/#register_debug","title":"register_debug","text":"<p>register_debug page contains information on how the affine transforms in register page were calculated. Page added to Notebook in pipeline/register.py</p> <ul> <li> <p>n_matches: Numpy integer array [n_tiles x n_rounds x n_channels].</p> <p>Number of matches found for each transform. A match is when distance between points is less than <code>config['register']['neighb_dist_thresh']</code>.</p> </li> <li> <p>n_matches_thresh: Numpy integer array [n_tiles x n_rounds x n_channels].</p> <p><code>n_matches[t, r, c]</code> must exceed <code>n_matches_thresh[t, r, c]</code> otherwise <code>failed[t, r, c] = True</code> and transform found again using regularisation.</p> </li> <li> <p>error: Numpy float array [n_tiles x n_rounds x n_channels].</p> <p>Average distance between neighbours closer than <code>config['register']['neighb_dist_thresh']</code> for each transform.</p> </li> <li> <p>failed: Numpy boolean array [n_tiles x n_rounds x n_channels].</p> <p><code>failed[t, r, c] = True</code> if <code>transform[t, r, c]</code> had too few matches or was anomalous compared to average. <code>n_matches_thresh</code> in this page and <code>scale_dev_thresh</code>, <code>shift_dev_thresh</code> in config file quantify the required matches / deviation.</p> </li> <li> <p>converged: Numpy boolean array [n_tiles x n_rounds x n_channels].</p> <p>This is <code>False</code> for transforms where the ICP algorithm reached <code>config['register']['n_iter']</code> iterations before transform converged.</p> </li> <li> <p>av_scaling: Numpy float array [n_channels x 3].</p> <p><code>av_scaling[c]</code> is the \\(yxz\\) chromatic aberration scale factor to channel \\(c\\) from the <code>ref_channel</code> averaged over all rounds and tiles. Expect the \\(y\\) and \\(x\\) scaling to be the same and all scalings to be approx 1.</p> </li> <li> <p>av_shifts: Numpy float array [n_tiles x n_rounds x 3].</p> <p><code>av_shifts[t, r]</code> is the \\(yxz\\) shift from tile \\(t\\), <code>ref_round</code> to tile \\(t\\), round \\(r\\) averaged over all channels. All three directions are in yx-pixel units.</p> </li> <li> <p>transform_outlier: Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3].</p> <p><code>[t, r, c]</code> is the final transform found for tile \\(t\\), round \\(r\\), channel \\(c\\) without regularisation. Regularisation only used for \\(t\\),\\(r\\),\\(c\\) indicated by failed and so <code>transform_outlier = 0</code> for others.</p> </li> </ul>"},{"location":"notebook_comments/#ref_spots","title":"ref_spots","text":"<p>ref_spots page contains gene assignments and info for spots found on reference round.  Page added to Notebook in pipeline/get_reference_spots.py.  The variables <code>gene_no</code>, <code>score</code>, <code>score_diff</code>, <code>intensity</code> will be set to <code>None</code> after <code>get_reference_spots</code>.  <code>call_reference_spots</code> should then be run to give their actual values. This is so if there is an error in  <code>call_reference_spots</code>, <code>get_reference_spots</code> won't have to be re-run.</p> <ul> <li> <p>local_yxz: Numpy int16 array [n_spots x 3].</p> <p><code>local_yxz[s]</code> are the \\(yxz\\) coordinates of spot \\(s\\) found on <code>tile[s]</code>, <code>ref_round</code>, <code>ref_channel</code>. To get <code>global_yxz</code>, add <code>nb.stitch.tile_origin[tile[s]]</code>.</p> </li> <li> <p>isolated: Numpy boolean array [n_spots].</p> <p><code>True</code> for spots that are well isolated i.e. surroundings have low intensity so no nearby spots.</p> </li> <li> <p>tile: Numpy int16 array [n_spots].</p> <p>Tile each spot was found on.</p> </li> <li> <p>colors: Numpy int32 array [n_spots x n_rounds x n_channels].</p> <p><code>[s, r, c]</code> is the intensity of spot \\(s\\) on round \\(r\\), channel \\(c\\). <code>-tile_pixel_value_shift</code> if that round/channel not used otherwise integer.</p> </li> <li> <p>gene_no: Numpy int16 array [n_spots].</p> <p><code>gene_no[s]</code> is the index of the gene assigned to spot \\(s\\).</p> </li> <li> <p>score: Numpy float32 array [n_spots].</p> <p><code>score[s]</code> is the dot product score, \\(\\Delta_{s0g}\\), between <code>colors[s]</code> and <code>bled_codes[gene_no[s]]</code>. Normalisation depends on <code>config['call_spots']['dot_product_method']</code>.</p> </li> <li> <p>score_diff: Numpy float16 array [n_spots].</p> <p><code>score_diff[s]</code> is <code>score[s]</code> minus the score for the second best gene assignement for spot \\(s\\).</p> </li> <li> <p>intensity: Numpy float32 array [n_spots].</p> <p>\\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) <code>colors[s, r]/color_norm_factor[r]</code>.</p> </li> </ul>"},{"location":"notebook_comments/#call_spots","title":"call_spots","text":"<p>call_spots page contains <code>bleed_matrix</code> and expected code for each gene. Page added to Notebook in pipeline/call_reference_spots.py</p> <ul> <li> <p>gene_names: Numpy string array [n_genes].</p> <p>Names of all genes in the code book provided.</p> </li> <li> <p>gene_codes: Numpy integer array [n_genes x n_rounds].</p> <p><code>gene_codes[g, r]</code> indicates the dye that should be present for gene \\(g\\) in round \\(r\\).</p> </li> <li> <p>color_norm_factor: Numpy float array [n_rounds x n_channels].</p> <p>Normalisation such that dividing <code>colors</code> by <code>color_norm_factor</code> equalizes intensity of channels. <code>config['call_spots']['bleed_matrix_method']</code> indicates whether normalisation is for rounds and channels or just channels.</p> </li> <li> <p>initial_raw_bleed_matrix: Numpy float array [n_rounds x n_channels x n_dyes].</p> <p><code>initial_raw_bleed_matrix[r, c, d]</code> is the estimate of the raw intensity of dye \\(d\\) in round \\(r\\), channel \\(c\\). All will be nan if separate dye for each channel.</p> </li> <li> <p>initial_bleed_matrix: Numpy float array [n_rounds x n_channels x n_dyes].</p> <p>Starting point for determination of bleed matrix. If separate dye for each channel, <code>initial_bleed_matrix[r]</code> will be the identity matrix for each \\(r\\). Otherwise, it will be <code>initial_raw_bleed_matrix</code> divided by <code>color_norm_factor</code>.</p> </li> <li> <p>bleed_matrix: Numpy float array [n_rounds x n_channels x n_dyes].</p> <p>For a spot, \\(s\\), which should be dye \\(d\\) in round \\(r\\), we expect <code>color[s, r]/color_norm_factor[r]</code> to be a constant multiple of <code>bleed_matrix[r, :, d]</code></p> </li> <li> <p>background_codes: Numpy float array [n_channels x n_rounds x n_channels].</p> <p>These are the background codes for which each spot has a <code>background_coef</code>. <code>background_codes[C, r, c] = 1</code> if <code>c==C</code> and 0 otherwise for all rounds \\(r\\). <code>nan</code> if \\(r\\)/\\(c\\) outside <code>use_rounds</code>/<code>use_channels</code>.</p> </li> <li> <p>bled_codes: Numpy float array [n_genes x n_rounds x n_channels].</p> <p><code>color[s, r]/color_norm_factor[r]</code> of spot, \\(s\\), corresponding to gene \\(g\\) is expected to be a constant multiple of <code>bled_codes[g, r]</code> in round \\(r\\). <code>nan</code> if \\(r\\)/\\(c\\) outside <code>use_rounds</code>/<code>use_channels</code> and 0 if <code>gene_codes[g,r]</code> outside <code>use_dyes</code>. All codes have L2 norm = 1 when summed across all <code>use_rounds</code> and <code>use_channels</code>.</p> </li> <li> <p>gene_efficiency: Numpy float array [n_genes x n_rounds].</p> <p><code>gene_efficiency[g,r]</code> gives the expected intensity of gene \\(g\\) in round \\(r\\) compared to that expected by the <code>bleed_matrix</code>. It is computed based on the average of isolated spot_colors assigned to that gene which exceed <code>score</code>, <code>score_diff</code> and <code>intensity</code> thresholds given in config file. For all \\(g\\), there is an <code>av_round[g]</code> such that <code>gene_efficiency[g, av_round[g]] = 1</code>. <code>nan</code> if \\(r\\) outside <code>use_rounds</code> and 1 if <code>gene_codes[g,r]</code> outside <code>use_dyes</code>.</p> </li> <li> <p>bled_codes_ge: Numpy float array [n_genes x n_rounds x n_channels].</p> <p><code>bled_codes</code> using <code>gene_efficiency</code> information i.e. <code>bled_codes * gene_efficiency</code>. All codes have L2 norm = 1 when summed across all <code>use_rounds</code> and <code>use_channels</code>.</p> </li> <li> <p>background_weight_shift: Float.</p> <p>Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round \\(r\\) for the fitting of the background vector for channel \\(c\\) is <code>1 / (spot_color[r, c] + background_weight_shift)</code> so <code>background_weight_shift</code> ensures this does not go to infinity for small <code>spot_color[r, c]</code>. Typical <code>spot_color[r, c]</code> is 1 for intense spot so <code>background_weight_shift</code> is small fraction of this.</p> </li> <li> <p>dp_norm_shift: Float.</p> <p>When calculating the dot product score, this is the small shift to apply when normalising <code>spot_colors</code> to ensure don't divide by zero. Value is for a single round and is multiplied by <code>sqrt(n_rounds_used)</code> when computing dot product score. Expected norm of a <code>spot_color</code> for a single round is 1 so <code>dp_norm_shift</code> is a small fraction of this.</p> </li> <li> <p>abs_intensity_percentile: Numpy float array [100] or None.</p> <p><code>abs_intensity_percentile[i]</code> is the i% percentile of absolute <code>pixel_colors</code> on <code>norm_shift_tile</code>,  <code>norm_shift_z</code>. </p> <p>This is used to compute <code>nb.omp.initial_intensity_thresh</code> if not provided.</p> </li> <li> <p>norm_shift_tile: Integer.</p> <p>Tile that is used to compute <code>abs_intensity_percentile</code> from which <code>dp_norm_shift</code>, <code>background_weight_shift</code>  and <code>intensity_thresh</code> are computed.</p> </li> <li> <p>norm_shift_z: Integer.</p> <p>z-plane that is used to compute <code>abs_intensity_percentile</code> from which <code>dp_norm_shift</code>, <code>background_weight_shift</code>  and <code>intensity_thresh</code> are computed.</p> </li> <li> <p>gene_efficiency_intensity_thresh: Float.</p> <p><code>gene_efficiency</code> is computed from spots with intensity greater than this. By default, it is set to the <code>config['call_spots']['gene_efficiency_intensity_thresh_percentile']</code>  percentile of the <code>intensity</code> computed for all pixels on the mid z-plane of the most central tile</p> </li> </ul>"},{"location":"notebook_comments/#omp","title":"omp","text":"<p>omp page contains gene assignments and info for  spots located at the local maxima of the gene coefficients returned by OMP. Also contains info about <code>spot_shape</code> which indicates the expected sign of the OMP coefficient in a neighbourhood centered on a spot.  Page added to Notebook in pipeline/call_spots_omp.py</p> <ul> <li> <p>initial_intensity_thresh: Float.</p> <p>To save time in <code>call_spots_omp</code>, coefficients only found for pixels with <code>intensity</code>  of absolute <code>spot_colors</code> greater than <code>initial_intensity_thresh</code>. This threshold is set to the <code>config['omp']['initial_intensity_thresh_percentile']</code> percentile  of the absolute <code>intensity</code> of all pixels on the mid z-plane of the central tile (uses <code>nb.call_spots.abs_intensity_percentile</code>). It is also clamped between the min and max values given in config file.</p> </li> <li> <p>shape_tile: Integer or None.</p> <p><code>spot_shape</code> was found from spots detected on this tile. <code>None</code> if <code>spot_shape</code> not computed in this experiment.</p> </li> <li> <p>shape_spot_local_yxz: Numpy integer array [n_shape_spots x 3] or None.</p> <p>\\(yxz\\) coordinates on <code>shape_tile</code>, <code>ref_round</code>/<code>ref_channel</code> of spots used to compute <code>spot_shape</code> <code>None</code> if <code>spot_shape</code> not computed in this experiment.</p> </li> <li> <p>shape_spot_gene_no: Numpy integer array [n_shape_spots] or None.</p> <p><code>shape_spot_gene_no[s]</code> is the gene that the spot at <code>shape_spot_local_yxz[s]</code> was assigned to. <code>None</code> if <code>spot_shape</code> not computed in this experiment.</p> </li> <li> <p>spot_shape_float: Numpy float array [shape_max_size[0] x shape_max_size[1] x shape_max_size[2]] or None.</p> <p>Mean of OMP coefficient sign in neighbourhood centered on spot. <code>None</code> if <code>spot_shape</code> not computed in this experiment.</p> </li> <li> <p>initial_pos_neighbour_thresh: Integer.</p> <p>Only spots with number of positive coefficient neighbours greater than this are saved to notebook. </p> <p>Typical = 4 in 2D and 40 in 3D (set to 10% of max number by default).</p> </li> <li> <p>spot_shape: Numpy integer array [shape_size_y x shape_size_y x shape_size_x].</p> <p>Expected sign of OMP coefficient in neighbourhood centered on spot. </p> <p>1 means expected positive coefficient. </p> <p>-1 means expected negative coefficient. </p> <p>0 means unsure of expected sign.</p> </li> <li> <p>local_yxz: Numpy int16 array [n_spots, 3].</p> <p><code>local_yxz[s]</code> are the \\(yxz\\) coordinates of spot \\(s\\) found on <code>tile[s]</code>, <code>ref_round</code>, <code>ref_channel</code>. To get <code>global_yxz</code>, add <code>nb.stitch.tile_origin[tile[s]]</code>.</p> </li> <li> <p>tile: Numpy int16 array [n_spots].</p> <p>Tile each spot was found on.</p> </li> <li> <p>colors: Numpy int32 array [n_spots x n_rounds x n_channels].</p> <p><code>[s, r, c]</code> is the intensity of spot \\(s\\) on round \\(r\\), channel \\(c\\). It will be <code>-tile_pixel_value_shift</code> if that round/channel not used otherwise integer.</p> </li> <li> <p>gene_no: Numpy int16 array [n_spots].</p> <p><code>gene_no[s]</code> is the index of the gene assigned to spot \\(s\\).</p> </li> <li> <p>n_neighbours_pos: Numpy int16 array [n_spots].</p> <p>Number of positive pixels around each spot in neighbourhood given by <code>spot_shape==1</code>. Max is <code>sum(spot_shape==1)</code>.</p> </li> <li> <p>n_neighbours_neg: Numpy int16 array [n_spots].</p> <p>Number of negative pixels around each spot in neighbourhood given by <code>spot_shape==-1</code>. Max is <code>sum(spot_shape==-1)</code>.</p> </li> <li> <p>intensity: Numpy float32 array [n_spots].</p> <p>\\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) <code>colors[s, r]/color_norm_factor[r]</code>.</p> </li> </ul>"},{"location":"notebook_comments/#thresholds","title":"thresholds","text":"<p>thresholds page contains quality thresholds which affect which spots plotted and which are exported to pciSeq.  Page added to Notebook when utils/pciseq/export_to_pciseq is run.</p> <ul> <li> <p>intensity: Float.</p> <p>Final accepted reference and OMP spots require <code>intensity &gt; thresholds[intensity]</code>. This is copied from <code>config[thresholds]</code> and if not given there, will be set to  <code>nb.call_spots.gene_efficiency_intensity_thresh</code>. intensity for a really intense spot is about 1 so intensity_thresh should be less than this.</p> </li> <li> <p>score_ref: Float.</p> <p>Final accepted reference spots are those which pass <code>quality_threshold</code> which is: </p> <p><code>nb.ref_spots.score &gt; thresholds[score_ref]</code> and <code>intensity &gt; thresholds[intensity]</code>. </p> <p>This is copied from <code>config[thresholds]</code>. Max score is 1 so <code>score_ref</code> should be less than this.</p> </li> <li> <p>score_omp: Float.</p> <p>Final accepted OMP spots are those which pass <code>quality_threshold</code> which is: </p> <p><code>score &gt; thresholds[score_omp]</code> and <code>intensity &gt; thresholds[intensity]</code>. </p> <p><code>score</code> is given by: </p> <p><code>score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) /  (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max)</code>. </p> <p>This is copied from <code>config[thresholds]</code>. Max score is 1 so <code>score_thresh</code> should be less than this.</p> </li> <li> <p>score_omp_multiplier: Float.</p> <p>Final accepted OMP spots are those which pass quality_threshold which is: </p> <p><code>score &gt; thresholds[score_omp]</code> and <code>intensity &gt; thresholds[intensity]</code>. </p> <p>score is given by: </p> <p><code>score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) /  (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max)</code>. </p> <p>This is copied from <code>config[thresholds]</code>.</p> </li> </ul>"},{"location":"run_code/","title":"Running the code","text":"<p>Once the configuration file has been set up with the path  /Users/user/coppafish/experiment/settings.ini, the code  can be run via the command line or using a python script:</p> Command LinePython Script <pre><code>python -m coppafish /Users/user/coppafish/experiment/settings.ini\n</code></pre> <pre><code>from coppafish import run_pipeline\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nnb = run_pipeline(ini_file)\n</code></pre> <p>If the pipeline has already been partially run and the notebook.npz file exists in the output directory,  the above will pick up the pipeline from the last stage it finished. So for a notebook that contains the pages  file_names, basic_info, extract, extract_debug and  find_spots, running the above code will start the pipeline at the stitch stage.</p>"},{"location":"run_code/#check-data-before-running","title":"Check data before running","text":"<p>The functions <code>view_raw</code>,  <code>view_filter</code> and <code>view_find_spots</code>  can be run before the Notebook is created if a valid configuration file is provided.</p> <p>So if there is a dataset of questionable quality, it may be worth running some of these first to see if it  looks ok. In particular, <code>view_raw</code> may be useful for checking the correct channels are used, or to see if specific tiles/z-planes should be  removed.</p>"},{"location":"run_code/#re-run-section","title":"Re-run section","text":"<p>If at any stage, if a section of the pipeline needs re-running, then the relevant NotebookPage must first be  removed from the Notebook, before the configuration file parameters for that  section can be altered.</p> Re-run <code>register_initial</code> <p>The code below illustrates how you can re-run the <code>register_initial</code> step of the pipeline with different configuration file parameters. </p> <p>If the last line is uncommented, the full pipeline will be run, starting with <code>register_initial</code>, and the  Notebook will be saved as <code>notebook_new.npz</code> in the output directory.</p> CodeOutputnb._config (saved to Notebook)/Users/user/coppafish/experiment/settings_new.ini <pre><code>from coppafish import Notebook, run_pipeline\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\n\n# Save new notebook with different name so it does not overwrite old notebook\n# Make sure notebook_name is specified in [file_names] section \n# of settings_new.ini file to be same as name given here.\nnb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz'\nini_file_new = '/Users/user/coppafish/experiment/settings_new.ini'\n\n# config_file not given so will use last one saved to Notebook\nnb = Notebook(nb_file)\nconfig = nb.get_config()['register_initial']\nprint('Using config file saved to notebook:')\nprint(f\"shift_max_range: {config['shift_max_range']}\")\nprint(f\"shift_score_thresh_multiplier: {config['shift_score_thresh_multiplier']}\")\n\n# Change register_initial\ndel nb.register_initial     # delete old register_initial\nnb.save(nb_file_new)        # save Notebook with no register_initial page to new file \n                            # so does not overwrite old Notebook\n# Load in new notebook with new config file\nnb_new = Notebook(nb_file_new, ini_file_new)\nconfig_new = nb_new.get_config()['register_initial']\nprint(f'Using new config file {ini_file_new}:')\nprint(f\"shift_max_range: {config_new['shift_max_range']}\")\nprint(f\"shift_score_thresh_multiplier: {config_new['shift_score_thresh_multiplier']}\")\n# nb = run_pipeline(ini_file_new)   # Uncomment this line to run pipeline starting from\n                                    # register_initial\n</code></pre> <pre><code>Using config file saved to notebook:\nshift_max_range: [500, 500, 10]\nshift_score_thresh_multiplier: 1.5\nUsing new config file /Users/user/coppafish/experiment/settings_new.ini:\nshift_max_range: [600, 600, 20]\nshift_score_thresh_multiplier: 1.2\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\nnotebook_name = notebook_new\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n\n[register_initial]\nshift_max_range = 600, 600, 20\nshift_score_thresh_multiplier = 1.2\n</code></pre> <p>If the section that needs running is <code>call_reference_spots</code>,  then the procedure is  slightly different because this step adds variables to the <code>ref_spots</code> page as well as creating the  <code>call_spots</code> page.</p> <p>The OMP section is slightly different too because it saves files to  the output directory.</p>"},{"location":"run_code/#exporting-to-pciseq","title":"Exporting to pciSeq","text":"<p>To save the results of the pipeline as a .csv file which can then be plotted with pciSeq, one  of  the following can be run (assuming path to the config file is /Users/user/coppafish/experiment/settings.ini  and the path to the notebook file is /Users/user/coppafish/experiment/notebook.npz):</p> Command LinePython Script Using Config PathPython Script Using Notebook Path <pre><code>python -m coppafish /Users/user/coppafish/experiment/settings.ini -export\n</code></pre> <pre><code>from coppafish import Notebook, export_to_pciseq\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nnb = Notebook(config_file=ini_file)\nexport_to_pciseq(nb)\n</code></pre> <pre><code>from coppafish import Notebook, export_to_pciseq\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\nexport_to_pciseq(nb)\n</code></pre> <p>This will save a csv file in the output_dir for each method  (omp and ref_spots) of finding spots, and assigning genes to them. The names of the files are specified through <code>config['file_names']['pciseq']</code>. Each file will contain:</p> <ul> <li>y - y coordinate of each spot in stitched coordinate system.</li> <li>x - x coordinate of each spot in stitched coordinate system.</li> <li>z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels).</li> <li>Gene - Name of gene each spot was assigned to.</li> </ul> <p>An example file is given here.</p>"},{"location":"run_code/#thresholding","title":"Thresholding","text":"<p>Only spots which pass  <code>quality_threshold</code> are saved. This depends on parameters given in <code>config['thresholds']</code>.</p> <p>For a reference spot, \\(s\\), to pass the thresholding, it must satisfy  the following:</p> \\[ \\displaylines{\\Delta_s &gt; \\Delta_{thresh}\\\\ \\chi_s &gt; \\chi_{thresh}} \\] <p>Where:</p> <ul> <li>\\(\\Delta_s\\) is the maximum dot product score on iteration 0   for spot \\(s\\) across all genes (i.e. \\(\\Delta_s = \\max_g(\\Delta_{s0g})\\)).</li> <li>\\(\\Delta_{thresh}\\) is <code>config['thresholds']['score_ref']</code>.</li> <li>\\(\\chi_s\\) is the intensity of spot \\(s\\).</li> <li>\\(\\chi_{thresh}\\) is <code>config['thresholds']['intensity']</code>. If this is not provided, it is set to <code>nb.call_spots.gene_efficiency_intensity_thresh</code>.</li> </ul> <p>For an OMP spot, \\(s\\), to pass the thresholding, it must satisfy the following:</p> \\[ \\displaylines{\\gamma_s &gt; \\gamma_{thresh}\\\\ \\chi_s &gt; \\chi_{thresh}} \\] <p>Where:</p> <ul> <li>\\(\\gamma_s\\) is the OMP score for spot \\(s\\).</li> <li>\\(\\gamma_{thresh}\\) is <code>config['thresholds']['score_omp']</code>.</li> <li>\\(\\chi_s\\) and \\(\\chi_{thresh}\\) are the same as for the reference spots.</li> </ul> <p>It is important that these thresholds are greater than 0, because when running the pipeline, we try to save a lot  of spots. The idea being this is that it is better to do the thresholding after the pipeline has been run, rather than  during the pipeline. This is because, if there were too few spots in the latter case, much of the pipeline would have to be re-run to obtain new spots, but in the former case, you can just change the threshold values.</p> <p>Once <code>export_to_pciseq</code> is run, the thresholds page will be added to the notebook. This inherits all the values from the thresholds section of the config file, the purpose of which is to remove the possibility of the thresholds section in the configuration file being changed once the results have been exported.</p>"},{"location":"view_results/","title":"Viewing the results","text":"<p>Once the pipeline has completed the <code>reference_spots</code> step  such that the Notebook contains the call_spots and  ref_spots pages, the gene assignments of the spots found can be visualised using <code>coppafish.Viewer</code>.</p> <p>This can be opened via the command line or using a python script. It requires either the path to the config file (/Users/user/coppafish/experiment/settings.ini) or the path to the notebook file  (/Users/user/coppafish/experiment/notebook.npz):</p> Command LinePython Script Using Config PathPython Script Using Notebook Path <pre><code>python -m coppafish /Users/user/coppafish/experiment/settings.ini -view\n</code></pre> <pre><code>from coppafish import Notebook, Viewer\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nnb = Notebook(config_file=ini_file)\nViewer(nb)\n</code></pre> <pre><code>from coppafish import Notebook, Viewer\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\nViewer(nb)\n</code></pre> <p>This will then open the napari viewer which will show the spots with a marker indicating which gene they  were assigned to. If the Notebook contains the omp page, the spots plotted will be those found with the OMP algorithm, otherwise it will show the  reference spots (those found on <code>nb.basic_info.ref_round</code>/<code>nb.basic_info.ref_channel</code>)  and their gene assignments found using <code>call_reference_spots</code>.  An example is shown below:</p> <p></p> Markers not visible <p>When the napari viewer first opens, the markers are often not visible because it is so far zoomed out. After zooming in by scrolling with the mouse, they should show up.</p> pydevd Warning <p>When opening the napari viewer, a series of warnings, starting with <pre><code>UserWarning: incompatible copy of pydevd already imported\n</code></pre> may occur e.g.  These can be removed by uninstalling debugpy: <pre><code>pip uninstall debugpy\n</code></pre></p>"},{"location":"view_results/#background-image","title":"Background Image","text":"<p>By default, the spots will be plotted on top of the stitched DAPI image (<code>config['file_names']['big_dapi_image']</code>)  if it exists, otherwise there will not be a background image.</p> <p>To use a particular background image, when calling <code>Viewer</code> a second argument needs to be given  (<code>Viewer(nb, background_image)</code>. There are several options:</p> <ul> <li><code>'dapi'</code>: Will use <code>config['file_names']['big_dapi_image']</code> if it exists, otherwise will be no background (default).</li> <li><code>'anchor'</code>: Will use <code>config['file_names']['big_anchor_image']</code> if it exists, otherwise will be no background.</li> <li>Path to .npy or .npz file: An example would be <code>'/Users/user/coppafish/experiment/background_image.npz'</code>.      This file must contain an image with axis in the order z-y-x (y-x if 2D).</li> <li>Numpy array: Can explicitly give the <code>[n_z x n_y x n_x]</code> (<code>[n_y x n_x]</code> in 2D) desired image. </li> </ul> <p>If a 2D image is provided for a 3D dataset, then this image will be used as the background image for each z-plane.</p>"},{"location":"view_results/#gene-markers","title":"Gene Markers","text":"<p>The color and marker used for each gene can be provided through a csv file e.g.</p> <pre><code>Viewer(nb, gene_marker_file='/Users/user/coppafish/experiment/gene_markers.csv')\n</code></pre> <p>This csv file must contain  6 columns with the following headers:</p> <ul> <li>GeneNames - <code>str</code>, name of gene with first letter capital.</li> <li>ColorR - <code>float</code>, Rgb color for plotting.</li> <li>ColorG - <code>float</code>, rGb color for plotting.</li> <li>ColorB - <code>float</code>, rgB color for plotting.</li> <li>napari_symbol - <code>str</code>, symbol used to plot in napari.</li> <li>mpl_symbol - <code>str</code>, equivalent of napari symbol in matplotlib.</li> </ul> <p>Only genes with names indicated in the GeneNames column will be shown in the viewer. If this file is not specified, then the default file will be used.</p>"},{"location":"view_results/#sidebar","title":"Sidebar","text":"<p>The sidebar on the left of the viewer includes various widgets which can change which spots are plotted.</p>"},{"location":"view_results/#select-genes","title":"Select Genes","text":"<ul> <li>To remove a gene from the plot, click on it in the gene legend. </li> <li>To add a gene that has been removed, click on it again.</li> <li>To view only one gene, right-click on it.</li> <li>To go back to viewing all genes, right-click on a gene which is the only gene selected.</li> </ul>"},{"location":"view_results/#image-contrast","title":"Image contrast","text":"<p>The image contrast slider controls the brightness of the background image.</p>"},{"location":"view_results/#method","title":"Method","text":"<p>If the notebook contains the omp page, a pair of buttons labelled OMP and Anchor will appear at the bottom of the  sidebar. Initially the OMP button is selected meaning the spots shown are those saved in the  omp page.  Pressing the Anchor button will change the spots shown to be those saved in the  ref_spots page.</p> <p>If the Notebook does not have the omp page, these buttons will not be present and the spots shown will be those  saved in the ref_spots page.</p>"},{"location":"view_results/#score-range","title":"Score Range","text":"<p>Only spots which pass a quality thresholding are shown in the viewer.</p> <p>Spots are assigned a <code>score</code> between 0 and 1 (can be larger for <code>ref_spots</code>) which indicates the likelihood that the  gene assignment is legitimate.  When the viewer is first opened, only spots with <code>score &gt; config['thresholds']['score_omp']</code>  (<code>score &gt; config['thresholds']['score_ref']</code> if no omp page in notebook) are shown and the lower value of the score slider is set to this.</p> <p>The slider can then be used to view only spots which satisfy:</p> <p><code>slider_low_value &lt; score &lt; slider_high_value</code></p> <p>Effect of changing Method on Score Range slider</p> <p>The <code>score</code> computed for spots using the omp method, \\(\\gamma\\), differs from that used  with the ref_spots method, \\(\\Delta\\). Thus, we keep a unique score range slider for each method so that when the method is changed using the buttons,  the score range slider values will also change to the last values used with that method.</p>"},{"location":"view_results/#intensity-threshold","title":"Intensity Threshold","text":"<p>As well as a score, each spot has an <code>intensity</code>  value.</p> <p>The quality thresholding also means  that only spots with <code>intensity &gt; intensity_thresh</code> are shown in the viewer.</p> <p>Initially, <code>intensity_thresh</code> will be set to <code>config['thresholds']['intensity']</code> and the slider can be  used to change it.</p> <p>Effect of changing Method on Intensity Threshold slider</p> <p>The <code>intensity</code> is computed in the same way for OMP  spots and reference spots. Thus the value of <code>intensity_thresh</code> will not change when the method is changed using the buttons.</p>"},{"location":"view_results/#omp-score-multiplier","title":"OMP Score Multiplier","text":"<p>This is the \\(\\rho\\) parameter used in the calculation of OMP score. It will not affect anything if the method is Anchor.</p>"},{"location":"view_results/#diagnostics","title":"Diagnostics","text":"<p>There are a few diagnostic plots which can be called with keyboard shortcuts while the viewer is open.</p>"},{"location":"view_results/#i-remove-background-image","title":"i: Remove background image","text":"<p>The background image can be removed from the viewer by pressing the i  key. Once it has been removed, it can be  put back by pressing i again.</p>"},{"location":"view_results/#b-view_bleed_matrix","title":"b: <code>view_bleed_matrix</code>","text":"<p>The bleed matrix computed for the experiment can be shown by pressing b.  This will then show the expected intensity of each dye in each channel. An example is shown below.</p> Normalised Bleed MatrixUn-normalised Bleed Matrix <p></p> <p></p> Norm Button <p>This plot as well as the <code>view_bled_codes</code>, <code>view_codes</code> and <code>view_spot</code> plots below have a Norm button. </p> <p>When these plots open the colorbar gives the intensity after  normalisation  has been applied to equalise the intensity between color channels i.e. weaker channels are boosted.</p> <p>To remove this normalisation, press the Norm button (Norm will go red).  The range of the colorbar will then change from approximately -1 to 1 to approximately -1000 to 1000.  This is then then the intensity that is read off from the filtered images saved as .npy files  in <code>config['file_names']['tile_dir']</code>.</p> <p>You can see above that in the un-normalised bleed matrix, channel 2 appears much weaker than it does in  the normalised version.</p> <p>In both the call_reference_spots and OMP sections of the pipeline, spots are assigned to genes by comparing the  <code>spot_color</code> to the <code>bled_code</code> of each gene. This is done using the normalised <code>spot_color</code> (with  background removed) and normalised <code>bled_codes</code>.</p> <p>This plot is useful to check that the dyes can be distinguished.  I.e. that each column above (in the normalised version) is relatively unique. This is required so that  genes can be distinguished based on their barcodes which indicate which dye each gene should appear with in each round.</p>"},{"location":"view_results/#g-view_bled_codes","title":"g: <code>view_bled_codes</code>","text":"<p>The <code>bled_code</code> for each gene can be shown by pressing g. This shows two plots for each gene as shown below for Plp1:</p> <p></p> <p>The bottom plot shows the predicted code for the gene based on its barcode  and the <code>bleed_matrix</code>.  For this experiment, Plp1 has the barcode 2364463 meaning the column for round 0 in the <code>bled_code</code>  is the column for dye 2 in the <code>bleed_matrix</code>, the column for round 1 above is the column for dye 3 in  the <code>bleed_matrix</code> etc. These <code>bled_codes</code> are saved as  <code>nb.call_spots.bled_codes</code> in the  <code>call_reference_spots</code> section of the pipeline.</p> <p>The top plot shows the <code>bled_code</code> incorporating the calculated  <code>gene_efficiency</code>.  The <code>gene_efficiency</code> is the expected strength of a gene in each round and is given in  brackets in the x-tick labels. These <code>bled_codes</code> are saved as  <code>nb.call_spots.bled_codes_ge</code> in the  <code>call_reference_spots</code> section of the pipeline. It is these <code>bled_codes</code> which are compared to <code>spot_colors</code> when assigning each spot to a particular gene.</p> <p>You can view other gene codes by scrolling up and down with the mouse when this plot is open.</p> <p>A green rectangle is added to each round/channel where the <code>bled_code</code> value is greater than 0.2. This indicates rounds/channels where the gene is particularly strong. It is also done in <code>view_codes</code> and <code>view_omp_fit</code>.</p>"},{"location":"view_results/#shift-g-gene_counts","title":"Shift-g: <code>gene_counts</code>","text":"<p>This plot indicates the number of reference spots assigned to each gene  which also have <code>nb.call_spots.score &gt; score_thresh</code> and <code>nb.call_spots.intensity &gt; intensity_thresh</code>.  The initial values of <code>score_thresh</code> and <code>intensity_thresh</code> used will be the current slider values.</p> <p>If the Notebook has the OMP page, then it will also show  the number of OMP spots assigned to each gene which also have \\(\\gamma_s &gt;\\) <code>omp_score_thresh</code> and  <code>nb.omp.intensity &gt; intensity_thresh</code>. The initial values of <code>omp_score_thresh</code>, <code>omp_score_multiplier</code> and  <code>intensity_thresh</code> used will be the current slider values.</p>"},{"location":"view_results/#h-histogram_score","title":"h: <code>histogram_score</code>","text":"<p>This plot shows the histogram of the score assigned to each spot  for the current method. If the current method is OMP,  the initial value of <code>omp_score_multiplier</code> will be the current slider value.</p>"},{"location":"view_results/#shift-h-histogram_2d_score","title":"Shift-h: <code>histogram_2d_score</code>","text":"<p>This plot shows the bivariate histogram to see the correlation  between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\) for spots detected with the  OMP algorithm. The initial value of <code>omp_score_multiplier</code> will be the current slider value.</p>"},{"location":"view_results/#k-view_scaled_k_means","title":"k: <code>view_scaled_k_means</code>","text":"<p>This plot shows how the <code>bleed_matrix</code> was computed.</p>"},{"location":"view_results/#space-change-to-select-mode","title":"space: Change to select mode","text":"<p>To run the diagnostics listed below, you need to change to select mode. This is done by pressing space-bar. In select mode, you won't be able to pan or zoom. To change back to pan/zoom mode, press space-bar again. On pressing space-bar, it should tell you in the bottom right corner of the viewer which mode you are in.</p> <p>When clicking on a spot in select mode, it should tell you in the bottom left corner, the spot_no of that spot and to which gene it was assigned. </p>"},{"location":"view_results/#c-view_codes","title":"c: <code>view_codes</code>","text":"<p>The <code>spot_color</code> for a particular spot can be compared to the <code>bled_code</code> (including <code>gene_efficiency</code>) of the gene it was assigned to by pressing c after clicking on the spot in select mode:</p> Background Not RemovedBackground Removed <p></p> <p></p> <p>Pressing the Background button (Background will turn red), shows what the <code>spot_color</code> looks like after the  background genes have been removed.</p> <p>The <code>spot_color</code> and <code>bled_code</code> that are used when computing the  dot product score are shown when the  Background button is red but the Norm button is white.</p> <p>The subsequent plots all show the same spot as used here.</p>"},{"location":"view_results/#s-view_spot","title":"s: <code>view_spot</code>","text":"<p>The intensity in the neighbourhood of a particular spot in each round/channel can be viewed by pressing s  after clicking on the spot in select mode:</p> <p></p> <p>For a 3D experiment, this will only show the neighbourhood on the z-plane where the spot was found. This is also the case for <code>view_omp</code>.</p> <p>The cross-hair is in green in each round/channel where the bled_code (with <code>gene_efficiency</code>)  of the predicted gene (Snca here) is greater than 0.2.</p>"},{"location":"view_results/#d-view_score","title":"d: <code>view_score</code>","text":"<p>This plot indicates how the dot product score, \\(\\Delta_s\\), was computed for a particular spot.</p>"},{"location":"view_results/#shift-i-view_intensity","title":"Shift-i: <code>view_intensity</code>","text":"<p>This plot shows how the intensity, \\(\\chi_s\\) was computed for a particular spot.</p>"},{"location":"view_results/#o-view_omp","title":"o: <code>view_omp</code>","text":"<p>The omp coefficients of all genes in neighbourhood of a particular spot can be viewed by pressing o after clicking on the spot in select mode:</p> <p></p> <p>If a gene is not plotted, it means hardly any pixels had a non-zero coefficient for that gene.</p> <p>The gene indicated by BG2 is the background code for channel 2, which is equal to 1 in all rounds of channel  2 and 0 otherwise. It is then normalised to have an L2 norm of 1. E.g. for an experiment with 7 rounds and 7 channels,  the \\(n_{rounds}\\times n_{channels}\\) code would be:</p> <pre><code>array([[0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.378, 0.   , 0.   , 0.   , 0.   ]])\n</code></pre> <p>The background codes are saved as  <code>nb.call_spots.background_codes</code> in the  <code>call_reference_spots</code> section of the pipeline. Each background vector is always fitted to each pixel by the omp algorithm, so they will always be shown.</p> OMP Diagnostics on Reference Spots <p>The functions <code>view_omp</code>, <code>view_omp_fit</code> and  <code>view_omp_score</code> can also be run for reference spots by pressing the relavent key once a spot has been selected in the Anchor method.</p> <p>The <code>view_omp_score</code> function requires the Notebook to have the  OMP page but the other two do not.</p>"},{"location":"view_results/#shift-o-view_omp_fit","title":"shift-o: <code>view_omp_fit</code>","text":"<p>The genes fitted to a particular spot at each stage of the OMP algorithm can be viewed by pressing shift-o after clicking on the spot in select mode:</p> <p></p> <p>The image shown at column \\(i\\) of the first row is the residual <code>spot_color</code> before the gene shown  at the column \\(i\\) of the second row has been fitted. </p> <p>The image shown at column \\(i+1\\) of the first row is the residual <code>spot_color</code> after the gene at column \\(i\\) of the second row has been removed.</p> <p>The image shown at column \\(i\\) of the second row is the gene that best explains the residual <code>spot_color</code>  shown at column \\(i\\) of the first row. </p> <p>The second row shows the <code>bled_code</code> (with <code>gene_efficiency</code>) for the gene multiplied by the coefficient found by the OMP algorithm for that gene at that iteration.</p> <p>The first plot of the second row shows the sum of the contribution of all background vectors. They are combined because there is no overlap between the different <code>background_codes</code>.</p> <p>The OMP algorithm stops when the absolute value of the <code>dot_product_score</code>  (DP in the title of images in the second row) to the best gene drops below the DP Threshold indicated in  the title (0.225 here, this value is <code>config['omp']['dp_thresh']</code>).  The gene shown in red is the first gene with a <code>dot_product_score</code> less than this and won't be fitted.</p> <p>Res in the title of images in the first row gives the L2 norm of the residual <code>spot_color</code> at that iteration  of the OMP algorithm. I.e. we expect this to decrease as the omp algorithm proceeds.</p> <p>If you right-click on a column,  it will run the <code>view_score</code> function to indicate how the dot product was calculated for that gene on that iteration.</p>"},{"location":"view_results/#shift-s-view_omp_score","title":"Shift-s: <code>view_omp_score</code>","text":"<p>This shows how the OMP score, \\(\\gamma_s\\), was computed for a particular spot.  The initial value of <code>omp_score_multiplier</code> will be the current slider value.</p>"},{"location":"code/sep_round_reg/","title":"Sep round reg","text":""},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.get_shift","title":"<code>get_shift(config, spot_yxz_base, spot_yxz_transform, z_scale_base, z_scale_transform, is_3d)</code>","text":"<p>Find shift from base to transform.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>register_initial section of config file corresponding to spot_yxz_base.</p> required <code>spot_yxz_base</code> <code>np.ndarray</code> <p>Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels.</p> required <code>spot_yxz_transform</code> <code>np.ndarray</code> <p>Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels.</p> required <code>z_scale_base</code> <code>float</code> <p>Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.</p> required <code>z_scale_transform</code> <code>float</code> <p>Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.</p> required <code>is_3d</code> <code>bool</code> <p>Whether pipeline is 3D or not.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>shift</code> - <code>float [shift_y, shift_x, shift_z]</code>. Best shift found.</p> <code>np.ndarray</code> <p><code>shift_score</code> - <code>float</code>. Score of best shift found.</p> <code>np.ndarray</code> <p><code>min_score</code> - <code>float</code>. Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this.</p> Source code in <code>docs/scripts/sep_round_reg.py</code> <pre><code>def get_shift(config: dict, spot_yxz_base: np.ndarray, spot_yxz_transform: np.ndarray, z_scale_base: float,\n              z_scale_transform: float, is_3d: bool) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, dict]:\n\"\"\"\n    Find shift from base to transform.\n\n    Args:\n        config: register_initial section of config file corresponding to spot_yxz_base.\n        spot_yxz_base: Point cloud want to find the shift from.\n            spot_yxz_base[:, 2] is the z coordinate in units of z-pixels.\n        spot_yxz_transform: Point cloud want to find the shift to.\n            spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels.\n        z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.\n        z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.\n        is_3d: Whether pipeline is 3D or not.\n\n    Returns:\n        `shift` - `float [shift_y, shift_x, shift_z]`.\n            Best shift found.\n        `shift_score` - `float`.\n            Score of best shift found.\n        `min_score` - `float`.\n            Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this.\n    \"\"\"\n\n    coords = ['y', 'x', 'z']\n    shifts = {}\n    for i in range(len(coords)):\n        shifts[coords[i]] = np.arange(config['shift_min'][i],\n                                      config['shift_max'][i] +\n                                      config['shift_step'][i] / 2, config['shift_step'][i]).astype(int)\n    if not is_3d:\n        config['shift_widen'][2] = 0  # so don't look for shifts in z direction\n        config['shift_max_range'][2] = 0\n        shifts['z'] = np.array([0], dtype=int)\n    shift, shift_score, shift_score_thresh, debug_info = \\\n        compute_shift(spot_yxz_base, spot_yxz_transform,\n                      config['shift_score_thresh'], config['shift_score_thresh_multiplier'],\n                      config['shift_score_thresh_min_dist'], config['shift_score_thresh_max_dist'],\n                      config['neighb_dist_thresh'], shifts['y'], shifts['x'], shifts['z'],\n                      config['shift_widen'], config['shift_max_range'], [z_scale_base, z_scale_transform],\n                      config['nz_collapse'], config['shift_step'][2])\n    return shift, np.asarray(shift_score), np.asarray(shift_score_thresh), debug_info\n</code></pre>"},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.run_sep_round_reg","title":"<code>run_sep_round_reg(config_file, config_file_full, channels_to_save, transform=None)</code>","text":"<p>This runs the pipeline for a separate round up till the end of the stitching stage and then finds the affine transform that takes it to the anchor image of the full pipeline run. It then saves the corresponding transformed images for the channels of the separate round indicated by <code>channels_to_save</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to config file for separate round. This should have only 1 round, that round being an anchor round and only one channel being used so filtering is only done on the anchor channel.</p> required <code>config_file_full</code> <code>str</code> <p>Path to config file for full pipeline run, for which full notebook exists.</p> required <code>channels_to_save</code> <code>List</code> <p>Channels of the separate round, that will be saved to the output directory in the same coordinate system as the anchor round of the full run.</p> required <code>transform</code> <code>Optional[np.ndarray]</code> <p><code>float [4 x 3]</code>. Can provide the affine transform which transforms the separate round onto the anchor image of the full pipeline run. If not provided, it will be computed.</p> <code>None</code> Source code in <code>docs/scripts/sep_round_reg.py</code> <pre><code>def run_sep_round_reg(config_file: str, config_file_full: str, channels_to_save: List,\n                      transform: Optional[np.ndarray] = None):\n\"\"\"\n    This runs the pipeline for a separate round up till the end of the stitching stage and then finds the\n    affine transform that takes it to the anchor image of the full pipeline run.\n    It then saves the corresponding transformed images for the channels of the separate round indicated by\n    `channels_to_save`.\n\n    Args:\n        config_file: Path to config file for separate round.\n            This should have only 1 round, that round being an anchor round and only one channel being used so\n            filtering is only done on the anchor channel.\n        config_file_full: Path to config file for full pipeline run, for which full notebook exists.\n        channels_to_save: Channels of the separate round, that will be saved to the output directory in\n            the same coordinate system as the anchor round of the full run.\n        transform: `float [4 x 3]`.\n            Can provide the affine transform which transforms the separate round onto the anchor\n            image of the full pipeline run. If not provided, it will be computed.\n    \"\"\"\n    # Get all information from full pipeline results - global spot positions and z scaling\n    nb_full = initialize_nb(config_file_full)\n    global_yxz_full = nb_full.ref_spots.local_yxz + nb_full.stitch.tile_origin[nb_full.ref_spots.tile]\n\n    # run pipeline to get as far as a set of global coordinates for the separate round anchor.\n    nb = initialize_nb(config_file)\n    config = nb.get_config()\n    run_extract(nb)\n    run_find_spots(nb)\n    if not nb.has_page(\"stitch\"):\n        nbp_stitch = stitch(config['stitch'], nb.basic_info, nb.find_spots.spot_details)\n        nb += nbp_stitch\n    else:\n        warnings.warn('stitch', utils.warnings.NotebookPageWarning)\n\n    # scale z coordinate so in units of xy pixels as other 2 coordinates are.\n    z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy\n    # z_scale_full = nb_full.basic_info.pixel_size_z / nb_full.basic_info.pixel_size_xy\n    # need both z_scales to be the same for final transform_image to work. Does not always seem to be the case though\n    z_scale_full = z_scale\n\n\n    # Compute centre of stitched image, as when running PCR, coordinates are centred first.\n    yx_origin = np.round(nb.stitch.tile_origin[:, :2]).astype(int)\n    z_origin = np.round(nb.stitch.tile_origin[:, 2]).astype(int).flatten()\n    yx_size = np.max(yx_origin, axis=0) + nb.basic_info.tile_sz\n    if nb.basic_info.is_3d:\n        z_size = z_origin.max() + nb.basic_info.nz\n        image_centre = np.floor(np.append(yx_size, z_size)/2).astype(int)\n    else:\n        image_centre = np.append(np.floor(yx_size/2).astype(int), 0)\n\n    if not nb.has_page('reg_to_anchor_info'):\n        nbp = setup.NotebookPage('reg_to_anchor_info')\n        if transform is not None:\n            nbp.transform = transform\n        else:\n            # remove duplicate spots\n            spot_local_yxz = nb.find_spots.spot_details[:, -3:]\n            spot_tile = nb.find_spots.spot_details[:, 0]\n            not_duplicate = get_non_duplicate(nb.stitch.tile_origin, nb.basic_info.use_tiles,\n                                              nb.basic_info.tile_centre, spot_local_yxz, spot_tile)\n            global_yxz = spot_local_yxz[not_duplicate] + nb.stitch.tile_origin[spot_tile[not_duplicate]]\n\n            # Only keep isolated points far from neighbour\n            if nb.basic_info.is_3d:\n                neighb_dist_thresh = config['register']['neighb_dist_thresh_3d']\n            else:\n                neighb_dist_thresh = config['register']['neighb_dist_thresh_2d']\n            n_iter = config['register']['n_iter']\n            isolated = get_isolated_points(global_yxz * [1, 1, z_scale], 2 * neighb_dist_thresh)\n            isolated_full = get_isolated_points(global_yxz_full * [1, 1, z_scale_full], 2 * neighb_dist_thresh)\n            global_yxz = global_yxz[isolated, :]\n            global_yxz_full = global_yxz_full[isolated_full, :]\n\n            # get initial shift from separate round to the full anchor image\n            nbp.shift, nbp.shift_score, nbp.shift_score_thresh, debug_info = \\\n                get_shift(config['register_initial'], global_yxz, global_yxz_full,\n                          z_scale, z_scale_full, nb.basic_info.is_3d)\n\n            # # Uncomment to produce plot showing best shift found\n            # view_shifts(debug_info['shifts_2d'], debug_info['scores_2d'], debug_info['shifts_3d'],\n            #             debug_info['scores_3d'], nbp.shift, debug_info['min_score_2d'],\n            #             debug_info['shift_2d_initial'],\n            #             nbp.shift_score_thresh, debug_info['shift_thresh'],\n            #             config['register_initial']['shift_score_thresh_min_dist'],\n            #             config['register_initial']['shift_score_thresh_max_dist'])\n\n            # Get affine transform from separate round to full anchor image\n            start_transform = np.eye(4, 3)  # no scaling just shift to start off icp\n            start_transform[3] = nbp.shift * [1, 1, z_scale]\n            nbp.transform, n_matches, error, is_converged = \\\n                get_single_affine_transform(global_yxz, global_yxz_full, z_scale, z_scale_full,\n                                            start_transform, neighb_dist_thresh, image_centre, n_iter)\n            nbp.n_matches = int(n_matches)\n            nbp.error = float(error)\n            nbp.is_converged = bool(is_converged)\n        nb += nbp  # save results of transform found\n    else:\n        nbp = nb.reg_to_anchor_info\n        if transform is not None:\n            if (transform != nb.reg_to_anchor_info.transform).any():\n                raise ValueError(f\"transform given is:\\n{transform}.\\nThis differs \"\n                                 f\"from nb.reg_to_anchor_info.transform:\\n{nb.reg_to_anchor_info.transform}\")\n\n    # save all the images\n    for c in channels_to_save:\n        im_file = os.path.join(nb.file_names.output_dir, f'sep_round_channel{c}_transformed.npz')\n        if c == nb.basic_info.ref_channel:\n            from_nd2 = False\n        else:\n            from_nd2 = True\n        image_stitch = utils.npy.save_stitched(None, nb.file_names, nb.basic_info, nb.stitch.tile_origin,\n                                               nb.basic_info.ref_round, c, from_nd2,\n                                               config['stitch']['save_image_zero_thresh'])\n\n        image_transform = transform_image(image_stitch, nbp.transform, image_centre[:image_stitch.ndim], z_scale)\n        if nb.basic_info.is_3d:\n            # Put z axis first for saving\n            image_transform = np.moveaxis(image_transform, -1, 0)\n        np.savez_compressed(im_file, image_transform)\n</code></pre>"},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.transform_image","title":"<code>transform_image(image, transform, image_centre, z_scale)</code>","text":"<p>This transforms <code>image</code> to a new coordinate system by applying <code>transform</code> to every pixel in <code>image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>int [n_y x n_x (x n_z)]</code>. image which is to be transformed.</p> required <code>transform</code> <code>np.ndarray</code> <p><code>float [4 x 3]</code>. Affine transform which transforms image which is applied to every pixel in image to form a new transformed image.</p> required <code>image_centre</code> <code>np.ndarray</code> <p><code>int [image.ndim]</code>. Pixel coordinates were centred by subtracting this first when computing affine transform. So when applying affine transform, pixels will also be shifted by this amount. z centre i.e. <code>image_centre[2]</code> is in units of z-pixels.</p> required <code>z_scale</code> <code>int</code> <p>Scaling to put z coordinates in same units as yx coordinates.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [n_y x n_x (x n_z)]</code>. <code>image</code> transformed according to <code>transform</code>.</p> Source code in <code>docs/scripts/sep_round_reg.py</code> <pre><code>def transform_image(image: np.ndarray, transform: np.ndarray, image_centre: np.ndarray, z_scale: int) -&gt; np.ndarray:\n\"\"\"\n    This transforms `image` to a new coordinate system by applying `transform` to every pixel in `image`.\n\n    Args:\n        image: `int [n_y x n_x (x n_z)]`.\n            image which is to be transformed.\n        transform: `float [4 x 3]`.\n            Affine transform which transforms image which is applied to every pixel in image to form a\n            new transformed image.\n        image_centre: `int [image.ndim]`.\n            Pixel coordinates were centred by subtracting this first when computing affine transform.\n            So when applying affine transform, pixels will also be shifted by this amount.\n            z centre i.e. `image_centre[2]` is in units of z-pixels.\n        z_scale: Scaling to put z coordinates in same units as yx coordinates.\n\n    Returns:\n        `int [n_y x n_x (x n_z)]`.\n            `image` transformed according to `transform`.\n\n    \"\"\"\n    im_transformed = np.zeros_like(image)\n    yxz = jnp.asarray(np.where(image != 0)).T.reshape(-1, image.ndim)\n    image_values = image[tuple([yxz[:, i] for i in range(image.ndim)])]\n    tile_size = jnp.asarray(im_transformed.shape)\n    if image.ndim == 2:\n        tile_size = jnp.append(tile_size, 1)\n        image_centre = np.append(image_centre, 0)\n        yxz = np.hstack((yxz, np.zeros((yxz.shape[0], 1))))\n\n    yxz_transform, in_range = apply_transform(yxz, jnp.asarray(transform), jnp.asarray(image_centre), z_scale,\n                                              tile_size)\n    yxz_transform = np.asarray(yxz_transform[in_range])\n    image_values = image_values[np.asarray(in_range)]\n    im_transformed[tuple([yxz_transform[:, i] for i in range(image.ndim)])] = image_values\n    return im_transformed\n</code></pre>"},{"location":"code/call_spots/background/","title":"Background","text":""},{"location":"code/call_spots/background/#coppafish.call_spots.background.fit_background","title":"<code>fit_background(spot_colors, weight_shift=0)</code>","text":"<p>This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this.</p> <p>Note</p> <p><code>background_vectors[i]</code> is 1 in channel <code>i</code> for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>weight_shift</code> <code>float</code> <p>shift to apply to weighting of each background vector to limit boost of weak spots.</p> <code>0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>residual - <code>float [n_spots x n_rounds x n_channels]</code>. <code>spot_colors</code> after background removed.</li> </ul> <code>np.ndarray</code> <ul> <li>coef - <code>float [n_spots, n_channels]</code>. coefficient value for each background vector found for each spot.</li> </ul> <code>np.ndarray</code> <ul> <li>background_vectors <code>float [n_channels x n_rounds x n_channels]</code>. background_vectors[c] is the background vector for channel c.</li> </ul> Source code in <code>coppafish/call_spots/background.py</code> <pre><code>def fit_background(spot_colors: np.ndarray, weight_shift: float = 0) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    This determines the coefficient of the background vectors for each spot.\n    Coefficients determined using a weighted dot product as to avoid overfitting\n    and accounting for the fact that background coefficients are not updated after this.\n\n    !!! note\n        `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise.\n        It is then normalised to have L2 norm of 1 when summed over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x n_rounds x n_channels]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots.\n\n    Returns:\n        - residual - `float [n_spots x n_rounds x n_channels]`.\n            `spot_colors` after background removed.\n        - coef - `float [n_spots, n_channels]`.\n            coefficient value for each background vector found for each spot.\n        - background_vectors `float [n_channels x n_rounds x n_channels]`.\n            background_vectors[c] is the background vector for channel c.\n\n    \"\"\"\n    if weight_shift &lt; 1e-20:\n        warnings.warn(f'weight_shift value given, {weight_shift} is below 1e-20.'\n                      f'Using weight_shift=1e-20 to stop blow up to infinity.')\n    weight_shift = np.clip(weight_shift, 1e-20, np.inf)  # ensure weight_shift &gt; 1e-20 to avoid blow up to infinity.\n\n    n_rounds, n_channels = spot_colors[0].shape\n    background_vectors = np.repeat(np.expand_dims(np.eye(n_channels), axis=1), n_rounds, axis=1)\n    # give background_vectors an L2 norm of 1 so can compare coefficients with other genes.\n    background_vectors = background_vectors / np.linalg.norm(background_vectors, axis=(1, 2), keepdims=True)\n\n    weight_factor = 1 / (np.abs(spot_colors) + weight_shift)\n    spot_weight = spot_colors * weight_factor\n    background_weight = np.ones((1, n_rounds, n_channels)) * background_vectors[0, 0, 0] * weight_factor\n    coef = np.sum(spot_weight * background_weight, axis=1) / np.sum(background_weight ** 2, axis=1)\n    residual = spot_colors - np.expand_dims(coef, 1) * np.ones((1, n_rounds, n_channels)) * background_vectors[0, 0, 0]\n\n    # # Old method, about 10x slower\n    # n_spots = spot_colors.shape[0]\n    # coef = np.zeros([n_spots, n_channels])\n    # background_contribution = np.zeros_like(spot_colors)\n    # background_vectors = np.zeros([n_channels, n_rounds, n_channels])\n    # for c in range(n_channels):\n    #     weight_factor = np.zeros([n_spots, n_rounds])\n    #     for r in range(n_rounds):\n    #         weight_factor[:, r] = 1 / (abs(spot_colors[:, r, c]) + weight_shift)\n    #     weight_factor = np.expand_dims(weight_factor, 2)\n    #\n    #     background_vector = np.zeros([1, n_rounds, n_channels])\n    #     background_vector[:, :, c] = 1\n    #     # give background_vector an L2 norm of 1 so can compare coefficients with other genes.\n    #     background_vector = background_vector / np.expand_dims(np.linalg.norm(background_vector, axis=(1, 2)), (1, 2))\n    #     background_vectors[c] = background_vector\n    #\n    #     background_weight = background_vector * weight_factor\n    #     spot_weight = spot_colors * weight_factor\n    #\n    #     coef[:, c] = np.sum(spot_weight * background_weight, axis=(1, 2)\n    #     ) / np.sum(background_weight ** 2, axis=(1, 2))\n    #     background_contribution[:, :, c] = np.expand_dims(coef[:, c], 1) * background_vector[0, 0, c]\n    #\n    # residual = spot_colors - background_contribution\n    return residual, coef, background_vectors\n</code></pre>"},{"location":"code/call_spots/background/#optimised","title":"Optimised","text":""},{"location":"code/call_spots/background/#coppafish.call_spots.background_optimised.fit_background","title":"<code>fit_background(spot_colors, weight_shift)</code>","text":"<p>This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this.</p> <p>Note</p> <p><code>background_vectors[i]</code> is 1 in channel <code>i</code> for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>jnp.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>weight_shift</code> <code>float</code> <p>shift to apply to weighting of each background vector to limit boost of weak spots.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [n_spots x n_rounds x n_channels]</code>. <code>spot_colors</code> after background removed.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coef - <code>float [n_spots, n_channels]</code>. coefficient value for each background vector found for each spot.</li> </ul> <code>jnp.ndarray</code> <ul> <li>background_vectors <code>float [n_channels x n_rounds x n_channels]</code>. background_vectors[c] is the background vector for channel c.</li> </ul> Source code in <code>coppafish/call_spots/background_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=1)\ndef fit_background(spot_colors: jnp.ndarray, weight_shift: float) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This determines the coefficient of the background vectors for each spot.\n    Coefficients determined using a weighted dot product as to avoid overfitting\n    and accounting for the fact that background coefficients are not updated after this.\n\n    !!! note\n        `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise.\n        It is then normalised to have L2 norm of 1 when summed over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x n_rounds x n_channels]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots.\n\n    Returns:\n        - residual - `float [n_spots x n_rounds x n_channels]`.\n            `spot_colors` after background removed.\n        - coef - `float [n_spots, n_channels]`.\n            coefficient value for each background vector found for each spot.\n        - background_vectors `float [n_channels x n_rounds x n_channels]`.\n            background_vectors[c] is the background vector for channel c.\n    \"\"\"\n    return jax.vmap(fit_background_single, in_axes=(0, None), out_axes=(0, 0, None))(spot_colors, weight_shift)\n</code></pre>"},{"location":"code/call_spots/background/#coppafish.call_spots.background_optimised.fit_background_single","title":"<code>fit_background_single(spot_color, weight_shift)</code>","text":"<p>This determines the coefficient of the background vectors. Coefficients determined using a weighted dot product as to avoid over-fitting and accounting for the fact that background coefficients are not updated after this.</p> <p>Note</p> <p><code>background_vectors[i]</code> is 1 in channel <code>i</code> for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_color</code> <code>jnp.ndarray</code> <p><code>float [n_rounds x n_channels]</code>. Spot color normalised to equalise intensities between channels (and rounds).</p> required <code>weight_shift</code> <code>float</code> <p>shift to apply to weighting of each background vector to limit boost of weak spots.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [n_rounds x n_channels]</code>. <code>spot_color</code> after background removed.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coefs - <code>float [n_channels]</code>. coefficient value for each background vector.</li> </ul> <code>jnp.ndarray</code> <ul> <li>background_vectors <code>float [n_channels x n_rounds x n_channels]</code>. background_vectors[c] is the background vector for channel c.</li> </ul> Source code in <code>coppafish/call_spots/background_optimised.py</code> <pre><code>def fit_background_single(spot_color: jnp.ndarray, weight_shift: float) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This determines the coefficient of the background vectors.\n    Coefficients determined using a weighted dot product as to avoid over-fitting\n    and accounting for the fact that background coefficients are not updated after this.\n\n    !!! note\n        `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise.\n        It is then normalised to have L2 norm of 1 when summed over all rounds and channels.\n\n    Args:\n        spot_color: `float [n_rounds x n_channels]`.\n            Spot color normalised to equalise intensities between channels (and rounds).\n        weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots.\n\n    Returns:\n        - residual - `float [n_rounds x n_channels]`.\n            `spot_color` after background removed.\n        - coefs - `float [n_channels]`.\n            coefficient value for each background vector.\n        - background_vectors `float [n_channels x n_rounds x n_channels]`.\n            background_vectors[c] is the background vector for channel c.\n    \"\"\"\n    n_rounds, n_channels = spot_color.shape\n    background_vectors = jnp.repeat(jnp.expand_dims(jnp.eye(n_channels), axis=1), n_rounds, axis=1)\n    # give background_vectors an L2 norm of 1 so can compare coefficients with other genes.\n    background_vectors = background_vectors / jnp.linalg.norm(background_vectors, axis=(1, 2), keepdims=True)\n    # array of correct shape containing the non-zero value of background_vectors everywhere.\n    background_nz_value = jnp.full((n_rounds, n_channels), background_vectors[0, 0, 0])\n\n    weight_factor = 1 / (jnp.abs(spot_color) + weight_shift)\n    spot_weight = spot_color * weight_factor\n    background_weight = background_nz_value * weight_factor\n    coefs = jnp.sum(spot_weight * background_weight, axis=0) / jnp.sum(background_weight ** 2, axis=0)\n    residual = spot_color - coefs * background_nz_value\n    return residual, coefs, background_vectors\n</code></pre>"},{"location":"code/call_spots/base/","title":"Base","text":""},{"location":"code/call_spots/base/#coppafish.call_spots.base.color_normalisation","title":"<code>color_normalisation(hist_values, hist_counts, thresh_intensities, thresh_probs, method)</code>","text":"<p>This finds the normalisations for each round, <code>r</code>, and channel, <code>c</code>, such that if <code>norm_spot_color[r,c] = spot_color[r,c] / norm_factor[r,c]</code>, the probability of <code>norm_spot_color</code> being larger than <code>thresh_intensities[i]</code> is less than <code>thresh_probs[i]</code> for every <code>i</code>. Where the probability is based on all pixels from all tiles in that round and channel.</p> <p>Parameters:</p> Name Type Description Default <code>hist_values</code> <code>np.ndarray</code> <p><code>int [n_pixel_values]</code>. All possible pixel values in saved tiff images i.e. n_pixel_values is approximately <code>np.iinfo(np.uint16).max</code> because tiffs saved as <code>uint16</code> images.</p> required <code>hist_counts</code> <code>np.ndarray</code> <p><code>int [n_pixel_values x n_rounds x n_channels]</code>. <code>hist_counts[i, r, c]</code> is the number of pixels across all tiles in round <code>r</code>, channel <code>c</code> which had the value <code>hist_values[i]</code>.</p> required <code>thresh_intensities</code> <code>Union[float, List[float], np.ndarray]</code> <p><code>float [n_thresholds]</code>. Thresholds such that the probability of having a normalised spot_color greater than this are quite low. Need to be ascending. Typical: <code>[0.5, 1, 5]</code> i.e. we want most of <code>normalised spot_colors</code> to be less than <code>0.5</code> so high normalised spot color is on the order of <code>1</code>.</p> required <code>thresh_probs</code> <code>Union[float, List[float], np.ndarray]</code> <p><code>float [n_thresholds]</code>. Probability of normalised spot color being greater than <code>thresh_intensities[i]</code> must be less than <code>thresh_probs[i]</code>. Needs to be same shape as thresh_intensities and descending. Typical: <code>[0.01, 5e-4, 1e-5]</code> i.e. want almost all non spot pixels to have normalised intensity less than <code>0.5</code>.</p> required <code>method</code> <code>str</code> <p>Must be one of the following:</p> <ul> <li><code>'single'</code> - A single normalisation factor is produced for all rounds of each channel     i.e. <code>norm_factor[r, b]</code> for a given <code>b</code> value, will be the same for all <code>r</code> values.</li> <li><code>'separate'</code> - A different normalisation factor is made for each round and channel.</li> </ul> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_rounds x n_channels]</code>. <code>norm_factor</code> such that <code>norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c]</code>.</p> Source code in <code>coppafish/call_spots/base.py</code> <pre><code>def color_normalisation(hist_values: np.ndarray, hist_counts: np.ndarray,\n                        thresh_intensities: Union[float, List[float], np.ndarray],\n                        thresh_probs: Union[float, List[float], np.ndarray], method: str) -&gt; np.ndarray:\n\"\"\"\n    This finds the normalisations for each round, ```r```, and channel, ```c```, such that if ```norm_spot_color[r,c] =\n    spot_color[r,c] / norm_factor[r,c]```, the probability of ```norm_spot_color``` being larger than ```\n    thresh_intensities[i]``` is less than ```thresh_probs[i]``` for every ```i```.\n    Where the probability is based on all pixels from all tiles in that round and channel.\n\n    Args:\n        hist_values: ```int [n_pixel_values]```.\n            All possible pixel values in saved tiff images i.e. n_pixel_values is approximately\n            ```np.iinfo(np.uint16).max``` because tiffs saved as ```uint16``` images.\n        hist_counts: ```int [n_pixel_values x n_rounds x n_channels]```.\n            ```hist_counts[i, r, c]``` is the number of pixels across all tiles in round ```r```, channel ```c```\n            which had the value ```hist_values[i]```.\n        thresh_intensities: ```float [n_thresholds]```.\n            Thresholds such that the probability of having a normalised spot_color greater than this are quite low.\n            Need to be ascending.\n            Typical: ```[0.5, 1, 5]``` i.e. we want most of ```normalised spot_colors``` to be less than ```0.5``` so\n            high normalised spot color is on the order of ```1```.\n        thresh_probs: ```float [n_thresholds]```.\n            Probability of normalised spot color being greater than ```thresh_intensities[i]``` must be less than\n            ```thresh_probs[i]```. Needs to be same shape as thresh_intensities and descending.\n            Typical: ```[0.01, 5e-4, 1e-5]``` i.e. want almost all non spot pixels to have\n            normalised intensity less than ```0.5```.\n        method: Must be one of the following:\n\n            - ```'single'``` - A single normalisation factor is produced for all rounds of each channel\n                i.e. ```norm_factor[r, b]``` for a given ```b``` value, will be the same for all ```r``` values.\n            - ```'separate'``` - A different normalisation factor is made for each round and channel.\n\n    Returns:\n        ```float [n_rounds x n_channels]```.\n            ```norm_factor``` such that ```norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c]```.\n    \"\"\"\n    if not utils.errors.check_shape(hist_values, hist_counts.shape[:1]):\n        raise utils.errors.ShapeError('hist_values', hist_values.shape, hist_counts.shape[:1])\n    # if only one value provided, turn to a list\n    if isinstance(thresh_intensities, (float, int)):\n        thresh_intensities = [thresh_intensities]\n    if isinstance(thresh_probs, (float, int)):\n        thresh_probs = [thresh_probs]\n    if not utils.errors.check_shape(np.array(thresh_intensities), np.array(thresh_probs).shape):\n        raise utils.errors.ShapeError('thresh_intensities', np.array(thresh_intensities).shape,\n                                      np.array(thresh_probs).shape)\n\n    # sort thresholds and check that thresh_probs descend as thresh_intensities increase\n    ind = np.argsort(thresh_intensities)\n    thresh_intensities = np.array(thresh_intensities)[ind]\n    thresh_probs = np.array(thresh_probs)[ind]\n    if not np.all(np.diff(thresh_probs) &lt;= 0):\n        raise ValueError(f\"thresh_probs given, {thresh_probs}, do not all descend as thresh_intensities,\"\n                         f\" {thresh_intensities}, increase.\")\n\n    n_rounds, n_channels = hist_counts.shape[1:]\n    norm_factor = np.zeros((n_rounds, n_channels))\n    for r_ind in range(n_rounds):\n        if method.lower() == 'single':\n            r = np.arange(n_rounds)\n        elif method.lower() == 'separate':\n            r = r_ind\n        else:\n            raise ValueError(f\"method given was {method} but should be either 'single' or 'separate'\")\n        for b in range(n_channels):\n            hist_counts_rb = np.sum(hist_counts[:, r, b].reshape(hist_values.shape[0], -1), axis=1)\n            # if not np.int32, get error in windows when cumsum goes negative.\n            cum_sum_rb = np.cumsum(hist_counts_rb.astype(np.int64))\n            n_pixels = cum_sum_rb[-1]\n            norm_factor_rb = -np.inf\n            for thresh_intensity, thresh_prob in zip(thresh_intensities, thresh_probs):\n                prob = np.sum(hist_counts_rb[hist_values &gt;= thresh_intensity * norm_factor_rb]) / n_pixels\n                if prob &gt; thresh_prob:\n                    norm_factor_rb = hist_values[np.where(cum_sum_rb &gt; (1 - thresh_prob) * n_pixels)[0][1]\n                                     ] / thresh_intensity\n            norm_factor[r, b] = norm_factor_rb\n        if r_ind == 0 and method.lower() == 'single':\n            break\n\n    return norm_factor\n</code></pre>"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_bled_codes","title":"<code>get_bled_codes(gene_codes, bleed_matrix)</code>","text":"<p>This gets <code>bled_codes</code> such that the spot_color of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan. Otherwise, will get confusion with dye indices in <code>gene_codes</code> being outside size of <code>bleed_matrix</code>.</p> <p>Note</p> <p>All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels with any nan values assumed to be 0.</p> <p>Parameters:</p> Name Type Description Default <code>gene_codes</code> <code>np.ndarray</code> <p><code>int [n_genes x n_rounds]</code>. <code>gene_codes[g, r]</code> indicates the dye that should be present for gene <code>g</code> in round <code>r</code>.</p> required <code>bleed_matrix</code> <code>np.ndarray</code> <p><code>float [n_rounds x n_channels x n_dyes]</code>. Expected intensity of dye <code>d</code> in round <code>r</code> is a constant multiple of <code>bleed_matrix[r, :, d]</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_genes x n_rounds x n_channels]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> Source code in <code>coppafish/call_spots/base.py</code> <pre><code>def get_bled_codes(gene_codes: np.ndarray, bleed_matrix: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    This gets ```bled_codes``` such that the spot_color of a gene ```g``` in round ```r``` is expected to be a constant\n    multiple of ```bled_codes[g, r]```.\n    This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan.\n    Otherwise, will get confusion with dye indices in `gene_codes` being outside size of `bleed_matrix`.\n\n    !!! note\n        All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels\n        with any nan values assumed to be 0.\n\n    Args:\n        gene_codes: ```int [n_genes x n_rounds]```.\n            ```gene_codes[g, r]``` indicates the dye that should be present for gene ```g``` in round ```r```.\n        bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```.\n            Expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, :, d]```.\n\n    Returns:\n        ```float [n_genes x n_rounds x n_channels]```.\n            ```bled_codes``` such that ```spot_color``` of a gene ```g```\n            in round ```r``` is expected to be a constant multiple of ```bled_codes[g, r]```.\n    \"\"\"\n    n_genes = gene_codes.shape[0]\n    n_rounds, n_channels, n_dyes = bleed_matrix.shape\n    if not utils.errors.check_shape(gene_codes, [n_genes, n_rounds]):\n        raise utils.errors.ShapeError('gene_codes', gene_codes.shape, (n_genes, n_rounds))\n    if gene_codes.max() &gt;= n_dyes:\n        ind_1, ind_2 = np.where(gene_codes == gene_codes.max())\n        raise ValueError(f\"gene_code for gene {ind_1[0]}, round {ind_2[0]} has a dye with index {gene_codes.max()}\"\n                         f\" but there are only {n_dyes} dyes.\")\n    if gene_codes.min() &lt; 0:\n        ind_1, ind_2 = np.where(gene_codes == gene_codes.min())\n        raise ValueError(f\"gene_code for gene {ind_1[0]}, round {ind_2[0]} has a dye with a negative index:\"\n                         f\" {gene_codes.min()}\")\n\n    bled_codes = np.zeros((n_genes, n_rounds, n_channels))\n    for g in range(n_genes):\n        for r in range(n_rounds):\n            for c in range(n_channels):\n                bled_codes[g, r, c] = bleed_matrix[r, c, gene_codes[g, r]]\n\n    # Give all bled codes an L2 norm of 1 assuming any nan values are 0\n    norm_factor = np.expand_dims(np.linalg.norm(np.nan_to_num(bled_codes), axis=(1, 2)), (1, 2))\n    norm_factor[norm_factor == 0] = 1   # For genes with no dye in any rounds, this avoids blow up on next line\n    bled_codes = bled_codes / norm_factor\n    return bled_codes\n</code></pre>"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_gene_efficiency","title":"<code>get_gene_efficiency(spot_colors, spot_gene_no, gene_codes, bleed_matrix, min_spots, max_gene_efficiency=np.inf, min_gene_efficiency=0, min_gene_efficiency_factor=1)</code>","text":"<p><code>gene_efficiency[g,r]</code> gives the expected intensity of gene <code>g</code> in round <code>r</code> compared to that expected by the <code>bleed_matrix</code>. It is computed based on the average of all <code>spot_colors</code> assigned to that gene.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>spot_gene_no</code> <code>np.ndarray</code> <p><code>int [n_spots]</code>. Gene each spot was assigned to.</p> required <code>gene_codes</code> <code>np.ndarray</code> <p><code>int [n_genes, n_rounds]</code>. <code>gene_codes[g, r]</code> indicates the dye that should be present for gene <code>g</code> in round <code>r</code>.</p> required <code>bleed_matrix</code> <code>np.ndarray</code> <p><code>float [n_rounds x n_channels x n_dyes]</code>. For a spot, <code>s</code> matched to gene with dye <code>d</code> in round <code>r</code>, we expect <code>spot_colors[s, r]</code>\", to be a constant multiple of <code>bleed_matrix[r, :, d]</code>\"</p> required <code>min_spots</code> <code>int</code> <p>If number of spots assigned to a gene less than or equal to this, <code>gene_efficiency[g]=1</code> for all rounds. Typical = 30.</p> required <code>max_gene_efficiency</code> <code>float</code> <p>Maximum allowed gene efficiency, i.e. any one round can be at most this times more important than the median round for every gene. Typical = 6.</p> <code>np.inf</code> <code>min_gene_efficiency</code> <code>float</code> <p>At most <code>ceil(min_gene_efficiency_factor * n_rounds)</code> rounds can have <code>gene_efficiency</code> below <code>min_gene_efficiency</code> for any given gene. Typical = 0.05</p> <code>0</code> <code>min_gene_efficiency_factor</code> <code>float</code> <p>At most <code>ceil(min_gene_efficiency_factor * n_rounds)</code> rounds can have <code>gene_efficiency</code> below <code>min_gene_efficiency</code> for any given gene. Typical = 0.2</p> <code>1</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_genes x n_rounds]</code>. <code>gene_efficiency[g,r]</code> gives the expected intensity of gene <code>g</code> in round <code>r</code> compared to that expected by the <code>bleed_matrix</code>.</p> Source code in <code>coppafish/call_spots/base.py</code> <pre><code>def get_gene_efficiency(spot_colors: np.ndarray, spot_gene_no: np.ndarray, gene_codes: np.ndarray,\n                        bleed_matrix: np.ndarray, min_spots: int,\n                        max_gene_efficiency: float = np.inf,\n                        min_gene_efficiency: float = 0,\n                        min_gene_efficiency_factor: float = 1) -&gt; np.ndarray:\n\"\"\"\n    `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected\n    by the `bleed_matrix`. It is computed based on the average of all `spot_colors` assigned to that gene.\n\n    Args:\n        spot_colors: `float [n_spots x n_rounds x n_channels]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        spot_gene_no: `int [n_spots]`.\n            Gene each spot was assigned to.\n        gene_codes: `int [n_genes, n_rounds]`.\n            `gene_codes[g, r]` indicates the dye that should be present for gene `g` in round `r`.\n        bleed_matrix: `float [n_rounds x n_channels x n_dyes]`.\n            For a spot, `s` matched to gene with dye `d` in round `r`, we expect `spot_colors[s, r]`\",\n            to be a constant multiple of `bleed_matrix[r, :, d]`\"\n        min_spots: If number of spots assigned to a gene less than or equal to this, `gene_efficiency[g]=1`\n            for all rounds. Typical = 30.\n        max_gene_efficiency: Maximum allowed gene efficiency, i.e. any one round can be at most this times more\n            important than the median round for every gene. Typical = 6.\n        min_gene_efficiency: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have\n            `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.05\n        min_gene_efficiency_factor: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have\n            `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.2\n\n    Returns:\n        `float [n_genes x n_rounds]`.\n            `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected\n            by the `bleed_matrix`.\n    \"\"\"\n    # Check n_spots, n_rounds, n_channels, n_genes consistent across all variables.\n    if not utils.errors.check_shape(spot_colors[0], bleed_matrix[:, :, 0].shape):\n        raise utils.errors.ShapeError('spot_colors', spot_colors.shape,\n                                      (spot_colors.shape[0],) + bleed_matrix[:, :, 0].shape)\n    if not utils.errors.check_shape(spot_colors[:, 0, 0], spot_gene_no.shape):\n        raise utils.errors.ShapeError('spot_colors', spot_colors.shape,\n                                      spot_gene_no.shape + bleed_matrix[:, :, 0].shape)\n    n_genes, n_rounds = gene_codes.shape\n    if not utils.errors.check_shape(spot_colors[0, :, 0].squeeze(), gene_codes[0].shape):\n        raise utils.errors.ShapeError('spot_colors', spot_colors.shape,\n                                      spot_gene_no.shape + (n_rounds,) + (bleed_matrix.shape[1],))\n\n    gene_no_oob = [val for val in spot_gene_no if val &lt; 0 or val &gt;= n_genes]\n    if len(gene_no_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"spot_gene_no\", gene_no_oob[0], 0, n_genes - 1)\n\n    gene_efficiency = np.ones([n_genes, n_rounds])\n    n_min_thresh = int(np.ceil(min_gene_efficiency_factor * n_rounds))\n    for g in range(n_genes):\n        use = spot_gene_no == g\n        if np.sum(use) &gt; min_spots:\n            round_strength = np.zeros([np.sum(use), n_rounds])\n            for r in range(n_rounds):\n                dye_ind = gene_codes[g, r]\n                # below is equivalent to MATLAB spot_colors / bleed_matrix.\n                round_strength[:, r] = np.linalg.lstsq(bleed_matrix[r, :, dye_ind:dye_ind + 1],\n                                                       spot_colors[use, r].transpose(), rcond=None)[0]\n\n            # find a reference round for each gene as that with median strength.\n            av_round_strength = np.median(round_strength, 0)\n            av_round = np.abs(av_round_strength - np.median(av_round_strength)).argmin()\n\n            # for each spot, find strength of each round relative to strength in\n            # av_round. Need relative strength not absolute strength\n            # because expect spot color to be constant multiple of bled code.\n            # So for all genes, gene_efficiency[g, av_round] = 1 but av_round is different between genes.\n\n            # Only use spots whose strength in RefRound is positive.\n            use = round_strength[:, av_round] &gt; 0\n            if np.sum(use) &gt; min_spots:\n                relative_round_strength = round_strength[use] / np.expand_dims(round_strength[use, av_round], 1)\n                # Only use spots with gene efficiency below the maximum allowed.\n                below_max = np.max(relative_round_strength, axis=1) &lt; max_gene_efficiency\n                # Only use spots with at most n_min_thresh rounds below the minimum.\n                above_min = np.sum(relative_round_strength &lt; min_gene_efficiency, axis=1) &lt;= n_min_thresh\n                use = np.array([below_max, above_min]).all(axis=0)\n                if np.sum(use) &gt; min_spots:\n                    gene_efficiency[g] = np.median(relative_round_strength[use], 0)\n\n    # set negative values to 0\n    gene_efficiency = np.clip(gene_efficiency, 0, np.inf)\n    return gene_efficiency\n</code></pre>"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_non_duplicate","title":"<code>get_non_duplicate(tile_origin, use_tiles, tile_centre, spot_local_yxz, spot_tile)</code>","text":"<p>Find duplicate spots as those detected on a tile which is not tile centre they are closest to.</p> <p>Parameters:</p> Name Type Description Default <code>tile_origin</code> <code>np.ndarray</code> <p><code>float [n_tiles x 3]</code>. <code>tile_origin[t,:]</code> is the bottom left yxz coordinate of tile <code>t</code>. yx coordinates in <code>yx_pixels</code> and z coordinate in <code>z_pixels</code>. This is saved in the <code>stitch</code> notebook page i.e. <code>nb.stitch.tile_origin</code>.</p> required <code>use_tiles</code> <code>List</code> <p><code>int [n_use_tiles]</code>. Tiles used in the experiment.</p> required <code>tile_centre</code> <code>np.ndarray</code> <p><code>float [3]</code> <code>tile_centre[:2]</code> are yx coordinates in <code>yx_pixels</code> of the centre of the tile that spots in <code>yxz</code> were found on. <code>tile_centre[2]</code> is the z coordinate in <code>z_pixels</code> of the centre of the tile. E.g. for tile of <code>yxz</code> dimensions <code>[2048, 2048, 51]</code>, <code>tile_centre = [1023.5, 1023.5, 25]</code> Each entry in <code>tile_centre</code> must be an integer multiple of <code>0.5</code>.</p> required <code>spot_local_yxz</code> <code>np.ndarray</code> <p><code>int [n_spots x 3]</code>. Coordinates of a spot s on tile spot_tile[s]. <code>yxz[s, :2]</code> are the yx coordinates in <code>yx_pixels</code> for spot <code>s</code>. <code>yxz[s, 2]</code> is the z coordinate in <code>z_pixels</code> for spot <code>s</code>.</p> required <code>spot_tile</code> <code>np.ndarray</code> <p><code>int [n_spots]</code>. Tile each spot was found on.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>bool [n_spots]</code>. Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to.</p> Source code in <code>coppafish/call_spots/base.py</code> <pre><code>def get_non_duplicate(tile_origin: np.ndarray, use_tiles: List, tile_centre: np.ndarray,\n                      spot_local_yxz: np.ndarray, spot_tile: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Find duplicate spots as those detected on a tile which is not tile centre they are closest to.\n\n    Args:\n        tile_origin: `float [n_tiles x 3]`.\n            `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`.\n            yx coordinates in `yx_pixels` and z coordinate in `z_pixels`.\n            This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`.\n        use_tiles: ```int [n_use_tiles]```.\n            Tiles used in the experiment.\n        tile_centre: ```float [3]```\n            ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in\n            ```yxz``` were found on.\n            ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile.\n            E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]```\n            Each entry in ```tile_centre``` must be an integer multiple of ```0.5```.\n        spot_local_yxz: ```int [n_spots x 3]```.\n            Coordinates of a spot s on tile spot_tile[s].\n            ```yxz[s, :2]``` are the yx coordinates in ```yx_pixels``` for spot ```s```.\n            ```yxz[s, 2]``` is the z coordinate in ```z_pixels``` for spot ```s```.\n        spot_tile: ```int [n_spots]```.\n            Tile each spot was found on.\n\n    Returns:\n        ```bool [n_spots]```.\n            Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to.\n    \"\"\"\n    tile_centres = tile_origin[use_tiles] + tile_centre\n    # Do not_duplicate search in 2D as overlap is only 2D\n    tree_tiles = KDTree(tile_centres[:, :2])\n    if np.isnan(tile_origin[np.unique(spot_tile)]).any():\n        nan_tiles = np.unique(spot_tile)[np.unique(np.where(np.isnan(tile_origin[np.unique(spot_tile)]))[0])]\n        raise ValueError(f\"tile_origin for tiles\\n{nan_tiles}\\ncontains nan values but some spot_tile \"\n                         f\"also contains these tiles. Maybe remove these from use_tiles to continue.\\n\"\n                         f\"Also, consider coppafish.plot.n_spots_grid to check if these tiles have few spots.\")\n    spot_global_yxz = spot_local_yxz + tile_origin[spot_tile]\n    all_nearest_tile_ind = tree_tiles.query(spot_global_yxz[:, :2])[1]\n    not_duplicate = np.asarray(use_tiles)[all_nearest_tile_ind.flatten()] == spot_tile\n    return not_duplicate\n</code></pre>"},{"location":"code/call_spots/bleed_matrix/","title":"Bleed Matrix","text":""},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.get_bleed_matrix","title":"<code>get_bleed_matrix(spot_colors, initial_bleed_matrix, method, score_thresh=0, min_cluster_size=10, n_iter=100, score_thresh_anneal=True, debug=-1)</code>","text":"<p>This returns a bleed matrix such that the expected intensity of dye <code>d</code> in round <code>r</code> is a constant multiple of <code>bleed_matrix[r, :, d]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Intensity found for each spot in each round and channel, normalized in some way to equalize channel intensities typically, the normalisation will be such that spot_colors vary between around <code>-5</code> to <code>10</code> with most near <code>0</code>.</p> required <code>initial_bleed_matrix</code> <code>np.ndarray</code> <p><code>float [n_rounds x n_channels x n_dyes]</code>. Initial guess for intensity we expect each dye to produce in each channel and round. Should be normalized in same way as spot_colors.</p> required <code>method</code> <code>str</code> <p>Must be one of the following:</p> <ul> <li><code>'single'</code> - A single bleed matrix is produced for all rounds.</li> <li><code>'separate'</code> - A different bleed matrix is made for each round.</li> </ul> required <code>score_thresh</code> <code>float</code> <p>Scalar between <code>0</code> and <code>1</code>. Threshold used for <code>scaled_k_means</code> affecting which spots contribute to bleed matrix estimate.</p> <code>0</code> <code>min_cluster_size</code> <code>int</code> <p>If less than this many points assigned to a dye, that dye mean vector will be set to <code>0</code>.</p> <code>10</code> <code>n_iter</code> <code>int</code> <p>Maximum number of iterations performed in <code>scaled_k_means</code>.</p> <code>100</code> <code>score_thresh_anneal</code> <code>bool</code> <p>If <code>True</code>, <code>scaled_k_means</code> will be performed twice. The second time starting with the output of the first and with <code>score_thresh</code> for cluster <code>i</code> set to the median of the scores assigned to cluster <code>i</code> in the first run. This limits the influence of bad spots to the bleed matrix.</p> <code>True</code> <code>debug</code> <code>int</code> <p>If this is &gt;=0, then the <code>debug_info</code> dictionary will also be returned. If <code>method == 'separate'</code>, this specifies the round of the <code>bleed_matrix</code> calculation to return debugging info for.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, dict]]</code> <p><code>bleed_matrix</code> - <code>float [n_rounds x n_channels x n_dyes]</code>. <code>bleed_matrix</code> such that the expected intensity of dye <code>d</code> in round <code>r</code> is a constant multiple of <code>bleed_matrix[r, _, d]</code>.</p> <code>Union[np.ndarray, Tuple[np.ndarray, dict]]</code> <p><code>debug_info</code> - dictionary containing useful information for debugging bleed matrix calculation. Each variable has size=3 in first dimension. <code>var[0]</code> refers to value with <code>initial_bleed_matrix</code>. <code>var[1]</code> refers to value after first <code>scaled_k_means</code>. <code>var[2]</code> refers to value after second k means (only if <code>score_thresh_anneal == True</code>).</p> <ul> <li><code>cluster_ind</code>: <code>int8 [3 x n_vectors]</code>. Index of dye each vector was assigned to in <code>scaled_k_means</code>.     <code>-1</code> means fell below score_thresh and not assigned.</li> <li><code>cluster_score</code>: <code>float16 [3 x n_vectors]</code>. Value of dot product between each vector and dye assigned to.</li> <li><code>bleed_matrix</code>: <code>float16 [3 x n_channels x n_dyes]</code>.     <code>bleed_matrix</code> computed at each stage of calculation.</li> </ul> Source code in <code>coppafish/call_spots/bleed_matrix.py</code> <pre><code>def get_bleed_matrix(spot_colors: np.ndarray, initial_bleed_matrix: np.ndarray, method: str, score_thresh: float = 0,\n                     min_cluster_size: int = 10, n_iter: int = 100, score_thresh_anneal: bool = True,\n                     debug: int = -1) -&gt; Union[np.ndarray, Tuple[np.ndarray, dict]]:\n\"\"\"\n    This returns a bleed matrix such that the expected intensity of dye ```d``` in round ```r```\n    is a constant multiple of ```bleed_matrix[r, :, d]```.\n\n    Args:\n        spot_colors: ```float [n_spots x n_rounds x n_channels]```.\n            Intensity found for each spot in each round and channel, normalized in some way to equalize channel\n            intensities typically, the normalisation will be such that spot_colors vary between around\n            ```-5``` to ```10``` with most near ```0```.\n        initial_bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```.\n            Initial guess for intensity we expect each dye to produce in each channel and round.\n            Should be normalized in same way as spot_colors.\n        method: Must be one of the following:\n\n            - ```'single'``` - A single bleed matrix is produced for all rounds.\n            - ```'separate'``` - A different bleed matrix is made for each round.\n        score_thresh: Scalar between ```0``` and ```1```.\n            Threshold used for ```scaled_k_means``` affecting which spots contribute to bleed matrix estimate.\n        min_cluster_size: If less than this many points assigned to a dye, that dye mean vector will be set to ```0```.\n        n_iter: Maximum number of iterations performed in ```scaled_k_means```.\n        score_thresh_anneal: If `True`, `scaled_k_means` will be performed twice.\n            The second time starting with the output of the first and with `score_thresh` for cluster `i`\n            set to the median of the scores assigned to cluster `i` in the first run.\n            This limits the influence of bad spots to the bleed matrix.\n        debug: If this is &gt;=0, then the `debug_info` dictionary will also be returned.\n            If `method == 'separate'`, this specifies the round of the `bleed_matrix` calculation to return\n            debugging info for.\n\n    Returns:\n        `bleed_matrix` - ```float [n_rounds x n_channels x n_dyes]```.\n            ```bleed_matrix``` such that the expected intensity of dye ```d``` in round ```r```\n            is a constant multiple of ```bleed_matrix[r, _, d]```.\n        `debug_info` - dictionary containing useful information for debugging bleed matrix calculation.\n            Each variable has size=3 in first dimension. `var[0]` refers to value with `initial_bleed_matrix`.\n            `var[1]` refers to value after first `scaled_k_means`.\n            `var[2]` refers to value after second k means (only if `score_thresh_anneal == True`).\n\n            - `cluster_ind`: `int8 [3 x n_vectors]`. Index of dye each vector was assigned to in `scaled_k_means`.\n                ```-1``` means fell below score_thresh and not assigned.\n            - `cluster_score`: `float16 [3 x n_vectors]`. Value of dot product between each vector and dye assigned to.\n            - `bleed_matrix`: `float16 [3 x n_channels x n_dyes]`.\n                `bleed_matrix` computed at each stage of calculation.\n    \"\"\"\n    n_rounds, n_channels = spot_colors.shape[1:]\n    n_dyes = initial_bleed_matrix.shape[2]\n    if not utils.errors.check_shape(initial_bleed_matrix, [n_rounds, n_channels, n_dyes]):\n        raise utils.errors.ShapeError('initial_bleed_matrix', initial_bleed_matrix.shape, (n_rounds, n_channels,\n                                                                                           n_dyes))\n\n    bleed_matrix = np.zeros((n_rounds, n_channels, n_dyes))  # Round, Measured, Real\n    debug_info = None\n    if method.lower() == 'separate':\n        for r in range(n_rounds):\n            spot_channel_intensity = spot_colors[:, r, :]\n            # get rid of any nan codes\n            spot_channel_intensity = spot_channel_intensity[~np.isnan(spot_channel_intensity).any(axis=1)]\n            if r == debug:\n                debug_info = {'cluster_ind': np.zeros((2+score_thresh_anneal, spot_channel_intensity.shape[0]),\n                                                      dtype=np.int8),\n                              'cluster_score': np.zeros((2+score_thresh_anneal, spot_channel_intensity.shape[0]),\n                                                        dtype=np.float16),\n                              'bleed_matrix': np.zeros((2+score_thresh_anneal, n_channels, n_dyes)),\n                              'round': r}\n            dye_codes, dye_eig_vals, cluster_ind, cluster_score, cluster_ind0, cluster_score0 = \\\n                scaled_k_means(spot_channel_intensity, initial_bleed_matrix[r].transpose(),\n                               score_thresh, min_cluster_size, n_iter)\n            if r == debug:\n                debug_info['bleed_matrix'][0] = initial_bleed_matrix[r]\n                debug_info['cluster_ind'][0] = cluster_ind0\n                debug_info['cluster_ind'][1] = cluster_ind\n                debug_info['cluster_score'][0] = cluster_score0\n                debug_info['cluster_score'][1] = cluster_score\n                for d in range(n_dyes):\n                    debug_info['bleed_matrix'][1, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n            if score_thresh_anneal:\n                # repeat with higher score_thresh so bad spots contribute less.\n                score_thresh2 = np.zeros(n_dyes)\n                for d in range(n_dyes):\n                    score_thresh2[d] = np.median(cluster_score[cluster_ind == d])\n                dye_codes, dye_eig_vals, cluster_ind, cluster_score = \\\n                    scaled_k_means(spot_channel_intensity, dye_codes, score_thresh2, min_cluster_size, n_iter)[:4]\n                if r == debug:\n                    debug_info['cluster_ind'][2] = cluster_ind\n                    debug_info['cluster_score'][2] = cluster_score\n                    for d in range(n_dyes):\n                        debug_info['bleed_matrix'][2, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n            for d in range(n_dyes):\n                bleed_matrix[r, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n    elif method.lower() == 'single':\n        initial_bleed_matrix_round_diff = initial_bleed_matrix.max(axis=0) - initial_bleed_matrix.min(axis=0)\n        if np.max(np.abs(initial_bleed_matrix_round_diff)) &gt; 1e-10:\n            raise ValueError(f\"method is {method}, but initial_bleed_matrix is different for different rounds.\")\n\n        spot_channel_intensity = spot_colors.reshape(-1, n_channels)\n        # get rid of any nan codes\n        spot_channel_intensity = spot_channel_intensity[~np.isnan(spot_channel_intensity).any(axis=1)]\n        if debug &gt;= 0:\n            debug_info = {'cluster_ind': np.zeros((2 + score_thresh_anneal, spot_channel_intensity.shape[0]),\n                                                  dtype=np.int8),\n                          'cluster_score': np.zeros((2 + score_thresh_anneal, spot_channel_intensity.shape[0]),\n                                                    dtype=np.float16),\n                          'bleed_matrix': np.zeros((2 + score_thresh_anneal, n_channels, n_dyes))}\n        dye_codes, dye_eig_vals, cluster_ind, cluster_score, cluster_ind0, cluster_score0 = \\\n            scaled_k_means(spot_channel_intensity, initial_bleed_matrix[0].transpose(),\n                           score_thresh, min_cluster_size, n_iter)\n        if debug &gt;= 0:\n            debug_info['bleed_matrix'][0] = initial_bleed_matrix[0]\n            debug_info['cluster_ind'][0] = cluster_ind0\n            debug_info['cluster_ind'][1] = cluster_ind\n            debug_info['cluster_score'][0] = cluster_score0\n            debug_info['cluster_score'][1] = cluster_score\n            for d in range(n_dyes):\n                debug_info['bleed_matrix'][1, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n        if score_thresh_anneal:\n            # repeat with higher score_thresh so bad spots contribute less.\n            score_thresh2 = np.zeros(n_dyes)\n            for d in range(n_dyes):\n                score_thresh2[d] = np.median(cluster_score[cluster_ind == d])\n            dye_codes, dye_eig_vals, cluster_ind, cluster_score = \\\n                scaled_k_means(spot_channel_intensity, dye_codes, score_thresh2, min_cluster_size, n_iter)[:4]\n            if debug &gt;= 0:\n                debug_info['cluster_ind'][2] = cluster_ind\n                debug_info['cluster_score'][2] = cluster_score\n                for d in range(n_dyes):\n                    debug_info['bleed_matrix'][2, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n        for r in range(n_rounds):\n            for d in range(n_dyes):\n                bleed_matrix[r, :, d] = dye_codes[d] * np.sqrt(dye_eig_vals[d])\n    else:\n        raise ValueError(f\"method given was {method} but should be either 'single' or 'separate'\")\n    if debug_info is not None:\n        return bleed_matrix, debug_info\n    else:\n        return bleed_matrix\n</code></pre>"},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.get_dye_channel_intensity_guess","title":"<code>get_dye_channel_intensity_guess(csv_file_name, dyes, cameras, lasers)</code>","text":"<p>This gets an estimate for the intensity of each dye in each channel (before any channel normalisation) which is then used as the starting point for the bleed matrix computation.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file_name</code> <code>str</code> <p>Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity:</p> <ul> <li>Dye is a column of names of different dyes</li> <li>Camera is a column of integers indicating the wavelength in nm of the camera.</li> <li>Laser is a column of integers indicating the wavelength in nm of the laser.</li> <li>Intensity<code>[i]</code> is the approximate intensity of Dye<code>[i]</code> in a channel with Camera<code>[i]</code> and     Laser<code>[i]</code>.</li> </ul> required <code>dyes</code> <code>Union[List[str], np.ndarray]</code> <p><code>str [n_dyes]</code>. Names of dyes used in particular experiment.</p> required <code>cameras</code> <code>Union[List[int], np.ndarray]</code> <p><code>int [n_channels]</code>. Wavelength of camera in nm used in each channel.</p> required <code>lasers</code> <code>Union[List[int], np.ndarray]</code> <p><code>int [n_channels]</code>. Wavelength of laser in nm used in each channel.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_dyes x n_channels]</code>. <code>[d, c]</code> is estimate of intensity of dye <code>d</code> in channel <code>c</code>.</p> Source code in <code>coppafish/call_spots/bleed_matrix.py</code> <pre><code>def get_dye_channel_intensity_guess(csv_file_name: str, dyes: Union[List[str], np.ndarray],\n                                    cameras: Union[List[int], np.ndarray],\n                                    lasers: Union[List[int], np.ndarray]) -&gt; np.ndarray:\n\"\"\"\n    This gets an estimate for the intensity of each dye in each channel (before any channel normalisation)\n    which is then used as the starting point for the bleed matrix computation.\n\n    Args:\n        csv_file_name: Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity:\n\n            - Dye is a column of names of different dyes\n            - Camera is a column of integers indicating the wavelength in nm of the camera.\n            - Laser is a column of integers indicating the wavelength in nm of the laser.\n            - Intensity```[i]``` is the approximate intensity of Dye```[i]``` in a channel with Camera```[i]``` and\n                Laser```[i]```.\n        dyes: ```str [n_dyes]```.\n            Names of dyes used in particular experiment.\n        cameras: ```int [n_channels]```.\n            Wavelength of camera in nm used in each channel.\n        lasers: ```int [n_channels]```.\n            Wavelength of laser in nm used in each channel.\n\n    Returns:\n        ```float [n_dyes x n_channels]```.\n            ```[d, c]``` is estimate of intensity of dye ```d``` in channel ```c```.\n    \"\"\"\n    n_dyes = len(dyes)\n    cameras = np.array(cameras)\n    lasers = np.array(lasers)\n    n_channels = cameras.shape[0]\n    if not utils.errors.check_shape(cameras, lasers.shape):\n        raise utils.errors.ShapeError('cameras', cameras.shape, lasers.shape)\n\n    # load in csv info\n    csv_dyes = np.genfromtxt(csv_file_name, delimiter=',', usecols=0, dtype=str, skip_header=1)\n    csv_cameras = np.genfromtxt(csv_file_name, delimiter=',', usecols=1, dtype=int, skip_header=1)\n    csv_lasers = np.genfromtxt(csv_file_name, delimiter=',', usecols=2, dtype=int, skip_header=1)\n    csv_intensities = np.genfromtxt(csv_file_name, delimiter=',', usecols=3, dtype=float, skip_header=1)\n\n    # read in intensity from csv info for desired dyes in each channel\n    dye_channel_intensity = np.zeros((n_dyes, n_channels))\n    for d in range(n_dyes):\n        correct_dye = csv_dyes == dyes[d].upper()\n        for c in range(n_channels):\n            correct_camera = csv_cameras == cameras[c]\n            correct_laser = csv_lasers == lasers[c]\n            correct_all = np.all((correct_dye, correct_camera, correct_laser), axis=0)\n            if sum(correct_all) != 1:\n                raise ValueError(f\"Expected intensity for dye {dyes[d]}, camera {cameras[c]} and laser {lasers[c]} \"\n                                 f\"to be found once in csv_file. Instead, it was found {sum(correct_all)} times.\")\n            dye_channel_intensity[d, c] = csv_intensities[np.where(correct_all)[0][0]]\n\n    return dye_channel_intensity\n</code></pre>"},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.scaled_k_means","title":"<code>scaled_k_means(x, initial_cluster_mean, score_thresh=0, min_cluster_size=10, n_iter=100)</code>","text":"<p>Does a clustering that minimizes the norm of <code>x[i] - g[i] * cluster_mean[cluster_ind[i]]</code> for each data point <code>i</code> in <code>x</code>, where <code>g</code> is the gain which is not explicitly computed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p><code>float [n_points x n_dims]</code>. Data set of vectors to build cluster means from.</p> required <code>initial_cluster_mean</code> <code>np.ndarray</code> <p><code>float [n_clusters x n_dims]</code>. Starting point of mean cluster vectors.</p> required <code>score_thresh</code> <code>Union[float, np.ndarray]</code> <p><code>float</code> or give different score for each cluster as <code>float [n_clusters]</code> Scalar between <code>0</code> and <code>1</code>. Points in <code>x</code> with dot product to a cluster mean vector greater than this contribute to new estimate of mean vector.</p> <code>0</code> <code>min_cluster_size</code> <code>int</code> <p>If less than this many points assigned to a cluster, that cluster mean vector will be set to <code>0</code>.</p> <code>10</code> <code>n_iter</code> <code>int</code> <p>Maximum number of iterations performed.</p> <code>100</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>norm_cluster_mean - <code>float [n_clusters x n_dims]</code>. Final normalised mean cluster vectors.</li> </ul> <code>np.ndarray</code> <ul> <li>cluster_eig_value - <code>float [n_clusters]</code>. First eigenvalue of outer product matrix for each cluster.</li> </ul> <code>np.ndarray</code> <ul> <li>cluster_ind - <code>int [n_points]</code>. Index of cluster each point was assigned to. <code>-1</code> means fell below score_thresh and not assigned.</li> </ul> <code>np.ndarray</code> <ul> <li>top_score - <code>float [n_points]</code>. <code>top_score[i]</code> is the dot product score between <code>x[i]</code> and <code>norm_cluster_mean[cluster_ind[i]]</code>.</li> </ul> <code>np.ndarray</code> <ul> <li>cluster_ind0 - <code>int [n_points]</code>. Index of cluster each point was assigned to on first iteration. <code>-1</code> means fell below score_thresh and not assigned.</li> </ul> <code>np.ndarray</code> <ul> <li>top_score0 - <code>float [n_points]</code>. <code>top_score0[i]</code> is the dot product score between <code>x[i]</code> and <code>initial_cluster_mean[cluster_ind0[i]]</code>.</li> </ul> Source code in <code>coppafish/call_spots/bleed_matrix.py</code> <pre><code>def scaled_k_means(x: np.ndarray, initial_cluster_mean: np.ndarray,\n                   score_thresh: Union[float, np.ndarray] = 0, min_cluster_size: int = 10,\n                   n_iter: int = 100) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Does a clustering that minimizes the norm of ```x[i] - g[i] * cluster_mean[cluster_ind[i]]```\n    for each data point ```i``` in ```x```, where ```g``` is the gain which is not explicitly computed.\n\n    Args:\n        x: ```float [n_points x n_dims]```.\n            Data set of vectors to build cluster means from.\n        initial_cluster_mean: ```float [n_clusters x n_dims]```.\n            Starting point of mean cluster vectors.\n        score_thresh: `float` or give different score for each cluster as `float [n_clusters]`\n            Scalar between ```0``` and ```1```.\n            Points in ```x``` with dot product to a cluster mean vector greater than this\n            contribute to new estimate of mean vector.\n        min_cluster_size: If less than this many points assigned to a cluster,\n            that cluster mean vector will be set to ```0```.\n        n_iter: Maximum number of iterations performed.\n\n    Returns:\n        - norm_cluster_mean - ```float [n_clusters x n_dims]```.\n            Final normalised mean cluster vectors.\n        - cluster_eig_value - ```float [n_clusters]```.\n            First eigenvalue of outer product matrix for each cluster.\n        - cluster_ind - ```int [n_points]```.\n            Index of cluster each point was assigned to. ```-1``` means fell below score_thresh and not assigned.\n        - top_score - ```float [n_points]```.\n            `top_score[i]` is the dot product score between `x[i]` and `norm_cluster_mean[cluster_ind[i]]`.\n        - cluster_ind0 - ```int [n_points]```.\n            Index of cluster each point was assigned to on first iteration.\n            ```-1``` means fell below score_thresh and not assigned.\n        - top_score0 - ```float [n_points]```.\n            `top_score0[i]` is the dot product score between `x[i]` and `initial_cluster_mean[cluster_ind0[i]]`.\n    \"\"\"\n    # normalise starting points and original data\n    norm_cluster_mean = initial_cluster_mean / np.linalg.norm(initial_cluster_mean, axis=1).reshape(-1, 1)\n    x_norm = x / np.linalg.norm(x, axis=1).reshape(-1, 1)\n    n_clusters = initial_cluster_mean.shape[0]\n    n_points, n_dims = x.shape\n    cluster_ind = np.ones(x.shape[0], dtype=int) * -2  # set all to -2 so won't end on first iteration\n    cluster_eig_val = np.zeros(n_clusters)\n\n    if not utils.errors.check_shape(initial_cluster_mean, [n_clusters, n_dims]):\n        raise utils.errors.ShapeError('initial_cluster_mean', initial_cluster_mean.shape, (n_clusters, n_dims))\n\n    if len(np.array([score_thresh]).flatten()) == 1:\n        # if single threshold, set the same for each cluster\n        score_thresh = np.ones(n_clusters) * score_thresh\n\n    if not utils.errors.check_shape(score_thresh, [n_clusters]):\n        raise utils.errors.ShapeError('score_thresh', score_thresh.shape, (n_clusters,))\n\n    for i in range(n_iter):\n        cluster_ind_old = cluster_ind.copy()\n\n        # project each point onto each cluster. Use normalized so we can interpret score\n        score = x_norm @ norm_cluster_mean.transpose()\n        cluster_ind = np.argmax(score, axis=1)  # find best cluster for each point\n        top_score = score[np.arange(n_points), cluster_ind]\n        top_score[np.where(np.isnan(top_score))[0]] = score_thresh.min()-1  # don't include nan values\n        cluster_ind[top_score &lt; score_thresh[cluster_ind]] = -1  # unclusterable points\n        if i == 0:\n            top_score0 = top_score.copy()\n            cluster_ind0 = cluster_ind.copy()\n\n        if (cluster_ind == cluster_ind_old).all():\n            break\n\n        for c in range(n_clusters):\n            my_points = x[cluster_ind == c]  # don't use normalized, to avoid overweighting weak points\n            n_my_points = my_points.shape[0]\n            if n_my_points &lt; min_cluster_size:\n                norm_cluster_mean[c] = 0\n                warnings.warn(f\"Cluster c only had {n_my_points} vectors assigned to it.\\n \"\n                              f\"This is less than min_cluster_size = {min_cluster_size} so setting this cluster to 0.\")\n                continue\n            eig_vals, eigs = np.linalg.eig(my_points.transpose() @ my_points / n_my_points)\n            best_eig_ind = np.argmax(eig_vals)\n            norm_cluster_mean[c] = eigs[:, best_eig_ind] * np.sign(eigs[:, best_eig_ind].mean())  # make them positive\n            cluster_eig_val[c] = eig_vals[best_eig_ind]\n\n    return norm_cluster_mean, cluster_eig_val, cluster_ind, top_score, cluster_ind0, top_score0\n</code></pre>"},{"location":"code/call_spots/dot_product/","title":"Dot Product","text":""},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product.dot_product_score","title":"<code>dot_product_score(spot_colors, bled_codes, norm_shift=0, weight_squared=None)</code>","text":"<p>Computes <code>sum(W**2(s * b) / W**2)</code> where <code>s</code> is a <code>spot_color</code>, <code>b</code> is a <code>bled_code</code> and <code>W**2</code> is weight_squared for a particular <code>spot_color</code>. Sum is over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> <code>0</code> <code>weight_squared</code> <code>Optional[np.ndarray]</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. squared weight to apply to each round/channel for each spot when computing dot product. If <code>None</code>, all rounds, channels treated equally.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_spots x n_genes]</code>. <code>score</code> such that <code>score[d, c]</code> gives dot product between <code>spot_colors</code> vector <code>d</code> with <code>bled_codes</code> vector <code>c</code>.</p> Source code in <code>coppafish/call_spots/dot_product.py</code> <pre><code>def dot_product_score(spot_colors: np.ndarray, bled_codes: np.ndarray, norm_shift: float = 0,\n                      weight_squared: Optional[np.ndarray] = None) -&gt; np.ndarray:\n\"\"\"\n    Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared\n    for a particular `spot_color`. Sum is over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x (n_rounds x n_channels)]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        weight_squared: `float [n_spots x (n_rounds x n_channels)]`.\n            squared weight to apply to each round/channel for each spot when computing dot product.\n            If `None`, all rounds, channels treated equally.\n\n    Returns:\n        `float [n_spots x n_genes]`.\n            `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d`\n            with `bled_codes` vector `c`.\n    \"\"\"\n    n_spots = spot_colors.shape[0]\n    n_genes, n_round_channels = bled_codes.shape\n    if not utils.errors.check_shape(spot_colors[0], bled_codes[0].shape):\n        raise utils.errors.ShapeError('spot_colors', spot_colors.shape,\n                                      (n_spots, n_round_channels))\n    spot_norm_factor = np.linalg.norm(spot_colors, axis=1, keepdims=True)\n    spot_norm_factor = spot_norm_factor + norm_shift\n    spot_colors = spot_colors / spot_norm_factor\n\n    gene_norm_factor = np.linalg.norm(bled_codes, axis=1, keepdims=True)\n    gene_norm_factor[gene_norm_factor == 0] = 1  # so don't blow up if bled_code is all 0 for a gene.\n    bled_codes = bled_codes / gene_norm_factor\n\n    if weight_squared is not None:\n        if not utils.errors.check_shape(weight_squared, spot_colors.shape):\n            raise utils.errors.ShapeError('weight', weight_squared.shape,\n                                          spot_colors.shape)\n        spot_colors = spot_colors * weight_squared\n\n    score = spot_colors @ bled_codes.transpose()\n\n    if weight_squared is not None:\n        score = score / np.expand_dims(np.sum(weight_squared, axis=1), 1)\n        score = score * n_round_channels  # make maximum score 1 if all weight the same and dot product perfect.\n\n    return score\n</code></pre>"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product.dot_product_score_no_weight","title":"<code>dot_product_score_no_weight(spot_colors, bled_codes, norm_shift=0)</code>","text":"<p>Computes <code>sum((s * b))</code> where <code>s</code> is a <code>spot_color</code>, <code>b</code> is a <code>bled_code</code>. Sum is over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> <code>0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_spots x n_genes]</code>. <code>score</code> such that <code>score[d, c]</code> gives dot product between <code>spot_colors</code> vector <code>d</code> with <code>bled_codes</code> vector <code>c</code>.</p> Source code in <code>coppafish/call_spots/dot_product.py</code> <pre><code>def dot_product_score_no_weight(spot_colors: np.ndarray, bled_codes: np.ndarray, norm_shift: float = 0) -&gt; np.ndarray:\n\"\"\"\n    Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`.\n    Sum is over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x (n_rounds x n_channels)]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n\n    Returns:\n        `float [n_spots x n_genes]`.\n            `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d`\n            with `bled_codes` vector `c`.\n    \"\"\"\n    n_spots = spot_colors.shape[0]\n    n_genes, n_round_channels = bled_codes.shape\n    if not utils.errors.check_shape(spot_colors[0], bled_codes[0].shape):\n        raise utils.errors.ShapeError('spot_colors', spot_colors.shape,\n                                      (n_spots, n_round_channels))\n    spot_norm_factor = np.linalg.norm(spot_colors, axis=1, keepdims=True)\n    spot_norm_factor = spot_norm_factor + norm_shift\n    spot_colors = spot_colors / spot_norm_factor\n\n    gene_norm_factor = np.linalg.norm(bled_codes, axis=1, keepdims=True)\n    gene_norm_factor[gene_norm_factor == 0] = 1  # so don't blow up if bled_code is all 0 for a gene.\n    bled_codes = bled_codes / gene_norm_factor\n\n    score = spot_colors @ bled_codes.transpose()\n    return score\n</code></pre>"},{"location":"code/call_spots/dot_product/#optimised","title":"Optimised","text":""},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product_optimised.dot_product_score","title":"<code>dot_product_score(spot_colors, bled_codes, norm_shift, weight_squared)</code>","text":"<p>Computes <code>sum(W**2(s * b) / W**2)</code> where <code>s</code> is a <code>spot_color</code>, <code>b</code> is a <code>bled_code</code> and <code>W**2</code> is weight_squared for a particular <code>spot_color</code>. Sum is over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>jnp.ndarray</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>weight_squared</code> <code>jnp.ndarray</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. squared weight to apply to each round/channel for each spot when computing dot product.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>float [n_spots x n_genes]</code>. <code>score</code> such that <code>score[d, c]</code> gives dot product between <code>spot_colors</code> vector <code>d</code> with <code>bled_codes</code> vector <code>c</code>.</p> Source code in <code>coppafish/call_spots/dot_product_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=2)\ndef dot_product_score(spot_colors: jnp.ndarray, bled_codes: jnp.ndarray, norm_shift: float,\n                      weight_squared: jnp.ndarray) -&gt; jnp.ndarray:\n\"\"\"\n    Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared\n    for a particular `spot_color`. Sum is over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x (n_rounds x n_channels)]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        weight_squared: `float [n_spots x (n_rounds x n_channels)]`.\n            squared weight to apply to each round/channel for each spot when computing dot product.\n\n    Returns:\n        `float [n_spots x n_genes]`.\n            `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d`\n            with `bled_codes` vector `c`.\n    \"\"\"\n    score = jax.vmap(dot_product_score_single, in_axes=(0, None, None, 0), out_axes=0)(spot_colors, bled_codes,\n                                                                                       norm_shift, weight_squared)\n    return score\n</code></pre>"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product_optimised.dot_product_score_no_weight","title":"<code>dot_product_score_no_weight(spot_colors, bled_codes, norm_shift)</code>","text":"<p>Computes <code>sum((s * b))</code> where <code>s</code> is a <code>spot_color</code>, <code>b</code> is a <code>bled_code</code>. Sum is over all rounds and channels.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>jnp.ndarray</code> <p><code>float [n_spots x (n_rounds x n_channels)]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>float [n_spots x n_genes]</code>. <code>score</code> such that <code>score[d, c]</code> gives dot product between <code>spot_colors</code> vector <code>d</code> with <code>bled_codes</code> vector <code>c</code>.</p> Source code in <code>coppafish/call_spots/dot_product_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=2)\ndef dot_product_score_no_weight(spot_colors: jnp.ndarray, bled_codes: jnp.ndarray, norm_shift: float) -&gt; jnp.ndarray:\n\"\"\"\n    Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`.\n    Sum is over all rounds and channels.\n\n    Args:\n        spot_colors: `float [n_spots x (n_rounds x n_channels)]`.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n\n    Returns:\n        `float [n_spots x n_genes]`.\n            `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d`\n            with `bled_codes` vector `c`.\n    \"\"\"\n    score = jax.vmap(dot_product_score_no_weight_single, in_axes=(0, None, None), out_axes=0)(spot_colors, bled_codes,\n                                                                                              norm_shift)\n    return score\n</code></pre>"},{"location":"code/call_spots/qual_check/","title":"Quality Check","text":""},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.get_intensity_thresh","title":"<code>get_intensity_thresh(nb)</code>","text":"<p>Gets threshold for intensity from parameters in <code>config file</code> or Notebook.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>call_spots</code> page.</p> required <p>Returns:</p> Type Description <code>float</code> <p>intensity threshold</p> Source code in <code>coppafish/call_spots/qual_check.py</code> <pre><code>def get_intensity_thresh(nb: Notebook) -&gt; float:\n\"\"\"\n    Gets threshold for intensity from parameters in `config file` or Notebook.\n\n    Args:\n        nb: Notebook containing at least the `call_spots` page.\n\n    Returns:\n        intensity threshold\n    \"\"\"\n    if nb.has_page('thresholds'):\n        intensity_thresh = nb.thresholds.intensity\n    else:\n        config = nb.get_config()['thresholds']\n        intensity_thresh = config['intensity']\n        if intensity_thresh is None:\n            intensity_thresh = nb.call_spots.gene_efficiency_intensity_thresh\n    return intensity_thresh\n</code></pre>"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.get_spot_intensity","title":"<code>get_spot_intensity(spot_colors)</code>","text":"<p>Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_spots]</code>. <code>[s]</code> is the intensity of spot <code>s</code>.</p> Source code in <code>coppafish/call_spots/qual_check.py</code> <pre><code>def get_spot_intensity(spot_colors: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Finds the max intensity for each imaging round across all imaging channels for each spot.\n    Then median of these max round intensities is returned.\n    Logic is that we expect spots that are genes to have at least one large intensity value in each round\n    so high spot intensity is more indicative of a gene.\n\n    Args:\n        spot_colors: ```float [n_spots x n_rounds x n_channels]```.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n\n    Returns:\n        ```float [n_spots]```.\n            ```[s]``` is the intensity of spot ```s```.\n    \"\"\"\n    check_spot = np.random.randint(spot_colors.shape[0])\n    diff_to_int = np.round(spot_colors[check_spot]).astype(int) - spot_colors[check_spot]\n    if np.abs(diff_to_int).max() == 0:\n        raise ValueError(f\"spot_intensities should be found using normalised spot_colors.\"\n                         f\"\\nBut for spot {check_spot}, spot_colors given are integers indicating they are \"\n                         f\"the raw intensities.\")\n    round_max_color = np.max(spot_colors, axis=2)\n    return np.median(round_max_color, axis=1)\n</code></pre>"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.omp_spot_score","title":"<code>omp_spot_score(nbp, score_multiplier, spot_no=None, n_neighbours_pos=None, n_neighbours_neg=None)</code>","text":"<p>Score for omp gene assignment</p> <p>Parameters:</p> Name Type Description Default <code>nbp</code> <code>NotebookPage</code> <p>OMP Notebook page</p> required <code>score_multiplier</code> <code>float</code> <p><code>score = score_multiplier * n_pos_neighb + n_neg_neighb</code>. So this influences the importance of positive coefficient neighbours vs negative.</p> required <code>spot_no</code> <code>Optional[Union[int, List, np.ndarray]]</code> <p>Which spots to get score for. If <code>None</code>, all scores will be found.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[float, np.ndarray]</code> <p>Score for each spot in spot_no if given, otherwise all spot scores.</p> Source code in <code>coppafish/call_spots/qual_check.py</code> <pre><code>def omp_spot_score(nbp: NotebookPage, score_multiplier: float,\n                   spot_no: Optional[Union[int, List, np.ndarray]] = None,\n                   n_neighbours_pos: Optional[Union[np.ndarray, int]] = None,\n                   n_neighbours_neg: Optional[Union[np.ndarray, int]] = None) -&gt; Union[float, np.ndarray]:\n\"\"\"\n    Score for omp gene assignment\n\n    Args:\n        nbp: OMP Notebook page\n        score_multiplier: `score = score_multiplier * n_pos_neighb + n_neg_neighb`.\n            So this influences the importance of positive coefficient neighbours vs negative.\n        spot_no: Which spots to get score for. If `None`, all scores will be found.\n\n    Returns:\n        Score for each spot in spot_no if given, otherwise all spot scores.\n    \"\"\"\n    max_score = score_multiplier * np.sum(nbp.spot_shape == 1) + np.sum(nbp.spot_shape == -1)\n    if n_neighbours_pos is None:\n        n_neighbours_pos = nbp.n_neighbours_pos\n    if n_neighbours_neg is None:\n        n_neighbours_neg = nbp.n_neighbours_neg\n    if spot_no is None:\n        score = (score_multiplier * n_neighbours_pos + n_neighbours_neg) / max_score\n    else:\n        score = (score_multiplier * n_neighbours_pos[spot_no] + n_neighbours_neg[spot_no]) / max_score\n    return score\n</code></pre>"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.quality_threshold","title":"<code>quality_threshold(nb, method='omp')</code>","text":"<p>Indicates which spots pass both the score and intensity quality thresholding.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>ref_spots</code> page.</p> required <code>method</code> <code>str</code> <p><code>'ref'</code> or <code>'omp'</code> indicating which spots to consider.</p> <code>'omp'</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>bool [n_spots]</code> indicating which spots pass quality thresholding.</p> Source code in <code>coppafish/call_spots/qual_check.py</code> <pre><code>def quality_threshold(nb: Notebook, method: str = 'omp') -&gt; np.ndarray:\n\"\"\"\n    Indicates which spots pass both the score and intensity quality thresholding.\n\n    Args:\n        nb: Notebook containing at least the `ref_spots` page.\n        method: `'ref'` or `'omp'` indicating which spots to consider.\n\n    Returns:\n        `bool [n_spots]` indicating which spots pass quality thresholding.\n\n    \"\"\"\n    if method.lower() != 'omp' and method.lower() != 'ref' and method.lower() != 'anchor':\n        raise ValueError(f\"method must be 'omp' or 'anchor' but {method} given.\")\n    intensity_thresh = get_intensity_thresh(nb)\n    if nb.has_page('thresholds'):\n        if method.lower() == 'omp':\n            score_thresh = nb.thresholds.score_omp\n            score_multiplier = nb.thresholds.score_omp_multiplier\n        else:\n            score_thresh = nb.thresholds.score_ref\n    else:\n        config = nb.get_config()['thresholds']\n        if method.lower() == 'omp':\n            score_thresh = config['score_omp']\n            score_multiplier = config['score_omp_multiplier']\n        else:\n            score_thresh = config['score_ref']\n    if method.lower() == 'omp':\n        intensity = nb.omp.intensity\n        score = omp_spot_score(nb.omp, score_multiplier)\n    else:\n        intensity = nb.ref_spots.intensity\n        score = nb.ref_spots.score\n    qual_ok = np.array([score &gt; score_thresh, intensity &gt; intensity_thresh]).all(axis=0)\n    return qual_ok\n</code></pre>"},{"location":"code/call_spots/qual_check/#optimised","title":"Optimised","text":""},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check_optimised.get_spot_intensity","title":"<code>get_spot_intensity(spot_colors)</code>","text":"<p>Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>jnp.ndarray</code> <p><code>float [n_spots x n_rounds x n_channels]</code>. Spot colors normalised to equalise intensities between channels (and rounds).</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>float [n_spots]</code>. <code>[s]</code> is the intensity of spot <code>s</code>.</p> Source code in <code>coppafish/call_spots/qual_check_optimised.py</code> <pre><code>@jax.jit\ndef get_spot_intensity(spot_colors: jnp.ndarray) -&gt; jnp.ndarray:\n\"\"\"\n    Finds the max intensity for each imaging round across all imaging channels for each spot.\n    Then median of these max round intensities is returned.\n    Logic is that we expect spots that are genes to have at least one large intensity value in each round\n    so high spot intensity is more indicative of a gene.\n\n    Args:\n        spot_colors: ```float [n_spots x n_rounds x n_channels]```.\n            Spot colors normalised to equalise intensities between channels (and rounds).\n\n    Returns:\n        ```float [n_spots]```.\n            ```[s]``` is the intensity of spot ```s```.\n    \"\"\"\n    return jax.vmap(lambda x: jnp.median(jnp.max(x, axis=1)), in_axes=0, out_axes=0)(spot_colors)\n</code></pre>"},{"location":"code/extract/base/","title":"Base","text":""},{"location":"code/extract/base/#coppafish.extract.base.get_extract_info","title":"<code>get_extract_info(image, auto_thresh_multiplier, hist_bin_edges, max_pixel_value, scale, z_info=None)</code>","text":"<p>Gets information from filtered scaled images useful for later in the pipeline. If 3D image, only z-plane used for <code>auto_thresh</code> and <code>hist_counts</code> calculation for speed and the that the exact value of these is not that important, just want a rough idea.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>int [n_y x n_x (x n_z)]</code> Image of tile after filtering and scaling.</p> required <code>auto_thresh_multiplier</code> <code>float</code> <p><code>auto_thresh</code> is set to <code>auto_thresh_multiplier * median(abs(image))</code> so that pixel values above this are likely spots. Typical = 10</p> required <code>hist_bin_edges</code> <code>np.ndarray</code> <p><code>float [len(nbp['hist_values']) + 1]</code> <code>hist_values</code> shifted by 0.5 to give bin edges not centres.</p> required <code>max_pixel_value</code> <code>int</code> <p>Maximum pixel value that image can contain when saving as tiff file. If no shift was applied, this would be <code>np.iinfo(np.uint16).max</code>.</p> required <code>scale</code> <code>float</code> <p>Factor by which, <code>image</code> has been multiplied in order to fill out available values in tiff file.</p> required <code>z_info</code> <code>Optional[int]</code> <p>z-plane to get <code>auto_thresh</code> and <code>hist_counts</code> from.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <ul> <li><code>auto_thresh</code> - <code>int</code> Pixel values above <code>auto_thresh</code> in <code>image</code> are likely spots.</li> </ul> <code>np.ndarray</code> <ul> <li><code>hist_counts</code> - <code>int [len(nbp['hist_values'])]</code>. <code>hist_counts[i]</code> is the number of pixels found in <code>image</code> with value equal to <code>hist_values[i]</code>.</li> </ul> <code>int</code> <ul> <li><code>n_clip_pixels</code> - <code>int</code> Number of pixels in <code>image</code> with value more than <code>max_pixel_value</code>.</li> </ul> <code>float</code> <ul> <li><code>clip_scale</code> - <code>float</code> Suggested scale factor to multiply un-scaled <code>image</code> by in order for <code>n_clip_pixels</code> to be 0.</li> </ul> Source code in <code>coppafish/extract/base.py</code> <pre><code>def get_extract_info(image: np.ndarray, auto_thresh_multiplier: float, hist_bin_edges: np.ndarray, max_pixel_value: int,\n                     scale: float, z_info: Optional[int] = None) -&gt; Tuple[float, np.ndarray, int, float]:\n\"\"\"\n    Gets information from filtered scaled images useful for later in the pipeline.\n    If 3D image, only z-plane used for `auto_thresh` and `hist_counts` calculation for speed and the that the\n    exact value of these is not that important, just want a rough idea.\n\n    Args:\n        image: ```int [n_y x n_x (x n_z)]```\n            Image of tile after filtering and scaling.\n        auto_thresh_multiplier: ```auto_thresh``` is set to ```auto_thresh_multiplier * median(abs(image))```\n            so that pixel values above this are likely spots. Typical = 10\n        hist_bin_edges: ```float [len(nbp['hist_values']) + 1]```\n            ```hist_values``` shifted by 0.5 to give bin edges not centres.\n        max_pixel_value: Maximum pixel value that image can contain when saving as tiff file.\n            If no shift was applied, this would be ```np.iinfo(np.uint16).max```.\n        scale: Factor by which, ```image``` has been multiplied in order to fill out available values in tiff file.\n        z_info: z-plane to get `auto_thresh` and `hist_counts` from.\n\n    Returns:\n        - ```auto_thresh``` - ```int``` Pixel values above ```auto_thresh``` in ```image``` are likely spots.\n        - ```hist_counts``` - ```int [len(nbp['hist_values'])]```.\n            ```hist_counts[i]``` is the number of pixels found in ```image``` with value equal to\n            ```hist_values[i]```.\n        - ```n_clip_pixels``` - ```int``` Number of pixels in ```image``` with value more than ```max_pixel_value```.\n        - ```clip_scale``` - ```float``` Suggested scale factor to multiply un-scaled ```image``` by in order for\n            ```n_clip_pixels``` to be 0.\n    \"\"\"\n    if image.ndim == 3:\n        if z_info is None:\n            raise ValueError(\"z_info not provided\")\n        auto_thresh = np.median(np.abs(image[:, :, z_info])) * auto_thresh_multiplier\n        hist_counts = np.histogram(image[:, :, z_info], hist_bin_edges)[0]\n    else:\n        auto_thresh = np.median(np.abs(image)) * auto_thresh_multiplier\n        hist_counts = np.histogram(image, hist_bin_edges)[0]\n    n_clip_pixels = np.sum(image &gt; max_pixel_value)\n    if n_clip_pixels &gt; 0:\n        # image has already been multiplied by scale hence inclusion of scale here\n        # max_pixel_value / image.max() is less than 1 so recommended scaling becomes smaller than scale.\n        clip_scale = scale * max_pixel_value / image.max()\n    else:\n        clip_scale = 0\n    return np.round(auto_thresh).astype(int), hist_counts, n_clip_pixels, clip_scale\n</code></pre>"},{"location":"code/extract/base/#coppafish.extract.base.get_pixel_length","title":"<code>get_pixel_length(length_microns, pixel_size)</code>","text":"<p>Converts a length in units of microns into a length in units of pixels</p> <p>Parameters:</p> Name Type Description Default <code>length_microns</code> <code>float</code> <p>Length in units of microns (microns)</p> required <code>pixel_size</code> <code>float</code> <p>Size of a pixel in microns (microns/pixels)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Desired length in units of pixels (pixels)</p> Source code in <code>coppafish/extract/base.py</code> <pre><code>def get_pixel_length(length_microns: float, pixel_size: float) -&gt; int:\n\"\"\"\n    Converts a length in units of microns into a length in units of pixels\n\n    Args:\n        length_microns: Length in units of microns (microns)\n        pixel_size: Size of a pixel in microns (microns/pixels)\n\n    Returns:\n        Desired length in units of pixels (pixels)\n\n    \"\"\"\n    return int(round(length_microns / pixel_size))\n</code></pre>"},{"location":"code/extract/base/#coppafish.extract.base.strip_hack","title":"<code>strip_hack(image)</code>","text":"<p>Finds all columns in image where each row is identical and then sets this column to the nearest normal column. Basically 'repeat padding'.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x (x n_z)]</code> Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>image</code> - <code>float [n_y x n_x (x n_z)]</code> Input array with change_columns set to nearest</li> </ul> <code>np.ndarray</code> <ul> <li><code>change_columns</code> - <code>int [n_changed_columns]</code> Indicates which columns have been changed.</li> </ul> Source code in <code>coppafish/extract/base.py</code> <pre><code>def strip_hack(image: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Finds all columns in image where each row is identical and then sets\n    this column to the nearest normal column. Basically 'repeat padding'.\n\n    Args:\n        image: ```float [n_y x n_x (x n_z)]```\n            Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z.\n\n    Returns:\n        - ```image``` - ```float [n_y x n_x (x n_z)]```\n            Input array with change_columns set to nearest\n        - ```change_columns``` - ```int [n_changed_columns]```\n            Indicates which columns have been changed.\n    \"\"\"\n    # all rows identical if standard deviation is 0\n    if np.ndim(image) == 3:\n        # assume each z-plane of 3d image has same bad columns\n        # seems to always be the case for our data\n        change_columns = np.where(np.std(image[:, :, 0], 0) == 0)[0]\n    else:\n        change_columns = np.where(np.std(image, 0) == 0)[0]\n    good_columns = np.setdiff1d(np.arange(np.shape(image)[1]), change_columns)\n    for col in change_columns:\n        nearest_good_col = good_columns[np.argmin(np.abs(good_columns - col))]\n        image[:, col] = image[:, nearest_good_col]\n    return image, change_columns\n</code></pre>"},{"location":"code/extract/base/#coppafish.extract.base.wait_for_data","title":"<code>wait_for_data(data_path, wait_time, dir=False)</code>","text":"<p>Waits for wait_time seconds to see if file/directory at data_path becomes available in that time.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to file or directory of interest</p> required <code>wait_time</code> <code>int</code> <p>Time to wait in seconds for file to become available.</p> required <code>dir</code> <code>bool</code> <p>If True, assumes data_path points to a directory, otherwise assumes points to a file.</p> <code>False</code> Source code in <code>coppafish/extract/base.py</code> <pre><code>def wait_for_data(data_path: str, wait_time: int, dir: bool = False):\n\"\"\"\n    Waits for wait_time seconds to see if file/directory at data_path becomes available in that time.\n\n    Args:\n        data_path: Path to file or directory of interest\n        wait_time: Time to wait in seconds for file to become available.\n        dir: If True, assumes data_path points to a directory, otherwise assumes points to a file.\n    \"\"\"\n    if dir:\n        check_data_func = lambda x: os.path.isdir(x)\n    else:\n        check_data_func = lambda x: os.path.isfile(x)\n    if not check_data_func(data_path):\n        # wait for file to become available\n        if wait_time &gt; 60 ** 2:\n            wait_time_print = round(wait_time / 60 ** 2, 1)\n            wait_time_unit = 'hours'\n        else:\n            wait_time_print = round(wait_time, 1)\n            wait_time_unit = 'seconds'\n        warnings.warn(f'\\nNo file named\\n{data_path}\\nexists. Waiting for {wait_time_print} {wait_time_unit}...')\n        with tqdm(total=wait_time, position=0) as pbar:\n            pbar.set_description(f\"Waiting for {data_path}\")\n            for i in range(wait_time):\n                time.sleep(1)\n                if check_data_func(data_path):\n                    break\n                pbar.update(1)\n        pbar.close()\n        if not check_data_func(data_path):\n            raise utils.errors.NoFileError(data_path)\n        print(\"file found!\\nWaiting for file to fully load...\")\n        # wait for file to stop loading\n        old_bytes = 0\n        new_bytes = 0.00001\n        while new_bytes &gt; old_bytes:\n            time.sleep(5)\n            old_bytes = new_bytes\n            new_bytes = os.path.getsize(data_path)\n        print(\"file loaded!\")\n</code></pre>"},{"location":"code/extract/deconvolution/","title":"Deconvolution","text":""},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_psf","title":"<code>get_psf(spot_images, annulus_width)</code>","text":"<p>This gets psf, which is average image of spot from individual images of spots. It is normalised so min value is 0 and max value is 1.</p> <p>Parameters:</p> Name Type Description Default <code>spot_images</code> <code>np.ndarray</code> <p><code>int [n_spots x y_diameter x x_diameter x z_diameter]</code>. <code>spot_images[s]</code> is the small image surrounding spot <code>s</code>.</p> required <code>annulus_width</code> <code>float</code> <p>Within each z-plane, this specifies how big an annulus to use, within which we expect all pixel values to be the same.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [y_diameter x x_diameter x z_diameter]</code>. Average small image about a spot. Normalised so min is 0 and max is 1.</p> Source code in <code>coppafish/extract/deconvolution.py</code> <pre><code>def get_psf(spot_images: np.ndarray, annulus_width: float) -&gt; np.ndarray:\n\"\"\"\n    This gets psf, which is average image of spot from individual images of spots.\n    It is normalised so min value is 0 and max value is 1.\n\n    Args:\n        spot_images: ```int [n_spots x y_diameter x x_diameter x z_diameter]```.\n            ```spot_images[s]``` is the small image surrounding spot ```s```.\n        annulus_width: Within each z-plane, this specifies how big an annulus to use,\n            within which we expect all pixel values to be the same.\n\n    Returns:\n        ```float [y_diameter x x_diameter x z_diameter]```.\n            Average small image about a spot. Normalised so min is 0 and max is 1.\n    \"\"\"\n    # normalise each z plane of each spot image first so each has median of 0 and max of 1.\n    # Found that this works well as taper psf anyway, which gives reduced intensity as move away from centre.\n    spot_images = spot_images - np.expand_dims(np.nanmedian(spot_images, axis=[1, 2]), [1, 2])\n    spot_images = spot_images / np.expand_dims(np.nanmax(spot_images, axis=(1, 2)), [1, 2])\n    psf = get_average_spot_image(spot_images, 'median', 'annulus_2d', annulus_width)\n    # normalise psf so min is 0 and max is 1.\n    psf = psf - psf.min()\n    psf = psf / psf.max()\n    return psf\n</code></pre>"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_psf_spots","title":"<code>get_psf_spots(nbp_file, nbp_basic, round, use_tiles, channel, use_z, radius_xy, radius_z, min_spots, intensity_thresh, intensity_auto_param, isolation_dist, shape)</code>","text":"<p>Finds spot_shapes about spots found in raw data, average of these then used for psf.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>round</code> <code>int</code> <p>Reference round to get spots from to determine psf. This should be the anchor round (last round) if using.</p> required <code>use_tiles</code> <code>List[int]</code> <p><code>int [n_use_tiles]</code>. tiff tile indices used in experiment.</p> required <code>channel</code> <code>int</code> <p>Reference channel to get spots from to determine psf.</p> required <code>use_z</code> <code>List[int]</code> <p><code>int [n_z]</code>. Z-planes used in the experiment.</p> required <code>radius_xy</code> <code>int</code> <p>Radius of dilation structuring element in xy plane (approximately spot radius).</p> required <code>radius_z</code> <code>int</code> <p>Radius of dilation structuring element in z direction (approximately spot radius)</p> required <code>min_spots</code> <code>int</code> <p>Minimum number of spots required to determine average shape from. Typical: 300</p> required <code>intensity_thresh</code> <code>Optional[float]</code> <p>Spots are local maxima in image with <code>pixel value &gt; intensity_thresh</code>. if <code>intensity_thresh = None</code>, will automatically compute it from mid z-plane of first tile.</p> required <code>intensity_auto_param</code> <code>float</code> <p>If <code>intensity_thresh = None</code> so is automatically computed, it is done using this.</p> required <code>isolation_dist</code> <code>float</code> <p>Spots are isolated if nearest neighbour is further away than this.</p> required <code>shape</code> <code>List[int]</code> <p><code>int [y_diameter, x_diameter, z_diameter]</code>. Desired size of image about each spot.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>spot_images</code> - <code>int [n_spots x y_diameter x x_diameter x z_diameter]</code>. <code>spot_images[s]</code> is the small image surrounding spot <code>s</code>.</li> </ul> <code>float</code> <ul> <li><code>intensity_thresh</code> - <code>float</code>. Only different from input if input was <code>None</code>.</li> </ul> <code>List[int]</code> <ul> <li><code>tiles_used</code> - <code>int [n_tiles_used]</code>. Tiles the spots were found on.</li> </ul> Source code in <code>coppafish/extract/deconvolution.py</code> <pre><code>def get_psf_spots(nbp_file: NotebookPage, nbp_basic: NotebookPage, round: int,\n                  use_tiles: List[int], channel: int, use_z: List[int], radius_xy: int, radius_z: int, min_spots: int,\n                  intensity_thresh: Optional[float], intensity_auto_param: float, isolation_dist: float,\n                  shape: List[int]) -&gt; Tuple[np.ndarray, float, List[int]]:\n\"\"\"\n    Finds spot_shapes about spots found in raw data, average of these then used for psf.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        round: Reference round to get spots from to determine psf.\n            This should be the anchor round (last round) if using.\n        use_tiles: ```int [n_use_tiles]```.\n            tiff tile indices used in experiment.\n        channel: Reference channel to get spots from to determine psf.\n        use_z: ```int [n_z]```. Z-planes used in the experiment.\n        radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius).\n        radius_z: Radius of dilation structuring element in z direction (approximately spot radius)\n        min_spots: Minimum number of spots required to determine average shape from. Typical: 300\n        intensity_thresh: Spots are local maxima in image with ```pixel value &gt; intensity_thresh```.\n            if ```intensity_thresh = None```, will automatically compute it from mid z-plane of first tile.\n        intensity_auto_param: If ```intensity_thresh = None``` so is automatically computed, it is done using this.\n        isolation_dist: Spots are isolated if nearest neighbour is further away than this.\n        shape: ```int [y_diameter, x_diameter, z_diameter]```. Desired size of image about each spot.\n\n    Returns:\n        - ```spot_images``` - ```int [n_spots x y_diameter x x_diameter x z_diameter]```.\n            ```spot_images[s]``` is the small image surrounding spot ```s```.\n        - ```intensity_thresh``` - ```float```. Only different from input if input was ```None```.\n        - ```tiles_used``` - ```int [n_tiles_used]```. Tiles the spots were found on.\n    \"\"\"\n    n_spots = 0\n    spot_images = np.zeros((0, shape[0], shape[1], shape[2]), dtype=int)\n    tiles_used = []\n    while n_spots &lt; min_spots:\n        t = scale.central_tile(nbp_basic.tilepos_yx, use_tiles)  # choose tile closet to centre\n        im = utils.raw.load(nbp_file, nbp_basic, None, round, channel, use_z)\n        mid_z = np.ceil(im.shape[2] / 2).astype(int)\n        median_im = np.median(im[:, :, mid_z])\n        if intensity_thresh is None:\n            intensity_thresh = median_im + np.median(np.abs(im[:, :, mid_z] - median_im)) * intensity_auto_param\n        elif intensity_thresh &lt;= median_im or intensity_thresh &gt;= np.iinfo(np.uint16).max:\n            raise utils.errors.OutOfBoundsError(\"intensity_thresh\", intensity_thresh, median_im,\n                                                np.iinfo(np.uint16).max)\n        spot_yxz, _ = detect_spots(im, intensity_thresh, radius_xy, radius_z, True)\n        # check fall off in intensity not too large\n        not_single_pixel = check_neighbour_intensity(im, spot_yxz, median_im)\n        isolated = get_isolated_points(spot_yxz, isolation_dist)\n        spot_yxz = spot_yxz[np.logical_and(isolated, not_single_pixel), :]\n        if n_spots == 0 and np.shape(spot_yxz)[0] &lt; min_spots / 4:\n            # raise error on first tile if looks like we are going to use more than 4 tiles\n            raise ValueError(f\"\\nFirst tile, {t}, only found {np.shape(spot_yxz)[0]} spots.\"\n                             f\"\\nMaybe consider lowering intensity_thresh from current value of {intensity_thresh}.\")\n        spot_images = np.append(spot_images, get_spot_images(im, spot_yxz, shape), axis=0)\n        n_spots = np.shape(spot_images)[0]\n        use_tiles = np.setdiff1d(use_tiles, t)\n        tiles_used.append(t)\n        if len(use_tiles) == 0 and n_spots &lt; min_spots:\n            raise ValueError(f\"\\nRequired min_spots = {min_spots}, but only found {n_spots}.\\n\"\n                             f\"Maybe consider lowering intensity_thresh from current value of {intensity_thresh}.\")\n    return spot_images, intensity_thresh.astype(float), tiles_used\n</code></pre>"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_wiener_filter","title":"<code>get_wiener_filter(psf, image_shape, constant)</code>","text":"<p>This tapers the psf so goes to 0 at edges and then computes wiener filter from it.</p> <p>Parameters:</p> Name Type Description Default <code>psf</code> <code>np.ndarray</code> <p><code>float [y_diameter x x_diameter x z_diameter]</code>. Average small image about a spot. Normalised so min is 0 and max is 1.</p> required <code>image_shape</code> <code>Union[np.ndarray, List[int]]</code> <p><code>int [n_im_y, n_im_x, n_im_z]</code>. Indicates the shape of the image to be convolved after padding.</p> required <code>constant</code> <code>float</code> <p>Constant used in wiener filter.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>complex128 [n_im_y x n_im_x x n_im_z]</code>. Wiener filter of same size as image.</p> Source code in <code>coppafish/extract/deconvolution.py</code> <pre><code>def get_wiener_filter(psf: np.ndarray, image_shape: Union[np.ndarray, List[int]], constant: float) -&gt; np.ndarray:\n\"\"\"\n    This tapers the psf so goes to 0 at edges and then computes wiener filter from it.\n\n    Args:\n        psf: ```float [y_diameter x x_diameter x z_diameter]```.\n            Average small image about a spot. Normalised so min is 0 and max is 1.\n        image_shape: ```int [n_im_y, n_im_x, n_im_z]```.\n            Indicates the shape of the image to be convolved after padding.\n        constant: Constant used in wiener filter.\n\n    Returns:\n        ```complex128 [n_im_y x n_im_x x n_im_z]```. Wiener filter of same size as image.\n    \"\"\"\n    # taper psf so smoothly goes to 0 at each edge.\n    psf = psf * np.hanning(psf.shape[0]).reshape(-1, 1, 1) * np.hanning(psf.shape[1]).reshape(1, -1, 1) * \\\n          np.hanning(psf.shape[2]).reshape(1, 1, -1)\n    psf = psf_pad(psf, image_shape)\n    psf_ft = np.fft.fftn(np.fft.ifftshift(psf))\n    return np.conj(psf_ft) / np.real((psf_ft * np.conj(psf_ft) + constant))\n</code></pre>"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.psf_pad","title":"<code>psf_pad(psf, image_shape)</code>","text":"<p>Pads psf with zeros so has same dimensions as image</p> <p>Parameters:</p> Name Type Description Default <code>psf</code> <code>np.ndarray</code> <p><code>float [y_shape x x_shape (x z_shape)]</code>. Point Spread Function with same shape as small image about each spot.</p> required <code>image_shape</code> <code>Union[np.ndarray, List[int]]</code> <p><code>int [psf.ndim]</code>. Number of pixels in <code>[y, x, (z)]</code> direction of padded image.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [image_shape[0] x image_shape[1] (x image_shape[2])]</code>.</p> <code>np.ndarray</code> <p>Array same size as image with psf centered on middle pixel.</p> Source code in <code>coppafish/extract/deconvolution.py</code> <pre><code>def psf_pad(psf: np.ndarray, image_shape: Union[np.ndarray, List[int]]) -&gt; np.ndarray:\n\"\"\"\n    Pads psf with zeros so has same dimensions as image\n\n    Args:\n        psf: ```float [y_shape x x_shape (x z_shape)]```.\n            Point Spread Function with same shape as small image about each spot.\n        image_shape: ```int [psf.ndim]```.\n            Number of pixels in ```[y, x, (z)]``` direction of padded image.\n\n    Returns:\n        ```float [image_shape[0] x image_shape[1] (x image_shape[2])]```.\n        Array same size as image with psf centered on middle pixel.\n    \"\"\"\n    # must pad with ceil first so that ifftshift puts central pixel to (0,0,0).\n    pre_pad = np.ceil((np.array(image_shape) - np.array(psf.shape)) / 2).astype(int)\n    post_pad = np.floor((np.array(image_shape) - np.array(psf.shape)) / 2).astype(int)\n    return np.pad(psf, [(pre_pad[i], post_pad[i]) for i in range(len(pre_pad))])\n</code></pre>"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.wiener_deconvolve","title":"<code>wiener_deconvolve(image, im_pad_shape, filter)</code>","text":"<p>This pads <code>image</code> so goes to median value of <code>image</code> at each edge. Then deconvolves using wiener filter.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>int [n_im_y x n_im_x x n_im_z]</code>. Image to be deconvolved.</p> required <code>im_pad_shape</code> <code>List[int]</code> <p><code>int [n_pad_y, n_pad_x, n_pad_z]</code>. How much to pad image in <code>[y, x, z]</code> directions.</p> required <code>filter</code> <code>np.ndarray</code> <p><code>complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z]</code>. Wiener filter to use.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [n_im_y x n_im_x x n_im_z]</code>. Deconvolved image.</p> Source code in <code>coppafish/extract/deconvolution.py</code> <pre><code>def wiener_deconvolve(image: np.ndarray, im_pad_shape: List[int], filter: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    This pads ```image``` so goes to median value of ```image``` at each edge. Then deconvolves using wiener filter.\n\n    Args:\n        image: ```int [n_im_y x n_im_x x n_im_z]```.\n            Image to be deconvolved.\n        im_pad_shape: ```int [n_pad_y, n_pad_x, n_pad_z]```.\n            How much to pad image in ```[y, x, z]``` directions.\n        filter: ```complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z]```.\n            Wiener filter to use.\n\n    Returns:\n        ```int [n_im_y x n_im_x x n_im_z]```. Deconvolved image.\n    \"\"\"\n    im_max = image.max()\n    im_min = image.min()\n    im_av = np.median(image[:, :, 0])\n    image = np.pad(image, [(im_pad_shape[i], im_pad_shape[i]) for i in range(len(im_pad_shape))], 'linear_ramp',\n                   end_values=[(im_av, im_av)] * 3)\n    im_deconvolved = np.real(np.fft.ifftn(np.fft.fftn(image) * filter))\n    im_deconvolved = im_deconvolved[im_pad_shape[0]:-im_pad_shape[0], im_pad_shape[1]:-im_pad_shape[1],\n                     im_pad_shape[2]:-im_pad_shape[2]]\n    # set min and max so it covers same range as input image\n    im_deconvolved = im_deconvolved - im_deconvolved.min()\n    return np.round(im_deconvolved * (im_max - im_min) / im_deconvolved.max() + im_min).astype(int)\n</code></pre>"},{"location":"code/extract/fstack/","title":"Fstack","text":""},{"location":"code/extract/fstack/#coppafish.extract.fstack.focus_stack","title":"<code>focus_stack(im_stack, nhsize=9, focus=None, alpha=0.2, sth=13)</code>","text":"<p>Generate extended depth-of-field image from focus sequence using noise-robust selective all-in-focus algorithm [1]. Input images may be grayscale or color. For color images, the algorithm is applied to each color plane independently.</p> <p>For further details, see: [1] Pertuz et. al. \"Generation of all-in-focus images by   noise-robust selective fusion of limited depth-of-field   images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013.</p> <p>S. Pertuz, Jan/2016</p> <p>Modified by Josh, 2021</p> <p>Parameters:</p> Name Type Description Default <code>im_stack</code> <code>np.ndarray</code> <p>Element <code>[:,:,p,:]</code> is greyscale or RGB image at z-plane <code>p</code>. RGB: <code>uint8 [M x N x P x 3]</code> Gray: <code>uint16 [M x N x P]</code></p> required <code>nhsize</code> <code>int</code> <p>Size of default window.</p> <code>9</code> <code>focus</code> <code>Optional[np.ndarray]</code> <p><code>int [P]</code> or <code>None</code>. Vector with focus of each frame. If <code>None</code>, will use <code>np.arange(P)</code>.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>A scalar in <code>[0,1]</code>. See [1] for details.</p> <code>0.2</code> <code>sth</code> <code>float</code> <p>A scalar. See [1] for details.</p> <code>13</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [M x N]</code>. All In Focus (AIF) image.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def focus_stack(im_stack: np.ndarray, nhsize: int = 9, focus: Optional[np.ndarray] = None, alpha: float = 0.2,\n                sth: float = 13) -&gt; np.ndarray:\n\"\"\"\n    Generate extended depth-of-field image from focus sequence\n    using noise-robust selective all-in-focus algorithm [1].\n    Input images may be grayscale or color. For color images,\n    the algorithm is applied to each color plane independently.\n\n    For further details, see:\n    [1] Pertuz et. al. \"Generation of all-in-focus images by\n      noise-robust selective fusion of limited depth-of-field\n      images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013.\n\n    S. Pertuz, Jan/2016\n\n    Modified by Josh, 2021\n\n    Args:\n        im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```.\n            RGB: ```uint8 [M x N x P x 3]```\n            Gray: ```uint16 [M x N x P]```\n        nhsize: Size of default window.\n        focus: ```int [P]``` or ```None```.\n            Vector with focus of each frame. If ```None```, will use ```np.arange(P)```.\n        alpha: A scalar in ```[0,1]```. See [1] for details.\n        sth: A scalar. See [1] for details.\n\n    Returns:\n        ```int [M x N]```. All In Focus (AIF) image.\n    \"\"\"\n    rgb = np.ndim(im_stack) == 4\n    if focus is None:\n        focus = np.arange(im_stack.shape[2])\n    if rgb:\n        imagesR = im_stack[:, :, :, 0].astype(float)\n        imagesG = im_stack[:, :, :, 1].astype(float)\n        imagesB = im_stack[:, :, :, 2].astype(float)\n    fm = get_fmeasure(im_stack, nhsize)\n    S, fm = get_smeasure(fm, nhsize, focus)\n    fm = get_weights(S, fm, alpha, sth)\n'''Fuse Images'''\n    fmn = np.sum(fm, 2)  # normalisation factor\n    if rgb:\n        im = np.zeros_like(im_stack[:, :, 0])\n        im[:, :, 0] = np.sum(imagesR * fm, 2) / fmn\n        im[:, :, 1] = np.sum(imagesG * fm, 2) / fmn\n        im[:, :, 2] = np.sum(imagesB * fm, 2) / fmn\n        im = np.round(im).astype(np.uint8)\n    else:\n        im = np.round((np.sum(im_stack.astype(float) * fm, 2) / fmn)).astype(int)\n    return im\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.gauss3p","title":"<code>gauss3p(x, y)</code>","text":"<p>Fast 3-point gaussian interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>np.ndarray</code> <p><code>int [P]</code>. Vector with focus of each frame. Typical: <code>np.arange(P)</code>.</p> required <code>y</code> <code>np.ndarray</code> <p><code>float [M x N x P]</code>. Image to interpolate.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>u</code> - <code>float [M x N]</code>. Mean value of gaussian function.</li> </ul> <code>np.ndarray</code> <ul> <li><code>s_squared</code> - <code>float [M x N]</code>. Variance.</li> </ul> <code>np.ndarray</code> <ul> <li><code>A</code> - <code>float [M x N]</code>. Max value of gaussian function.</li> </ul> <code>np.ndarray</code> <ul> <li><code>y_max</code> - <code>float [M x N]</code>. Max projection of image <code>y</code>.</li> </ul> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def gauss3p(x: np.ndarray, y: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Fast 3-point gaussian interpolation.\n\n    Args:\n        x: ```int [P]```.\n            Vector with focus of each frame. Typical: ```np.arange(P)```.\n        y: ```float [M x N x P]```.\n            Image to interpolate.\n\n    Returns:\n        - ```u``` - ```float [M x N]```.\n            Mean value of gaussian function.\n        - ```s_squared``` - ```float [M x N]```.\n            Variance.\n        - ```A``` - ```float [M x N]```.\n            Max value of gaussian function.\n        - ```y_max``` - ```float [M x N]```.\n            Max projection of image ```y```.\n    \"\"\"\n    step = 2  # internal parameter\n    M, N, P = np.shape(y)\n    y_max = np.max(y, axis=2)\n    I = np.argmax(y, axis=2)\n    IN, IM = np.meshgrid(np.arange(N), np.arange(M))\n    Ic = I.flatten()  # transpose before FLATTEN to give same as MATLAB\n    Ic[Ic &lt;= step - 1] = step\n    Ic[Ic &gt;= P - 1 - step] = P - 1 - step\n    index1 = np.ravel_multi_index((IM.flatten(), IN.flatten(), Ic - step), (M, N, P))\n    index2 = np.ravel_multi_index((IM.flatten(), IN.flatten(), Ic), (M, N, P))\n    index3 = np.ravel_multi_index((IM.flatten(), IN.flatten(), Ic + step), (M, N, P))\n    index1[I.flatten() &lt;= step - 1] = index3[I.flatten() &lt;= step - 1]\n    index3[I.flatten() &gt;= step - 1] = index1[I.flatten() &gt;= step - 1]\n    x1 = x[Ic.flatten() - step].reshape(M, N)\n    x2 = x[Ic.flatten()].reshape(M, N)\n    x3 = x[Ic.flatten() + step].reshape(M, N)\n    y1 = np.log(y[np.unravel_index(index1, np.shape(y))].reshape(M, N))\n    y2 = np.log(y[np.unravel_index(index2, np.shape(y))].reshape(M, N))\n    y3 = np.log(y[np.unravel_index(index3, np.shape(y))].reshape(M, N))\n    c = ((y1 - y2) * (x2 - x3) - (y2 - y3) * (x1 - x2)) / (\n            (x1 ** 2 - x2 ** 2) * (x2 - x3) - (x2 ** 2 - x3 ** 2) * (x1 - x2))\n    b = ((y2 - y3) - c * (x2 - x3) * (x2 + x3 + 2)) / (x2 - x3)  # +2 is due to python vs matlab indexing\n    s_squared = -1 / (2 * c)\n    u = b * s_squared\n    a = y1 - b * (x1 + 1) - c * (x1 + 1) ** 2  # +1 is due to python vs matlab indexing\n    A = np.exp(a + u ** 2. / (2 * s_squared))\n    return u, s_squared, A, y_max\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_fmeasure","title":"<code>get_fmeasure(im_stack, nhsize)</code>","text":"<p>Returns focus measure value for each pixel.</p> <p>Parameters:</p> Name Type Description Default <code>im_stack</code> <code>np.ndarray</code> <p>Element <code>[:,:,p,:]</code> is greyscale or RGB image at z-plane <code>p</code>. RGB: <code>uint8 [M x N x P x 3]</code>. Gray: <code>uint16 [M x N x P]</code>.</p> required <code>nhsize</code> <code>int</code> <p>Size of focus measure window. Typical: <code>M/200</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [M x N x P]</code>. Focus measure image.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def get_fmeasure(im_stack: np.ndarray, nhsize: int) -&gt; np.ndarray:\n\"\"\"\n    Returns focus measure value for each pixel.\n\n    Args:\n        im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```.\n            RGB: ```uint8 [M x N x P x 3]```.\n            Gray: ```uint16 [M x N x P]```.\n        nhsize: Size of focus measure window. Typical: ```M/200```.\n\n    Returns:\n        ```float [M x N x P]```. Focus measure image.\n    \"\"\"\n    rgb = np.ndim(im_stack) == 4\n    im_shape = np.shape(im_stack)\n    fm = np.zeros((im_shape[:3]))\n    for p in range(im_shape[2]):\n        if rgb:\n            im = rgb2gray(im_stack[:, :, p])\n        else:\n            im = im_stack[:, :, p]\n        fm[:, :, p] = gfocus(im2double(im), nhsize)\n    return fm\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_smeasure","title":"<code>get_smeasure(fm, nhsize, focus)</code>","text":"<p>Returns selectivity measure value for each pixel.</p> <p>Parameters:</p> Name Type Description Default <code>fm</code> <code>np.ndarray</code> <p><code>float [M x N x P]</code>. Focus Measure Image.</p> required <code>nhsize</code> <code>int</code> <p>Size of focus measure window. Typical: <code>M/200</code>.</p> required <code>focus</code> <code>np.ndarray</code> <p><code>int [P]</code>. Vector with focus of each frame. Typical: <code>np.arange(P)</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>S</code> - <code>float [M x N]</code>. Selectivity measure image.</li> </ul> <code>np.ndarray</code> <ul> <li><code>fm</code> - <code>float [M x N x P]</code>. Normalised focus measure image.</li> </ul> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def get_smeasure(fm: np.ndarray, nhsize: int, focus: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Returns selectivity measure value for each pixel.\n\n    Args:\n        fm: ```float [M x N x P]```.\n            Focus Measure Image.\n        nhsize: Size of focus measure window. Typical: ```M/200```.\n        focus: ```int [P]```.\n            Vector with focus of each frame. Typical: ```np.arange(P)```.\n\n    Returns:\n        - ```S``` - ```float [M x N]```. Selectivity measure image.\n        - ```fm``` - ```float [M x N x P]```. Normalised focus measure image.\n    \"\"\"\n    M, N, P = np.shape(fm)\n    u, s_squared, A, fmax = gauss3p(focus, fm)\n    # Aprox. RMS of error signal as sum|Signal-Noise| instead of sqrt(sum(Signal-noise)^2):\n    err = np.zeros((M, N))\n    for p in range(P):\n        # +1 for self.focus is due to python vs matlab indexing\n        # this bit is twice as slow vectorized for 2048 x 2048 x 51 image\n        err = err + np.abs(fm[:, :, p] - A * np.exp(-(focus[p] + 1 - u) ** 2 / (2 * s_squared)))\n    fm = fm / np.expand_dims(fmax, 2)\n    h = np.ones((nhsize, nhsize)) / (nhsize ** 2)\n    inv_psnr = ndimage.correlate(err / (P * fmax), h, mode='nearest')\n    # inv_psnr = cv2.filter2D(err / (P * fmax), -1, h, borderType=cv2.BORDER_REPLICATE)\n    S = 20 * np.log10(1 / inv_psnr)\n    S[np.isnan(S)] = np.nanmin(S)\n    return S, fm\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_weights","title":"<code>get_weights(S, fm, alpha, sth)</code>","text":"<p>Computes sharpening parameter phi and then returns cut off frequency for high pass convolve_2d, <code>omega</code>.</p> <p>Parameters:</p> Name Type Description Default <code>S</code> <code>np.ndarray</code> <p><code>float [M x N]</code>. Selectivity measure image.</p> required <code>fm</code> <code>np.ndarray</code> <p><code>float [M x N x P]</code>. Normalised focus measure image.</p> required <code>alpha</code> <code>float</code> <p>A scalar in <code>[0, 1]</code>. Typical: <code>0.2</code>.</p> required <code>sth</code> <code>float</code> <p>A scalar. Typical: <code>13</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [M x N]</code>. Cut off frequency for high pass convolve_2d.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def get_weights(S: np.ndarray, fm: np.ndarray, alpha: float, sth: float) -&gt; np.ndarray:\n\"\"\"\n    Computes sharpening parameter phi and then\n    returns cut off frequency for high pass convolve_2d, ```omega```.\n\n    Args:\n        S: ```float [M x N]```.\n            Selectivity measure image.\n        fm: ```float [M x N x P]```.\n            Normalised focus measure image.\n        alpha: A scalar in ```[0, 1]```. Typical: ```0.2```.\n        sth: A scalar. Typical: ```13```.\n\n    Returns:\n        ```float [M x N]```. Cut off frequency for high pass convolve_2d.\n    \"\"\"\n    phi = 0.5 * (1 + np.tanh(alpha * (S - sth))) / alpha\n    phi = medfilt2d(phi, 3)\n    omega = 0.5 + 0.5 * np.tanh(np.expand_dims(phi, 2) * (fm - 1))\n    return omega\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.gfocus","title":"<code>gfocus(im, w_size)</code>","text":"<p>Compute focus measure using gray level local variance.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>np.ndarray</code> <p><code>float [M x N]</code>. Gray scale image.</p> required <code>w_size</code> <code>int</code> <p>Size of convolve_2d window. Typical: <code>M/200</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [M x N]</code>. Focus measure image.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def gfocus(im: np.ndarray, w_size: int) -&gt; np.ndarray:\n\"\"\"\n    Compute focus measure using gray level local variance.\n\n    Args:\n        im: ```float [M x N]```.\n            Gray scale image.\n        w_size: Size of convolve_2d window. Typical: ```M/200```.\n\n    Returns:\n        ```float [M x N]```. Focus measure image.\n    \"\"\"\n    mean_f = np.ones((w_size, w_size)) / (w_size ** 2)\n    #u = ndimage.correlate(im, mean_f, mode='nearest')\n    u = cv2.filter2D(im, -1, mean_f, borderType=cv2.BORDER_REPLICATE)\n    fm = (im - u) ** 2\n    fm = cv2.filter2D(fm, -1, mean_f, borderType=cv2.BORDER_REPLICATE)\n    #fm = ndimage.correlate(fm, mean_f, mode='nearest')\n    return fm\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.im2double","title":"<code>im2double(im)</code>","text":"<p>Equivalent to Matlab im2double function. Follows answer here.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>np.ndarray</code> <p><code>int</code>/<code>uint``````[m x n x ...]</code>. Image tom convert.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [m x n x ...]</code>. Converted image.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def im2double(im: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Equivalent to Matlab im2double function.\n    Follows answer\n    [here](https://stackoverflow.com/questions/29100722/equivalent-im2double-function-in-opencv-python/29104511).\n\n    Args:\n        im: ```int```/```uint``````[m x n x ...]```. Image tom convert.\n\n    Returns:\n        ```float [m x n x ...]```. Converted image.\n    \"\"\"\n    info = np.iinfo(im.dtype)  # Get the data type of the input image\n    return im.astype(float) / info.max  # Divide all values by the largest possible value in the datatype\n</code></pre>"},{"location":"code/extract/fstack/#coppafish.extract.fstack.rgb2gray","title":"<code>rgb2gray(im)</code>","text":"<p>Converts RGB <code>m x n x 3</code> color image into <code>m x n</code> greyscale image. Uses weighted sum indicated here.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>np.ndarray</code> <p><code>float [m x n x 3]</code>. RGB image</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [m x n]</code>. Greyscale image.</p> Source code in <code>coppafish/extract/fstack.py</code> <pre><code>def rgb2gray(im: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Converts RGB ```m x n x 3``` color image into ```m x n``` greyscale image.\n    Uses weighted sum indicated [here](https://www.mathworks.com/help/matlab/ref/rgb2gray.html).\n\n    Args:\n        im: ```float [m x n x 3]```. RGB image\n\n    Returns:\n        ```float [m x n]```. Greyscale image.\n\n    \"\"\"\n    im_R = im[:, :, 0]\n    im_G = im[:, :, 1]\n    im_B = im[:, :, 2]\n    if np.issubdtype(im.dtype,np.integer):\n        im_output = np.round((0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B)).astype(im.dtype)\n    else:\n        im_output = (0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B).astype(im.dtype)\n    return im_output\n</code></pre>"},{"location":"code/extract/scale/","title":"Scale","text":""},{"location":"code/extract/scale/#coppafish.extract.scale.central_tile","title":"<code>central_tile(tilepos_yx, use_tiles)</code>","text":"<p>returns tile in use_tiles closest to centre.</p> <p>Parameters:</p> Name Type Description Default <code>tilepos_yx</code> <code>np.ndarray</code> <p><code>int [n_tiles x 2]</code>. tiff tile positions (index <code>0</code> refers to <code>[0,0]</code>).</p> required <code>use_tiles</code> <code>List[int]</code> <p><code>int [n_use_tiles]</code>. Tiles used in the experiment.</p> required <p>Returns:</p> Type Description <code>int</code> <p>tile in <code>use_tiles</code> closest to centre.</p> Source code in <code>coppafish/extract/scale.py</code> <pre><code>def central_tile(tilepos_yx: np.ndarray, use_tiles: List[int]) -&gt; int:\n\"\"\"\n    returns tile in use_tiles closest to centre.\n\n    Args:\n        tilepos_yx: ```int [n_tiles x 2]```.\n            tiff tile positions (index ```0``` refers to ```[0,0]```).\n        use_tiles: ```int [n_use_tiles]```.\n            Tiles used in the experiment.\n\n    Returns:\n        tile in ```use_tiles``` closest to centre.\n    \"\"\"\n    mean_yx = np.round(np.mean(tilepos_yx, 0))\n    nearest_t = np.linalg.norm(tilepos_yx[use_tiles] - mean_yx, axis=1).argmin()\n    return int(use_tiles[nearest_t])\n</code></pre>"},{"location":"code/extract/scale/#coppafish.extract.scale.get_scale","title":"<code>get_scale(nbp_file, nbp_basic, r, use_tiles, use_channels, use_z, scale_norm, filter_kernel, smooth_kernel=None)</code>","text":"<p>Convolves the image for tile <code>t</code>, channel <code>c</code>, z-plane <code>z</code> with <code>filter_kernel</code> then gets the multiplier to apply to filtered nd2 images by dividing <code>scale_norm</code> by the max value of this filtered image.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>r</code> <code>int</code> <p>Round to get <code>scale</code> from. Should be 0 to determine <code>scale</code> and the anchor round (last round) to determine <code>scale_anchor</code>.</p> required <code>use_tiles</code> <code>List[int]</code> <p><code>int [n_use_tiles]</code>. tiff tile indices to consider when finding tile.</p> required <code>use_channels</code> <code>List[int]</code> <p><code>int [n_use_channels]</code>. Channels to consider when finding channel.</p> required <code>use_z</code> <code>List[int]</code> <p><code>int [n_z]</code>. Z-planes to consider when finding z_plane.</p> required <code>scale_norm</code> <code>int</code> <p>Desired maximum pixel value of npy images. Typical: <code>40000</code>.</p> required <code>filter_kernel</code> <code>np.ndarray</code> <p><code>float [ny_kernel x nx_kernel]</code>. Kernel to convolve nd2 data with to produce npy tiles. Typical shape: <code>[13 x 13]</code>.</p> required <code>smooth_kernel</code> <code>Optional[np.ndarray]</code> <p><code>float [ny_smooth x nx_smooth]</code>. 2D kernel to smooth filtered image with npy with. Typical shape: <code>[3 x 3]</code>. If None, no smoothing is applied</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <ul> <li><code>t</code> - <code>int</code>. npy tile index (index <code>0</code> refers to <code>tilepos_yx_npy=[MaxY, MaxX]</code>) scale found from.</li> </ul> <code>int</code> <ul> <li><code>c</code> - <code>int</code>. Channel scale found from.</li> </ul> <code>int</code> <ul> <li><code>z</code> - <code>int</code>. Z-plane scale found from.</li> </ul> <code>float</code> <ul> <li><code>scale</code> - <code>float</code>. Multiplier to apply to filtered nd2 images before saving as npy so full npy <code>uint16</code> range occupied.</li> </ul> Source code in <code>coppafish/extract/scale.py</code> <pre><code>def get_scale(nbp_file: NotebookPage, nbp_basic: NotebookPage, r: int, use_tiles: List[int],\n              use_channels: List[int], use_z: List[int], scale_norm: int,\n              filter_kernel: np.ndarray, smooth_kernel: Optional[np.ndarray] = None) -&gt; Tuple[int, int, int, float]:\n\"\"\"\n    Convolves the image for tile ```t```, channel ```c```, z-plane ```z``` with ```filter_kernel```\n    then gets the multiplier to apply to filtered nd2 images by dividing ```scale_norm``` by the max value of this\n    filtered image.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        r: Round to get `scale` from.\n            Should be 0 to determine `scale` and the anchor round (last round) to determine `scale_anchor`.\n        use_tiles: ```int [n_use_tiles]```.\n            tiff tile indices to consider when finding tile.\n        use_channels: ```int [n_use_channels]```.\n            Channels to consider when finding channel.\n        use_z: ```int [n_z]```.\n            Z-planes to consider when finding z_plane.\n        scale_norm: Desired maximum pixel value of npy images. Typical: ```40000```.\n        filter_kernel: ```float [ny_kernel x nx_kernel]```.\n            Kernel to convolve nd2 data with to produce npy tiles. Typical shape: ```[13 x 13]```.\n        smooth_kernel: ```float [ny_smooth x nx_smooth]```.\n            2D kernel to smooth filtered image with npy with. Typical shape: ```[3 x 3]```.\n            If None, no smoothing is applied\n\n    Returns:\n        - ```t``` - ```int```.\n            npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) scale found from.\n        - ```c``` - ```int```.\n            Channel scale found from.\n        - ```z``` - ```int```.\n            Z-plane scale found from.\n        - ```scale``` - ```float```.\n            Multiplier to apply to filtered nd2 images before saving as npy so full npy ```uint16``` range occupied.\n    \"\"\"\n    # tile to get scale from is central tile\n    t = central_tile(nbp_basic.tilepos_yx, use_tiles)\n    # find z-plane with max pixel across all channels of tile t\n    c, z, image = get_z_plane(nbp_file, nbp_basic, r, t, use_channels, use_z)\n    # convolve_2d image in same way we convolve_2d before saving tiff files\n    im_filtered = utils.morphology.convolve_2d(image, filter_kernel)\n    if smooth_kernel is not None:\n        im_filtered = utils.morphology.imfilter(im_filtered, smooth_kernel, oa=False)\n    scale = scale_norm / im_filtered.max()\n    return t, c, z, float(scale)\n</code></pre>"},{"location":"code/extract/scale/#coppafish.extract.scale.get_scale_from_txt","title":"<code>get_scale_from_txt(txt_file, scale, scale_anchor, tol=0.001)</code>","text":"<p>This checks whether <code>scale</code> and <code>scale_anchor</code> values used for producing npy files in tile_dir match values used and saved to <code>txt_file</code> on previous run.</p> <p>Will raise error if they are different.</p> <p>Parameters:</p> Name Type Description Default <code>txt_file</code> <code>str</code> <p><code>nb.file_names.scale</code>, path to text file where scale values are saved. File contains two values, <code>scale</code> first and <code>scale_anchor</code> second. Values will be 0 if not used or not yet computed.</p> required <code>scale</code> <code>Optional[float]</code> <p>Value of <code>scale</code> used for current run of extract method i.e. <code>config['extract']['scale']</code>.</p> required <code>scale_anchor</code> <code>Optional[float]</code> <p>Value of <code>scale_anchor</code> used for current run of extract method i.e. <code>config['extract']['scale_anchor']</code>.</p> required <code>tol</code> <code>float</code> <p>Two scale values will be considered the same if they are closer than this.</p> <code>0.001</code> <p>Returns:</p> Type Description <code>float</code> <p>scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value.</p> <code>float</code> <p>scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value.</p> Source code in <code>coppafish/extract/scale.py</code> <pre><code>def get_scale_from_txt(txt_file: str, scale: Optional[float], scale_anchor: Optional[float],\n                       tol: float = 0.001) -&gt; Tuple[float, float]:\n\"\"\"\n    This checks whether `scale` and `scale_anchor` values used for producing npy files in *tile_dir* match\n    values used and saved to `txt_file` on previous run.\n\n    Will raise error if they are different.\n\n    Args:\n        txt_file: `nb.file_names.scale`, path to text file where scale values are saved.\n            File contains two values, `scale` first and `scale_anchor` second.\n            Values will be 0 if not used or not yet computed.\n        scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']`.\n        scale_anchor: Value of `scale_anchor` used for current run of extract method\n            i.e. `config['extract']['scale_anchor']`.\n        tol: Two scale values will be considered the same if they are closer than this.\n\n    Returns:\n        scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value.\n        scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value.\n    \"\"\"\n    if os.path.isfile(txt_file):\n        scale_saved, scale_anchor_saved = np.genfromtxt(txt_file)\n        if np.abs(scale_saved) &lt; tol:\n            pass  # 0 means scale not used so do nothing\n        elif scale is None:\n            warnings.warn(\"Using value of scale = {:.2f} saved in\\n\".format(scale_saved) + txt_file)\n            scale = float(scale_saved)  # Set to saved value used up till now if not specified\n        elif np.abs(scale - scale_saved) &gt; tol:\n            raise ValueError(f\"\\nImaging round (Not anchor) tiles saved so far were calculated with scale = \"\n                             f\"{scale_saved}\\nas saved in {txt_file}\\n\"\n                             f\"This is different from config['extract']['scale'] = {scale}.\")\n        if np.abs(scale_anchor_saved) &lt; tol:\n            pass  # 0 means scale_anchor not computed yet so do nothing\n        elif scale_anchor is None:\n            warnings.warn(\"Using value of scale_anchor = {:.2f} saved in\\n\".format(scale_anchor_saved) + txt_file)\n            scale_anchor = float(scale_anchor_saved)  # Set to saved value used up till now if not specified\n        elif np.abs(scale_anchor - scale_anchor_saved) &gt; tol:\n            raise ValueError(f\"\\nAnchor round tiles saved so far were calculated with scale_anchor = \"\n                             f\"{scale_anchor_saved}\\nas saved in {txt_file}\\n\"\n                             f\"This is different from config['extract']['scale_anchor'] = {scale_anchor}.\")\n    return scale, scale_anchor\n</code></pre>"},{"location":"code/extract/scale/#coppafish.extract.scale.get_z_plane","title":"<code>get_z_plane(nbp_file, nbp_basic, r, t, use_channels, use_z)</code>","text":"<p>Finds z plane and channel that has maximum pixel value for given round and tile.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>r</code> <code>int</code> <p>Round to consider.</p> required <code>t</code> <code>int</code> <p>npy tile index (index <code>0</code> refers to <code>tilepos_yx_npy=[MaxY, MaxX]</code>) to find z-plane from.</p> required <code>use_channels</code> <code>List[int]</code> <p><code>int [n_use_channels]</code>. Channels to consider.</p> required <code>use_z</code> <code>List[int]</code> <p><code>int [n_z]</code>. Z-planes to consider.</p> required <p>Returns:</p> Type Description <code>int</code> <ul> <li><code>max_channel</code> - <code>int</code>. Channel to which image with max pixel value corresponds.</li> </ul> <code>int</code> <ul> <li><code>max_z</code> - <code>int</code>. Z-plane to which image with max pixel value corresponds.</li> </ul> <code>np.ndarray</code> <ul> <li><code>image</code> - <code>int [tile_sz x tile_sz]</code>. Corresponding image.</li> </ul> Source code in <code>coppafish/extract/scale.py</code> <pre><code>def get_z_plane(nbp_file: NotebookPage, nbp_basic: NotebookPage, r: int, t: int, use_channels: List[int],\n                use_z: List[int]) -&gt; Tuple[int, int, np.ndarray]:\n\"\"\"\n    Finds z plane and channel that has maximum pixel value for given round and tile.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        r: Round to consider.\n        t: npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) to find z-plane from.\n        use_channels: ```int [n_use_channels]```.\n            Channels to consider.\n        use_z: ```int [n_z]```.\n            Z-planes to consider.\n\n    Returns:\n        - ```max_channel``` - ```int```.\n            Channel to which image with max pixel value corresponds.\n        - ```max_z``` - ```int```.\n            Z-plane to which image with max pixel value corresponds.\n        - ```image``` - ```int [tile_sz x tile_sz]```.\n            Corresponding image.\n    \"\"\"\n    round_dask_array = utils.raw.load(nbp_file, nbp_basic, r=r)\n    image_max = np.zeros((len(use_channels), len(use_z)))\n    for i in range(len(use_channels)):\n        image_max[i, :] = np.max(np.max(utils.raw.load(nbp_file, nbp_basic, round_dask_array, r,\n                                                       t, use_channels[i], use_z), axis=0), axis=0)\n    max_channel = use_channels[np.max(image_max, axis=1).argmax()]\n    max_z = use_z[np.max(image_max, axis=0).argmax()]\n    return max_channel, max_z, utils.raw.load(nbp_file, nbp_basic, round_dask_array, r, t, max_channel, max_z)\n</code></pre>"},{"location":"code/extract/scale/#coppafish.extract.scale.save_scale","title":"<code>save_scale(txt_file, scale, scale_anchor)</code>","text":"<p>This saves <code>scale</code> and <code>scale_anchor</code> to <code>txt_file</code>. If either <code>scale</code> and <code>scale_anchor</code> are <code>None</code>, they will be set to 0 when saving.</p> <p>Parameters:</p> Name Type Description Default <code>txt_file</code> <code>str</code> <p><code>nb.file_names.scale</code>, path to text file where scale values are to be saved. File will contain two values, <code>scale</code> first and <code>scale_anchor</code> second. Values will be 0 if not used or not yet computed.</p> required <code>scale</code> <code>Optional[float]</code> <p>Value of <code>scale</code> used for current run of extract method i.e. <code>config['extract']['scale']</code> or value computed from <code>get_scale</code>.</p> required <code>scale_anchor</code> <code>Optional[float]</code> <p>Value of <code>scale_anchor</code> used for current run of extract method i.e. <code>config['extract']['scale_anchor']</code> or value computed from <code>get_scale</code>.</p> required Source code in <code>coppafish/extract/scale.py</code> <pre><code>def save_scale(txt_file: str, scale: Optional[float], scale_anchor: Optional[float]):\n\"\"\"\n    This saves `scale` and `scale_anchor` to `txt_file`. If either `scale` and `scale_anchor` are `None`,\n    they will be set to 0 when saving.\n\n    Args:\n        txt_file: `nb.file_names.scale`, path to text file where scale values are to be saved.\n            File will contain two values, `scale` first and `scale_anchor` second.\n            Values will be 0 if not used or not yet computed.\n        scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']` or\n            value computed from `get_scale`.\n        scale_anchor: Value of `scale_anchor` used for current run of extract method\n            i.e. `config['extract']['scale_anchor']` or value computed from `get_scale`.\n\n    \"\"\"\n    scale, scale_anchor = get_scale_from_txt(txt_file, scale, scale_anchor)  # check if match current saved values\n    if scale is None:\n        scale = 0\n    if scale_anchor is None:\n        scale_anchor = 0\n    np.savetxt(txt_file, [scale, scale_anchor], header='scale followed by scale_anchor')\n</code></pre>"},{"location":"code/find_spots/base/","title":"Base","text":""},{"location":"code/find_spots/base/#coppafish.find_spots.base.check_neighbour_intensity","title":"<code>check_neighbour_intensity(image, spot_yxz, thresh=0)</code>","text":"<p>Checks whether a neighbouring pixel to those indicated in <code>spot_yxz</code> has intensity less than <code>thresh</code>. The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x x n_z]</code>. image spots were found on.</p> required <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found. If axis 1 dimension is more than <code>image.ndim</code>, only first <code>image.ndim</code> dimensions used i.e. if supply yxz, with 2d image, only yx position used.</p> required <code>thresh</code> <code>float</code> <p>Spots are indicated as <code>False</code> if intensity at neighbour to spot location is less than this.</p> <code>0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_peaks]</code>. <code>True</code> if no neighbours below thresh.</p> Source code in <code>coppafish/find_spots/base.py</code> <pre><code>def check_neighbour_intensity(image: np.ndarray, spot_yxz: np.ndarray, thresh: float = 0) -&gt; np.ndarray:\n\"\"\"\n    Checks whether a neighbouring pixel to those indicated in ```spot_yxz``` has intensity less than ```thresh```.\n    The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot.\n\n    Args:\n        image: ```float [n_y x n_x x n_z]```.\n            image spots were found on.\n        spot_yxz: ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found.\n            If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used\n            i.e. if supply yxz, with 2d image, only yx position used.\n        thresh: Spots are indicated as ```False``` if intensity at neighbour to spot location is less than this.\n\n    Returns:\n        ```float [n_peaks]```.\n            ```True``` if no neighbours below thresh.\n    \"\"\"\n    if image.ndim == 3:\n        transforms = [[1, 0, 0], [0, 1, 0], [-1, 0, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]]\n    elif image.ndim == 2:\n        transforms = [[1, 0], [0, 1], [-1, 0], [0, -1]]\n    else:\n        raise ValueError(f\"image has to have two or three dimensions but given image has {image.ndim} dimensions.\")\n    keep = np.zeros((spot_yxz.shape[0], len(transforms)), dtype=bool)\n    for i, t in enumerate(transforms):\n        mod_spot_yx = spot_yxz + t\n        for j in range(image.ndim):\n            mod_spot_yx[:, j] = np.clip(mod_spot_yx[:, j], 0, image.shape[j]-1)\n        keep[:, i] = image[tuple([mod_spot_yx[:, j] for j in range(image.ndim)])] &gt; thresh\n    return keep.min(axis=1)\n</code></pre>"},{"location":"code/find_spots/base/#coppafish.find_spots.base.get_isolated","title":"<code>get_isolated(image, spot_yxz, thresh, radius_inner, radius_xy, radius_z=None)</code>","text":"<p>Determines whether each spot in <code>spot_yxz</code> is isolated by getting the value of image after annular filtering at each location in <code>spot_yxz</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x x n_z]</code>. image spots were found on.</p> required <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found. If axis 1 dimension is more than <code>image.ndim</code>, only first <code>image.ndim</code> dimensions used i.e. if supply yxz, with 2d image, only yx position used.</p> required <code>thresh</code> <code>float</code> <p>Spots are isolated if annulus filtered image at spot location less than this.</p> required <code>radius_inner</code> <code>float</code> <p>Inner radius of annulus filtering kernel within which values are all zero.</p> required <code>radius_xy</code> <code>float</code> <p>Outer radius of annulus filtering kernel in xy direction.</p> required <code>radius_z</code> <code>Optional[float]</code> <p>Outer radius of annulus filtering kernel in z direction. If <code>None</code>, 2D filter is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>bool [n_peaks]</code>. Whether each spot is isolated or not.</p> Source code in <code>coppafish/find_spots/base.py</code> <pre><code>def get_isolated(image: np.ndarray, spot_yxz: np.ndarray, thresh: float, radius_inner: float, radius_xy: float,\n                 radius_z: Optional[float] = None) -&gt; np.ndarray:\n\"\"\"\n    Determines whether each spot in ```spot_yxz``` is isolated by getting the value of image after annular filtering\n    at each location in ```spot_yxz```.\n\n    Args:\n        image: ```float [n_y x n_x x n_z]```.\n            image spots were found on.\n        spot_yxz: ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found.\n            If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used\n            i.e. if supply yxz, with 2d image, only yx position used.\n        thresh: Spots are isolated if annulus filtered image at spot location less than this.\n        radius_inner: Inner radius of annulus filtering kernel within which values are all zero.\n        radius_xy: Outer radius of annulus filtering kernel in xy direction.\n        radius_z: Outer radius of annulus filtering kernel in z direction.\n            If ```None```, 2D filter is used.\n\n    Returns:\n        ```bool [n_peaks]```.\n            Whether each spot is isolated or not.\n\n    \"\"\"\n    se = utils.strel.annulus(radius_inner, radius_xy, radius_z)\n    # With just coords, takes about 3s for 50 z-planes.\n    isolated = utils.morphology.imfilter_coords(image, se, spot_yxz, padding=0, corr_or_conv='corr') / np.sum(se)\n    return isolated &lt; thresh\n</code></pre>"},{"location":"code/find_spots/base/#coppafish.find_spots.base.get_isolated_points","title":"<code>get_isolated_points(spot_yxz, isolation_dist)</code>","text":"<p>Get the isolated points in a point cloud as those whose neighbour is far.</p> <p>Parameters:</p> Name Type Description Default <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found in image.</p> required <code>isolation_dist</code> <code>float</code> <p>Spots are isolated if nearest neighbour is further away than this.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>bool [n_peaks]</code>. <code>True</code> for points far from any other point in <code>spot_yx</code>.</p> Source code in <code>coppafish/find_spots/base.py</code> <pre><code>def get_isolated_points(spot_yxz: np.ndarray, isolation_dist: float) -&gt; np.ndarray:\n\"\"\"\n    Get the isolated points in a point cloud as those whose neighbour is far.\n\n    Args:\n        spot_yxz: ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found in image.\n        isolation_dist: Spots are isolated if nearest neighbour is further away than this.\n\n    Returns:\n        ```bool [n_peaks]```. ```True``` for points far from any other point in ```spot_yx```.\n\n    \"\"\"\n    tree = KDTree(spot_yxz)\n    # for distances more than isolation_dist, distances will be set to infinity i.e. will be &gt; isolation_dist.\n    distances = tree.query(spot_yxz, k=[2], distance_upper_bound=isolation_dist)[0].squeeze()\n    return distances &gt; isolation_dist\n</code></pre>"},{"location":"code/find_spots/base/#coppafish.find_spots.base.spot_yxz","title":"<code>spot_yxz(spot_details, tile, round, channel, return_isolated=False)</code>","text":"<p>Function which gets yxz positions (and whether isolated) of spots on a particular <code>tile</code>, <code>round</code>, <code>channel</code> from <code>spot_details</code> in find_spots notebook page.</p> <p>Parameters:</p> Name Type Description Default <code>spot_details</code> <code>np.ndarray</code> <p><code>int16 [n_spots x 7]</code>. <code>spot_details[s]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> of spot <code>s</code>.</p> required <code>tile</code> <code>int</code> <p>Tile of desired spots.</p> required <code>round</code> <code>int</code> <p>Round of desired spots.</p> required <code>channel</code> <code>int</code> <p>Channel of desired spots.</p> required <code>return_isolated</code> <code>bool</code> <p>Whether to return isolated status of each spot.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li><code>spot_yxz</code> - <code>int16 [n_trc_spots x 3]</code>. yxz coordinates of spots on chosen <code>tile</code>, <code>round</code> and <code>channel</code>.</li> </ul> <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li><code>spot_isolated</code> - <code>bool [n_trc_spots]</code> (Only returned if <code>return_isolated = True</code>). Isolated status (<code>1</code> if isolated, <code>0</code> if not) of the spots.</li> </ul> Source code in <code>coppafish/find_spots/base.py</code> <pre><code>def spot_yxz(spot_details: np.ndarray, tile: int, round: int, channel: int,\n             return_isolated: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n\"\"\"\n    Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```, ```\n    channel``` from ```spot_details``` in find_spots notebook page.\n\n    Args:\n        spot_details: ```int16 [n_spots x 7]```.\n            ```spot_details[s]``` is ```[tile, round, channel, isolated, y, x, z]``` of spot ```s```.\n        tile: Tile of desired spots.\n        round: Round of desired spots.\n        channel: Channel of desired spots.\n        return_isolated: Whether to return isolated status of each spot.\n\n    Returns:\n        - ```spot_yxz``` - ```int16 [n_trc_spots x 3]```.\n            yxz coordinates of spots on chosen ```tile```, ```round``` and ```channel```.\n        - ```spot_isolated``` - ```bool [n_trc_spots]``` (Only returned if ```return_isolated = True```).\n            Isolated status (```1``` if isolated, ```0``` if not) of the spots.\n    \"\"\"\n    #     Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```,\n    #     ```channel``` from ```spot_details``` in find_spots notebook page.\n    use = np.all((spot_details[:, 0] == tile, spot_details[:, 1] == round, spot_details[:, 2] == channel), axis=0)\n    if return_isolated:\n        return spot_details[use, 4:], spot_details[use, 3]\n    else:\n        return spot_details[use, 4:]\n</code></pre>"},{"location":"code/find_spots/check_spots/","title":"Check Spots","text":""},{"location":"code/find_spots/check_spots/#coppafish.find_spots.check_spots.check_n_spots","title":"<code>check_n_spots(nb)</code>","text":"<p>This checks that a decent number of spots are detected on:</p> <ul> <li>Each channel across all rounds and tiles.</li> <li>Each tile across all rounds and channels.</li> <li>Each round across all tile and channels.</li> </ul> <p>An error will be raised if any of these conditions are violated.</p> <p><code>config['find_spots']['n_spots_warn_fraction']</code> and <code>config['find_spots']['n_spots_error_fraction']</code> are the parameters which determine if warnings/errors will be raised.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing <code>find_spots</code> page.</p> required Source code in <code>coppafish/find_spots/check_spots.py</code> <pre><code>def check_n_spots(nb: Notebook):\n\"\"\"\n    This checks that a decent number of spots are detected on:\n\n    * Each channel across all rounds and tiles.\n    * Each tile across all rounds and channels.\n    * Each round across all tile and channels.\n\n    An error will be raised if any of these conditions are violated.\n\n    `config['find_spots']['n_spots_warn_fraction']` and `config['find_spots']['n_spots_error_fraction']`\n    are the parameters which determine if warnings/errors will be raised.\n\n    Args:\n        nb: *Notebook* containing `find_spots` page.\n    \"\"\"\n    # TODO: show example of what error looks like in the docs\n    config = nb.get_config()['find_spots']\n    if nb.basic_info.is_3d:\n        n_spots_warn = config['n_spots_warn_fraction'] * config['max_spots_3d'] * nb.basic_info.nz\n    else:\n        n_spots_warn = config['n_spots_warn_fraction'] * config['max_spots_2d']\n    n_spots_warn = int(np.ceil(n_spots_warn))\n    use_tiles = np.asarray(nb.basic_info.use_tiles)\n    error_message = \"\"\n\n    if len(nb.basic_info.use_rounds) &gt; 0:\n        use_rounds = np.asarray(nb.basic_info.use_rounds)  # don't consider anchor in this analysis\n        use_channels = np.asarray(nb.basic_info.use_channels)\n        spot_no = nb.find_spots.spot_no[np.ix_(use_tiles, use_rounds, use_channels)]\n\n        # Consider bad channels first as most likely to have consistently low spot counts in a channel\n        n_images = len(use_tiles) * len(use_rounds)\n        n_images_error = int(np.floor(n_images * config['n_spots_error_fraction']))\n        n_bad_images = np.zeros(len(use_channels), dtype=int)\n        for c in range(len(use_channels)):\n            bad_images = np.vstack(np.where(spot_no[:, :, c] &lt; n_spots_warn)).T\n            n_bad_images[c] = bad_images.shape[0]\n            if n_bad_images[c] &gt; 0:\n                bad_images[:, 0] = use_tiles[bad_images[:, 0]]\n                bad_images[:, 1] = use_rounds[bad_images[:, 1]]\n                warnings.warn(f\"\\nChannel {use_channels[c]} - {n_bad_images[c]} tiles/rounds with n_spots &lt; {n_spots_warn}:\"\n                              f\"\\n{bad_images}\")\n\n        fail_inds = np.where(n_bad_images &gt;= n_images_error)[0]\n        if len(fail_inds) &gt; 0:\n            error_message = error_message + f\"\\nChannels that failed: {use_channels[fail_inds]}\\n\" \\\n                                            f\"This is because out of {n_images} tiles/rounds, these channels had \" \\\n                                            f\"respectively:\\n{n_bad_images[fail_inds]}\\ntiles/rounds with \" \\\n                                            f\"n_spots &lt; {n_spots_warn}. These are all more than the error threshold of \" \\\n                                            f\"{n_images_error}.\\nConsider removing these from use_channels.\"\n            # don't consider failed channels for subsequent warnings/errors\n            use_channels = np.setdiff1d(use_channels, use_channels[fail_inds])\n            spot_no = nb.find_spots.spot_no[np.ix_(use_tiles, use_rounds, use_channels)]\n\n        # Consider bad tiles next as second most likely to have consistently low spot counts in a tile\n        n_images = len(use_channels) * len(use_rounds)\n        n_images_error = int(np.floor(n_images * config['n_spots_error_fraction']))\n        n_bad_images = np.zeros(len(use_tiles), dtype=int)\n        for t in range(len(use_tiles)):\n            bad_images = np.vstack(np.where(spot_no[t] &lt; n_spots_warn)).T\n            n_bad_images[t] = bad_images.shape[0]\n        fail_inds = np.where(n_bad_images &gt;= n_images_error)[0]\n        if len(fail_inds) &gt; 0:\n            error_message = error_message + f\"\\nTiles that failed: {use_tiles[fail_inds]}\\n\" \\\n                                            f\"This is because out of {n_images} rounds/channels, these tiles had \" \\\n                                            f\"respectively:\\n{n_bad_images[fail_inds]}\\nrounds/channels with \" \\\n                                            f\"n_spots &lt; {n_spots_warn}. These are all more than the error threshold of \" \\\n                                            f\"{n_images_error}.\\nConsider removing these from use_tiles.\"\n            # don't consider failed channels for subsequent warnings/errors\n            use_tiles = np.setdiff1d(use_tiles, use_tiles[fail_inds])\n            spot_no = nb.find_spots.spot_no[np.ix_(use_tiles, use_rounds, use_channels)]\n\n        # Consider bad rounds last as least likely to have consistently low spot counts in a round\n        n_images = len(use_channels) * len(use_tiles)\n        n_images_error = int(np.floor(n_images * config['n_spots_error_fraction']))\n        n_bad_images = np.zeros(len(use_rounds), dtype=int)\n        for r in range(len(use_rounds)):\n            bad_images = np.vstack(np.where(spot_no[:, r] &lt; n_spots_warn)).T\n            n_bad_images[r] = bad_images.shape[0]\n        fail_inds = np.where(n_bad_images &gt;= n_images_error)[0]\n        if len(fail_inds) &gt; 0:\n            error_message = error_message + f\"\\nRounds that failed: {use_rounds[fail_inds]}\\n\" \\\n                                            f\"This is because out of {n_images} tiles/channels, these tiles had \" \\\n                                            f\"respectively:\\n{n_bad_images[fail_inds]}\\ntiles/channels with \" \\\n                                            f\"n_spots &lt; {n_spots_warn}. These are all more than the error threshold \" \\\n                                            f\"of {n_images_error}.\\nConsider removing these from use_rounds.\"\n\n    # Consider anchor\n    if nb.basic_info.use_anchor:\n        spot_no = nb.find_spots.spot_no[use_tiles, nb.basic_info.anchor_round, nb.basic_info.anchor_channel]\n        n_images = len(use_tiles)\n        n_images_error = int(np.floor(n_images * config['n_spots_error_fraction']))\n        bad_images = np.where(spot_no &lt; n_spots_warn)[0]\n        n_bad_images = len(bad_images)\n        if n_bad_images &gt; 0:\n            bad_images = use_tiles[bad_images]\n            warnings.warn(\n                f\"\\nAnchor - {n_bad_images} tiles with n_spots &lt; {n_spots_warn}:\\n\"\n                f\"{bad_images}\")\n\n        if n_bad_images &gt;= n_images_error:\n            error_message = error_message + f\"\\nAnchor - tiles {bad_images} all had n_spots &lt; {n_spots_warn}. \" \\\n                                            f\"{n_bad_images}/{n_images} tiles failed which is more than the \" \\\n                                            f\"error threshold of {n_images_error}.\\n\" \\\n                                            f\"Consider removing these tiles from use_tiles.\"\n\n    if len(error_message) &gt; 0:\n        error_message = error_message + f\"\\nThe function coppafish.plot.view_find_spots may be useful for \" \\\n                                        f\"investigating why the above tiles/rounds/channels had so few spots detected.\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"code/find_spots/detect/","title":"Detect","text":""},{"location":"code/find_spots/detect/#coppafish.find_spots.detect.detect_spots","title":"<code>detect_spots(image, intensity_thresh, radius_xy, radius_z=None, remove_duplicates=False, se=None)</code>","text":"<p>Finds local maxima in image exceeding <code>intensity_thresh</code>. This is achieved through a dilation being run on the whole image. Should use for a large se.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x x n_z]</code>. <code>image</code> to find spots on.</p> required <code>intensity_thresh</code> <code>float</code> <p>Spots are local maxima in image with <code>pixel_value &gt; intensity_thresh</code>.</p> required <code>radius_xy</code> <code>Optional[int]</code> <p>Radius of dilation structuring element in xy plane (approximately spot radius).</p> required <code>radius_z</code> <code>Optional[int]</code> <p>Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If <code>None</code>, 2D filter is used.</p> <code>None</code> <code>remove_duplicates</code> <code>bool</code> <p>Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image.</p> <code>False</code> <code>se</code> <code>Optional[np.ndarray]</code> <p><code>int [se_sz_y x se_sz_x x se_sz_z]</code>. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>peak_yxz</code> - <code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found.</li> </ul> <code>np.ndarray</code> <ul> <li><code>peak_intensity</code> - <code>float [n_peaks]</code>. Pixel value of spots found.</li> </ul> Source code in <code>coppafish/find_spots/detect.py</code> <pre><code>def detect_spots(image: np.ndarray, intensity_thresh: float, radius_xy: Optional[int],\n                 radius_z: Optional[int] = None, remove_duplicates: bool = False,\n                 se: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Finds local maxima in image exceeding ```intensity_thresh```.\n    This is achieved through a dilation being run on the whole image.\n    Should use for a large se.\n\n    Args:\n        image: ```float [n_y x n_x x n_z]```.\n            ```image``` to find spots on.\n        intensity_thresh: Spots are local maxima in image with ```pixel_value &gt; intensity_thresh```.\n        radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius).\n        radius_z: Radius of dilation structuring element in z direction (approximately spot radius).\n            Must be more than 1 to be 3D.\n            If ```None```, 2D filter is used.\n        remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have\n            same intensity. Only works with integer image.\n        se: ```int [se_sz_y x se_sz_x x se_sz_z]```.\n            Can give structuring element manually rather than using a cuboid element.\n            Must only contain zeros and ones.\n\n    Returns:\n        - ```peak_yxz``` - ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found.\n        - ```peak_intensity``` - ```float [n_peaks]```.\n            Pixel value of spots found.\n    \"\"\"\n    if se is None:\n        # Default is a cuboid se of all ones as is quicker than disk and very similar results.\n        if radius_z is not None:\n            se = np.ones((2*radius_xy-1, 2*radius_xy-1, 2*radius_z-1), dtype=int)\n        else:\n            se = np.ones((2*radius_xy-1, 2*radius_xy-1), dtype=int)\n    if image.ndim == 2 and se.ndim == 3:\n        mid_z = int(np.floor((se.shape[2]-1)/2))\n        warnings.warn(f\"2D image provided but 3D filter asked for.\\n\"\n                      f\"Using the middle plane ({mid_z}) of this filter.\")\n        se = se[:, :, mid_z]\n\n    small = 1e-6  # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image).\n    if remove_duplicates:\n        # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly.\n        # hence when find maxima, will only get one of the pixels not both.\n        rng = np.random.default_rng(0)   # So shift is always the same.\n        # rand_shift must be larger than small to detect a single spot.\n        rand_im_shift = rng.uniform(low=small*2, high=0.2, size=image.shape)\n        image = image + rand_im_shift\n\n    dilate = utils.morphology.dilate(image, se)\n    spots = np.logical_and(image + small &gt; dilate, image &gt; intensity_thresh)\n    peak_pos = np.where(spots)\n    peak_yxz = np.concatenate([coord.reshape(-1, 1) for coord in peak_pos], axis=1)\n    peak_intensity = image[spots]\n    return peak_yxz, peak_intensity\n</code></pre>"},{"location":"code/find_spots/detect/#optimised","title":"Optimised","text":""},{"location":"code/find_spots/detect/#coppafish.find_spots.detect_optimised.detect_spots","title":"<code>detect_spots(image, intensity_thresh, radius_xy, radius_z=None, remove_duplicates=False, se=None)</code>","text":"<p>Finds local maxima in image exceeding <code>intensity_thresh</code>. This is achieved by looking at neighbours of pixels above intensity_thresh. Should use for a small <code>se</code> and high <code>intensity_thresh</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x x n_z]</code>. <code>image</code> to find spots on.</p> required <code>intensity_thresh</code> <code>float</code> <p>Spots are local maxima in image with <code>pixel_value &gt; intensity_thresh</code>.</p> required <code>radius_xy</code> <code>Optional[int]</code> <p>Radius of dilation structuring element in xy plane (approximately spot radius).</p> required <code>radius_z</code> <code>Optional[int]</code> <p>Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If <code>None</code>, 2D filter is used.</p> <code>None</code> <code>remove_duplicates</code> <code>bool</code> <p>Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image.</p> <code>False</code> <code>se</code> <code>Optional[np.ndarray]</code> <p><code>int [se_sz_y x se_sz_x x se_sz_z]</code>. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>peak_yxz</code> - <code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found.</li> </ul> <code>np.ndarray</code> <ul> <li><code>peak_intensity</code> - <code>float [n_peaks]</code>. Pixel value of spots found.</li> </ul> Source code in <code>coppafish/find_spots/detect_optimised.py</code> <pre><code>def detect_spots(image: np.ndarray, intensity_thresh: float, radius_xy: Optional[int], radius_z: Optional[int] = None,\n                 remove_duplicates: bool = False, se: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Finds local maxima in image exceeding ```intensity_thresh```.\n    This is achieved by looking at neighbours of pixels above intensity_thresh.\n    Should use for a small `se` and high `intensity_thresh`.\n\n    Args:\n        image: ```float [n_y x n_x x n_z]```.\n            ```image``` to find spots on.\n        intensity_thresh: Spots are local maxima in image with ```pixel_value &gt; intensity_thresh```.\n        radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius).\n        radius_z: Radius of dilation structuring element in z direction (approximately spot radius).\n            Must be more than 1 to be 3D.\n            If ```None```, 2D filter is used.\n        remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have\n            same intensity. Only works with integer image.\n        se: ```int [se_sz_y x se_sz_x x se_sz_z]```.\n            Can give structuring element manually rather than using a cuboid element.\n            Must only contain zeros and ones.\n\n    Returns:\n        - ```peak_yxz``` - ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found.\n        - ```peak_intensity``` - ```float [n_peaks]```.\n            Pixel value of spots found.\n    \"\"\"\n    if se is None:\n        # Default is a cuboid se of all ones as is quicker than disk and very similar results.\n        if radius_z is not None:\n            se = np.ones((2*radius_xy-1, 2*radius_xy-1, 2*radius_z-1), dtype=int)\n            pad_size_z = radius_z-1\n        else:\n            se = np.ones((2*radius_xy-1, 2*radius_xy-1), dtype=int)\n            pad_size_z = 0\n        pad_size_y = radius_xy - 1\n        pad_size_x = radius_xy - 1\n    else:\n        se = utils.morphology.ensure_odd_kernel(se)\n        pad_size_y = int((se.shape[0]-1)/2)\n        pad_size_x = int((se.shape[1]-1)/2)\n        if se.ndim == 3:\n            pad_size_z = int((se.shape[2] - 1) / 2)\n        else:\n            pad_size_z = 0\n    if image.ndim == 2 and se.ndim == 3:\n        mid_z = int(np.floor((se.shape[2]-1)/2))\n        warnings.warn(f\"2D image provided but 3D filter asked for.\\n\"\n                      f\"Using the middle plane ({mid_z}) of this filter.\")\n        se = se[:, :, mid_z]\n\n    # set central pixel to 0\n    se[np.ix_(*[(np.floor((se.shape[i] - 1) / 2).astype(int),) for i in range(se.ndim)])] = 0\n    se_shifts = utils.morphology.filter_optimised.get_shifts_from_kernel(se)\n\n    consider_yxz = np.where(image &gt; intensity_thresh)\n    n_consider = consider_yxz[0].shape[0]\n    if remove_duplicates:\n        # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly.\n        # hence when find maxima, will only get one of the pixels not both.\n        rng = np.random.default_rng(0)   # So shift is always the same.\n        # rand_shift must be larger than small to detect a single spot.\n        rand_im_shift = rng.uniform(low=2e-6, high=0.2, size=n_consider).astype(np.float32)\n        image = image.astype(np.float32)\n        image[consider_yxz] = image[consider_yxz] + rand_im_shift\n\n    consider_intensity = image[consider_yxz]\n    consider_yxz = list(consider_yxz)\n\n    keep = np.asarray(get_local_maxima_jax(image, se_shifts, pad_size_y, pad_size_x, pad_size_z, consider_yxz,\n                                           consider_intensity))\n    if remove_duplicates:\n        peak_intensity = np.round(consider_intensity[keep]).astype(int)\n    else:\n        peak_intensity = consider_intensity[keep]\n    peak_yxz = np.array(consider_yxz).transpose()[keep]\n    return peak_yxz, peak_intensity\n</code></pre>"},{"location":"code/find_spots/detect/#coppafish.find_spots.detect_optimised.get_local_maxima_jax","title":"<code>get_local_maxima_jax(image, se_shifts, pad_size_y, pad_size_x, pad_size_z, consider_yxz, consider_intensity)</code>","text":"<p>Finds the local maxima from a given set of pixels to consider.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>jnp.ndarray</code> <p><code>float [n_y x n_x x n_z]</code>. <code>image</code> to find spots on.</p> required <code>se_shifts</code> <code>Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]</code> <p><code>(image.ndim x  int [n_shifts])</code>. y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out.</p> required <code>pad_size_y</code> <code>int</code> <p>How much to zero pad image in y.</p> required <code>pad_size_x</code> <code>int</code> <p>How much to zero pad image in x.</p> required <code>pad_size_z</code> <code>int</code> <p>How much to zero pad image in z.</p> required <code>consider_yxz</code> <code>List[jnp.ndarray]</code> <p><code>[3 x int [n_consider]]</code>. All yxz coordinates where value in image is greater than an intensity threshold.</p> required <code>consider_intensity</code> <code>jnp.ndarray</code> <p><code>float [n_consider]</code>. Value of image at coordinates given by <code>consider_yxz</code>.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>bool [n_consider]</code> Whether each point in <code>consider_yxz</code> is a local maxima or not.</p> Source code in <code>coppafish/find_spots/detect_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=(2, 3, 4))\ndef get_local_maxima_jax(image: jnp.ndarray, se_shifts: Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray],\n                         pad_size_y: int, pad_size_x: int, pad_size_z: int,\n                         consider_yxz: List[jnp.ndarray], consider_intensity: jnp.ndarray) -&gt; jnp.ndarray:\n\"\"\"\n    Finds the local maxima from a given set of pixels to consider.\n\n    Args:\n        image: ```float [n_y x n_x x n_z]```.\n            ```image``` to find spots on.\n        se_shifts: `(image.ndim x  int [n_shifts])`.\n            y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out.\n        pad_size_y: How much to zero pad image in y.\n        pad_size_x: How much to zero pad image in x.\n        pad_size_z: How much to zero pad image in z.\n        consider_yxz: `[3 x int [n_consider]]`.\n            All yxz coordinates where value in image is greater than an intensity threshold.\n        consider_intensity: `float [n_consider]`.\n            Value of image at coordinates given by `consider_yxz`.\n\n    Returns:\n        `bool [n_consider]`\n            Whether each point in `consider_yxz` is a local maxima or not.\n    \"\"\"\n    pad_size = [(pad_size_y, pad_size_y), (pad_size_x, pad_size_x), (pad_size_z, pad_size_z)][:image.ndim]\n    image = jnp.pad(image, pad_size)\n    for i in range(len(pad_size)):\n        consider_yxz[i] = consider_yxz[i] + pad_size[i][0]\n    keep = jnp.ones(consider_yxz[0].shape[0], dtype=bool)\n    for i in range(se_shifts[0].shape[0]):\n        # Note that in each iteration, only consider coordinates which can still possibly be local maxima.\n        keep = keep * (image[tuple([consider_yxz[j] + se_shifts[j][i] for j in range(image.ndim)])] &lt;=\n                       consider_intensity)\n    return keep\n</code></pre>"},{"location":"code/omp/base/","title":"Base","text":""},{"location":"code/omp/base/#coppafish.omp.base.get_initial_intensity_thresh","title":"<code>get_initial_intensity_thresh(config, nbp)</code>","text":"<p>Gets absolute intensity threshold from config file. OMP will only be run on pixels with absolute intensity greater than this.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p><code>omp</code> section of config file.</p> required <code>nbp</code> <code>NotebookPage</code> <p><code>call_spots</code> NotebookPage</p> required <p>Returns:</p> Type Description <code>float</code> <p>Either <code>config['initial_intensity_thresh']</code> or <code>nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']]</code>.</p> Source code in <code>coppafish/omp/base.py</code> <pre><code>def get_initial_intensity_thresh(config: dict, nbp: NotebookPage) -&gt; float:\n\"\"\"\n    Gets absolute intensity threshold from config file. OMP will only be run on\n    pixels with absolute intensity greater than this.\n\n    Args:\n        config: `omp` section of config file.\n        nbp: `call_spots` *NotebookPage*\n\n    Returns:\n        Either `config['initial_intensity_thresh']` or\n            `nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']]`.\n\n    \"\"\"\n    initial_intensity_thresh = config['initial_intensity_thresh']\n    if initial_intensity_thresh is None:\n        config['initial_intensity_thresh'] = \\\n            round_any(nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']],\n                      config['initial_intensity_precision'])\n    initial_intensity_thresh = \\\n        float(np.clip(config['initial_intensity_thresh'], config['initial_intensity_thresh_min'],\n                      config['initial_intensity_thresh_max']))\n    return initial_intensity_thresh\n</code></pre>"},{"location":"code/omp/coefs/","title":"Coefficients","text":""},{"location":"code/omp/coefs/#coppafish.omp.coefs.fit_coefs","title":"<code>fit_coefs(bled_codes, pixel_colors, genes)</code>","text":"<p>Old method before Jax. This finds the least squared solution for how the <code>n_genes</code> <code>bled_codes</code> can best explain each <code>pixel_color</code>. Can also find weighted least squared solution if <code>weight</code> provided.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_colors</code> <code>np.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_pixels]</code> if <code>n_genes==1</code> otherwise  <code>float [(n_rounds x n_channels)]</code>. Flattened then transposed pixel colors which usually has the shape <code>[n_pixels x n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>np.ndarray</code> <p><code>int [n_pixels x n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>residual - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel_colors after removing bled_codes with coefficients specified by coef.</li> </ul> <code>np.ndarray</code> <ul> <li>coefs - <code>float [n_pixels x n_genes_add]</code> if n_genes == 1 otherwise <code>float [n_genes]</code> if n_pixels == 1. coefficient found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def fit_coefs(bled_codes: np.ndarray, pixel_colors: np.ndarray, genes: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Old method before Jax.\n    This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`.\n    Can also find weighted least squared solution if `weight` provided.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1`\n            otherwise  `float [(n_rounds x n_channels)]`.\n            Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`.\n        genes: `int [n_pixels x n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.\n\n    Returns:\n        - residual - `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel_colors after removing bled_codes with coefficients specified by coef.\n        - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1.\n            coefficient found through least squares fitting for each gene.\n\n    \"\"\"\n    n_pixels = pixel_colors.shape[1]\n    residual = np.zeros((n_pixels, pixel_colors.shape[0]))\n    coefs = np.zeros_like(genes, dtype=float)\n    for s in range(n_pixels):\n        coefs[s] = np.linalg.lstsq(bled_codes[:, genes[s]], pixel_colors[:, s], rcond=None)[0]\n        residual[s] = pixel_colors[:, s] - bled_codes[:, genes[s]] @ coefs[s]\n    return residual, coefs\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs.fit_coefs_weight","title":"<code>fit_coefs_weight(bled_codes, pixel_colors, genes, weight)</code>","text":"<p>Old method before Jax. This finds the least squared solution for how the <code>n_genes</code> <code>bled_codes</code> can best explain each <code>pixel_color</code>. Can also find weighted least squared solution if <code>weight</code> provided.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_colors</code> <code>np.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_pixels]</code> if <code>n_genes==1</code> otherwise  <code>float [(n_rounds x n_channels)]</code>. Flattened then transposed pixel colors which usually has the shape <code>[n_pixels x n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>np.ndarray</code> <p><code>int [n_pixels x n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.</p> required <code>weight</code> <code>np.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. <code>weight[s, i]</code> is the weight to be applied to round_channel <code>i</code> when computing coefficient of each <code>bled_code</code> for pixel <code>s</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>residual - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel_colors after removing bled_codes with coefficients specified by coef.</li> </ul> <code>np.ndarray</code> <ul> <li>coefs - <code>float [n_pixels x n_genes_add]</code> if n_genes == 1 otherwise <code>float [n_genes]</code> if n_pixels == 1. coefficient found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def fit_coefs_weight(bled_codes: np.ndarray, pixel_colors: np.ndarray, genes: np.ndarray,\n                     weight: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Old method before Jax.\n    This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`.\n    Can also find weighted least squared solution if `weight` provided.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1`\n            otherwise  `float [(n_rounds x n_channels)]`.\n            Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`.\n        genes: `int [n_pixels x n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.\n        weight: `float [n_pixels x (n_rounds x n_channels)]`.\n            `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each\n            `bled_code` for pixel `s`.\n\n    Returns:\n        - residual - `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel_colors after removing bled_codes with coefficients specified by coef.\n        - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1.\n            coefficient found through least squares fitting for each gene.\n\n    \"\"\"\n    n_pixels = pixel_colors.shape[1]\n    residual = np.zeros((n_pixels, pixel_colors.shape[0]))\n    coefs = np.zeros_like(genes, dtype=float)\n    pixel_colors = pixel_colors * weight.transpose()\n    for s in range(n_pixels):\n        bled_codes_s = bled_codes[:, genes[s]] * weight[s][:, np.newaxis]\n        coefs[s] = np.linalg.lstsq(bled_codes_s, pixel_colors[:, s], rcond=None)[0]\n        residual[s] = pixel_colors[:, s] - bled_codes_s @ coefs[s]\n    residual = residual / weight\n    return residual, coefs\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_all_coefs","title":"<code>get_all_coefs(pixel_colors, bled_codes, background_shift, dp_shift, dp_thresh, alpha, beta, max_genes, weight_coef_fit=False, track=False)</code>","text":"<p>This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes.</p> <p>Note</p> <p>Background vectors are fitted first and then not updated again.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_colors</code> <code>np.ndarray</code> <p><code>float [n_pixels x n_rounds x n_channels]</code>. Pixel colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x n_rounds x n_channels]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>background_shift</code> <code>float</code> <p>When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels.</p> required <code>dp_shift</code> <code>float</code> <p>When finding <code>dot_product_score</code> between residual <code>pixel_colors</code> and <code>bled_codes</code>, this is applied to normalisation of <code>pixel_colors</code> to limit boost of weak pixels.</p> required <code>dp_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added at each iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_standard_deviation</code>, by how much to increase variance as genes added.</p> required <code>beta</code> <code>float</code> <p>Used for <code>fitting_standard_deviation</code>, the variance with no genes added (<code>coef=0</code>) is <code>beta**2</code>.</p> required <code>max_genes</code> <code>int</code> <p>Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP.</p> required <code>weight_coef_fit</code> <code>bool</code> <p>If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor.</p> <code>False</code> <code>track</code> <code>bool</code> <p>If <code>True</code> and one pixel, info about genes added at each step returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, dict]]</code> <p>gene_coefs - <code>float32 [n_pixels x n_genes]</code>. <code>gene_coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>g</code> found by the omp algorithm. Most are zero.</p> <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, dict]]</code> <p>background_coefs - <code>float32 [n_pixels x n_channels]</code>. coefficient value for each background vector found for each pixel.</p> <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, dict]]</code> <p>track_info - dictionary containing info about genes added at each step returned if <code>track == True</code> -</p> <ul> <li><code>background_codes</code> - <code>float [n_channels x n_rounds x n_channels]</code>.     <code>background_codes[c]</code> is the background vector for channel <code>c</code> with L2 norm of 1.</li> <li><code>background_coefs</code> - <code>float [n_channels]</code>.     <code>background_coefs[c]</code> is the coefficient value for <code>background_codes[c]</code>.</li> <li><code>gene_added</code> - <code>int [n_genes_added + 2]</code>.     <code>gene_added[0]</code> and <code>gene_added[1]</code> are -1.     <code>gene_added[2+i]</code> is the <code>ith</code> gene that was added.</li> <li><code>residual</code> - <code>float [(n_genes_added + 2) x n_rounds x n_channels]</code>.     <code>residual[0]</code> is the initial <code>pixel_color</code>.     <code>residual[1]</code> is the post background <code>pixel_color</code>.     <code>residual[2+i]</code> is the <code>pixel_color</code> after removing gene <code>gene_added[2+i]</code>.</li> <li><code>coef</code> - <code>float [(n_genes_added + 2) x n_genes]</code>.     <code>coef[0]</code> and <code>coef[1]</code> are all 0.     <code>coef[2+i]</code> are the coefficients for all genes after the ith gene has been added.</li> <li><code>dot_product</code> - <code>float [n_genes_added + 2]</code>.     <code>dot_product[0]</code> and <code>dot_product[1]</code> are 0.     <code>dot_product[2+i]</code> is the dot product for the gene <code>gene_added[2+i]</code>.</li> <li><code>inverse_var</code> - <code>float [(n_genes_added + 2) x n_rounds x n_channels]</code>.     <code>inverse_var[0]</code> and <code>inverse_var[1]</code> are all 0.     <code>inverse_var[2+i]</code> is the weighting used to compute <code>dot_product[2+i]</code>,      which down-weights rounds/channels for which a gene has already been fitted.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def get_all_coefs(pixel_colors: np.ndarray, bled_codes: np.ndarray, background_shift: float,\n                  dp_shift: float, dp_thresh: float, alpha: float, beta: float, max_genes: int,\n                  weight_coef_fit: bool = False,\n                  track: bool = False) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, dict]]:\n\"\"\"\n    This performs omp on every pixel, the stopping criterion is that the dot_product_score\n    when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes.\n\n    !!! note\n        Background vectors are fitted first and then not updated again.\n\n    Args:\n        pixel_colors: `float [n_pixels x n_rounds x n_channels]`.\n            Pixel colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x n_rounds x n_channels]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        background_shift: When fitting background,\n            this is applied to weighting of each background vector to limit boost of weak pixels.\n        dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`,\n            this is applied to normalisation of `pixel_colors` to limit boost of weak pixels.\n        dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added at each iteration.\n        alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added.\n        beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`.\n        max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP.\n        weight_coef_fit: If False, coefs are found through normal least squares fitting.\n            If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor.\n        track: If `True` and one pixel, info about genes added at each step returned.\n\n    Returns:\n        gene_coefs - `float32 [n_pixels x n_genes]`.\n            `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero.\n        background_coefs - `float32 [n_pixels x n_channels]`.\n            coefficient value for each background vector found for each pixel.\n        track_info - dictionary containing info about genes added at each step returned if `track == True` -\n\n            - `background_codes` - `float [n_channels x n_rounds x n_channels]`.\n                `background_codes[c]` is the background vector for channel `c` with L2 norm of 1.\n            - `background_coefs` - `float [n_channels]`.\n                `background_coefs[c]` is the coefficient value for `background_codes[c]`.\n            - `gene_added` - `int [n_genes_added + 2]`.\n                `gene_added[0]` and `gene_added[1]` are -1.\n                `gene_added[2+i]` is the `ith` gene that was added.\n            - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`.\n                `residual[0]` is the initial `pixel_color`.\n                `residual[1]` is the post background `pixel_color`.\n                `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`.\n            - `coef` - `float [(n_genes_added + 2) x n_genes]`.\n                `coef[0]` and `coef[1]` are all 0.\n                `coef[2+i]` are the coefficients for all genes after the ith gene has been added.\n            - `dot_product` - `float [n_genes_added + 2]`.\n                `dot_product[0]` and `dot_product[1]` are 0.\n                `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`.\n            - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`.\n                `inverse_var[0]` and `inverse_var[1]` are all 0.\n                `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`,\n                 which down-weights rounds/channels for which a gene has already been fitted.\n\n    \"\"\"\n    n_pixels = pixel_colors.shape[0]\n    n_genes, n_rounds, n_channels = bled_codes.shape\n\n    no_verbose = n_pixels &lt; 1000  # show progress bar with more than 1000 pixels.\n    if track:\n        return_track = True\n        if n_pixels == 1:\n            track_info = {'residual': np.zeros((max_genes+3, n_rounds, n_channels)),\n                          'background_codes': None, 'background_coefs': None,\n                          'inverse_var': np.zeros((max_genes+3, n_rounds, n_channels)),\n                          'coef': np.zeros((max_genes+3, n_genes+n_channels)), 'dot_product': np.zeros(max_genes+3),\n                          'gene_added': np.ones(max_genes+3, dtype=int) * -1}\n            track_info['residual'][0] = pixel_colors[0]\n        else:\n            warnings.warn(f'Can only get track info if running on one pixel, but there are {n_pixels} pixels '\n                          f'so not getting track info.')\n            track = False\n            track_info = None\n    else:\n        return_track = False\n\n    # Fit background and override initial pixel_colors\n    gene_coefs = np.zeros((n_pixels, n_genes), dtype=np.float32)  # coefs of all genes and background\n    pixel_colors, background_coefs, background_codes = fit_background(pixel_colors, background_shift)\n\n    if track:\n        track_info['residual'][1] = pixel_colors[0]\n        track_info['background_codes'] = background_codes\n        track_info['background_coefs'] = background_coefs[0]\n\n    background_genes = np.arange(n_genes, n_genes + n_channels)\n\n    # colors and codes for get_best_gene function\n    # Includes background as if background is the best gene, iteration ends.\n    # uses residual color as used to find next gene to add.\n    bled_codes = bled_codes.reshape((n_genes, -1))\n    all_codes = np.concatenate((bled_codes, background_codes.reshape(n_channels, -1)))\n    bled_codes = bled_codes.transpose()\n\n    # colors and codes for fit_coefs function (No background as this is not updated again).\n    # always uses post background color as coefficients for all genes re-estimated at each iteration.\n    pixel_colors = pixel_colors.reshape((n_pixels, -1))\n\n    continue_pixels = np.arange(n_pixels)\n    with tqdm(total=max_genes, disable=no_verbose) as pbar:\n        pbar.set_description('Finding OMP coefficients for each pixel')\n        for i in range(max_genes):\n            if i == 0:\n                # Background coefs don't change, hence contribution to variance won't either.\n                added_genes, pass_score_thresh, background_variance, best_score = \\\n                    get_best_gene_first_iter(pixel_colors, all_codes, background_coefs, dp_shift,\n                                             dp_thresh, alpha, beta, background_genes)\n                inverse_var = 1 / background_variance\n                pixel_colors = pixel_colors.transpose()\n            else:\n                # only continue with pixels for which dot product score exceeds threshold\n                i_added_genes, pass_score_thresh, inverse_var, best_score = \\\n                    get_best_gene(residual_pixel_colors, all_codes, i_coefs, added_genes, dp_shift,\n                                  dp_thresh, alpha, background_genes, background_variance)\n\n                # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding.\n                fail_score_thresh = np.invert(pass_score_thresh)\n                gene_coefs[np.asarray(continue_pixels[fail_score_thresh])[:, np.newaxis],\n                           np.asarray(added_genes[fail_score_thresh])] = i_coefs[fail_score_thresh]\n\n            continue_pixels = continue_pixels[pass_score_thresh]\n            n_continue = np.size(continue_pixels)\n            pbar.set_postfix({'n_pixels': n_continue})\n            if n_continue == 0:\n                if track:\n                    track_info['inverse_var'][i + 2] = inverse_var.reshape(n_rounds, n_channels)\n                    track_info['dot_product'][i + 2] = best_score[0]\n                    if i == 0:\n                        track_info['gene_added'][i + 2] = added_genes\n                    else:\n                        track_info['gene_added'][i + 2] = i_added_genes\n                        added_genes_fail = np.hstack((added_genes, i_added_genes[:, np.newaxis]))\n                        # Need to usee all_codes here to deal with case where the best gene is background\n                        if weight_coef_fit:\n                            residual_pixel_colors_fail, i_coefs_fail = \\\n                                fit_coefs_weight(all_codes.T, pixel_colors, added_genes_fail, np.sqrt(inverse_var))\n                        else:\n                            residual_pixel_colors_fail, i_coefs_fail = fit_coefs(all_codes.T, pixel_colors, added_genes_fail)\n                        track_info['residual'][i + 2] = residual_pixel_colors_fail.reshape(n_rounds, n_channels)\n                        track_info['coef'][i + 2][added_genes_fail] = i_coefs_fail\n                    # Only save info where gene is actually added or for final case where not added.\n                    for key in track_info.keys():\n                        if 'background' not in key:\n                            track_info[key] = track_info[key][:i+3]\n                break\n\n            if i == 0:\n                added_genes = added_genes[pass_score_thresh, np.newaxis]\n            else:\n                added_genes = np.hstack((added_genes[pass_score_thresh], i_added_genes[pass_score_thresh, np.newaxis]))\n            pixel_colors = pixel_colors[:, pass_score_thresh]\n            background_variance = background_variance[pass_score_thresh]\n            inverse_var = inverse_var[pass_score_thresh]\n\n            if weight_coef_fit:\n                residual_pixel_colors, i_coefs = fit_coefs_weight(bled_codes, pixel_colors, added_genes,\n                                                                  np.sqrt(inverse_var))\n            else:\n                residual_pixel_colors, i_coefs = fit_coefs(bled_codes, pixel_colors, added_genes)\n\n            if i == max_genes-1:\n                # Add pixels to final gene_coefs when reach end of iteration.\n                gene_coefs[continue_pixels[:, np.newaxis], added_genes] = i_coefs\n\n            if track:\n                track_info['residual'][i + 2] = residual_pixel_colors.reshape(n_rounds, n_channels)\n                track_info['inverse_var'][i + 2] = inverse_var.reshape(n_rounds, n_channels)\n                track_info['coef'][i + 2][added_genes] = i_coefs\n                track_info['dot_product'][i + 2] = best_score[0]\n                track_info['gene_added'][i + 2] = added_genes[0][-1]\n\n            pbar.update(1)\n    pbar.close()\n    if track:\n        # Only return\n        no_gene_add_ind = np.where(track_info['gene_added'] == -1)[0]\n        no_gene_add_ind = no_gene_add_ind[no_gene_add_ind &gt;= 2]\n        if len(no_gene_add_ind) &gt; 0:\n            final_ind = no_gene_add_ind.min()\n    if return_track:\n        return gene_coefs.astype(np.float32), background_coefs.astype(np.float32), track_info\n    else:\n        return gene_coefs.astype(np.float32), background_coefs.astype(np.float32)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene","title":"<code>get_best_gene(residual_pixel_colors, all_bled_codes, coefs, genes_added, norm_shift, score_thresh, alpha, background_genes, background_var)</code>","text":"<p>Finds the <code>best_gene</code> to add next to each pixel based on the dot product score with each <code>bled_code</code>. If <code>best_gene[s]</code> is in <code>background_genes</code>, already in <code>genes_added[s]</code> or <code>best_score[s] &lt; score_thresh</code>, then <code>pass_score_thresh[s] = False</code>.</p> <p>Note</p> <p>The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_colors</code> <code>np.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel colors from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>coefs</code> <code>np.ndarray</code> <p><code>float [n_pixels x n_genes_added]</code>. <code>coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>genes_added[g]</code> found by the omp algorithm on previous iteration. All are non-zero.</p> required <code>genes_added</code> <code>np.array</code> <p><code>int [n_pixels x n_genes_added]</code> Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel <code>s</code> is set to one of <code>genes_added[s]</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>background_genes</code> <code>np.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel <code>s</code> is set to one of <code>background_genes</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <code>background_var</code> <code>np.array</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Contribution of background genes to variance (which does not change throughout omp iterations)  i.e. <code>background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>best_gene - <code>int [n_pixels]</code>. <code>best_gene[s]</code> is the best gene to add to pixel <code>s</code> next.</li> </ul> <code>np.ndarray</code> <ul> <li>pass_score_thresh - <code>bool [n_pixels]</code>. <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>np.ndarray</code> <ul> <li>inverse_var - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution.</li> </ul> <code>np.ndarray</code> <ul> <li>best_score - <code>float [n_pixels]</code>. <code>dot_product_score</code> for spot <code>s</code> with gene <code>best_gene[s]</code>.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def get_best_gene(residual_pixel_colors: np.ndarray, all_bled_codes: np.ndarray, coefs: np.ndarray,\n                  genes_added: np.array, norm_shift: float, score_thresh: float, alpha: float,\n                  background_genes: np.ndarray,\n                  background_var: np.array) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`.\n    If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] &lt; score_thresh`,\n    then `pass_score_thresh[s] = False`.\n\n    !!!note\n        The variance computed is based on maximum likelihood estimation - it accounts for all genes and background\n        fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a\n        weighting for omp fitting or choosing the next gene,\n        the rounds/channels which already have genes in will contribute less.\n\n    Args:\n        residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel colors from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        coefs: `float [n_pixels x n_genes_added]`.\n            `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous\n            iteration. All are non-zero.\n        genes_added: `int [n_pixels x n_genes_added]`\n            Indices of genes added to each pixel from previous iteration of omp.\n            If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False.\n        background_var: `float [n_pixels x (n_rounds x n_channels)]`.\n            Contribution of background genes to variance (which does not change throughout omp iterations)  i.e.\n            `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`.\n\n    Returns:\n        - best_gene - `int [n_pixels]`.\n            `best_gene[s]` is the best gene to add to pixel `s` next.\n        - pass_score_thresh - `bool [n_pixels]`.\n            `True` if `best_score &gt; score_thresh`.\n        - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`.\n            Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration.\n            Includes both background and gene contribution.\n        - best_score - `float [n_pixels]`.\n            `dot_product_score` for spot `s` with gene `best_gene[s]`.\n    \"\"\"\n\n    n_pixels, n_genes_added = genes_added.shape\n    n_genes = all_bled_codes.shape[0]\n    coefs_all = np.zeros((n_pixels, n_genes))\n    pixel_ind = np.repeat(np.arange(n_pixels), n_genes_added)\n    coefs_all[(pixel_ind, genes_added.flatten())] = coefs.flatten()\n\n    inverse_var = 1 / (coefs_all ** 2 @ all_bled_codes ** 2 * alpha + background_var)\n    ignore_genes = np.concatenate((genes_added, np.tile(background_genes, [n_pixels, 1])), axis=1)\n    best_gene, pass_score_thresh, best_score = \\\n        get_best_gene_base(residual_pixel_colors, all_bled_codes, norm_shift, score_thresh, inverse_var, ignore_genes)\n\n    return best_gene, pass_score_thresh, inverse_var, best_score\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene_base","title":"<code>get_best_gene_base(residual_pixel_colors, all_bled_codes, norm_shift, score_thresh, inverse_var, ignore_genes)</code>","text":"<p>Computes the <code>dot_product_score</code> between <code>residual_pixel_color</code> and each code in <code>all_bled_codes</code>. If <code>best_score</code> is less than <code>score_thresh</code> or if the corresponding <code>best_gene</code> is in <code>ignore_genes</code>, then <code>pass_score_thresh</code> will be False.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_colors</code> <code>np.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel color from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>inverse_var</code> <code>np.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as <code>weight_squared</code> when computing <code>dot_product_score</code>.</p> required <code>ignore_genes</code> <code>np.ndarray</code> <p><code>int [n_pixels x n_genes_ignore]</code>. If <code>best_gene[s]</code> is one of these, <code>pass_score_thresh[s]</code> will be <code>False</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>best_gene - <code>int [n_pixels]</code>. <code>best_gene[s]</code> is the best gene to add to pixel <code>s</code> next.</li> </ul> <code>np.ndarray</code> <ul> <li>pass_score_thresh - <code>bool [n_pixels]</code>. <code>True</code> if <code>best_score[s] &gt; score_thresh</code> and <code>best_gene[s]</code> not in <code>ignore_genes</code>.</li> </ul> <code>np.ndarray</code> <ul> <li>best_score - <code>float [n_pixels]</code>. <code>dot_product_score</code> for spot <code>s</code> with gene <code>best_gene[s]</code>.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def get_best_gene_base(residual_pixel_colors: np.ndarray, all_bled_codes: np.ndarray,\n                       norm_shift: float, score_thresh: float, inverse_var: np.ndarray,\n                       ignore_genes: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`.\n    If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`,\n    then `pass_score_thresh` will be False.\n\n    Args:\n        residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel color from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        inverse_var: `float [(n_rounds x n_channels)]`.\n            Inverse of variance in each round/channel based on genes fit on previous iteration.\n            Used as `weight_squared` when computing `dot_product_score`.\n        ignore_genes: `int [n_pixels x n_genes_ignore]`.\n            If `best_gene[s]` is one of these, `pass_score_thresh[s]` will be `False`.\n\n    Returns:\n        - best_gene - `int [n_pixels]`.\n            `best_gene[s]` is the best gene to add to pixel `s` next.\n        - pass_score_thresh - `bool [n_pixels]`.\n            `True` if `best_score[s] &gt; score_thresh` and `best_gene[s]` not in `ignore_genes`.\n        - best_score - `float [n_pixels]`.\n            `dot_product_score` for spot `s` with gene `best_gene[s]`.\n\n    \"\"\"\n    # calculate score including background genes as if best gene is background, then stop iteration.\n    all_scores = dot_product_score(residual_pixel_colors, all_bled_codes, norm_shift, inverse_var)\n    best_gene = np.argmax(np.abs(all_scores), axis=1)\n    # if best_gene is in ignore_gene, set score below score_thresh.\n    is_ignore_gene = (best_gene[:, np.newaxis] == ignore_genes).any(axis=1)\n    best_score = all_scores[(np.arange(best_gene.shape[0]), best_gene)] * np.invert(is_ignore_gene)\n    pass_score_thresh = np.abs(best_score) &gt; score_thresh\n    return best_gene, pass_score_thresh, best_score\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene_first_iter","title":"<code>get_best_gene_first_iter(residual_pixel_colors, all_bled_codes, background_coefs, norm_shift, score_thresh, alpha, beta, background_genes)</code>","text":"<p>Finds the <code>best_gene</code> to add next based on the dot product score with each <code>bled_code</code>. If <code>best_gene</code> is in <code>background_genes</code> or <code>best_score &lt; score_thresh</code> then <code>pass_score_thresh = False</code>. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause <code>pass_score_thresh</code> to be <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_colors</code> <code>np.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel color from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>background_coefs</code> <code>np.ndarray</code> <p><code>float [n_pixels x n_channels]</code>. <code>coefs[g]</code> is the weighting for gene <code>background_genes[g]</code> found by the omp algorithm.  All are non-zero.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>beta</code> <code>float</code> <p>Used for <code>fitting_variance</code>, the variance with no genes added (<code>coef=0</code>) is <code>beta**2</code>.</p> required <code>background_genes</code> <code>np.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel <code>s</code> is set to one of <code>background_genes</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>best_gene - <code>int [n_pixels]</code>. <code>best_gene[s]</code> is the best gene to add to pixel <code>s</code> next.</li> </ul> <code>np.ndarray</code> <ul> <li>pass_score_thresh - <code>bool [n_pixels]</code>. <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>np.ndarray</code> <ul> <li>background_var - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Variance in each round/channel based on just the background.</li> </ul> <code>np.ndarray</code> <ul> <li>best_score - <code>float [n_pixels]</code>. <code>dot_product_score</code> for spot <code>s</code> with gene <code>best_gene[s]</code>.</li> </ul> Source code in <code>coppafish/omp/coefs.py</code> <pre><code>def get_best_gene_first_iter(residual_pixel_colors: np.ndarray, all_bled_codes: np.ndarray,\n                             background_coefs: np.ndarray, norm_shift: float,\n                             score_thresh: float, alpha: float, beta: float,\n                             background_genes: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next based on the dot product score with each `bled_code`.\n    If `best_gene` is in `background_genes` or `best_score &lt; score_thresh` then `pass_score_thresh = False`.\n    Different for first iteration as no actual non-zero gene coefficients to consider when computing variance\n    or genes that can be added which will cause `pass_score_thresh` to be `False`.\n\n    Args:\n        residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel color from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        background_coefs: `float [n_pixels x n_channels]`.\n            `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm.\n             All are non-zero.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False.\n\n    Returns:\n        - best_gene - `int [n_pixels]`.\n            `best_gene[s]` is the best gene to add to pixel `s` next.\n        - pass_score_thresh - `bool [n_pixels]`.\n            `True` if `best_score &gt; score_thresh`.\n        - background_var - `float [n_pixels x (n_rounds x n_channels)]`.\n            Variance in each round/channel based on just the background.\n        - best_score - `float [n_pixels]`.\n            `dot_product_score` for spot `s` with gene `best_gene[s]`.\n\n    \"\"\"\n    background_var = np.square(background_coefs) @ np.square(all_bled_codes[background_genes]) * alpha + beta ** 2\n    ignore_genes = np.tile(background_genes, [background_var.shape[0], 1])\n    best_gene, pass_score_thresh, best_score = \\\n        get_best_gene_base(residual_pixel_colors, all_bled_codes, norm_shift, score_thresh, 1 / background_var,\n                           ignore_genes)\n    return best_gene, pass_score_thresh, background_var, best_score\n</code></pre>"},{"location":"code/omp/coefs/#optimised","title":"Optimised","text":""},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs","title":"<code>fit_coefs(bled_codes, pixel_colors, genes)</code>","text":"<p>This finds the least squared solution for how the <code>n_genes_add</code> <code>bled_codes</code> indicated by <code>genes[s]</code> can best explain <code>pixel_colors[:, s]</code> for each pixel s.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_colors</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_pixels]</code>. Flattened then transposed <code>pixel_colors</code> which usually has the shape <code>[n_pixels x n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>jnp.ndarray</code> <p><code>int [n_pixels x n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel_colors after removing bled_codes with coefficients specified by coefs.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coefs - <code>float [n_pixels x n_genes_add]</code>. Coefficients found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>@jax.jit\ndef fit_coefs(bled_codes: jnp.ndarray, pixel_colors: jnp.ndarray,\n              genes: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]`\n    can best explain `pixel_colors[:, s]` for each pixel s.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`.\n            Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`.\n        genes: `int [n_pixels x n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.\n\n    Returns:\n        - residual - `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel_colors after removing bled_codes with coefficients specified by coefs.\n        - coefs - `float [n_pixels x n_genes_add]`.\n            Coefficients found through least squares fitting for each gene.\n    \"\"\"\n    return jax.vmap(fit_coefs_single, in_axes=(None, 1, 0), out_axes=(0, 0))(bled_codes, pixel_colors, genes)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_single","title":"<code>fit_coefs_single(bled_codes, pixel_color, genes)</code>","text":"<p>This finds the least squared solution for how the <code>n_genes_add</code> <code>bled_codes</code> indicated by <code>genes</code> can best explain <code>pixel_color</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_color</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Flattened <code>pixel_color</code> which usually has the shape <code>[n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>jnp.ndarray</code> <p><code>int [n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain pixel_color.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [(n_rounds x n_channels)]</code>. Residual pixel_color after removing bled_codes with coefficients specified by coefs.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coefs - <code>float [n_genes_add]</code>. Coefficients found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def fit_coefs_single(bled_codes: jnp.ndarray, pixel_color: jnp.ndarray,\n                     genes: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes`\n    can best explain `pixel_color`.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_color: `float [(n_rounds x n_channels)]`.\n            Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`.\n        genes: `int [n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain pixel_color.\n\n    Returns:\n        - residual - `float [(n_rounds x n_channels)]`.\n            Residual pixel_color after removing bled_codes with coefficients specified by coefs.\n        - coefs - `float [n_genes_add]`.\n            Coefficients found through least squares fitting for each gene.\n    \"\"\"\n    coefs = jnp.linalg.lstsq(bled_codes[:, genes], pixel_color)[0]\n    residual = pixel_color - jnp.matmul(bled_codes[:, genes], coefs)\n    return residual, coefs\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_weight","title":"<code>fit_coefs_weight(bled_codes, pixel_colors, genes, weight)</code>","text":"<p>This finds the weighted least squared solution for how the <code>n_genes_add</code> <code>bled_codes</code> indicated by <code>genes[s]</code> can best explain <code>pixel_colors[:, s]</code> for each pixel s. The <code>weight</code> indicates which rounds/channels should have more influence when finding the coefficients of each gene.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_colors</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_pixels]</code>. Flattened then transposed <code>pixel_colors</code> which usually has the shape <code>[n_pixels x n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>jnp.ndarray</code> <p><code>int [n_pixels x n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.</p> required <code>weight</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. <code>weight[s, i]</code> is the weight to be applied to round_channel <code>i</code> when computing coefficient of each <code>bled_code</code> for pixel <code>s</code>.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel_colors after removing bled_codes with coefficients specified by coefs.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coefs - <code>float [n_pixels x n_genes_add]</code>. Coefficients found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>@jax.jit\ndef fit_coefs_weight(bled_codes: jnp.ndarray, pixel_colors: jnp.ndarray, genes: jnp.ndarray,\n                     weight: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]`\n    can best explain `pixel_colors[:, s]` for each pixel s. The `weight` indicates which rounds/channels should\n    have more influence when finding the coefficients of each gene.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`.\n            Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`.\n        genes: `int [n_pixels x n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain each pixel_color.\n        weight: `float [n_pixels x (n_rounds x n_channels)]`.\n            `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each\n            `bled_code` for pixel `s`.\n\n    Returns:\n        - residual - `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel_colors after removing bled_codes with coefficients specified by coefs.\n        - coefs - `float [n_pixels x n_genes_add]`.\n            Coefficients found through least squares fitting for each gene.\n    \"\"\"\n    return jax.vmap(fit_coefs_weight_single, in_axes=(None, 1, 0, 0), out_axes=(0, 0))(bled_codes, pixel_colors, genes,\n                                                                                       weight)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_weight_single","title":"<code>fit_coefs_weight_single(bled_codes, pixel_color, genes, weight)</code>","text":"<p>This finds the weighted least squared solution for how the <code>n_genes_add</code> <code>bled_codes</code> indicated by <code>genes</code> can best explain <code>pixel_color</code>. The <code>weight</code> indicates which rounds/channels should have more influence when finding the coefficients of each gene.</p> <p>Parameters:</p> Name Type Description Default <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels) x n_genes]</code>. Flattened then transposed bled codes which usually has the shape <code>[n_genes x n_rounds x n_channels]</code>.</p> required <code>pixel_color</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Flattened <code>pixel_color</code> which usually has the shape <code>[n_rounds x n_channels]</code>.</p> required <code>genes</code> <code>jnp.ndarray</code> <p><code>int [n_genes_add]</code>. Indices of codes in bled_codes to find coefficients for which best explain pixel_color.</p> required <code>weight</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. <code>weight[i]</code> is the weight to be applied to round_channel <code>i</code> when computing coefficient of each <code>bled_code</code>.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>residual - <code>float [(n_rounds x n_channels)]</code>. Residual pixel_color after removing bled_codes with coefficients specified by coefs.</li> </ul> <code>jnp.ndarray</code> <ul> <li>coefs - <code>float [n_genes_add]</code>. Coefficients found through least squares fitting for each gene.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def fit_coefs_weight_single(bled_codes: jnp.ndarray, pixel_color: jnp.ndarray, genes: jnp.ndarray,\n                            weight: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes`\n    can best explain `pixel_color`. The `weight` indicates which rounds/channels should have more influence when finding\n    the coefficients of each gene.\n\n    Args:\n        bled_codes: `float [(n_rounds x n_channels) x n_genes]`.\n            Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`.\n        pixel_color: `float [(n_rounds x n_channels)]`.\n            Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`.\n        genes: `int [n_genes_add]`.\n            Indices of codes in bled_codes to find coefficients for which best explain pixel_color.\n        weight: `float [(n_rounds x n_channels)]`.\n            `weight[i]` is the weight to be applied to round_channel `i` when computing coefficient of each\n            `bled_code`.\n\n    Returns:\n        - residual - `float [(n_rounds x n_channels)]`.\n            Residual pixel_color after removing bled_codes with coefficients specified by coefs.\n        - coefs - `float [n_genes_add]`.\n            Coefficients found through least squares fitting for each gene.\n    \"\"\"\n    coefs = jnp.linalg.lstsq(bled_codes[:, genes] * weight[:, jnp.newaxis], pixel_color * weight)[0]\n    residual = pixel_color * weight - jnp.matmul(bled_codes[:, genes] * weight[:, jnp.newaxis], coefs)\n    return residual / weight, coefs\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_all_coefs","title":"<code>get_all_coefs(pixel_colors, bled_codes, background_shift, dp_shift, dp_thresh, alpha, beta, max_genes, weight_coef_fit=False)</code>","text":"<p>This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes.</p> <p>Note</p> <p>Background vectors are fitted first and then not updated again.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_colors</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x n_rounds x n_channels]</code>. Pixel colors normalised to equalise intensities between channels (and rounds).</p> required <code>bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x n_rounds x n_channels]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>.</p> required <code>background_shift</code> <code>float</code> <p>When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels.</p> required <code>dp_shift</code> <code>float</code> <p>When finding <code>dot_product_score</code> between residual <code>pixel_colors</code> and <code>bled_codes</code>, this is applied to normalisation of <code>pixel_colors</code> to limit boost of weak pixels.</p> required <code>dp_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added at each iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_standard_deviation</code>, by how much to increase variance as genes added.</p> required <code>beta</code> <code>float</code> <p>Used for <code>fitting_standard_deviation</code>, the variance with no genes added (<code>coef=0</code>) is <code>beta**2</code>.</p> required <code>max_genes</code> <code>int</code> <p>Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP.</p> required <code>weight_coef_fit</code> <code>bool</code> <p>If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor.</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>gene_coefs - <code>float32 [n_pixels x n_genes]</code>. <code>gene_coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>g</code> found by the omp algorithm. Most are zero.</li> </ul> <code>np.ndarray</code> <ul> <li>background_coefs - <code>float32 [n_pixels x n_channels]</code>. coefficient value for each background vector found for each pixel.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def get_all_coefs(pixel_colors: jnp.ndarray, bled_codes: jnp.ndarray, background_shift: float,\n                  dp_shift: float, dp_thresh: float, alpha: float, beta: float, max_genes: int,\n                  weight_coef_fit: bool = False) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    This performs omp on every pixel, the stopping criterion is that the dot_product_score\n    when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes.\n\n    !!! note\n        Background vectors are fitted first and then not updated again.\n\n    Args:\n        pixel_colors: `float [n_pixels x n_rounds x n_channels]`.\n            Pixel colors normalised to equalise intensities between channels (and rounds).\n        bled_codes: `float [n_genes x n_rounds x n_channels]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n        background_shift: When fitting background,\n            this is applied to weighting of each background vector to limit boost of weak pixels.\n        dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`,\n            this is applied to normalisation of `pixel_colors` to limit boost of weak pixels.\n        dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added at each iteration.\n        alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added.\n        beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`.\n        max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP.\n        weight_coef_fit: If False, coefs are found through normal least squares fitting.\n            If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor.\n\n    Returns:\n        - gene_coefs - `float32 [n_pixels x n_genes]`.\n            `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero.\n        - background_coefs - `float32 [n_pixels x n_channels]`.\n            coefficient value for each background vector found for each pixel.\n    \"\"\"\n    n_pixels = pixel_colors.shape[0]\n\n    check_spot = np.random.randint(n_pixels)\n    diff_to_int = jnp.round(pixel_colors[check_spot]).astype(int) - pixel_colors[check_spot]\n    if jnp.abs(diff_to_int).max() == 0:\n        raise ValueError(f\"pixel_coefs should be found using normalised pixel_colors.\"\n                         f\"\\nBut for pixel {check_spot}, pixel_colors given are integers indicating they are \"\n                         f\"the raw intensities.\")\n\n    n_genes, n_rounds, n_channels = bled_codes.shape\n    if not utils.errors.check_shape(pixel_colors, [n_pixels, n_rounds, n_channels]):\n        raise utils.errors.ShapeError('pixel_colors', pixel_colors.shape, (n_pixels, n_rounds, n_channels))\n    no_verbose = n_pixels &lt; 1000  # show progress bar with more than 1000 pixels.\n\n    # Fit background and override initial pixel_colors\n    gene_coefs = np.zeros((n_pixels, n_genes), dtype=np.float32)  # coefs of all genes and background\n    pixel_colors, background_coefs, background_codes = fit_background(pixel_colors,\n                                                                      background_shift)\n\n    background_genes = jnp.arange(n_genes, n_genes + n_channels)\n\n    # colors and codes for get_best_gene function\n    # Includes background as if background is the best gene, iteration ends.\n    # uses residual color as used to find next gene to add.\n    bled_codes = bled_codes.reshape((n_genes, -1))\n    all_codes = jnp.concatenate((bled_codes, background_codes.reshape(n_channels, -1)))\n    bled_codes = bled_codes.transpose()\n\n    # colors and codes for fit_coefs function (No background as this is not updated again).\n    # always uses post background color as coefficients for all genes re-estimated at each iteration.\n    pixel_colors = pixel_colors.reshape((n_pixels, -1))\n\n    continue_pixels = jnp.arange(n_pixels)\n    with tqdm(total=max_genes, disable=no_verbose) as pbar:\n        pbar.set_description('Finding OMP coefficients for each pixel')\n        for i in range(max_genes):\n            if i == 0:\n                # Background coefs don't change, hence contribution to variance won't either.\n                added_genes, pass_score_thresh, background_variance = \\\n                    get_best_gene_first_iter(pixel_colors, all_codes, background_coefs, dp_shift,\n                                             dp_thresh, alpha, beta, background_genes)\n                inverse_var = 1 / background_variance\n                pixel_colors = pixel_colors.transpose()\n            else:\n                # only continue with pixels for which dot product score exceeds threshold\n                i_added_genes, pass_score_thresh, inverse_var = \\\n                    get_best_gene(residual_pixel_colors, all_codes, i_coefs, added_genes, dp_shift,\n                                  dp_thresh, alpha, background_genes, background_variance)\n\n                # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding.\n                fail_score_thresh = jnp.invert(pass_score_thresh)\n                # gene_coefs[np.asarray(continue_pixels[fail_score_thresh])] = np.asarray(i_coefs[fail_score_thresh])\n                gene_coefs[np.asarray(continue_pixels[fail_score_thresh])[:, np.newaxis],\n                           np.asarray(added_genes[fail_score_thresh])] = np.asarray(i_coefs[fail_score_thresh])\n\n            continue_pixels = continue_pixels[pass_score_thresh]\n            n_continue = jnp.size(continue_pixels)\n            pbar.set_postfix({'n_pixels': n_continue})\n            if n_continue == 0:\n                break\n            if i == 0:\n                added_genes = added_genes[pass_score_thresh, np.newaxis]\n            else:\n                added_genes = jnp.hstack((added_genes[pass_score_thresh], i_added_genes[pass_score_thresh, jnp.newaxis]))\n            pixel_colors = pixel_colors[:, pass_score_thresh]\n            background_variance = background_variance[pass_score_thresh]\n            inverse_var = inverse_var[pass_score_thresh]\n\n            # Maybe add different fit_coefs for i==0 i.e. can do multiple pixels at once for same gene added.\n            if weight_coef_fit:\n                residual_pixel_colors, i_coefs = fit_coefs_weight(bled_codes, pixel_colors, added_genes,\n                                                                  jnp.sqrt(inverse_var))\n            else:\n                residual_pixel_colors, i_coefs = fit_coefs(bled_codes, pixel_colors, added_genes)\n\n            if i == max_genes-1:\n                # Add pixels to final gene_coefs when reach end of iteration.\n                gene_coefs[np.asarray(continue_pixels)[:, np.newaxis], np.asarray(added_genes)] = np.asarray(i_coefs)\n\n            pbar.update(1)\n    pbar.close()\n\n    return gene_coefs.astype(np.float32), np.asarray(background_coefs).astype(np.float32)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene","title":"<code>get_best_gene(residual_pixel_colors, all_bled_codes, coefs, genes_added, norm_shift, score_thresh, alpha, background_genes, background_var)</code>","text":"<p>Finds the <code>best_gene</code> to add next to each pixel based on the dot product score with each <code>bled_code</code>. If <code>best_gene[s]</code> is in <code>background_genes</code>, already in <code>genes_added[s]</code> or <code>best_score[s] &lt; score_thresh</code>, then <code>pass_score_thresh[s] = False</code>.</p> <p>Note</p> <p>The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_colors</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel colors from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>coefs</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x n_genes_added]</code>. <code>coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>genes_added[g]</code> found by the omp algorithm on previous iteration. All are non-zero.</p> required <code>genes_added</code> <code>jnp.array</code> <p><code>int [n_pixels x n_genes_added]</code> Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel <code>s</code> is set to one of <code>genes_added[s]</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>background_genes</code> <code>jnp.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel <code>s</code> is set to one of <code>background_genes</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <code>background_var</code> <code>jnp.array</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Contribution of background genes to variance (which does not change throughout omp iterations)  i.e. <code>background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2</code>.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>best_gene - <code>int [n_pixels]</code>. <code>best_gene[s]</code> is the best gene to add to pixel <code>s</code> next.</li> </ul> <code>jnp.ndarray</code> <ul> <li>pass_score_thresh - <code>bool [n_pixels]</code>. <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>jnp.ndarray</code> <ul> <li>inverse_var - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=(4, 5, 6))\ndef get_best_gene(residual_pixel_colors: jnp.ndarray, all_bled_codes: jnp.ndarray, coefs: jnp.ndarray,\n                  genes_added: jnp.array, norm_shift: float, score_thresh: float, alpha: float,\n                  background_genes: jnp.ndarray,\n                  background_var: jnp.array) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`.\n    If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] &lt; score_thresh`,\n    then `pass_score_thresh[s] = False`.\n\n    !!!note\n        The variance computed is based on maximum likelihood estimation - it accounts for all genes and background\n        fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a\n        weighting for omp fitting or choosing the next gene,\n        the rounds/channels which already have genes in will contribute less.\n\n    Args:\n        residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel colors from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        coefs: `float [n_pixels x n_genes_added]`.\n            `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous\n            iteration. All are non-zero.\n        genes_added: `int [n_pixels x n_genes_added]`\n            Indices of genes added to each pixel from previous iteration of omp.\n            If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False.\n        background_var: `float [n_pixels x (n_rounds x n_channels)]`.\n            Contribution of background genes to variance (which does not change throughout omp iterations)  i.e.\n            `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`.\n\n    Returns:\n        - best_gene - `int [n_pixels]`.\n            `best_gene[s]` is the best gene to add to pixel `s` next.\n        - pass_score_thresh - `bool [n_pixels]`.\n            `True` if `best_score &gt; score_thresh`.\n        - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`.\n            Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration.\n            Includes both background and gene contribution.\n    \"\"\"\n    return jax.vmap(get_best_gene_single, in_axes=(0, None, 0, 0, None, None, None, None, 0),\n                    out_axes=(0, 0, 0))(residual_pixel_colors, all_bled_codes, coefs, genes_added, norm_shift,\n                                        score_thresh, alpha, background_genes, background_var)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_base","title":"<code>get_best_gene_base(residual_pixel_color, all_bled_codes, norm_shift, score_thresh, inverse_var, ignore_genes)</code>","text":"<p>Computes the <code>dot_product_score</code> between <code>residual_pixel_color</code> and each code in <code>all_bled_codes</code>. If <code>best_score</code> is less than <code>score_thresh</code> or if the corresponding <code>best_gene</code> is in <code>ignore_genes</code>, then <code>pass_score_thresh</code> will be False.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_color</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Residual pixel color from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>inverse_var</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as <code>weight_squared</code> when computing <code>dot_product_score</code>.</p> required <code>ignore_genes</code> <code>jnp.ndarray</code> <p><code>int [n_genes_ignore]</code>. If <code>best_gene</code> is one of these, <code>pass_score_thresh</code> will be <code>False</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <ul> <li>best_gene - The best gene to add next.</li> </ul> <code>bool</code> <ul> <li>pass_score_thresh - <code>True</code> if <code>best_score &gt; score_thresh</code> and <code>best_gene</code> not in <code>ignore_genes</code>.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def get_best_gene_base(residual_pixel_color: jnp.ndarray, all_bled_codes: jnp.ndarray,\n                       norm_shift: float, score_thresh: float, inverse_var: jnp.ndarray,\n                       ignore_genes: jnp.ndarray) -&gt; Tuple[int, bool]:\n\"\"\"\n    Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`.\n    If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`,\n    then `pass_score_thresh` will be False.\n\n    Args:\n        residual_pixel_color: `float [(n_rounds x n_channels)]`.\n            Residual pixel color from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        inverse_var: `float [(n_rounds x n_channels)]`.\n            Inverse of variance in each round/channel based on genes fit on previous iteration.\n            Used as `weight_squared` when computing `dot_product_score`.\n        ignore_genes: `int [n_genes_ignore]`.\n            If `best_gene` is one of these, `pass_score_thresh` will be `False`.\n\n    Returns:\n        - best_gene - The best gene to add next.\n        - pass_score_thresh - `True` if `best_score &gt; score_thresh` and `best_gene` not in `ignore_genes`.\n\n    \"\"\"\n    # calculate score including background genes as if best gene is background, then stop iteration.\n    all_scores = dot_product_score_single(residual_pixel_color, all_bled_codes, norm_shift, inverse_var)\n    best_gene = jnp.argmax(jnp.abs(all_scores))\n    # if best_gene is background, set score below score_thresh.\n    best_score = all_scores[best_gene] * jnp.isin(best_gene, ignore_genes, invert=True)\n    pass_score_thresh = jnp.abs(best_score) &gt; score_thresh\n    return best_gene, pass_score_thresh\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_first_iter","title":"<code>get_best_gene_first_iter(residual_pixel_colors, all_bled_codes, background_coefs, norm_shift, score_thresh, alpha, beta, background_genes)</code>","text":"<p>Finds the <code>best_gene</code> to add next to each pixel based on the dot product score with each <code>bled_code</code>. If <code>best_gene[s]</code> is in <code>background_genes</code> or <code>best_score[s] &lt; score_thresh</code> then <code>pass_score_thresh[s] = False</code>. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause <code>pass_score_thresh</code> to be <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_colors</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x (n_rounds x n_channels)]</code>. Residual pixel colors from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>background_coefs</code> <code>jnp.ndarray</code> <p><code>float [n_pixels x n_channels]</code>. <code>coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>background_genes[g]</code> found by the omp algorithm.  All are non-zero.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>beta</code> <code>float</code> <p>Used for <code>fitting_variance</code>, the variance with no genes added (<code>coef=0</code>) is <code>beta**2</code>.</p> required <code>background_genes</code> <code>jnp.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel <code>s</code> is set to one of <code>background_genes</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li>best_gene - <code>int [n_pixels]</code>. <code>best_gene[s]</code> is the best gene to add to pixel <code>s</code> next.</li> </ul> <code>jnp.ndarray</code> <ul> <li>pass_score_thresh - <code>bool [n_pixels]</code>. <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>jnp.ndarray</code> <ul> <li>background_var - <code>float [n_pixels x (n_rounds x n_channels)]</code>. Variance of each pixel in each round/channel based on just the background.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=(3, 4, 5, 6))\ndef get_best_gene_first_iter(residual_pixel_colors: jnp.ndarray, all_bled_codes: jnp.ndarray,\n                             background_coefs: jnp.ndarray, norm_shift: float,\n                             score_thresh: float, alpha: float, beta: float,\n                             background_genes: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`.\n    If `best_gene[s]` is in `background_genes` or `best_score[s] &lt; score_thresh` then `pass_score_thresh[s] = False`.\n    Different for first iteration as no actual non-zero gene coefficients to consider when computing variance\n    or genes that can be added which will cause `pass_score_thresh` to be `False`.\n\n    Args:\n        residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`.\n            Residual pixel colors from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        background_coefs: `float [n_pixels x n_channels]`.\n            `coefs[s, g]` is the weighting of pixel `s` for gene `background_genes[g]` found by the omp algorithm.\n             All are non-zero.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False.\n\n    Returns:\n        - best_gene - `int [n_pixels]`.\n            `best_gene[s]` is the best gene to add to pixel `s` next.\n        - pass_score_thresh - `bool [n_pixels]`.\n            `True` if `best_score &gt; score_thresh`.\n        - background_var - `float [n_pixels x (n_rounds x n_channels)]`.\n            Variance of each pixel in each round/channel based on just the background.\n\n    \"\"\"\n    return jax.vmap(get_best_gene_first_iter_single, in_axes=(0, None, 0, None, None, None, None, None),\n                    out_axes=(0, 0, 0))(residual_pixel_colors, all_bled_codes, background_coefs, norm_shift,\n                                           score_thresh, alpha, beta, background_genes)\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_first_iter_single","title":"<code>get_best_gene_first_iter_single(residual_pixel_color, all_bled_codes, background_coefs, norm_shift, score_thresh, alpha, beta, background_genes)</code>","text":"<p>Finds the <code>best_gene</code> to add next based on the dot product score with each <code>bled_code</code>. If <code>best_gene</code> is in <code>background_genes</code> or <code>best_score &lt; score_thresh</code> then <code>pass_score_thresh = False</code>. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause <code>pass_score_thresh</code> to be <code>False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_color</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Residual pixel color from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>background_coefs</code> <code>jnp.ndarray</code> <p><code>float [n_channels]</code>. <code>coefs[g]</code> is the weighting for gene <code>background_genes[g]</code> found by the omp algorithm.  All are non-zero.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>beta</code> <code>float</code> <p>Used for <code>fitting_variance</code>, the variance with no genes added (<code>coef=0</code>) is <code>beta**2</code>.</p> required <code>background_genes</code> <code>jnp.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel <code>s</code> is set to one of <code>background_genes</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <p>Returns:</p> Type Description <code>int</code> <ul> <li>best_gene - The best gene to add next.</li> </ul> <code>bool</code> <ul> <li>pass_score_thresh - <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>jnp.ndarray</code> <ul> <li>background_var - <code>float [(n_rounds x n_channels)]</code>. Variance in each round/channel based on just the background.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def get_best_gene_first_iter_single(residual_pixel_color: jnp.ndarray, all_bled_codes: jnp.ndarray,\n                                    background_coefs: jnp.ndarray, norm_shift: float,\n                                    score_thresh: float, alpha: float, beta: float,\n                                    background_genes: jnp.ndarray) -&gt; Tuple[int, bool, jnp.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next based on the dot product score with each `bled_code`.\n    If `best_gene` is in `background_genes` or `best_score &lt; score_thresh` then `pass_score_thresh = False`.\n    Different for first iteration as no actual non-zero gene coefficients to consider when computing variance\n    or genes that can be added which will cause `pass_score_thresh` to be `False`.\n\n    Args:\n        residual_pixel_color: `float [(n_rounds x n_channels)]`.\n            Residual pixel color from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        background_coefs: `float [n_channels]`.\n            `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm.\n             All are non-zero.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False.\n\n    Returns:\n        - best_gene - The best gene to add next.\n        - pass_score_thresh - `True` if `best_score &gt; score_thresh`.\n        - background_var - `float [(n_rounds x n_channels)]`.\n            Variance in each round/channel based on just the background.\n\n    \"\"\"\n    background_var = jnp.square(background_coefs) @ jnp.square(all_bled_codes[background_genes]) * alpha + beta ** 2\n    best_gene, pass_score_thresh = get_best_gene_base(residual_pixel_color, all_bled_codes, norm_shift, score_thresh,\n                                                      1 / background_var, background_genes)\n    return best_gene, pass_score_thresh, background_var\n</code></pre>"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_single","title":"<code>get_best_gene_single(residual_pixel_color, all_bled_codes, coefs, genes_added, norm_shift, score_thresh, alpha, background_genes, background_var)</code>","text":"<p>Finds the <code>best_gene</code> to add next to each pixel based on the dot product score with each <code>bled_code</code>. If <code>best_gene</code> is in <code>background_genes</code>, already in <code>genes_added</code> or <code>best_score &lt; score_thresh</code>, then <code>pass_score_thresh = False</code>.</p> <p>Note</p> <p>The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less.</p> <p>Parameters:</p> Name Type Description Default <code>residual_pixel_color</code> <code>jnp.ndarray</code> <p><code>float [(n_rounds x n_channels)]</code>. Residual pixel color from previous iteration of omp.</p> required <code>all_bled_codes</code> <code>jnp.ndarray</code> <p><code>float [n_genes x (n_rounds x n_channels)]</code>. <code>bled_codes</code> such that <code>spot_color</code> of a gene <code>g</code> in round <code>r</code> is expected to be a constant multiple of <code>bled_codes[g, r]</code>. Includes codes of genes and background.</p> required <code>coefs</code> <code>jnp.ndarray</code> <p><code>float [n_genes_added]</code>. <code>coefs[g]</code> is the weighting for gene <code>genes_added[g]</code> found by the omp algorithm on previous iteration.  All are non-zero.</p> required <code>genes_added</code> <code>jnp.array</code> <p><code>int [n_genes_added]</code> Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel <code>s</code> is set to one of <code>genes_added[s]</code>, <code>pass_score_thresh[s]</code> will be False.</p> required <code>norm_shift</code> <code>float</code> <p>shift to apply to normalisation of spot_colors to limit boost of weak spots.</p> required <code>score_thresh</code> <code>float</code> <p><code>dot_product_score</code> of the best gene for a pixel must exceed this for that gene to be added in the current iteration.</p> required <code>alpha</code> <code>float</code> <p>Used for <code>fitting_variance</code>, by how much to increase variance as genes added.</p> required <code>background_genes</code> <code>jnp.ndarray</code> <p><code>int [n_channels]</code>. Indices of codes in all_bled_codes which correspond to background. If the best gene is set to one of <code>background_genes</code>, <code>pass_score_thresh</code> will be False.</p> required <code>background_var</code> <code>jnp.array</code> <p><code>float [(n_rounds x n_channels)]</code>. Contribution of background genes to variance (which does not change throughout omp iterations)  i.e. <code>background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <ul> <li>best_gene - The best gene to add next.</li> </ul> <code>bool</code> <ul> <li>pass_score_thresh - <code>True</code> if <code>best_score &gt; score_thresh</code>.</li> </ul> <code>jnp.ndarray</code> <ul> <li>inverse_var - <code>float [(n_rounds x n_channels)]</code>. Inverse of variance in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution.</li> </ul> Source code in <code>coppafish/omp/coefs_optimised.py</code> <pre><code>def get_best_gene_single(residual_pixel_color: jnp.ndarray, all_bled_codes: jnp.ndarray, coefs: jnp.ndarray,\n                         genes_added: jnp.array, norm_shift: float, score_thresh: float, alpha: float,\n                         background_genes: jnp.ndarray, background_var: jnp.array) -&gt; Tuple[int, bool, jnp.ndarray]:\n\"\"\"\n    Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`.\n    If `best_gene` is in `background_genes`, already in `genes_added` or `best_score &lt; score_thresh`,\n    then `pass_score_thresh = False`.\n\n    !!!note\n        The variance computed is based on maximum likelihood estimation - it accounts for all genes and background\n        fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a\n        weighting for omp fitting or choosing the next gene,\n        the rounds/channels which already have genes in will contribute less.\n\n    Args:\n        residual_pixel_color: `float [(n_rounds x n_channels)]`.\n            Residual pixel color from previous iteration of omp.\n        all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`.\n            `bled_codes` such that `spot_color` of a gene `g`\n            in round `r` is expected to be a constant multiple of `bled_codes[g, r]`.\n            Includes codes of genes and background.\n        coefs: `float [n_genes_added]`.\n            `coefs[g]` is the weighting for gene `genes_added[g]` found by the omp algorithm on previous iteration.\n             All are non-zero.\n        genes_added: `int [n_genes_added]`\n            Indices of genes added to each pixel from previous iteration of omp.\n            If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False.\n        norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots.\n        score_thresh: `dot_product_score` of the best gene for a pixel must exceed this\n            for that gene to be added in the current iteration.\n        alpha: Used for `fitting_variance`, by how much to increase variance as genes added.\n        background_genes: `int [n_channels]`.\n            Indices of codes in all_bled_codes which correspond to background.\n            If the best gene is set to one of `background_genes`, `pass_score_thresh` will be False.\n        background_var: `float [(n_rounds x n_channels)]`.\n            Contribution of background genes to variance (which does not change throughout omp iterations)  i.e.\n            `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`.\n\n    Returns:\n        - best_gene - The best gene to add next.\n        - pass_score_thresh - `True` if `best_score &gt; score_thresh`.\n        - inverse_var - `float [(n_rounds x n_channels)]`.\n            Inverse of variance in each round/channel based on genes fit on previous iteration.\n            Includes both background and gene contribution.\n    \"\"\"\n    inverse_var = 1 / (jnp.square(coefs) @ jnp.square(all_bled_codes[genes_added]) * alpha + background_var)\n    # calculate score including background genes as if best gene is background, then stop iteration.\n    best_gene, pass_score_thresh = get_best_gene_base(residual_pixel_color, all_bled_codes, norm_shift, score_thresh,\n                                                      inverse_var, jnp.append(background_genes, genes_added))\n    return best_gene, pass_score_thresh, inverse_var\n</code></pre>"},{"location":"code/omp/spots/","title":"Spots","text":""},{"location":"code/omp/spots/#coppafish.omp.spots.count_spot_neighbours","title":"<code>count_spot_neighbours(image, spot_yxz, kernel)</code>","text":"<p>Counts the number of positive (and negative) pixels in a neighbourhood about each spot. If <code>filter</code> contains only 1 and 0, then number of positive pixels returned near each spot. If <code>filter</code> contains only -1 and 0, then number of negative pixels returned near each spot. If <code>filter</code> contains -1, 0 and 1, then number of positive and negative pixels returned near each spot.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [n_y x n_x (x n_z)]</code>. image spots were found on.</p> required <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_spots x image.ndim]</code>. yx or yxz location of spots found.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>int [filter_sz_y x filter_sz_x (x filter_sz_z)]</code>. Number of positive (and negative) pixels counted in this neighbourhood about each spot in image. Only contains values 0 and 1 (and -1).</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li>n_pos_neighbours - <code>int [n_spots]</code> (Only if <code>filter</code> contains 1). Number of positive pixels around each spot in neighbourhood given by <code>pos_filter</code>.</li> </ul> <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li>n_neg_neighbours - <code>int [n_spots]</code> (Only if <code>filter</code> contains -1). Number of negative pixels around each spot in neighbourhood given by <code>neg_filter</code>.</li> </ul> Source code in <code>coppafish/omp/spots.py</code> <pre><code>def count_spot_neighbours(image: np.ndarray, spot_yxz: np.ndarray,\n                          kernel: np.ndarray) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n\"\"\"\n    Counts the number of positive (and negative) pixels in a neighbourhood about each spot.\n    If `filter` contains only 1 and 0, then number of positive pixels returned near each spot.\n    If `filter` contains only -1 and 0, then number of negative pixels returned near each spot.\n    If `filter` contains -1, 0 and 1, then number of positive and negative pixels returned near each spot.\n\n    Args:\n        image: `float [n_y x n_x (x n_z)]`.\n            image spots were found on.\n        spot_yxz: `int [n_spots x image.ndim]`.\n            yx or yxz location of spots found.\n        kernel: `int [filter_sz_y x filter_sz_x (x filter_sz_z)]`.\n            Number of positive (and negative) pixels counted in this neighbourhood about each spot in image.\n            Only contains values 0 and 1 (and -1).\n\n    Returns:\n        - n_pos_neighbours - `int [n_spots]` (Only if `filter` contains 1).\n            Number of positive pixels around each spot in neighbourhood given by `pos_filter`.\n        - n_neg_neighbours - `int [n_spots]` (Only if `filter` contains -1).\n            Number of negative pixels around each spot in neighbourhood given by `neg_filter`.\n    \"\"\"\n    # Correct for 2d cases where an empty dimension has been used for some variables.\n    if all([image.ndim == spot_yxz.shape[1] - 1, np.max(np.abs(spot_yxz[:, -1])) == 0]):\n        # Image 2D but spots 3D\n        spot_yxz = spot_yxz[:, :image.ndim]\n    if all([image.ndim == spot_yxz.shape[1] + 1, image.shape[-1] == 1]):\n        # Image 3D but spots 2D\n        image = np.mean(image, axis=image.ndim - 1)  # average over last dimension just means removing it.\n    if all([image.ndim == kernel.ndim - 1, kernel.shape[-1] == 1]):\n        # Image 2D but pos_filter 3D\n        kernel = np.mean(kernel, axis=kernel.ndim - 1)\n\n    # Check kernel contains right values.\n    kernel_vals = np.unique(kernel)\n    if not np.isin(kernel_vals, [-1, 0, 1]).all():\n        raise ValueError('filter contains values other than -1, 0 or 1.')\n\n    # Check all spots in image\n    max_yxz = np.array(image.shape) - 1\n    spot_oob = [val for val in spot_yxz if val.min() &lt; 0 or any(val &gt; max_yxz)]\n    if len(spot_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"spot_yxz\", spot_oob[0], [0] * image.ndim, max_yxz)\n\n    if np.isin([-1, 1], kernel_vals).all():\n        # Return positive and negative counts\n        n_pos = utils.morphology.imfilter_coords(image &gt; 0, kernel &gt; 0, spot_yxz)\n        n_neg = utils.morphology.imfilter_coords(image &lt; 0, kernel &lt; 0, spot_yxz)\n        return n_pos, n_neg\n    elif np.isin(-1, kernel_vals):\n        # Return negative counts\n        return utils.morphology.imfilter_coords(image &lt; 0, kernel &lt; 0, spot_yxz).astype(int)\n    elif np.isin(1, kernel_vals):\n        # Return positive counts\n        return utils.morphology.imfilter_coords(image &gt; 0, kernel &gt; 0, spot_yxz).astype(int)\n    else:\n        raise ValueError('filter contains only 0.')\n</code></pre>"},{"location":"code/omp/spots/#coppafish.omp.spots.cropped_coef_image","title":"<code>cropped_coef_image(pixel_yxz, pixel_coefs)</code>","text":"<p>Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_yxz</code> <code>np.ndarray</code> <p><code>int [n_pixels x 3]</code> <code>pixel_yxz[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for pixel <code>s</code>. <code>pixel_yxz[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for pixel <code>s</code>.</p> required <code>pixel_coefs</code> <code>Union[csr_matrix, np.array]</code> <p><code>float32 [n_pixels x 1]</code>. <code>pixel_coefs[s]</code> is the weighting of pixel <code>s</code> for a given gene found by the omp algorithm.  Most are zero hence sparse form used.</p> required <p>Returns:</p> Type Description <code>Optional[np.ndarray]</code> <ul> <li>coef_image - <code>float32 [im_size_y x im_size_x x im_size_z]</code> cropped omp coefficient. Will be <code>None</code> if there are no non-zero coefficients.</li> </ul> <code>Optional[np.ndarray]</code> <ul> <li>coord_shift - <code>int [3]</code>. yxz shift subtracted from pixel_yxz to build coef_image. Will be <code>None</code> if there are no non-zero coefficients.</li> </ul> Source code in <code>coppafish/omp/spots.py</code> <pre><code>def cropped_coef_image(pixel_yxz: np.ndarray,\n                       pixel_coefs: Union[csr_matrix, np.array]) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n\"\"\"\n    Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included.\n\n    Args:\n        pixel_yxz: `int [n_pixels x 3]`\n            ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```.\n            ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```.\n        pixel_coefs: `float32 [n_pixels x 1]`.\n            `pixel_coefs[s]` is the weighting of pixel `s` for a given gene found by the omp algorithm.\n             Most are zero hence sparse form used.\n\n    Returns:\n        - coef_image - `float32 [im_size_y x im_size_x x im_size_z]`\n            cropped omp coefficient.\n            Will be `None` if there are no non-zero coefficients.\n        - coord_shift - `int [3]`.\n            yxz shift subtracted from pixel_yxz to build coef_image.\n            Will be `None` if there are no non-zero coefficients.\n    \"\"\"\n    if isinstance(pixel_coefs, csr_matrix):\n        nz_ind = pixel_coefs.nonzero()[0]\n        nz_pixel_coefs = pixel_coefs[nz_ind].toarray().flatten()\n    else:\n        nz_ind = pixel_coefs != 0\n        nz_pixel_coefs = pixel_coefs[nz_ind]\n    if nz_pixel_coefs.size == 0:\n        # If no non-zero coefficients, return nothing\n        return None, None\n    else:\n        nz_pixel_yxz = pixel_yxz[nz_ind, :]\n\n        # shift nz_pixel_yxz so min is 0 in each axis so smaller image can be formed.\n        coord_shift = nz_pixel_yxz.min(axis=0)\n        nz_pixel_yxz = nz_pixel_yxz - coord_shift\n        n_y, n_x, n_z = nz_pixel_yxz.max(axis=0) + 1\n\n        # coef_image at pixels other than nz_pixel_yxz is set to 0.\n        if n_z == 1:\n            coef_image = np.zeros((n_y, n_x), dtype=np.float32)\n        else:\n            coef_image = np.zeros((n_y, n_x, n_z), dtype=np.float32)\n        coef_image[tuple([nz_pixel_yxz[:, j] for j in range(coef_image.ndim)])] = nz_pixel_coefs\n        return coef_image, coord_shift\n</code></pre>"},{"location":"code/omp/spots/#coppafish.omp.spots.get_spots","title":"<code>get_spots(pixel_coefs, pixel_yxz, radius_xy, radius_z, coef_thresh=0, spot_shape=None, pos_neighbour_thresh=0, spot_yxzg=None)</code>","text":"<p>Finds all local maxima in <code>coef_image</code> of each gene with coefficient exceeding <code>coef_thresh</code> and returns corresponding <code>yxz</code> position and <code>gene_no</code>. If provide <code>spot_shape</code>, also counts number of positive and negative pixels in neighbourhood of each spot.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_coefs</code> <code>Union[csr_matrix, np.array]</code> <p><code>float [n_pixels x n_genes]</code>. <code>pixel_coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>g</code> found by the omp algorithm.  Most are zero hence sparse form used.</p> required <code>pixel_yxz</code> <code>np.ndarray</code> <p><code>int [n_pixels x 3]</code>. <code>pixel_yxz[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for pixel <code>s</code>. <code>pixel_yxz[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for pixel <code>s</code>.</p> required <code>radius_xy</code> <code>int</code> <p>Radius of dilation structuring element in xy plane (approximately spot radius).</p> required <code>radius_z</code> <code>Optional[int]</code> <p>Radius of dilation structuring element in z direction (approximately spot radius). If <code>None</code>, 2D filter is used.</p> required <code>coef_thresh</code> <code>float</code> <p>Local maxima in <code>coef_image</code> exceeding this value are considered spots.</p> <code>0</code> <code>spot_shape</code> <code>Optional[np.ndarray]</code> <p><code>int [shape_size_y x shape_size_x x shape_size_z]</code> or <code>None</code>. Indicates expected sign of coefficients in neighbourhood of spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign so ignore.</p> <code>None</code> <code>pos_neighbour_thresh</code> <code>int</code> <p>Only spots with number of positive neighbours exceeding this will be kept if <code>spot_shape</code> provided.</p> <code>0</code> <code>spot_yxzg</code> <code>Optional[np.ndarray]</code> <p><code>float [n_spots x 4]</code>. Can provide location and gene identity of spots if already computed. Where spots are local maxima above <code>coef_thresh</code> in <code>pixel_coefs</code> image for each gene. If None, spots are determined from <code>pixel_coefs</code>. <code>spot_yxzg[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for spot <code>s</code>. <code>spot_yxzg[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for spot <code>s</code>. <code>spot_yxzg[s, 3]</code> is the gene number of spot <code>s</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]</code> <ul> <li>spot_yxz - <code>int [n_spots x 3]</code> <code>spot_yxz[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for spot <code>s</code>. <code>spot_yxz[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for spot <code>s</code>.</li> </ul> <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]</code> <ul> <li>spot_gene_no - <code>int [n_spots]</code>. <code>spot_gene_no[s]</code> is the gene that spot s is assigned to.</li> </ul> <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]</code> <ul> <li>n_pos_neighbours - <code>int [n_spots]</code> (Only if <code>spot_shape</code> given). Number of positive pixels around each spot in neighbourhood given by <code>spot_shape==1</code>.</li> </ul> <code>Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]</code> <ul> <li>n_neg_neighbours - <code>int [n_spots]</code> (Only if <code>spot_shape</code> given). Number of negative pixels around each spot in neighbourhood given by <code>spot_shape==-1</code>.</li> </ul> Source code in <code>coppafish/omp/spots.py</code> <pre><code>def get_spots(pixel_coefs: Union[csr_matrix, np.array], pixel_yxz: np.ndarray, radius_xy: int, radius_z: Optional[int],\n              coef_thresh: float = 0, spot_shape: Optional[np.ndarray] = None,\n              pos_neighbour_thresh: int = 0, spot_yxzg: Optional[np.ndarray] = None\n              ) -&gt; Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:\n\"\"\"\n    Finds all local maxima in `coef_image` of each gene with coefficient exceeding `coef_thresh`\n    and returns corresponding `yxz` position and `gene_no`.\n    If provide `spot_shape`, also counts number of positive and negative pixels in neighbourhood of each spot.\n\n    Args:\n        pixel_coefs: `float [n_pixels x n_genes]`.\n            `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm.\n             Most are zero hence sparse form used.\n        pixel_yxz: ```int [n_pixels x 3]```.\n            ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```.\n            ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```.\n        radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius).\n        radius_z: Radius of dilation structuring element in z direction (approximately spot radius).\n            If ```None```, 2D filter is used.\n        coef_thresh: Local maxima in `coef_image` exceeding this value are considered spots.\n        spot_shape: `int [shape_size_y x shape_size_x x shape_size_z]` or `None`.\n            Indicates expected sign of coefficients in neighbourhood of spot.\n            1 means expected positive coefficient.\n            -1 means expected negative coefficient.\n            0 means unsure of expected sign so ignore.\n        pos_neighbour_thresh: Only spots with number of positive neighbours exceeding this will be kept\n            if `spot_shape` provided.\n        spot_yxzg: `float [n_spots x 4]`.\n            Can provide location and gene identity of spots if already computed.\n            Where spots are local maxima above `coef_thresh` in `pixel_coefs` image for each gene.\n            If None, spots are determined from `pixel_coefs`.\n            ```spot_yxzg[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```.\n            ```spot_yxzg[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```.\n            ```spot_yxzg[s, 3]``` is the gene number of spot ```s```.\n\n    Returns:\n        - spot_yxz - `int [n_spots x 3]`\n            ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```.\n            ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```.\n        - spot_gene_no - `int [n_spots]`.\n            ```spot_gene_no[s]``` is the gene that spot s is assigned to.\n        - n_pos_neighbours - `int [n_spots]` (Only if `spot_shape` given).\n            Number of positive pixels around each spot in neighbourhood given by `spot_shape==1`.\n        - n_neg_neighbours - `int [n_spots]` (Only if `spot_shape` given).\n            Number of negative pixels around each spot in neighbourhood given by `spot_shape==-1`.\n    \"\"\"\n\n    n_pixels, n_genes = pixel_coefs.shape\n    if not utils.errors.check_shape(pixel_yxz, [n_pixels, 3]):\n        raise utils.errors.ShapeError('pixel_yxz', pixel_yxz.shape,\n                                      (n_pixels, 3))\n\n    if spot_shape is None:\n        spot_info = np.zeros((0, 4), dtype=int)\n    else:\n        if np.sum(spot_shape == 1) == 0:\n            raise ValueError(f\"spot_shape contains no pixels with a value of 1 which indicates the \"\n                             f\"neighbourhood about a spot where we expect a positive coefficient.\")\n        if np.sum(spot_shape == -1) == 0:\n            raise ValueError(f\"spot_shape contains no pixels with a value of -1 which indicates the \"\n                             f\"neighbourhood about a spot where we expect a negative coefficient.\")\n        if pos_neighbour_thresh &lt; 0 or pos_neighbour_thresh &gt;= np.sum(spot_shape &gt; 0):\n            # Out of bounds if threshold for positive neighbours is above the maximum possible.\n            raise utils.errors.OutOfBoundsError(\"pos_neighbour_thresh\", pos_neighbour_thresh, 0,\n                                                np.sum(spot_shape &gt; 0)-1)\n        spot_info = np.zeros((0, 6), dtype=int)\n\n    if spot_yxzg is not None:\n        # check pixel coefficient is positive for random subset of 500 spots.\n        spots_to_check = np.random.choice(range(spot_yxzg.shape[0]), np.clip(500, 0, spot_yxzg.shape[0]), replace=False)\n        pixel_index = numpy_indexed.indices(pixel_yxz, spot_yxzg[spots_to_check, :3].astype(pixel_yxz.dtype))\n        spot_coefs_check = pixel_coefs[pixel_index, spot_yxzg[spots_to_check, 3]]\n        if spot_coefs_check.min() &lt;= coef_thresh:\n            bad_spot = spots_to_check[spot_coefs_check.argmin()]\n            raise ValueError(f\"spot_yxzg provided but gene {spot_yxzg[bad_spot, 3]} coefficient for spot {bad_spot}\\n\"\n                             f\"at yxz = {spot_yxzg[bad_spot, :3]} is {spot_coefs_check.min()} \\n\"\n                             f\"whereas it should be more than coef_thresh = {coef_thresh} as it is listed as a spot.\")\n    with tqdm(total=n_genes) as pbar:\n        # TODO: if 2D can do all genes together.\n        pbar.set_description(f\"Finding spots for all {n_genes} genes from omp_coef images.\")\n        for g in range(n_genes):\n            # shift nzg_pixel_yxz so min is 0 in each axis so smaller image can be formed.\n            # Note size of image will be different for each gene.\n            coef_image, coord_shift = cropped_coef_image(pixel_yxz, pixel_coefs[:, g])\n            if coef_image is None:\n                # If no non-zero coefficients, go to next gene\n                continue\n            if spot_yxzg is None:\n                spot_yxz = detect_spots(coef_image, coef_thresh, radius_xy, radius_z, False)[0]\n            else:\n                # spot_yxz match pixel_yxz so if crop pixel_yxz need to crop spot_yxz too.\n                spot_yxz = spot_yxzg[spot_yxzg[:, 3] == g, :coef_image.ndim] - coord_shift[:coef_image.ndim]\n            if spot_yxz.shape[0] &gt; 0:\n                if spot_shape is None:\n                    keep = np.ones(spot_yxz.shape[0], dtype=bool)\n                    spot_info_g = np.zeros((np.sum(keep), 4), dtype=int)\n                else:\n                    n_pos_neighb, n_neg_neighb = count_spot_neighbours(coef_image, spot_yxz, spot_shape)\n                    keep = n_pos_neighb &gt; pos_neighbour_thresh\n                    spot_info_g = np.zeros((np.sum(keep), 6), dtype=int)\n                    spot_info_g[:, 4] = n_pos_neighb[keep]\n                    spot_info_g[:, 5] = n_neg_neighb[keep]\n\n                spot_info_g[:, :coef_image.ndim] = spot_yxz[keep]\n                spot_info_g[:, :3] = spot_info_g[:, :3] + coord_shift  # shift spot_yxz back\n                spot_info_g[:, 3] = g\n                spot_info = np.append(spot_info, spot_info_g, axis=0)\n            pbar.update(1)\n    pbar.close()\n\n    if spot_shape is None:\n        return spot_info[:, :3], spot_info[:, 3]\n    else:\n        return spot_info[:, :3], spot_info[:, 3], spot_info[:, 4], spot_info[:, 5]\n</code></pre>"},{"location":"code/omp/spots/#coppafish.omp.spots.spot_neighbourhood","title":"<code>spot_neighbourhood(pixel_coefs, pixel_yxz, spot_yxz, spot_gene_no, max_size, pos_neighbour_thresh, isolation_dist, z_scale, mean_sign_thresh)</code>","text":"<p>Finds the expected sign the coefficient should have in the neighbourhood about a spot.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_coefs</code> <code>Union[csr_matrix, np.array]</code> <p><code>float [n_pixels x n_genes]</code>. <code>pixel_coefs[s, g]</code> is the weighting of pixel <code>s</code> for gene <code>g</code> found by the omp algorithm.  Most are zero hence sparse form used.</p> required <code>pixel_yxz</code> <code>np.ndarray</code> <p><code>int [n_pixels x 3]</code>. <code>pixel_yxz[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for pixel <code>s</code>. <code>pixel_yxz[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for pixel <code>s</code>.</p> required <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_spots x 3]</code>. <code>spot_yxz[s, :2]</code> are the local yx coordinates in <code>yx_pixels</code> for spot <code>s</code>. <code>spot_yxz[s, 2]</code> is the local z coordinate in <code>z_pixels</code> for spot <code>s</code>.</p> required <code>spot_gene_no</code> <code>np.ndarray</code> <p><code>int [n_spots]</code>. <code>spot_gene_no[s]</code> is the gene that this spot is assigned to.</p> required <code>max_size</code> <code>Union[np.ndarray, List]</code> <p><code>int [3]</code>. max YXZ size of spot shape returned. Zeros at extremities will be cropped in <code>av_spot_image</code>.</p> required <code>pos_neighbour_thresh</code> <code>int</code> <p>For spot to be used to find av_spot_image, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Typical = 9.</p> required <code>isolation_dist</code> <code>float</code> <p>Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find av_spot_image.</p> required <code>z_scale</code> <code>float</code> <p>Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. <code>z_scale = pixel_size_z / pixel_size_yx</code> where both are measured in microns. typically, <code>z_scale &gt; 1</code> because <code>z_pixels</code> are larger than the <code>yx_pixels</code>.</p> required <code>mean_sign_thresh</code> <code>float</code> <p>If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in av_spot_image to be 0.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li>av_spot_image - <code>int8 [av_shape_y x av_shape_x x av_shape_z]</code> Expected sign of omp coefficient in neighbourhood centered on spot.</li> </ul> <code>np.ndarray</code> <ul> <li>spot_indices_used - <code>int [n_spots_used]</code>. indices of spots in <code>spot_yxzg</code> used to make av_spot_image.</li> </ul> <code>np.ndarray</code> <ul> <li>av_spot_image_float - <code>float [max_size[0] x max_size[1] x max_size[2]]</code> Mean of omp coefficient sign in neighbourhood centered on spot. This is before cropping and thresholding.</li> </ul> Source code in <code>coppafish/omp/spots.py</code> <pre><code>def spot_neighbourhood(pixel_coefs: Union[csr_matrix, np.array], pixel_yxz: np.ndarray, spot_yxz: np.ndarray,\n                       spot_gene_no: np.ndarray, max_size: Union[np.ndarray, List], pos_neighbour_thresh: int,\n                       isolation_dist: float, z_scale: float,\n                       mean_sign_thresh: float) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Finds the expected sign the coefficient should have in the neighbourhood about a spot.\n\n    Args:\n        pixel_coefs: `float [n_pixels x n_genes]`.\n            `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm.\n             Most are zero hence sparse form used.\n        pixel_yxz: ```int [n_pixels x 3]```.\n            ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```.\n            ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```.\n        spot_yxz: ```int [n_spots x 3]```.\n            ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```.\n            ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```.\n        spot_gene_no: ```int [n_spots]```.\n            ```spot_gene_no[s]``` is the gene that this spot is assigned to.\n        max_size: `int [3]`.\n            max YXZ size of spot shape returned. Zeros at extremities will be cropped in `av_spot_image`.\n        pos_neighbour_thresh: For spot to be used to find av_spot_image, it must have this many pixels\n            around it on the same z-plane that have a positive coefficient.\n            If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value).\n            Typical = 9.\n        isolation_dist: Spots are isolated if nearest neighbour (across all genes) is further away than this.\n            Only isolated spots are used to find av_spot_image.\n        z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels.\n            I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns.\n            typically, ```z_scale &gt; 1``` because ```z_pixels``` are larger than the ```yx_pixels```.\n        mean_sign_thresh: If the mean absolute coefficient sign is less than this in a region near a spot,\n            we set the expected coefficient in av_spot_image to be 0.\n\n    Returns:\n        - av_spot_image - `int8 [av_shape_y x av_shape_x x av_shape_z]`\n            Expected sign of omp coefficient in neighbourhood centered on spot.\n        - spot_indices_used - `int [n_spots_used]`.\n            indices of spots in `spot_yxzg` used to make av_spot_image.\n        - av_spot_image_float - `float [max_size[0] x max_size[1] x max_size[2]]`\n            Mean of omp coefficient sign in neighbourhood centered on spot.\n            This is before cropping and thresholding.\n    \"\"\"\n    # TODO: Maybe provide pixel_coef_sign instead of pixel_coef as less memory or use csr_matrix.\n    n_pixels, n_genes = pixel_coefs.shape\n    if not utils.errors.check_shape(pixel_yxz, [n_pixels, 3]):\n        raise utils.errors.ShapeError('pixel_yxz', pixel_yxz.shape,\n                                      (n_pixels, 3))\n    n_spots = spot_gene_no.shape[0]\n    if not utils.errors.check_shape(spot_yxz, [n_spots, 3]):\n        raise utils.errors.ShapeError('spot_yxz', spot_yxz.shape,\n                                      (n_spots, 3))\n\n    n_z = pixel_yxz.max(axis=0)[2] + 1\n\n    pos_filter_shape_yx = np.ceil(np.sqrt(pos_neighbour_thresh)).astype(int)\n    if pos_filter_shape_yx % 2 == 0:\n        # Shape must be odd\n        pos_filter_shape_yx = pos_filter_shape_yx + 1\n    if n_z &lt;= 2:\n        pos_filter_shape_z = 1\n    else:\n        pos_filter_shape_z = 3\n    pos_filter = np.zeros((pos_filter_shape_yx, pos_filter_shape_yx, pos_filter_shape_z), dtype=int)\n    pos_filter[:, :, np.floor(pos_filter_shape_z/2).astype(int)] = 1\n    if pos_filter_shape_z == 3:\n        mid_yx = np.floor(pos_filter_shape_yx/2).astype(int)\n        pos_filter[mid_yx, mid_yx, 0] = 1\n        pos_filter[mid_yx, mid_yx, 2] = 1\n\n    max_size = np.array(max_size)\n    if n_z == 1:\n        max_size[2] = 1\n    max_size_odd_loc = np.where(np.array(max_size) % 2 == 0)[0]\n    if max_size_odd_loc.size &gt; 0:\n        max_size[max_size_odd_loc] += 1  # ensure shape is odd\n\n    # get image centred on each spot.\n    # Big image shape which will be cropped later.\n    spot_images = np.zeros((0, *max_size), dtype=int)\n    spots_used = np.zeros(n_spots, dtype=bool)\n    for g in range(n_genes):\n        use = spot_gene_no == g\n        if use.any():\n            # Note size of image will be different for each gene.\n            coef_sign_image, coord_shift = cropped_coef_image(pixel_yxz, pixel_coefs[:, g])\n            if coef_sign_image is None:\n                # Go to next gene if no non-zero coefficients for this gene\n                continue\n            coef_sign_image = np.sign(coef_sign_image).astype(int)\n            g_spot_yxz = spot_yxz[use] - coord_shift\n\n            # Only keep spots with all neighbourhood having positive coefficient.\n            n_pos_neighb = count_spot_neighbours(coef_sign_image, g_spot_yxz, pos_filter)\n            g_use = n_pos_neighb == pos_filter.sum()\n            use[np.where(use)[0][np.invert(g_use)]] = False\n            if coef_sign_image.ndim == 2:\n                coef_sign_image = coef_sign_image[:, :, np.newaxis]\n            if use.any():\n                # nan_to_num sets nan to zero i.e. if out of range of coef_sign_image, coef assumed zero.\n                # This is what we want as have cropped coef_sign_image to exclude zero coefficients.\n                spot_images = np.append(\n                    spot_images, np.nan_to_num(get_spot_images(coef_sign_image, g_spot_yxz[g_use], max_size)\n                                               ).astype(int), axis=0)\n                spots_used[use] = True\n\n    if not spots_used.any():\n        raise ValueError(\"No spots found to make average spot image from.\")\n    # Compute average spot image from all isolated spots\n    isolated = get_isolated_points(spot_yxz[spots_used] * [1, 1, z_scale], isolation_dist)\n    # get_average below ignores the nan values.\n    av_spot_image = get_average_spot_image(spot_images[isolated].astype(float), 'mean', 'annulus_3d')\n    av_spot_image_float = av_spot_image.copy()\n    spot_indices_used = np.where(spots_used)[0][isolated]\n\n    # Where mean sign is low, set to 0.\n    av_spot_image[np.abs(av_spot_image) &lt; mean_sign_thresh] = 0\n    av_spot_image = np.sign(av_spot_image).astype(np.int8)\n\n    # Crop image to remove zeros at extremities\n    # may get issue here if there is a positive sign pixel further away than negative but think unlikely.\n    av_spot_image = av_spot_image[:, :, ~np.all(av_spot_image == 0, axis=(0, 1))]\n    av_spot_image = av_spot_image[:, ~np.all(av_spot_image == 0, axis=(0, 2)), :]\n    av_spot_image = av_spot_image[~np.all(av_spot_image == 0, axis=(1, 2)), :, :]\n\n    if np.sum(av_spot_image == 1) == 0:\n        warnings.warn(f\"In av_spot_image, no pixels have a value of 1.\\n\"\n                      f\"Maybe mean_sign_thresh = {mean_sign_thresh} is too high.\")\n    if np.sum(av_spot_image == -1) == 0:\n        warnings.warn(f\"In av_spot_image, no pixels have a value of -1.\\n\"\n                      f\"Maybe mean_sign_thresh = {mean_sign_thresh} is too high.\")\n    if np.sum(av_spot_image == 0) == 0:\n        warnings.warn(f\"In av_spot_image, no pixels have a value of 0.\\n\"\n                      f\"Maybe mean_sign_thresh = {mean_sign_thresh} is too low.\")\n\n    return av_spot_image, spot_indices_used, av_spot_image_float\n</code></pre>"},{"location":"code/pipeline/basic_info/","title":"Basic Info","text":""},{"location":"code/pipeline/basic_info/#coppafish.pipeline.basic_info.set_basic_info","title":"<code>set_basic_info(config_file, config_basic)</code>","text":"<p>Adds info from <code>'basic_info'</code> section of config file to notebook page.</p> <p>To <code>basic_info</code> page, the following is also added: <code>anchor_round</code>, <code>n_rounds</code>, <code>n_extra_rounds</code>, <code>n_tiles</code>, <code>n_channels</code>, <code>nz</code>, <code>tile_sz</code>, <code>tilepos_yx</code>, <code>tilepos_yx_nd2</code>, <code>pixel_size_xy</code>, <code>pixel_size_z</code>, <code>tile_centre</code>, <code>use_anchor</code>.</p> <p>See <code>'basic_info'</code> sections of <code>notebook_comments.json</code> file for description of the variables.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>dict</code> <p>Dictionary obtained from <code>'file_names'</code> section of config file.</p> required <code>config_basic</code> <code>dict</code> <p>Dictionary obtained from <code>'basic_info'</code> section of config file.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <ul> <li><code>NotebookPage[basic_info]</code> - Page contains information that is used at all stages of the pipeline.</li> </ul> Source code in <code>coppafish/pipeline/basic_info.py</code> <pre><code>def set_basic_info(config_file: dict, config_basic: dict) -&gt; NotebookPage:\n\"\"\"\n    Adds info from `'basic_info'` section of config file to notebook page.\n\n    To `basic_info` page, the following is also added:\n    `anchor_round`, `n_rounds`, `n_extra_rounds`, `n_tiles`, `n_channels`, `nz`, `tile_sz`, `tilepos_yx`,\n    `tilepos_yx_nd2`, `pixel_size_xy`, `pixel_size_z`, `tile_centre`, `use_anchor`.\n\n    See `'basic_info'` sections of `notebook_comments.json` file\n    for description of the variables.\n\n    Args:\n        config_file: Dictionary obtained from `'file_names'` section of config file.\n        config_basic: Dictionary obtained from `'basic_info'` section of config file.\n\n    Returns:\n        - `NotebookPage[basic_info]` - Page contains information that is used at all stages of the pipeline.\n    \"\"\"\n    nbp = NotebookPage('basic_info')\n    nbp.is_3d = config_basic['is_3d']\n\n    # Deal with case where no imaging rounds, just want to run anchor round.\n    if config_file['round'] is None:\n        if config_file['anchor'] is None:\n            raise ValueError(f'Neither imaging rounds nor anchor_round provided')\n        config_file['round'] = []\n    n_rounds = len(config_file['round'])\n\n    # Set ref/anchor round/channel\n    if config_file['anchor'] is None:\n        config_basic['anchor_channel'] = None  # set anchor channel to None if no anchor round\n        config_basic['dapi_channel'] = None  # set dapi channel to None if no anchor round\n        if config_basic['ref_round'] is None:\n            raise ValueError('No anchor round used, but ref_round not specified')\n        if config_basic['ref_channel'] is None:\n            raise ValueError('No anchor round used, but ref_channel not specified')\n        nbp.anchor_round = None\n        nbp.n_extra_rounds = 0\n        nbp.use_anchor = False\n        warnings.warn(f\"Anchor file not given.\"\n                      f\"\\nWill use round {config_basic['ref_round']}, \"\n                      f\"channel {config_basic['ref_channel']} as reference\")\n    else:\n        if config_basic['anchor_channel'] is None:\n            raise ValueError('Using anchor round, but anchor_channel not specified')\n        # always have anchor as first round after imaging rounds\n        nbp.anchor_round = n_rounds\n        config_basic['ref_round'] = nbp.anchor_round\n        config_basic['ref_channel'] = config_basic['anchor_channel']\n        nbp.n_extra_rounds = 1\n        nbp.use_anchor = True\n        warnings.warn(f\"Anchor file given and anchor channel specified.\"\n                      f\"\\nWill use anchor round, channel {config_basic['anchor_channel']} as reference\")\n    nbp.anchor_channel = config_basic['anchor_channel']\n    nbp.dapi_channel = config_basic['dapi_channel']\n    nbp.ref_round = config_basic['ref_round']\n    nbp.ref_channel = config_basic['ref_channel']\n\n    if config_basic['use_rounds'] is None:\n        config_basic['use_rounds'] = list(np.arange(n_rounds))\n    nbp.use_rounds = config_basic['use_rounds']\n    nbp.use_rounds.sort()  # ensure ascending\n    use_rounds_oob = [val for val in nbp.use_rounds if val &lt; 0 or val &gt;= n_rounds]\n    if len(use_rounds_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"use_rounds\", use_rounds_oob[0], 0, n_rounds - 1)\n\n    if len(config_file['round']) &gt; 0:\n        first_round_raw = os.path.join(config_file['input_dir'], config_file['round'][0])\n    else:\n        first_round_raw = os.path.join(config_file['input_dir'], config_file['anchor'])\n    if config_file['raw_extension'] == '.nd2':\n        # load in metadata of nd2 file corresponding to first round\n        # Test for number of rounds in case of separate round registration and load metadata\n        # from anchor round in that case\n        metadata = utils.nd2.get_metadata(first_round_raw + config_file['raw_extension'])\n    elif config_file['raw_extension'] == '.npy':\n        # Load in metadata as dictionary from a json file\n        config_file['raw_metadata'] = config_file['raw_metadata'].replace('.json', '')\n        metadata_file = os.path.join(config_file['input_dir'], config_file['raw_metadata'] + '.json')\n        metadata = json.load(open(metadata_file))\n        # Check metadata info matches that in first round npy file.\n        use_tiles_nd2 = utils.raw.metadata_sanity_check(metadata, first_round_raw)\n    else:\n        raise ValueError(f\"config_file['raw_extension'] should be either '.nd2' or '.npy' but it is \"\n                         f\"{config_file['raw_extension']}.\")\n\n\n\n    # get channel info\n    n_channels = metadata['sizes']['c']\n    if config_basic['use_channels'] is None:\n        config_basic['use_channels'] = list(np.arange(n_channels))\n    nbp.use_channels = config_basic['use_channels']\n    nbp.use_channels.sort()\n    use_channels_oob = [val for val in nbp.use_channels if val &lt; 0 or val &gt;= n_channels]\n    if len(use_channels_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"use_channels\", use_channels_oob[0], 0, n_channels - 1)\n\n    # get z info\n    if config_basic['use_z'] is None:\n        config_basic['use_z'] = list(np.arange(metadata['sizes']['z']))\n    elif len(config_basic['use_z']) == 2:\n        # use consecutive values if only 2 given.\n        config_basic['use_z'] = list(np.arange(config_basic['use_z'][0], config_basic['use_z'][1] + 1))\n    if config_basic['ignore_first_z_plane'] and 0 in config_basic['use_z']:\n        config_basic['use_z'].remove(0)\n    nbp.use_z = config_basic['use_z']\n    nbp.use_z.sort()\n    use_z_oob = [val for val in nbp.use_z if val &lt; 0 or val &gt;= metadata['sizes']['z']]\n    if len(use_z_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"use_z\", use_z_oob[0], 0, metadata['sizes']['z'] - 1)\n\n    # get tile info\n    tile_sz = metadata['sizes']['x']\n    n_tiles = metadata['sizes']['t']\n    tilepos_yx_nd2, tilepos_yx = setup.get_tilepos(np.asarray(metadata['xy_pos']), tile_sz)\n    nbp.tilepos_yx_nd2 = tilepos_yx_nd2  # numpy array, yx coordinate of tile with nd2 index.\n    nbp.tilepos_yx = tilepos_yx  # and with npy index\n\n    if config_file['raw_extension'] == '.npy':\n        # Read tile indices from raw data folder and set to use_tiles if not specified already.\n        use_tiles_folder = utils.npy.get_npy_tile_ind(use_tiles_nd2, tilepos_yx_nd2, tilepos_yx)\n        if config_basic['use_tiles'] is None:\n            config_basic['use_tiles'] = use_tiles_folder\n        elif np.setdiff1d(config_basic['use_tiles'], use_tiles_folder).size &gt; 0:\n            raise ValueError(f\"config_basic['use_tiles'] = {config_basic['use_tiles']}\\n\"\n                             f\"But in the folder:\\n{first_round_raw}\\nTiles Available are {use_tiles_folder}.\")\n    if config_basic['use_tiles'] is None:\n        config_basic['use_tiles'] = list(np.arange(n_tiles))\n    if config_basic['ignore_tiles'] is not None:\n        config_basic['use_tiles'] = list(np.setdiff1d(config_basic['use_tiles'], config_basic['ignore_tiles']))\n    nbp.use_tiles = config_basic['use_tiles']\n    nbp.use_tiles.sort()\n    use_tiles_oob = [val for val in nbp.use_tiles if val &lt; 0 or val &gt;= n_tiles]\n    if len(use_tiles_oob) &gt; 0:\n        raise utils.errors.OutOfBoundsError(\"use_tiles\", use_tiles_oob[0], 0, n_tiles - 1)\n\n    # get dye info\n    if config_basic['dye_names'] is None:\n        warnings.warn(f\"dye_names not specified so assuming separate dye for each channel.\")\n        n_dyes = n_channels\n    else:\n        # Ensure channel_camera/channel_laser are correct sizes\n        n_dyes = len(config_basic['dye_names'])\n        if config_basic['channel_camera'] is None:\n            raise ValueError('dye_names specified but channel_camera is not.')\n        elif len(config_basic['channel_camera']) != n_channels:\n            raise ValueError(f\"channel_camera contains {len(config_basic['channel_camera'])} values.\\n\"\n                             f\"But there must be a value for each channel and there are {n_channels} channels.\")\n        if config_basic['channel_laser'] is None:\n            raise ValueError('dye_names specified but channel_laser is not.')\n        elif len(config_basic['channel_laser']) != n_channels:\n            raise ValueError(f\"channel_laser contains {len(config_basic['channel_camera'])} values.\\n\"\n                             f\"But there must be a value for each channel and there are {n_channels} channels.\")\n\n    if config_basic['use_dyes'] is None:\n        if config_basic['dye_names'] is None:\n            config_basic['use_dyes'] = nbp.use_channels\n        else:\n            config_basic['use_dyes'] = list(np.arange(n_dyes))\n    nbp.use_dyes = config_basic['use_dyes']\n    nbp.dye_names = config_basic['dye_names']\n    nbp.channel_camera = config_basic['channel_camera']\n    nbp.channel_laser = config_basic['channel_laser']\n\n    nbp.tile_pixel_value_shift = config_basic['tile_pixel_value_shift']\n\n    # Add size info obtained from raw metadata to notebook page\n    nbp.n_rounds = n_rounds  # int, number of imaging rounds\n    nbp.tile_sz = tile_sz  # xy dimension of tiles in pixels.\n    nbp.n_tiles = n_tiles  # int, number of tiles\n    nbp.n_channels = n_channels  # int, number of imaging channels\n    nbp.nz = len(nbp.use_z)  # number of z planes in npy file (not necessarily the same as in nd2)\n    nbp.n_dyes = n_dyes  # int, number of dyes\n\n    # subtract tile_centre from local pixel coordinates to get centered local tile coordinates\n    if not nbp.is_3d:\n        nz = 1\n    else:\n        nz = nbp.nz\n    nbp.tile_centre = (np.array([tile_sz, tile_sz, nz]) - 1) / 2\n    nbp.pixel_size_xy = metadata['pixel_microns']  # pixel size in microns in xy\n    nbp.pixel_size_z = metadata['pixel_microns_z']  # and z directions.\n\n    # Make sure reference rounds/channels are in range of data provided.\n    if nbp.use_anchor:\n        if not 0 &lt;= nbp.ref_channel &lt;= n_channels - 1:\n            raise utils.errors.OutOfBoundsError(\"ref_channel\", nbp.ref_channel, 0, n_channels - 1)\n        if nbp.dapi_channel is not None:\n            if not 0 &lt;= nbp.dapi_channel &lt;= n_channels - 1:\n                raise utils.errors.OutOfBoundsError(\"dapi_channel\", nbp.ref_channel, 0, n_channels - 1)\n    else:\n        # Seen as ref_channel is an imaging channel if anchor not used, ref_channel must be an imaging channels i.e.\n        # must be in use_channels. Same goes for ref_round.\n        if not np.isin(nbp.ref_channel, nbp.use_channels):\n            raise ValueError(f\"ref_channel is {nbp.ref_channel} which is not in use_channels = {nbp.use_channels}.\")\n        if not np.isin(nbp.ref_round, nbp.use_rounds):\n            raise ValueError(f\"ref_round is {nbp.ref_round} which is not in use_rounds = {nbp.use_rounds}.\")\n\n    return nbp\n</code></pre>"},{"location":"code/pipeline/call_reference_spots/","title":"Call Reference Spots","text":""},{"location":"code/pipeline/call_reference_spots/#coppafish.pipeline.call_reference_spots.call_reference_spots","title":"<code>call_reference_spots(config, nbp_file, nbp_basic, nbp_ref_spots, hist_values, hist_counts, transform, overwrite_ref_spots=False)</code>","text":"<p>This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a simple dot product for spots found on the reference round.</p> <p>Returns the <code>call_spots</code> notebook page and adds the following variables to the <code>ref_spots</code> page: <code>gene_no</code>, <code>score</code>, <code>score_diff</code>, <code>intensity</code>.</p> <p>See <code>'call_spots'</code> and <code>'ref_spots'</code> sections of <code>notebook_comments.json</code> file for description of the variables in each page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'call_spots'</code> section of config file.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>nbp_ref_spots</code> <code>NotebookPage</code> <p><code>ref_spots</code> notebook page containing all variables produced in <code>pipeline/reference_spots.py</code> i.e. <code>local_yxz</code>, <code>isolated</code>, <code>tile</code>, <code>colors</code>. <code>gene_no</code>, <code>score</code>, <code>score_diff</code>, <code>intensity</code> should all be <code>None</code> to add them here, unless <code>overwrite_ref_spots == True</code>.</p> required <code>hist_values</code> <code>np.ndarray</code> <p><code>int [n_pixel_values]</code>. All possible pixel values in saved tiff images i.e. <code>n_pixel_values</code> is approximately <code>np.iinfo(np.uint16).max</code> because tiffs saved as <code>uint16</code> images. This is saved in the extract notebook page i.e. <code>nb.extract.hist_values</code>.</p> required <code>hist_counts</code> <code>np.ndarray</code> <p><code>int [n_pixel_values x n_rounds x n_channels]</code>. <code>hist_counts[i, r, c]</code> is the number of pixels across all tiles in round <code>r</code>, channel <code>c</code> which had the value <code>hist_values[i]</code>. This is saved in extract notebook page i.e. <code>nb.extract.hist_counts</code>.</p> required <code>transform</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x 4 x 3]</code>. <code>transform[t, r, c]</code> is the affine transform to get from tile <code>t</code>, <code>ref_round</code>, <code>ref_channel</code> to tile <code>t</code>, round <code>r</code>, channel <code>c</code>. This is saved in the register notebook page i.e. <code>nb.register.transform</code>.</p> required <code>overwrite_ref_spots</code> <code>bool</code> <p>If <code>True</code>, the variables:</p> <ul> <li><code>gene_no</code></li> <li><code>score</code></li> <li><code>score_diff</code></li> <li><code>intensity</code></li> </ul> <p>in <code>nbp_ref_spots</code> will be overwritten if they exist. If this is <code>False</code>, they will only be overwritten if they are all set to <code>None</code>, otherwise an error will occur.</p> <code>False</code> <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[call_spots]</code> - Page contains bleed matrix and expected code for each gene.</p> <code>NotebookPage</code> <p><code>NotebookPage[ref_spots]</code> - Page contains gene assignments and info for spots found on reference round. Parameters added are: intensity, score, gene_no, score_diff</p> Source code in <code>coppafish/pipeline/call_reference_spots.py</code> <pre><code>def call_reference_spots(config: dict, nbp_file: NotebookPage, nbp_basic: NotebookPage, nbp_ref_spots: NotebookPage,\n                         hist_values: np.ndarray, hist_counts: np.ndarray,\n                         transform: np.ndarray, overwrite_ref_spots: bool = False) -&gt; Tuple[NotebookPage, NotebookPage]:\n\"\"\"\n    This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a\n    simple dot product for spots found on the reference round.\n\n    Returns the `call_spots` notebook page and adds the following variables to the `ref_spots` page:\n    `gene_no`, `score`, `score_diff`, `intensity`.\n\n    See `'call_spots'` and `'ref_spots'` sections of `notebook_comments.json` file\n    for description of the variables in each page.\n\n    Args:\n        config: Dictionary obtained from `'call_spots'` section of config file.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        nbp_ref_spots: `ref_spots` notebook page containing all variables produced in `pipeline/reference_spots.py` i.e.\n            `local_yxz`, `isolated`, `tile`, `colors`.\n            `gene_no`, `score`, `score_diff`, `intensity` should all be `None` to add them here, unless\n            `overwrite_ref_spots == True`.\n        hist_values: `int [n_pixel_values]`.\n            All possible pixel values in saved tiff images i.e. `n_pixel_values` is approximately\n            `np.iinfo(np.uint16).max` because tiffs saved as `uint16` images.\n            This is saved in the extract notebook page i.e. `nb.extract.hist_values`.\n        hist_counts: `int [n_pixel_values x n_rounds x n_channels]`.\n            `hist_counts[i, r, c]` is the number of pixels across all tiles in round `r`, channel `c`\n            which had the value `hist_values[i]`.\n            This is saved in extract notebook page i.e. `nb.extract.hist_counts`.\n        transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`.\n            `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to\n            tile `t`, round `r`, channel `c`.\n            This is saved in the register notebook page i.e. `nb.register.transform`.\n        overwrite_ref_spots: If `True`, the variables:\n\n            * `gene_no`\n            * `score`\n            * `score_diff`\n            * `intensity`\n\n            in `nbp_ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten\n            if they are all set to `None`, otherwise an error will occur.\n\n    Returns:\n        `NotebookPage[call_spots]` - Page contains bleed matrix and expected code for each gene.\n        `NotebookPage[ref_spots]` - Page contains gene assignments and info for spots found on reference round.\n            Parameters added are: intensity, score, gene_no, score_diff\n    \"\"\"\n    if overwrite_ref_spots:\n        warnings.warn(\"\\noverwrite_ref_spots = True so will overwrite:\\ngene_no, score, score_diff, intensity\"\n                      \"\\nin nbp_ref_spots.\")\n    else:\n        # Raise error if data in nbp_ref_spots already exists that will be overwritted in this function.\n        error_message = \"\"\n        for var in ['gene_no', 'score', 'score_diff', 'intensity']:\n            if hasattr(nbp_ref_spots, var) and nbp_ref_spots.__getattribute__(var) is not None:\n                error_message += f\"\\nnbp_ref_spots.{var} is not None but this function will overwrite {var}.\" \\\n                                 f\"\\nRun with overwrite_ref_spots = True to get past this error.\"\n        if len(error_message) &gt; 0:\n            raise ValueError(error_message)\n\n    nbp_ref_spots.finalized = False  # So we can add and delete ref_spots page variables\n    # delete all variables in ref_spots set to None so can add them later.\n    for var in ['gene_no', 'score', 'score_diff', 'intensity']:\n        if hasattr(nbp_ref_spots, var):\n            nbp_ref_spots.__delattr__(var)\n    nbp = NotebookPage(\"call_spots\")\n\n    # get color norm factor\n    rc_ind = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels)\n    hist_counts_use = np.moveaxis(np.moveaxis(hist_counts, 0, -1)[rc_ind], -1, 0)\n    color_norm_factor = np.ones((nbp_basic.n_rounds, nbp_basic.n_channels)) * np.nan\n    color_norm_factor[rc_ind] = color_normalisation(hist_values, hist_counts_use, config['color_norm_intensities'],\n                                                    config['color_norm_probs'], config['bleed_matrix_method'])\n\n    # get initial bleed matrix\n    initial_raw_bleed_matrix = np.ones((nbp_basic.n_rounds, nbp_basic.n_channels, nbp_basic.n_dyes)) * np.nan\n    rcd_ind = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels, nbp_basic.use_dyes)\n    if nbp_basic.dye_names is not None:\n        # if specify dyes, will initialize bleed matrix using prior data\n        dye_names_use = np.array(nbp_basic.dye_names)[nbp_basic.use_dyes]\n        camera_use = np.array(nbp_basic.channel_camera)[nbp_basic.use_channels]\n        laser_use = np.array(nbp_basic.channel_laser)[nbp_basic.use_channels]\n        initial_raw_bleed_matrix[rcd_ind] = get_dye_channel_intensity_guess(nbp_file.dye_camera_laser,\n                                                                            dye_names_use, camera_use,\n                                                                            laser_use).transpose()\n        initial_bleed_matrix = initial_raw_bleed_matrix / np.expand_dims(color_norm_factor, 2)\n    else:\n        if nbp_basic.n_dyes != nbp_basic.n_channels:\n            raise ValueError(f\"'dye_names' were not specified so expect each dye to correspond to a different channel.\"\n                             f\"\\nBut n_channels={nbp_basic.n_channels} and n_dyes={nbp_basic.n_dyes}\")\n        if nbp_basic.use_channels != nbp_basic.use_dyes:\n            raise ValueError(f\"'dye_names' were not specified so expect each dye to correspond to a different channel.\"\n                             f\"\\nBleed matrix computation requires use_channels and use_dyes to be the same to work.\"\n                             f\"\\nBut use_channels={nbp_basic.use_channels} and use_dyes={nbp_basic.use_dyes}\")\n        initial_bleed_matrix = initial_raw_bleed_matrix.copy()\n        initial_bleed_matrix[rcd_ind] = np.tile(np.expand_dims(np.eye(nbp_basic.n_channels), 0),\n                                                (nbp_basic.n_rounds, 1, 1))[rcd_ind]\n\n    # Get norm_shift and intensity_thresh from middle tile/ z-plane average intensity\n    # This is because these variables are all a small fraction of a spot_color L2 norm in one round.\n    # Hence, use average pixel as example of low intensity spot.\n    # get central tile\n    nbp.norm_shift_tile = scale.central_tile(nbp_basic.tilepos_yx, nbp_basic.use_tiles)\n    if nbp_basic.is_3d:\n        nbp.norm_shift_z = int(np.floor(nbp_basic.nz / 2))  # central z-plane to get info from.\n    else:\n        nbp.norm_shift_z = 0\n    pixel_colors = get_spot_colors(all_pixel_yxz(nbp_basic.tile_sz, nbp_basic.tile_sz, nbp.norm_shift_z),\n                                   nbp.norm_shift_tile, transform, nbp_file, nbp_basic, return_in_bounds=True)[0]\n    pixel_intensity = get_spot_intensity(np.abs(pixel_colors) / color_norm_factor[rc_ind])\n    nbp.abs_intensity_percentile = np.percentile(pixel_intensity, np.arange(1, 101))\n    if config['background_weight_shift'] is None:\n        # Set to median absolute pixel intensity\n        config['background_weight_shift'] = float(round_any(nbp.abs_intensity_percentile[50],\n                                                            config['norm_shift_precision'], 'ceil'))\n    median_round_l2_norm = np.median(np.linalg.norm(pixel_colors / color_norm_factor[rc_ind], axis=2))\n    if config['dp_norm_shift'] is None:\n        config['dp_norm_shift'] = float(round_any(median_round_l2_norm, config['norm_shift_precision']))\n    # intensity thresh is just a very low threshold, would basically be the same if set to 0\n    # but found it to be slightly better on ground truth\n    pixel_intensity = get_spot_intensity(pixel_colors / color_norm_factor[rc_ind])\n    if config['gene_efficiency_intensity_thresh'] is None:\n        config['gene_efficiency_intensity_thresh'] = \\\n            float(round_any(np.percentile(pixel_intensity, config['gene_efficiency_intensity_thresh_percentile']),\n                            config['gene_efficiency_intensity_thresh_precision']))\n    nbp.dp_norm_shift = float(np.clip(config['dp_norm_shift'], config['norm_shift_min'], config['norm_shift_max']))\n    nbp.background_weight_shift = float(np.clip(config['background_weight_shift'],\n                                                config['norm_shift_min'], config['norm_shift_max']))\n    nbp.gene_efficiency_intensity_thresh = \\\n        float(np.clip(config['gene_efficiency_intensity_thresh'],\n                      config['gene_efficiency_intensity_thresh_min'],\n                      config['gene_efficiency_intensity_thresh_max']))\n\n    # get bleed matrix\n    spot_colors_use = np.moveaxis(np.moveaxis(nbp_ref_spots.colors, 0, -1)[rc_ind], -1, 0) / color_norm_factor[rc_ind]\n    nbp_ref_spots.intensity = np.asarray(get_spot_intensity(spot_colors_use).astype(np.float32))\n    # Remove background first\n    background_coef = np.ones((spot_colors_use.shape[0], nbp_basic.n_channels)) * np.nan\n    background_codes = np.ones((nbp_basic.n_channels, nbp_basic.n_rounds, nbp_basic.n_channels)) * np.nan\n    crc_ind = np.ix_(nbp_basic.use_channels, nbp_basic.use_rounds, nbp_basic.use_channels)\n    spot_colors_use, background_coef[:, nbp_basic.use_channels], background_codes[crc_ind] = \\\n        fit_background(spot_colors_use, nbp.background_weight_shift)\n    spot_colors_use = np.asarray(spot_colors_use)  # in case using jax\n    bleed_matrix = initial_raw_bleed_matrix.copy()\n    bleed_matrix[rcd_ind] = get_bleed_matrix(spot_colors_use[nbp_ref_spots.isolated], initial_bleed_matrix[rcd_ind],\n                                             config['bleed_matrix_method'], config['bleed_matrix_score_thresh'],\n                                             config['bleed_matrix_min_cluster_size'], config['bleed_matrix_n_iter'],\n                                             config['bleed_matrix_anneal'])\n\n    # get gene codes\n    gene_names, gene_codes = np.genfromtxt(nbp_file.code_book, dtype=(str, str)).transpose()\n    if config['channel_order'] is None:\n        gene_code_map = {i : i for i in range(0, 100)}\n    else:\n        co = config['channel_order']\n        assert len(co) == len(set(co)) # All unique\n        gene_code_map = {co[i]: i for i in range(0, len(co))}\n    gene_codes = np.array([[gene_code_map[int(i)] for i in gene_codes[j]] for j in range(len(gene_codes))])\n    # bled_codes[g,r,c] returned below is nan where r/c/gene_codes[g,r] outside use_rounds/channels/dyes\n    bled_codes = get_bled_codes(gene_codes, bleed_matrix)\n\n    # get bled_codes_use with no nan values and L2 norm=1\n    bled_codes_use = np.moveaxis(np.moveaxis(bled_codes, 0, -1)[rc_ind], -1, 0)\n    bled_codes_use[np.isnan(bled_codes_use)] = 0  # set all round vectors where dye is not in use_dyes to 0.\n    # Give all bled codes an L2 norm of 1 across use_rounds and use_channels\n    norm_factor = np.expand_dims(np.linalg.norm(bled_codes_use, axis=(1, 2)), (1, 2))\n    norm_factor[norm_factor == 0] = 1  # For genes with no dye in use_dye, this avoids blow up on next line\n    bled_codes_use = bled_codes_use / norm_factor\n\n    # bled_codes[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes\n    n_genes = bled_codes_use.shape[0]\n    bled_codes = np.ones((nbp_basic.n_rounds, nbp_basic.n_channels, n_genes)) * np.nan\n    bled_codes[rc_ind] = np.moveaxis(bled_codes_use, 0, -1)\n    bled_codes = np.moveaxis(bled_codes, -1, 0)\n\n    nbp.gene_names = gene_names\n    nbp.gene_codes = gene_codes\n    nbp.color_norm_factor = color_norm_factor\n    nbp.initial_raw_bleed_matrix = initial_raw_bleed_matrix\n    nbp.initial_bleed_matrix = initial_bleed_matrix\n    nbp.bleed_matrix = bleed_matrix\n    nbp.bled_codes = bled_codes\n    nbp.background_codes = background_codes\n\n    # shift in config file is just for one round.\n    n_spots, n_rounds_use, n_channels_use = spot_colors_use.shape\n    dp_norm_shift = nbp.dp_norm_shift * np.sqrt(n_rounds_use)\n\n    # Down-weight round/channels with high background when compute dot product\n    alpha = config['alpha']\n    beta = config['beta']\n    background_codes = background_codes[crc_ind].reshape(n_channels_use, -1)\n    background_var = background_coef[:, nbp_basic.use_channels]**2 @ background_codes**2 * alpha + beta ** 2\n\n    # find spot assignments to genes and gene efficiency\n    n_iter = config['gene_efficiency_n_iter'] + 1\n    pass_intensity_thresh = nbp_ref_spots.intensity &gt; nbp.gene_efficiency_intensity_thresh\n    use_ge_last = np.zeros(n_spots).astype(bool)\n    bled_codes_ge_use = bled_codes_use.copy()\n    for i in range(n_iter):\n        scores = np.asarray(dot_product_score(spot_colors_use.reshape(n_spots, -1),\n                                              bled_codes_ge_use.reshape(n_genes, -1), dp_norm_shift, 1/background_var))\n        spot_gene_no = np.argmax(scores, 1)\n        spot_score = scores[np.arange(np.shape(scores)[0]), spot_gene_no]\n        pass_score_thresh = spot_score &gt; config['gene_efficiency_score_thresh']\n\n        sort_gene_inds = np.argsort(scores, axis=1)\n        gene_no_second_best = sort_gene_inds[:, -2]\n        score_second_best = scores[np.arange(np.shape(scores)[0]), gene_no_second_best]\n        spot_score_diff = spot_score - score_second_best\n        pass_score_diff_thresh = spot_score_diff &gt; config['gene_efficiency_score_diff_thresh']\n        # only use isolated spots which pass strict thresholding to compute gene_efficiencies\n        use_ge = np.array([nbp_ref_spots.isolated, pass_intensity_thresh, pass_score_thresh,\n                           pass_score_diff_thresh]).all(axis=0)\n        # nan_to_num line below converts nan in bleed_matrix to 0.\n        # This basically just says that for dyes not in use_dyes, we expect intensity to be 0.\n        gene_efficiency_use = get_gene_efficiency(spot_colors_use[use_ge], spot_gene_no[use_ge],\n                                                  gene_codes[:, nbp_basic.use_rounds],\n                                                  np.nan_to_num(bleed_matrix[rc_ind]),\n                                                  config['gene_efficiency_min_spots'],\n                                                  config['gene_efficiency_max'],\n                                                  config['gene_efficiency_min'],\n                                                  config['gene_efficiency_min_factor'])\n\n        # get new bled codes using gene efficiency with L2 norm = 1.\n        multiplier_ge = np.tile(np.expand_dims(gene_efficiency_use, 2), [1, 1, n_channels_use])\n        bled_codes_ge_use = bled_codes_use * multiplier_ge\n        norm_factor = np.expand_dims(np.linalg.norm(bled_codes_ge_use, axis=(1, 2)), (1, 2))\n        norm_factor[norm_factor == 0] = 1  # For genes with no dye in use_dye, this avoids blow up on next line\n        bled_codes_ge_use = bled_codes_ge_use / norm_factor\n\n        if np.sum(use_ge != use_ge_last) &lt; 3:\n            # if less than 3 spots different in spots used for ge computation, end.\n            break\n        use_ge_last = use_ge.copy()\n\n    if config['gene_efficiency_n_iter'] &gt; 0:\n        # Compute score with final gene efficiency\n        scores = np.asarray(dot_product_score(spot_colors_use.reshape(n_spots, -1),\n                                              bled_codes_ge_use.reshape(n_genes, -1), dp_norm_shift, 1/background_var))\n        spot_gene_no = np.argmax(scores, 1)\n        spot_score = scores[np.arange(np.shape(scores)[0]), spot_gene_no]\n    else:\n        bled_codes_ge_use = bled_codes_use.copy()\n\n    # save score using the latest gene efficiency and diff to second best gene\n    nbp_ref_spots.score = spot_score.astype(np.float32)\n    nbp_ref_spots.gene_no = spot_gene_no.astype(np.int16)\n    sort_gene_inds = np.argsort(scores, axis=1)\n    gene_no_second_best = sort_gene_inds[:, -2]\n    score_second_best = scores[np.arange(np.shape(scores)[0]), gene_no_second_best]\n    nbp_ref_spots.score_diff = (nbp_ref_spots.score - score_second_best).astype(np.float16)\n\n    # save gene_efficiency[g,r] with nan when r outside use_rounds and 1 when gene_codes[g,r] outside use_dyes.\n    gene_efficiency = np.ones((n_genes, nbp_basic.n_rounds)) * np.nan\n    gene_efficiency[:, nbp_basic.use_rounds] = gene_efficiency_use\n    nbp.gene_efficiency = gene_efficiency\n\n    # bled_codes_ge[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes\n    bled_codes_ge = np.ones((nbp_basic.n_rounds, nbp_basic.n_channels, n_genes)) * np.nan\n    bled_codes_ge[rc_ind] = np.moveaxis(bled_codes_ge_use, 0, -1)\n    bled_codes_ge = np.moveaxis(bled_codes_ge, -1, 0)\n    nbp.bled_codes_ge = bled_codes_ge\n\n    ge_fail_genes = np.where(np.min(gene_efficiency_use,axis=1) == 1)[0]\n    n_fail_ge = len(ge_fail_genes)\n    if n_fail_ge &gt; 0:\n        fail_genes_str = [str(ge_fail_genes[i]) + ': ' + gene_names[ge_fail_genes][i] for i in range(n_fail_ge)]\n        fail_genes_str = '\\n'.join(fail_genes_str)\n        warnings.warn(f\"\\nGene Efficiency could not be calculated for {n_fail_ge}/{n_genes} \"\n                      f\"genes:\\n{fail_genes_str}\")\n    nbp_ref_spots.finalized = True\n    return nbp, nbp_ref_spots\n</code></pre>"},{"location":"code/pipeline/extract_run/","title":"Extract","text":""},{"location":"code/pipeline/extract_run/#coppafish.pipeline.extract_run.extract_and_filter","title":"<code>extract_and_filter(config, nbp_file, nbp_basic)</code>","text":"<p>This reads in images from the raw <code>nd2</code> files, filters them and then saves them as npy files in the tile directory. Also gets <code>auto_thresh</code> for use in turning images to point clouds and <code>hist_values</code>, <code>hist_counts</code> required for normalisation between channels.</p> <p>Returns the <code>extract</code> and <code>extract_debug</code> notebook pages.</p> <p>See <code>'extract'</code> and <code>'extract_debug'</code> sections of <code>notebook_comments.json</code> file for description of the variables in each page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'extract'</code> section of config file.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <ul> <li><code>NotebookPage[extract]</code> - Page containing <code>auto_thresh</code> for use in turning images to point clouds and <code>hist_values</code>, <code>hist_counts</code> required for normalisation between channels.</li> </ul> <code>NotebookPage</code> <ul> <li><code>NotebookPage[extract_debug]</code> - Page containing variables which are not needed later in the pipeline but may be useful for debugging purposes.</li> </ul> Source code in <code>coppafish/pipeline/extract_run.py</code> <pre><code>def extract_and_filter(config: dict, nbp_file: NotebookPage,\n                       nbp_basic: NotebookPage) -&gt; Tuple[NotebookPage, NotebookPage]:\n\"\"\"\n    This reads in images from the raw `nd2` files, filters them and then saves them as npy files in the tile directory.\n    Also gets `auto_thresh` for use in turning images to point clouds and `hist_values`, `hist_counts` required for\n    normalisation between channels.\n\n    Returns the `extract` and `extract_debug` notebook pages.\n\n    See `'extract'` and `'extract_debug'` sections of `notebook_comments.json` file\n    for description of the variables in each page.\n\n    Args:\n        config: Dictionary obtained from `'extract'` section of config file.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n\n    Returns:\n        - `NotebookPage[extract]` - Page containing `auto_thresh` for use in turning images to point clouds and\n            `hist_values`, `hist_counts` required for normalisation between channels.\n        - `NotebookPage[extract_debug]` - Page containing variables which are not needed later in the pipeline\n            but may be useful for debugging purposes.\n    \"\"\"\n    # Check scaling won't cause clipping when saving as uint16\n    scale_norm_max = np.iinfo(np.uint16).max - nbp_basic.tile_pixel_value_shift\n    if config['scale_norm'] &gt;= scale_norm_max:\n        raise ValueError(f\"\\nconfig['extract']['scale_norm'] = {config['scale_norm']} but it must be below \"\n                         f\"{scale_norm_max}\")\n\n    # initialise notebook pages\n    if not nbp_basic.is_3d:\n        config['deconvolve'] = False  # only deconvolve if 3d pipeline\n    nbp = NotebookPage(\"extract\")\n    nbp_debug = NotebookPage(\"extract_debug\")\n    # initialise output of this part of pipeline as 'vars' key\n    nbp.auto_thresh = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds + nbp_basic.n_extra_rounds,\n                                nbp_basic.n_channels), dtype=int)\n    nbp.hist_values = np.arange(-nbp_basic.tile_pixel_value_shift, np.iinfo(np.uint16).max -\n                                nbp_basic.tile_pixel_value_shift + 2, 1)\n    nbp.hist_counts = np.zeros((len(nbp.hist_values), nbp_basic.n_rounds, nbp_basic.n_channels), dtype=int)\n    hist_bin_edges = np.concatenate((nbp.hist_values - 0.5, nbp.hist_values[-1:] + 0.5))\n    # initialise debugging info as 'debug' page\n    nbp_debug.n_clip_pixels = np.zeros_like(nbp.auto_thresh, dtype=int)\n    nbp_debug.clip_extract_scale = np.zeros_like(nbp.auto_thresh, dtype=np.float32)\n    if nbp_basic.is_3d:\n        nbp_debug.z_info = int(np.floor(nbp_basic.nz / 2))  # central z-plane to get info from.\n    else:\n        nbp_debug.z_info = 0\n\n    # update config params in notebook. All optional parameters in config are added to debug page\n    if config['r1'] is None:\n        config['r1'] = extract.get_pixel_length(config['r1_auto_microns'], nbp_basic.pixel_size_xy)\n    if config['r2'] is None:\n        config['r2'] = config['r1'] * 2\n    if config['r_dapi'] is None:\n        if config['r_dapi_auto_microns'] is not None:\n            config['r_dapi'] = extract.get_pixel_length(config['r_dapi_auto_microns'], nbp_basic.pixel_size_xy)\n    nbp_debug.r1 = config['r1']\n    nbp_debug.r2 = config['r2']\n    nbp_debug.r_dapi = config['r_dapi']\n\n    filter_kernel = utils.morphology.hanning_diff(nbp_debug.r1, nbp_debug.r2)\n    if nbp_debug.r_dapi is not None:\n        filter_kernel_dapi = utils.strel.disk(nbp_debug.r_dapi)\n    else:\n        filter_kernel_dapi = None\n\n    if config['r_smooth'] is not None:\n        if len(config['r_smooth']) == 2:\n            if nbp_basic.is_3d:\n                warnings.warn(f\"Running 3D pipeline but only 2D smoothing requested with r_smooth\"\n                              f\" = {config['r_smooth']}.\")\n        elif len(config['r_smooth']) == 3:\n            if not nbp_basic.is_3d:\n                raise ValueError(\"Running 2D pipeline but 3D smoothing requested.\")\n        else:\n            raise ValueError(f\"r_smooth provided was {config['r_smooth']}.\\n\"\n                             f\"But it needs to be a 2 radii for 2D smoothing or 3 radii for 3D smoothing.\\n\"\n                             f\"I.e. it is the wrong shape.\")\n        if config['r_smooth'][0] &gt; config['r2']:\n            raise ValueError(f\"Smoothing radius, {config['r_smooth'][0]}, is larger than the outer radius of the\\n\"\n                             f\"hanning filter, {config['r2']}, making the filtering step redundant.\")\n\n        # smooth_kernel = utils.strel.fspecial(*tuple(config['r_smooth']))\n        smooth_kernel = np.ones(tuple(np.array(config['r_smooth'], dtype=int) * 2 - 1))\n        smooth_kernel = smooth_kernel / np.sum(smooth_kernel)\n\n        if np.max(config['r_smooth']) == 1:\n            warnings.warn('Max radius of smooth filter was 1, so not using.')\n            config['r_smooth'] = None\n\n    if config['deconvolve']:\n        if not os.path.isfile(nbp_file.psf):\n            spot_images, config['psf_intensity_thresh'], psf_tiles_used = \\\n                extract.get_psf_spots(nbp_file, nbp_basic, nbp_basic.ref_round,\n                                      nbp_basic.use_tiles, nbp_basic.ref_channel, nbp_basic.use_z,\n                                      config['psf_detect_radius_xy'], config['psf_detect_radius_z'],\n                                      config['psf_min_spots'], config['psf_intensity_thresh'],\n                                      config['auto_thresh_multiplier'], config['psf_isolation_dist'],\n                                      config['psf_shape'])\n            psf = extract.get_psf(spot_images, config['psf_annulus_width'])\n            np.save(nbp_file.psf, np.moveaxis(psf, 2, 0))  # save with z as first axis\n        else:\n            # Know psf only computed for 3D pipeline hence know ndim=3\n            psf = np.moveaxis(np.load(nbp_file.psf), 0, 2)  # Put z to last index\n            psf_tiles_used = None\n        # normalise psf so min is 0 and max is 1.\n        psf = psf - psf.min()\n        psf = psf / psf.max()\n        pad_im_shape = np.array([nbp_basic.tile_sz, nbp_basic.tile_sz, nbp_basic.nz]) + \\\n                       np.array(config['wiener_pad_shape']) * 2\n        wiener_filter = extract.get_wiener_filter(psf, pad_im_shape, config['wiener_constant'])\n        nbp_debug.psf = psf\n        nbp_debug.psf_intensity_thresh = config['psf_intensity_thresh']\n        nbp_debug.psf_tiles_used = psf_tiles_used\n    else:\n        nbp_debug.psf = None\n        nbp_debug.psf_intensity_thresh = None\n        nbp_debug.psf_tiles_used = None\n\n    # check to see if scales have already been computed\n    config['scale'], config['scale_anchor'] = extract.get_scale_from_txt(nbp_file.scale, config['scale'],\n                                                                         config['scale_anchor'])\n\n    if config['scale'] is None and len(nbp_file.round) &gt; 0:\n        # ensure scale_norm value is reasonable so max pixel value in tiff file is significant factor of max of uint16\n        scale_norm_min = (np.iinfo('uint16').max - nbp_basic.tile_pixel_value_shift) / 5\n        scale_norm_max = np.iinfo('uint16').max - nbp_basic.tile_pixel_value_shift\n        if not scale_norm_min &lt;= config['scale_norm'] &lt;= scale_norm_max:\n            raise utils.errors.OutOfBoundsError(\"scale_norm\", config['scale_norm'], scale_norm_min, scale_norm_max)\n        # If using smoothing, apply this as well before scal\n        if config['r_smooth'] is None:\n            smooth_kernel_2d = None\n        else:\n            if smooth_kernel.ndim == 3:\n                # take central plane of smooth filter if 3D as scale is found from a single z-plane.\n                smooth_kernel_2d = smooth_kernel[:, :, config['r_smooth'][2] - 1]\n            else:\n                smooth_kernel_2d = smooth_kernel.copy()\n            # smoothing is averaging so to average in 2D, need to re normalise filter\n            smooth_kernel_2d = smooth_kernel_2d / np.sum(smooth_kernel_2d)\n            if np.max(config['r_smooth'][:2]) &lt;= 1:\n                smooth_kernel_2d = None  # If dimensions of 2D kernel are [1, 1] is equivalent to no smoothing\n        nbp_debug.scale_tile, nbp_debug.scale_channel, nbp_debug.scale_z, config['scale'] = \\\n            extract.get_scale(nbp_file, nbp_basic, 0, nbp_basic.use_tiles, nbp_basic.use_channels, nbp_basic.use_z,\n                              config['scale_norm'], filter_kernel, smooth_kernel_2d)\n    else:\n        nbp_debug.scale_tile = None\n        nbp_debug.scale_channel = None\n        nbp_debug.scale_z = None\n        smooth_kernel_2d = None\n    nbp_debug.scale = config['scale']\n    print(f\"Scale: {nbp_debug.scale}\")\n\n    # save scale values incase need to re-run\n    extract.save_scale(nbp_file.scale, nbp_debug.scale, config['scale_anchor'])\n\n    # get rounds to iterate over\n    use_channels_anchor = [c for c in [nbp_basic.dapi_channel, nbp_basic.anchor_channel] if c is not None]\n    use_channels_anchor.sort()\n    if filter_kernel_dapi is None:\n        # If not filtering DAPI, skip over the DAPI channel.\n        use_channels_anchor = np.setdiff1d(use_channels_anchor, nbp_basic.dapi_channel)\n    if nbp_basic.use_anchor:\n        # always have anchor as first round after imaging rounds\n        round_files = nbp_file.round + [nbp_file.anchor]\n        use_rounds = nbp_basic.use_rounds + [nbp_basic.n_rounds]\n        n_images = (len(use_rounds) - 1) * len(nbp_basic.use_tiles) * len(nbp_basic.use_channels) + \\\n                   len(nbp_basic.use_tiles) * len(use_channels_anchor)\n    else:\n        round_files = nbp_file.round\n        use_rounds = nbp_basic.use_rounds\n        n_images = len(use_rounds) * len(nbp_basic.use_tiles) * len(nbp_basic.use_channels)\n\n    n_clip_error_images = 0\n    if config['n_clip_error'] is None:\n        # default is 1% of pixels on single z-plane\n        config['n_clip_error'] = int(nbp_basic.tile_sz * nbp_basic.tile_sz / 100)\n\n    with tqdm(total=n_images) as pbar:\n        pbar.set_description(f'Loading in tiles from {nbp_file.raw_extension}, filtering and saving as .npy')\n        for r in use_rounds:\n            # set scale and channels to use\n            im_file = os.path.join(nbp_file.input_dir, round_files[r])\n            if nbp_file.raw_extension == '.npy':\n                extract.wait_for_data(im_file, config['wait_time'], dir=True)\n            else:\n                extract.wait_for_data(im_file + nbp_file.raw_extension, config['wait_time'])\n            round_dask_array = utils.raw.load(nbp_file, nbp_basic, r=r)\n            if r == nbp_basic.anchor_round:\n                n_clip_error_images = 0  # reset for anchor as different scale used.\n                if config['scale_anchor'] is None:\n                    nbp_debug.scale_anchor_tile, _, nbp_debug.scale_anchor_z, config['scale_anchor'] = \\\n                        extract.get_scale(nbp_file, nbp_basic, nbp_basic.anchor_round, nbp_basic.use_tiles,\n                                          [nbp_basic.anchor_channel], nbp_basic.use_z,\n                                          config['scale_norm'], filter_kernel, smooth_kernel_2d)\n                    # save scale values incase need to re-run\n                    extract.save_scale(nbp_file.scale, nbp_debug.scale, config['scale_anchor'])\n                else:\n                    nbp_debug.scale_anchor_tile = None\n                    nbp_debug.scale_anchor_z = None\n                nbp_debug.scale_anchor = config['scale_anchor']\n                print(f\"Scale_anchor: {nbp_debug.scale_anchor}\")\n                scale = nbp_debug.scale_anchor\n                use_channels = use_channels_anchor\n            else:\n                scale = nbp_debug.scale\n                use_channels = nbp_basic.use_channels\n\n            # convolve_2d each image\n            for t in nbp_basic.use_tiles:\n                if not nbp_basic.is_3d:\n                    # for 2d all channels in same file\n                    file_exists = os.path.isfile(nbp_file.tile[t][r])\n                    if file_exists:\n                        # mmap load in image for all channels if tiff exists\n                        im_all_channels_2d = np.load(nbp_file.tile[t][r], mmap_mode='r')\n                    else:\n                        # Only save 2d data when all channels collected\n                        # For channels not used, keep all pixels 0.\n                        im_all_channels_2d = np.zeros((nbp_basic.n_channels, nbp_basic.tile_sz,\n                                                       nbp_basic.tile_sz), dtype=np.int32)\n                for c in use_channels:\n                    if r == nbp_basic.anchor_round and c == nbp_basic.anchor_channel:\n                        # max value that can be saved and no shifting done for DAPI\n                        max_npy_pixel_value = np.iinfo(np.uint16).max\n                    else:\n                        max_npy_pixel_value = np.iinfo(np.uint16).max - nbp_basic.tile_pixel_value_shift\n                    if nbp_basic.is_3d:\n                        file_exists = os.path.isfile(nbp_file.tile[t][r][c])\n                    pbar.set_postfix({'round': r, 'tile': t, 'channel': c, 'exists': str(file_exists)})\n                    if file_exists:\n                        if r == nbp_basic.anchor_round and c == nbp_basic.dapi_channel:\n                            pass\n                        else:\n                            # Only need to load in mid-z plane if 3D.\n                            if nbp_basic.is_3d:\n                                im = utils.npy.load_tile(nbp_file, nbp_basic, t, r, c,\n                                                         yxz=[None, None, nbp_debug.z_info])\n                            else:\n                                im = im_all_channels_2d[c].astype(np.int32) - nbp_basic.tile_pixel_value_shift\n                            nbp.auto_thresh[t, r, c], hist_counts_trc, nbp_debug.n_clip_pixels[t, r, c], \\\n                            nbp_debug.clip_extract_scale[t, r, c] = \\\n                                extract.get_extract_info(im, config['auto_thresh_multiplier'], hist_bin_edges,\n                                                         max_npy_pixel_value, scale)\n                            if r != nbp_basic.anchor_round:\n                                nbp.hist_counts[:, r, c] += hist_counts_trc\n                    else:\n                        im = utils.raw.load(nbp_file, nbp_basic, round_dask_array, r, t, c, nbp_basic.use_z)\n                        if not nbp_basic.is_3d:\n                            im = extract.focus_stack(im)\n                        im, bad_columns = extract.strip_hack(im)  # find faulty columns\n                        if config['deconvolve']:\n                            im = extract.wiener_deconvolve(im, config['wiener_pad_shape'], wiener_filter)\n                        if r == nbp_basic.anchor_round and c == nbp_basic.dapi_channel:\n                            im = utils.morphology.top_hat(im, filter_kernel_dapi)\n                            im[:, bad_columns] = 0\n                        else:\n                            # im converted to float in convolve_2d so no point changing dtype beforehand.\n                            im = utils.morphology.convolve_2d(im, filter_kernel) * scale\n                            if config['r_smooth'] is not None:\n                                # oa convolve uses lots of memory and much slower here.\n                                im = utils.morphology.imfilter(im, smooth_kernel, oa=False)\n                            im[:, bad_columns] = 0\n                            # get_info is quicker on int32 so do this conversion first.\n                            im = np.rint(im, np.zeros_like(im, dtype=np.int32), casting='unsafe')\n                            # only use image unaffected by strip_hack to get information from tile\n                            good_columns = np.setdiff1d(np.arange(nbp_basic.tile_sz), bad_columns)\n                            nbp.auto_thresh[t, r, c], hist_counts_trc, nbp_debug.n_clip_pixels[t, r, c], \\\n                            nbp_debug.clip_extract_scale[t, r, c] = \\\n                                extract.get_extract_info(im[:, good_columns], config['auto_thresh_multiplier'],\n                                                         hist_bin_edges, max_npy_pixel_value, scale,\n                                                         nbp_debug.z_info)\n\n                            # Deal with pixels outside uint16 range when saving\n                            if nbp_debug.n_clip_pixels[t, r, c] &gt; config['n_clip_warn']:\n                                warnings.warn(f\"\\nTile {t}, round {r}, channel {c} has \"\n                                              f\"{nbp_debug.n_clip_pixels[t, r, c]} pixels\\n\"\n                                              f\"that will be clipped when converting to uint16.\")\n                            if nbp_debug.n_clip_pixels[t, r, c] &gt; config['n_clip_error']:\n                                n_clip_error_images += 1\n                                message = f\"\\nNumber of images for which more than {config['n_clip_error']} pixels \" \\\n                                          f\"clipped in conversion to uint16 is {n_clip_error_images}.\"\n                                if n_clip_error_images &gt;= config['n_clip_error_images_thresh']:\n                                    # create new Notebook to save info obtained so far\n                                    nb_fail_name = os.path.join(nbp_file.output_dir, 'notebook_extract_error.npz')\n                                    nb_fail = Notebook(nb_fail_name, None)\n                                    # change names of pages so can add extra properties not in json file.\n                                    nbp.name = 'extract_fail'\n                                    nbp_debug.name = 'extract_debug_fail'\n                                    nbp.fail_trc = np.array([t, r, c])  # record where failure occurred\n                                    nbp_debug.fail_trc = np.array([t, r, c])\n                                    nb_fail += nbp\n                                    nb_fail += nbp_debug\n                                    raise ValueError(f\"{message}\\nResults up till now saved as {nb_fail_name}.\")\n                                else:\n                                    warnings.warn(f\"{message}\\nWhen this reaches {config['n_clip_error_images_thresh']}\"\n                                                  f\", the extract step of the algorithm will be interrupted.\")\n\n                            if r != nbp_basic.anchor_round:\n                                nbp.hist_counts[:, r, c] += hist_counts_trc\n                        if nbp_basic.is_3d:\n                            utils.npy.save_tile(nbp_file, nbp_basic, im, t, r, c)\n                        else:\n                            im_all_channels_2d[c] = im\n                    pbar.update(1)\n                if not nbp_basic.is_3d:\n                    utils.npy.save_tile(nbp_file, nbp_basic, im_all_channels_2d, t, r)\n    pbar.close()\n    if not nbp_basic.use_anchor:\n        nbp_debug.scale_anchor_tile = None\n        nbp_debug.scale_anchor_z = None\n        nbp_debug.scale_anchor = None\n    return nbp, nbp_debug\n</code></pre>"},{"location":"code/pipeline/find_spots/","title":"Find Spots","text":""},{"location":"code/pipeline/find_spots/#coppafish.pipeline.find_spots.find_spots","title":"<code>find_spots(config, nbp_file, nbp_basic, auto_thresh)</code>","text":"<p>This function turns each tiff file in the tile directory into a point cloud, saving the results as <code>spot_details</code> in the <code>find_spots</code> notebook page.</p> <p>See <code>'find_spots'</code> section of <code>notebook_comments.json</code> file for description of the variables in the page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'find_spots'</code> section of config file.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>auto_thresh</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels]</code>. <code>auto_thresh[t, r, c]</code> is the threshold for the tiff file corresponding to tile <code>t</code>, round <code>r</code>, channel <code>c</code> such that all local maxima with pixel values greater than this are considered spots.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[find_spots]</code> - Page containing point cloud of all tiles, rounds and channels.</p> Source code in <code>coppafish/pipeline/find_spots.py</code> <pre><code>def find_spots(config: dict, nbp_file: NotebookPage, nbp_basic: NotebookPage, auto_thresh: np.ndarray) -&gt; NotebookPage:\n\"\"\"\n    This function turns each tiff file in the tile directory into a point cloud, saving the results\n    as `spot_details` in the `find_spots` notebook page.\n\n    See `'find_spots'` section of `notebook_comments.json` file\n    for description of the variables in the page.\n\n    Args:\n        config: Dictionary obtained from `'find_spots'` section of config file.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        auto_thresh: `float [n_tiles x n_rounds x n_channels]`.\n            `auto_thresh[t, r, c]` is the threshold for the tiff file corresponding to tile `t`, round `r`, channel `c`\n            such that all local maxima with pixel values greater than this are considered spots.\n\n    Returns:\n        `NotebookPage[find_spots]` - Page containing point cloud of all tiles, rounds and channels.\n    \"\"\"\n    nbp = NotebookPage(\"find_spots\")\n    if nbp_basic.is_3d is False:\n        # set z details to None if using 2d pipeline\n        config['radius_z'] = None\n        config['isolation_radius_z'] = None\n        max_spots = config['max_spots_2d']\n    else:\n        max_spots = config['max_spots_3d']\n\n    # record threshold for isolated spots in each tile of reference round/channel\n    if config['isolation_thresh'] is None:\n        nbp.isolation_thresh = auto_thresh[:, nbp_basic.ref_round, nbp_basic.anchor_channel] * \\\n                                  config['auto_isolation_thresh_multiplier']\n    else:\n        nbp.isolation_thresh = np.ones_like(auto_thresh[:, nbp_basic.ref_round, nbp_basic.anchor_channel]) * \\\n                                  config['isolation_thresh']\n\n    # have to save spot_yxz and spot_isolated as table to stop pickle issues associated with numpy object arrays.\n    # columns of spot_details are: tile, channel, round, isolated, y, x, z\n    # max value is y or x coordinate of around 2048 hence can use int16.\n    spot_details = np.empty((0, 7), dtype=np.int16)\n    nbp.spot_no = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds+nbp_basic.n_extra_rounds,\n                            nbp_basic.n_channels), dtype=np.int32)\n    use_rounds = nbp_basic.use_rounds\n    n_images = len(use_rounds) * len(nbp_basic.use_tiles) * len(nbp_basic.use_channels)\n    if nbp_basic.use_anchor:\n        use_rounds = use_rounds + [nbp_basic.anchor_round]\n        n_images = n_images + len(nbp_basic.use_tiles)\n    n_z = np.max([1, nbp_basic.is_3d * nbp_basic.nz])\n    with tqdm(total=n_images) as pbar:\n        pbar.set_description(f\"Detecting spots on filtered images saved as npy\")\n        for r in use_rounds:\n            if r == nbp_basic.anchor_round:\n                use_channels = [nbp_basic.anchor_channel]\n            else:\n                use_channels = nbp_basic.use_channels\n            for t in nbp_basic.use_tiles:\n                for c in use_channels:\n                    pbar.set_postfix({'round': r, 'tile': t, 'channel': c})\n                    # Find local maxima on shifted uint16 images to save time avoiding conversion to int32.\n                    # Then need to shift the detect_spots and check_neighb_intensity thresh correspondingly.\n                    image = utils.npy.load_tile(nbp_file, nbp_basic, t, r, c, apply_shift=False)\n                    spot_yxz, spot_intensity = fs.detect_spots(image,\n                                                               auto_thresh[t, r, c] + nbp_basic.tile_pixel_value_shift,\n                                                               config['radius_xy'], config['radius_z'], True)\n                    no_negative_neighbour = fs.check_neighbour_intensity(image, spot_yxz,\n                                                                         thresh=nbp_basic.tile_pixel_value_shift)\n                    spot_yxz = spot_yxz[no_negative_neighbour]\n                    spot_intensity = spot_intensity[no_negative_neighbour]\n                    if r == nbp_basic.ref_round:\n                        spot_isolated = fs.get_isolated(image.astype(np.int32) - nbp_basic.tile_pixel_value_shift,\n                                                        spot_yxz, nbp.isolation_thresh[t],\n                                                        config['isolation_radius_inner'],\n                                                        config['isolation_radius_xy'],\n                                                        config['isolation_radius_z'])\n\n                    else:\n                        # if imaging round, only keep the highest intensity spots on each z plane\n                        # as only used for registration\n                        keep = np.ones(spot_yxz.shape[0], dtype=bool)\n                        for z in range(n_z):\n                            if nbp_basic.is_3d:\n                                in_z = spot_yxz[:, 2] == z\n                            else:\n                                in_z = np.ones(spot_yxz.shape[0], dtype=bool)\n                            if np.sum(in_z) &gt; max_spots:\n                                intensity_thresh = np.sort(spot_intensity[in_z])[-max_spots]\n                                keep[np.logical_and(in_z, spot_intensity &lt; intensity_thresh)] = False\n                        spot_yxz = spot_yxz[keep]\n                        # don't care if these spots isolated so say they are not\n                        spot_isolated = np.zeros(spot_yxz.shape[0], dtype=bool)\n                    spot_details_trc = np.zeros((spot_yxz.shape[0], spot_details.shape[1]), dtype=np.int16)\n                    spot_details_trc[:, :3] = [t, r, c]\n                    spot_details_trc[:, 3] = spot_isolated\n                    spot_details_trc[:, 4:4+spot_yxz.shape[1]] = spot_yxz  # if 2d pipeline, z coordinate set to 0.\n                    spot_details = np.append(spot_details, spot_details_trc, axis=0)\n                    nbp.spot_no[t, r, c] = spot_yxz.shape[0]\n                    pbar.update(1)\n    nbp.spot_details = spot_details\n    return nbp\n</code></pre>"},{"location":"code/pipeline/get_reference_spots/","title":"Get Reference Spots","text":""},{"location":"code/pipeline/get_reference_spots/#coppafish.pipeline.get_reference_spots.get_reference_spots","title":"<code>get_reference_spots(nbp_file, nbp_basic, spot_details, tile_origin, transform)</code>","text":"<p>This takes each spot found on the reference round/channel and computes the corresponding intensity in each of the imaging rounds/channels.</p> <p>See <code>'ref_spots'</code> section of <code>notebook_comments.json</code> file for description of the variables in the page. The following variables:</p> <ul> <li><code>gene_no</code></li> <li><code>score</code></li> <li><code>score_diff</code></li> <li><code>intensity</code></li> </ul> <p>will be set to <code>None</code> so the page can be added to a Notebook. <code>call_reference_spots</code> should then be run to give their actual values. This is so if there is an error in <code>call_reference_spots</code>, <code>get_reference_spots</code> won't have to be re-run.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>spot_details</code> <code>np.ndarray</code> <p><code>int [n_spots x 7]</code>. <code>spot_details[s]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> of spot <code>s</code>. This is saved in the find_spots notebook page i.e. <code>nb.find_spots.spot_details</code>.</p> required <code>tile_origin</code> <code>np.ndarray</code> <p><code>float [n_tiles x 3]</code>. <code>tile_origin[t,:]</code> is the bottom left yxz coordinate of tile <code>t</code>. yx coordinates in <code>yx_pixels</code> and z coordinate in <code>z_pixels</code>. This is saved in the <code>stitch</code> notebook page i.e. <code>nb.stitch.tile_origin</code>.</p> required <code>transform</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x 4 x 3]</code>. <code>transform[t, r, c]</code> is the affine transform to get from tile <code>t</code>, <code>ref_round</code>, <code>ref_channel</code> to tile <code>t</code>, round <code>r</code>, channel <code>c</code>. This is saved in the register notebook page i.e. <code>nb.register.transform</code>.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[ref_spots]</code> - Page containing intensity of each reference spot on each imaging round/channel.</p> Source code in <code>coppafish/pipeline/get_reference_spots.py</code> <pre><code>def get_reference_spots(nbp_file: NotebookPage, nbp_basic: NotebookPage, spot_details: np.ndarray,\n                        tile_origin: np.ndarray, transform: np.ndarray) -&gt; NotebookPage:\n\"\"\"\n    This takes each spot found on the reference round/channel and computes the corresponding intensity\n    in each of the imaging rounds/channels.\n\n    See `'ref_spots'` section of `notebook_comments.json` file\n    for description of the variables in the page.\n    The following variables:\n\n    * `gene_no`\n    * `score`\n    * `score_diff`\n    * `intensity`\n\n    will be set to `None` so the page can be added to a *Notebook*. `call_reference_spots` should then be run\n    to give their actual values. This is so if there is an error in `call_reference_spots`,\n    `get_reference_spots` won't have to be re-run.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        spot_details: `int [n_spots x 7]`.\n            `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`.\n            This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`.\n        tile_origin: `float [n_tiles x 3]`.\n            `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`.\n            yx coordinates in `yx_pixels` and z coordinate in `z_pixels`.\n            This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`.\n        transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`.\n            `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to\n            tile `t`, round `r`, channel `c`.\n            This is saved in the register notebook page i.e. `nb.register.transform`.\n\n    Returns:\n        `NotebookPage[ref_spots]` - Page containing intensity of each reference spot on each imaging round/channel.\n    \"\"\"\n    nbp = NotebookPage(\"ref_spots\")\n    r = nbp_basic.ref_round\n    c = nbp_basic.ref_channel\n\n    # all means all spots found on the reference round / channel\n    all_local_yxz = np.zeros((0, 3), dtype=np.int16)\n    all_isolated = np.zeros(0, dtype=bool)\n    all_local_tile = np.zeros(0, dtype=np.int16)\n    for t in nbp_basic.use_tiles:\n        t_local_yxz, t_isolated = spot_yxz(spot_details, t, r, c, return_isolated=True)\n        if np.shape(t_local_yxz)[0] &gt; 0:\n            all_local_yxz = np.append(all_local_yxz, t_local_yxz, axis=0)\n            all_isolated = np.append(all_isolated, t_isolated.astype(bool), axis=0)\n            all_local_tile = np.append(all_local_tile, np.ones_like(t_isolated, dtype=np.int16) * t)\n\n    # find duplicate spots as those detected on a tile which is not tile centre they are closest to\n    not_duplicate = get_non_duplicate(tile_origin, nbp_basic.use_tiles, nbp_basic.tile_centre, all_local_yxz,\n                                      all_local_tile)\n\n    # nd means all spots that are not duplicate\n    nd_local_yxz = all_local_yxz[not_duplicate]\n    nd_isolated = all_isolated[not_duplicate]\n    nd_local_tile = all_local_tile[not_duplicate]\n    invalid_value = -nbp_basic.tile_pixel_value_shift\n    # Only save used rounds/channels initially\n    n_use_rounds = len(nbp_basic.use_rounds)\n    n_use_channels = len(nbp_basic.use_channels)\n    use_tiles = np.array(nbp_basic.use_tiles.copy())\n    n_use_tiles = len(use_tiles)\n    nd_spot_colors_use = np.zeros((nd_local_tile.shape[0], n_use_rounds, n_use_channels), dtype=np.int32)\n    transform = jnp.asarray(transform)\n    print('Reading in spot_colors for ref_round spots')\n    for t in nbp_basic.use_tiles:\n        in_tile = nd_local_tile == t\n        if np.sum(in_tile) &gt; 0:\n            print(f\"Tile {np.where(use_tiles==t)[0][0]+1}/{n_use_tiles}\")\n            # this line will return invalid_value for spots outside tile bounds on particular r/c.\n            nd_spot_colors_use[in_tile] = get_spot_colors(jnp.asarray(nd_local_yxz[in_tile]), t,\n                                                          transform, nbp_file, nbp_basic)\n    # good means all spots that were in bounds of tile on every imaging round and channel that was used.\n    # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use, 0, -1)\n    # use_rc_index = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels)\n    # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use[use_rc_index], -1, 0)\n    good = ~np.any(nd_spot_colors_use == invalid_value, axis=(1, 2))\n\n    good_local_yxz = nd_local_yxz[good]\n    good_isolated = nd_isolated[good]\n    good_local_tile = nd_local_tile[good]\n    # add in un-used rounds with invalid_value\n    n_good = np.sum(good)\n    good_spot_colors = np.full((n_good, nbp_basic.n_rounds,\n                                nbp_basic.n_channels), invalid_value, dtype=np.int32)\n    good_spot_colors[np.ix_(np.arange(n_good), nbp_basic.use_rounds, nbp_basic.use_channels)] = nd_spot_colors_use[good]\n\n    # save spot info to notebook\n    nbp.local_yxz = good_local_yxz\n    nbp.isolated = good_isolated\n    nbp.tile = good_local_tile\n    nbp.colors = good_spot_colors\n\n    # Set variables added in call_reference_spots to None so can save to Notebook.\n    # I.e. if call_reference_spots hit error, but we did not do this,\n    # we would have to run get_reference_spots again.\n    nbp.gene_no = None\n    nbp.score = None\n    nbp.score_diff = None\n    nbp.intensity = None\n    return nbp\n</code></pre>"},{"location":"code/pipeline/omp/","title":"OMP","text":""},{"location":"code/pipeline/omp/#coppafish.pipeline.omp.call_spots_omp","title":"<code>call_spots_omp(config, nbp_file, nbp_basic, nbp_call_spots, tile_origin, transform, shape_tile)</code>","text":"<p>This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel.</p> <p>From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene. Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate.</p> <p>See <code>'omp'</code> section of <code>notebook_comments.json</code> file for description of the variables in the omp page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'omp'</code> section of config file.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>nbp_call_spots</code> <code>NotebookPage</code> required <code>tile_origin</code> <code>np.ndarray</code> <p><code>float [n_tiles x 3]</code>. <code>tile_origin[t,:]</code> is the bottom left yxz coordinate of tile <code>t</code>. yx coordinates in <code>yx_pixels</code> and z coordinate in <code>z_pixels</code>. This is saved in the <code>stitch</code> notebook page i.e. <code>nb.stitch.tile_origin</code>.</p> required <code>transform</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x 4 x 3]</code>. <code>transform[t, r, c]</code> is the affine transform to get from tile <code>t</code>, <code>ref_round</code>, <code>ref_channel</code> to tile <code>t</code>, round <code>r</code>, channel <code>c</code>. This is saved in the register notebook page i.e. <code>nb.register.transform</code>.</p> required <code>shape_tile</code> <code>Optional[int]</code> <p>Tile to use to compute the expected shape of a spot in the gene coefficient images. Should be the tile, for which the most spots where found in the <code>call_reference_spots</code> step. If <code>None</code>, will be set to the centre tile.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[omp]</code> - Page contains gene assignments and info for spots using omp.</p> Source code in <code>coppafish/pipeline/omp.py</code> <pre><code>def call_spots_omp(config: dict, nbp_file: NotebookPage, nbp_basic: NotebookPage,\n                   nbp_call_spots: NotebookPage, tile_origin: np.ndarray,\n                   transform: np.ndarray, shape_tile: Optional[int]) -&gt; NotebookPage:\n\"\"\"\n    This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel.\n\n    From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene.\n    Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate.\n\n    See `'omp'` section of `notebook_comments.json` file for description of the variables in the omp page.\n\n    Args:\n        config: Dictionary obtained from `'omp'` section of config file.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        nbp_call_spots:\n        tile_origin: `float [n_tiles x 3]`.\n            `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`.\n            yx coordinates in `yx_pixels` and z coordinate in `z_pixels`.\n            This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`.\n        transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`.\n            `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to\n            tile `t`, round `r`, channel `c`.\n            This is saved in the register notebook page i.e. `nb.register.transform`.\n        shape_tile: Tile to use to compute the expected shape of a spot in the gene coefficient images.\n            Should be the tile, for which the most spots where found in the `call_reference_spots` step.\n            If `None`, will be set to the centre tile.\n\n    Returns:\n        `NotebookPage[omp]` - Page contains gene assignments and info for spots using omp.\n\n    \"\"\"\n    nbp = NotebookPage(\"omp\")\n\n    # use bled_codes with gene efficiency incorporated and only use_rounds/channels\n    rc_ind = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels)\n    bled_codes = np.moveaxis(np.moveaxis(nbp_call_spots.bled_codes_ge, 0, -1)[rc_ind], -1, 0)\n    utils.errors.check_color_nan(bled_codes, nbp_basic)\n    norm_bled_codes = np.linalg.norm(bled_codes, axis=(1, 2))\n    if np.abs(norm_bled_codes - 1).max() &gt; 1e-6:\n        raise ValueError(\"nbp_call_spots.bled_codes_ge don't all have an L2 norm of 1 over \"\n                         \"use_rounds and use_channels.\")\n    bled_codes = jnp.asarray(bled_codes)\n    transform = jnp.asarray(transform)\n    color_norm_factor = jnp.asarray(nbp_call_spots.color_norm_factor[rc_ind])\n    n_genes, n_rounds_use, n_channels_use = bled_codes.shape\n    dp_norm_shift = nbp_call_spots.dp_norm_shift * np.sqrt(n_rounds_use)\n\n    if nbp_basic.is_3d:\n        detect_radius_z = config['radius_z']\n        n_z = nbp_basic.nz\n    else:\n        detect_radius_z = None\n        n_z = 1\n        config['use_z'] = np.arange(n_z)\n\n    if config['use_z'] is not None:\n        use_z_oob = [val for val in config['use_z'] if val &lt; 0 or val &gt;= n_z]\n        if len(use_z_oob) &gt; 0:\n            raise utils.errors.OutOfBoundsError(\"use_z\", use_z_oob[0], 0, n_z - 1)\n        if len(config['use_z']) == 2:\n            # use consecutive values if only 2 given.\n            config['use_z'] = list(np.arange(config['use_z'][0], config['use_z'][1] + 1))\n        use_z = np.array(config['use_z'])\n    else:\n        use_z = np.arange(n_z)\n\n    # determine initial_intensity_thresh from average intensity over all pixels on central z-plane.\n    nbp.initial_intensity_thresh = omp.get_initial_intensity_thresh(config, nbp_call_spots)\n\n    use_tiles = np.array(nbp_basic.use_tiles.copy())\n    if not os.path.isfile(nbp_file.omp_spot_shape):\n        # Set tile order so do shape_tile first to compute spot_shape from it.\n        if shape_tile is None:\n            shape_tile = scale.central_tile(nbp_basic.tilepos_yx, nbp_basic.use_tiles)\n        if shape_tile not in nbp_basic.use_tiles:\n            raise ValueError(f\"shape_tile, {shape_tile} is not in nbp_basic.use_tiles, {nbp_basic.use_tiles}\")\n        shape_tile_ind = np.where(np.array(nbp_basic.use_tiles) == shape_tile)[0][0]\n        use_tiles[0], use_tiles[shape_tile_ind] = use_tiles[shape_tile_ind], use_tiles[0]\n        spot_shape = None\n    else:\n        nbp.shape_tile = None\n        nbp.shape_spot_local_yxz = None\n        nbp.shape_spot_gene_no = None\n        nbp.spot_shape_float = None\n        # -1 because saved as uint16 so convert 0, 1, 2 to -1, 0, 1.\n        spot_shape = np.load(nbp_file.omp_spot_shape)  # Put z to last index\n        if spot_shape.ndim == 3:\n            spot_shape = np.moveaxis(spot_shape, 0, 2)  # Put z to last index\n\n    # Deal with case where algorithm has been run for some tiles and data saved\n    if os.path.isfile(nbp_file.omp_spot_info) and os.path.isfile(nbp_file.omp_spot_coef):\n        if spot_shape is None:\n            raise ValueError(f'OMP information already exists for some tiles but spot_shape tiff file does not:\\n'\n                             f'{nbp_file.omp_spot_shape}\\nEither add spot_shape tiff or delete the files:\\n'\n                             f'{nbp_file.omp_spot_info} and {nbp_file.omp_spot_coef}.')\n        spot_coefs = sparse.load_npz(nbp_file.omp_spot_coef)\n        spot_info = np.load(nbp_file.omp_spot_info)\n        if spot_coefs.shape[0] &gt; spot_info.shape[0]:\n            # Case where bugged out after saving spot_coefs but before saving spot_info, delete all excess spot_coefs.\n            warnings.warn(f\"Have spot_coefs for {spot_coefs.shape[0]} spots but only spot_info for {spot_info.shape[0]}\"\n                          f\" spots.\\nSo deleting the excess spot_coefs and re-saving to {nbp_file.omp_spot_coef}.\")\n            spot_coefs = spot_coefs[:spot_info.shape[0]]\n            sparse.save_npz(nbp_file.omp_spot_coef, spot_coefs)\n        elif spot_coefs.shape[0] &lt; spot_info.shape[0]:\n            # If more spots in info than coefs then likely because duplicates removed from coefs but not spot_info.\n            not_duplicate = get_non_duplicate(tile_origin, nbp_basic.use_tiles, nbp_basic.tile_centre,\n                                              spot_info[:, :3], spot_info[:, 6])\n            if not_duplicate.size == spot_info.shape[0]:\n                warnings.warn(f'There were less spots in\\n{nbp_file.omp_spot_info}\\nthan\\n{nbp_file.omp_spot_coef} '\n                              f'because duplicates were deleted for spot_coefs but not for spot_info.\\n'\n                              f'Now, spot_info duplicates have also been deleted.')\n                spot_info = spot_info[not_duplicate]\n                np.save(nbp_file.omp_spot_info, spot_info)\n            else:\n                raise ValueError(f\"Have spot_info for {spot_info.shape[0]} spots but only spot_coefs for \"\n                                 f\"{spot_coefs.shape[0]}\\nNeed to delete both {nbp_file.omp_spot_coef} and \"\n                                 f\"{nbp_file.omp_spot_info} to get past this error.\")\n        else:\n            prev_found_tiles = np.unique(spot_info[:, -1])\n            use_tiles = np.setdiff1d(use_tiles, prev_found_tiles)\n            warnings.warn(f'Already have OMP results for tiles {prev_found_tiles} so now just running on tiles '\n                          f'{use_tiles}.')\n        del spot_coefs, spot_info\n    elif os.path.isfile(nbp_file.omp_spot_coef):\n        # If only have information only file but not the other, need to delete all files and start again.\n        raise ValueError(f'The file {nbp_file.omp_spot_coef} exists but the file {nbp_file.omp_spot_info} does not.\\n'\n                         f'Delete or re-name the file {nbp_file.omp_spot_coef} to run omp part from scratch.')\n    elif os.path.isfile(nbp_file.omp_spot_info):\n        raise ValueError(f'The file {nbp_file.omp_spot_info} exists but the file {nbp_file.omp_spot_coef} does not.\\n'\n                         f'Delete or re-name the file {nbp_file.omp_spot_info} to run omp part from scratch.')\n\n    print(f'Finding OMP coefficients for all pixels on tiles {use_tiles}:')\n    initial_pos_neighbour_thresh = config['initial_pos_neighbour_thresh']\n    for t in use_tiles:\n        pixel_yxz_t = np.zeros((0, 3), dtype=np.int16)\n        pixel_coefs_t = sparse.csr_matrix(np.zeros((0, n_genes), dtype=np.float32))\n        for z in use_z:\n            print(f\"Tile {np.where(use_tiles == t)[0][0] + 1}/{len(use_tiles)},\"\n                  f\" Z-plane {np.where(use_z == z)[0][0] + 1}/{len(use_z)}\")\n            # While iterating through tiles, only save info for rounds/channels using\n            # - add all rounds/channels back in later. This returns colors in use_rounds/channels only and no invalid.\n            pixel_colors_tz, pixel_yxz_tz = get_spot_colors(all_pixel_yxz(nbp_basic.tile_sz, nbp_basic.tile_sz,\n                                                                          int(z)), int(t), transform,\n                                                            nbp_file, nbp_basic, return_in_bounds=True)\n            if pixel_colors_tz.shape[0] == 0:\n                continue\n            pixel_colors_tz = pixel_colors_tz / color_norm_factor\n\n            # Only keep pixels with significant absolute intensity to save memory.\n            # absolute because important to find negative coefficients as well.\n            # pixel_intensity_tz = get_spot_intensity(jnp.abs(pixel_colors_tz))\n            pixel_intensity_tz = get_spot_intensity(jnp.abs(pixel_colors_tz))\n            keep = pixel_intensity_tz &gt; nbp.initial_intensity_thresh\n            if not keep.any():\n                continue\n            pixel_colors_tz = pixel_colors_tz[keep]\n            pixel_yxz_tz = pixel_yxz_tz[keep]\n            del pixel_intensity_tz, keep\n\n            pixel_coefs_tz = sparse.csr_matrix(\n                omp.get_all_coefs(pixel_colors_tz, bled_codes,\n                                  nbp_call_spots.background_weight_shift, dp_norm_shift, config['dp_thresh'],\n                                  config['alpha'], config['beta'], config['max_genes'], config['weight_coef_fit'])[0])\n            del pixel_colors_tz\n            # Only keep pixels for which at least one gene has non-zero coefficient.\n            keep = (np.abs(pixel_coefs_tz).max(axis=1) &gt; 0).nonzero()[0]  # nonzero as is sparse matrix.\n            if len(keep) == 0:\n                continue\n            # TODO: check order of np.asarray and keep, which is quicker - think this is quickest though\n            pixel_yxz_t = np.append(pixel_yxz_t, np.asarray(pixel_yxz_tz[keep]), axis=0)\n            del pixel_yxz_tz\n            pixel_coefs_t = sparse.vstack((pixel_coefs_t, pixel_coefs_tz[keep]))\n            del pixel_coefs_tz, keep\n\n        if spot_shape is None:\n            nbp.shape_tile = int(t)\n            spot_yxz, spot_gene_no = omp.get_spots(pixel_coefs_t, pixel_yxz_t, config['radius_xy'], detect_radius_z)\n            z_scale = nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy\n            spot_shape, spots_used, spot_shape_float = \\\n                omp.spot_neighbourhood(pixel_coefs_t, pixel_yxz_t, spot_yxz, spot_gene_no, config['shape_max_size'],\n                                       config['shape_pos_neighbour_thresh'], config['shape_isolation_dist'], z_scale,\n                                       config['shape_sign_thresh'])\n            if not np.isin(-1, spot_shape):\n                # Rase error if computed average spot shape has no negative values\n                error_file = nbp_file.omp_spot_shape.replace('.npy', '_float_ERROR.npy')\n                if spot_shape_float.ndim == 3:\n                    # put z axis to front before saving if 3D\n                    np.save(error_file, np.moveaxis(spot_shape_float, 2, 0))\n                else:\n                    np.save(error_file, spot_shape_float)\n                shape_neg_values = spot_shape_float[spot_shape_float &lt; 0]\n                message = f\"Error when computing nb.omp.spot_shape:\\n\" \\\n                          f\"Average spot_shape in OMP Coefficient images was found with {spots_used.size} spots.\\n\" \\\n                          f\"However, it contains no pixels with a value of -1.\\n\" \\\n                          f\"nb.omp.spot_shape_float was saved as\\n{error_file}\\n\"\n                if len(shape_neg_values) == 0:\n                    message += \"This contains no negative values either, so OMP section needs re-running with \" \\\n                               \"config['file_names']['omp_spot_shape'] specified\"\n                else:\n                    max_neg_value = round_any(np.abs(shape_neg_values).max(), 0.001, 'floor')\n                    message += f\"OMP section needs re-running with\\n\" \\\n                               f\"config['omp']['shape_sign_thresh'] &lt; {max_neg_value} or \" \\\n                               f\"config['file_names']['omp_spot_shape'] specified\"\n                raise ValueError(message)\n            nbp.spot_shape_float = spot_shape_float\n            nbp.shape_spot_local_yxz = spot_yxz[spots_used]\n            nbp.shape_spot_gene_no = spot_gene_no[spots_used]\n            if spot_shape.ndim == 3:\n                # put z axis to front before saving if 3D\n                np.save(nbp_file.omp_spot_shape, np.moveaxis(spot_shape, 2, 0))\n            else:\n                np.save(nbp_file.omp_spot_shape, spot_shape)\n            # already found spots so don't find again.\n            spot_yxzg = np.append(spot_yxz, spot_gene_no.reshape(-1, 1), axis=1)\n            del spot_yxz, spot_gene_no, spots_used\n        else:\n            spot_yxzg = None\n\n        if initial_pos_neighbour_thresh is None:\n            # Only save spots which have 10% of max possible number of positive neighbours\n            initial_pos_neighbour_thresh = config['initial_pos_neighbour_thresh_param'] * np.sum(spot_shape &gt; 0)\n            initial_pos_neighbour_thresh = np.floor(initial_pos_neighbour_thresh)\n            initial_pos_neighbour_thresh = int(np.clip(initial_pos_neighbour_thresh,\n                                                       config['initial_pos_neighbour_thresh_min'],\n                                                       config['initial_pos_neighbour_thresh_max']))\n        spot_info_t = \\\n            omp.get_spots(pixel_coefs_t, pixel_yxz_t, config['radius_xy'], detect_radius_z, 0, spot_shape,\n                          initial_pos_neighbour_thresh, spot_yxzg)\n        del spot_yxzg\n        n_spots = spot_info_t[0].shape[0]\n        spot_info_t = np.concatenate([spot_var.reshape(n_spots, -1).astype(np.int16) for spot_var in spot_info_t],\n                                     axis=1)\n        spot_info_t = np.append(spot_info_t, np.ones((n_spots, 1), dtype=np.int16) * t, axis=1)\n\n        # find index of each spot in pixel array to add colors and coefs\n        pixel_index = numpy_indexed.indices(pixel_yxz_t, spot_info_t[:, :3])\n\n        # append this tile info to all tile info\n        if os.path.isfile(nbp_file.omp_spot_info) and os.path.isfile(nbp_file.omp_spot_coef):\n            # After ran on one tile, need to load in spot_coefs and spot_info, append and then save again.\n            spot_coefs = sparse.load_npz(nbp_file.omp_spot_coef)\n            spot_coefs = sparse.vstack((spot_coefs, pixel_coefs_t[pixel_index]))\n            del pixel_coefs_t, pixel_index\n            sparse.save_npz(nbp_file.omp_spot_coef, spot_coefs)\n            del spot_coefs\n            spot_info = np.load(nbp_file.omp_spot_info)\n            spot_info = np.append(spot_info, spot_info_t, axis=0)\n            del spot_info_t\n            np.save(nbp_file.omp_spot_info, spot_info)\n            del spot_info\n        else:\n            # 1st tile, need to create files to save to\n            sparse.save_npz(nbp_file.omp_spot_coef, pixel_coefs_t[pixel_index])\n            del pixel_coefs_t, pixel_index\n            np.save(nbp_file.omp_spot_info, spot_info_t.astype(np.int16))\n            del spot_info_t\n\n    nbp.spot_shape = spot_shape\n    nbp.initial_pos_neighbour_thresh = initial_pos_neighbour_thresh\n\n    spot_info = np.load(nbp_file.omp_spot_info)\n    # find duplicate spots as those detected on a tile which is not tile centre they are closest to\n    not_duplicate = get_non_duplicate(tile_origin, nbp_basic.use_tiles, nbp_basic.tile_centre,\n                                      spot_info[:, :3], spot_info[:, 6])\n\n    # Add spot info to notebook page\n    nbp.local_yxz = spot_info[not_duplicate, :3]\n    nbp.tile = spot_info[not_duplicate, 6]\n\n    # Get colors, background_coef and intensity of final spots.\n    n_spots = np.sum(not_duplicate)\n    invalid_value = -nbp_basic.tile_pixel_value_shift\n    # Only read in used colors first for background/intensity calculation.\n    nd_spot_colors_use = np.ones((n_spots, n_rounds_use, n_channels_use), dtype=np.int32) * invalid_value\n    for t in nbp_basic.use_tiles:\n        in_tile = nbp.tile == t\n        if np.sum(in_tile) &gt; 0:\n            nd_spot_colors_use[in_tile] = get_spot_colors(jnp.asarray(nbp.local_yxz[in_tile]), t,\n                                                          transform, nbp_file, nbp_basic)\n\n    spot_colors_norm = jnp.array(nd_spot_colors_use) / color_norm_factor\n    nbp.intensity = np.asarray(get_spot_intensity(spot_colors_norm))\n    del spot_colors_norm\n\n    # When saving to notebook, include unused rounds/channels.\n    nd_spot_colors = np.ones((n_spots, nbp_basic.n_rounds, nbp_basic.n_channels), dtype=np.int32) * invalid_value\n    nd_spot_colors[np.ix_(np.arange(n_spots), nbp_basic.use_rounds, nbp_basic.use_channels)] = nd_spot_colors_use\n    nbp.colors = nd_spot_colors\n    del nd_spot_colors_use\n\n    nbp.gene_no = spot_info[not_duplicate, 3]\n    nbp.n_neighbours_pos = spot_info[not_duplicate, 4]\n    nbp.n_neighbours_neg = spot_info[not_duplicate, 5]\n\n    return nbp\n</code></pre>"},{"location":"code/pipeline/register/","title":"Register","text":""},{"location":"code/pipeline/register/#coppafish.pipeline.register.register","title":"<code>register(config, nbp_basic, spot_details, initial_shift)</code>","text":"<p>This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. It uses iterative closest point and the starting shifts found in <code>pipeline/register_initial.py</code>.</p> <p>See <code>'register'</code> and <code>'register_debug'</code> sections of <code>notebook_comments.json</code> file for description of the variables in each page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'register'</code> section of config file.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>spot_details</code> <code>np.ndarray</code> <p><code>int [n_spots x 7]</code>. <code>spot_details[s]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> of spot <code>s</code>. This is saved in the find_spots notebook page i.e. <code>nb.find_spots.spot_details</code>.</p> required <code>initial_shift</code> <code>np.ndarray</code> <p><code>int [n_tiles x n_rounds x 3]</code>. <code>initial_shift[t, r]</code> is the yxz shift found that is applied to tile <code>t</code>, <code>ref_round</code> to take it to tile <code>t</code>, round <code>r</code>. Units: <code>[yx_pixels, yx_pixels, z_pixels]</code>. This is saved in the <code>register_initial_debug</code> notebook page i.e. <code>nb.register_initial_debug.shift</code>.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <ul> <li><code>NotebookPage[register]</code> - Page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile.</li> </ul> <code>NotebookPage</code> <ul> <li><code>NotebookPage[register_debug]</code> - Page contains information on how the affine transforms were calculated.</li> </ul> Source code in <code>coppafish/pipeline/register.py</code> <pre><code>def register(config: dict, nbp_basic: NotebookPage, spot_details: np.ndarray,\n             initial_shift: np.ndarray) -&gt; Tuple[NotebookPage, NotebookPage]:\n\"\"\"\n    This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile.\n    It uses iterative closest point and the starting shifts found in `pipeline/register_initial.py`.\n\n    See `'register'` and `'register_debug'` sections of `notebook_comments.json` file\n    for description of the variables in each page.\n\n    Args:\n        config: Dictionary obtained from `'register'` section of config file.\n        nbp_basic: `basic_info` notebook page\n        spot_details: `int [n_spots x 7]`.\n            `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`.\n            This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`.\n        initial_shift: `int [n_tiles x n_rounds x 3]`.\n            `initial_shift[t, r]` is the yxz shift found that is applied to tile `t`, `ref_round` to take it to\n            tile `t`, round `r`. Units: `[yx_pixels, yx_pixels, z_pixels]`.\n            This is saved in the `register_initial_debug` notebook page i.e. `nb.register_initial_debug.shift`.\n\n    Returns:\n        - `NotebookPage[register]` - Page contains the affine transforms to go from the ref round/channel to\n            each imaging round/channel for every tile.\n        - `NotebookPage[register_debug]` - Page contains information on how the affine transforms were calculated.\n    \"\"\"\n    nbp = NotebookPage(\"register\")\n    nbp_debug = NotebookPage(\"register_debug\")\n    nbp.initial_shift = initial_shift.copy()\n\n    if nbp_basic.is_3d:\n        neighb_dist_thresh = config['neighb_dist_thresh_3d']\n    else:\n        neighb_dist_thresh = config['neighb_dist_thresh_2d']\n\n    # centre and scale spot yxz coordinates\n    z_scale = [1, 1, nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy]\n    spot_yxz_ref = np.zeros(nbp_basic.n_tiles, dtype=object)\n    spot_yxz_imaging = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds, nbp_basic.n_channels), dtype=object)\n    n_matches_thresh = np.zeros_like(spot_yxz_imaging, dtype=float)\n    initial_shift = initial_shift.astype(float)\n    for t in nbp_basic.use_tiles:\n        spot_yxz_ref[t] = spot_yxz(spot_details, t, nbp_basic.ref_round, nbp_basic.ref_channel)\n        spot_yxz_ref[t] = (spot_yxz_ref[t] - nbp_basic.tile_centre) * z_scale\n        for r in nbp_basic.use_rounds:\n            initial_shift[t, r] = initial_shift[t, r] * z_scale  # put z initial shift into xy pixel units\n            for c in nbp_basic.use_channels:\n                spot_yxz_imaging[t, r, c] = spot_yxz(spot_details, t, r, c)\n                spot_yxz_imaging[t, r, c] = (spot_yxz_imaging[t, r, c] - nbp_basic.tile_centre) * z_scale\n                if neighb_dist_thresh &lt; 50:\n                    # only keep isolated spots, those whose second neighbour is far away\n                    isolated = get_isolated_points(spot_yxz_imaging[t, r, c], 2 * neighb_dist_thresh)\n                    spot_yxz_imaging[t, r, c] = spot_yxz_imaging[t, r, c][isolated, :]\n                n_matches_thresh[t, r, c] = (config['matches_thresh_fract'] *\n                                             np.min([spot_yxz_ref[t].shape[0], spot_yxz_imaging[t, r, c].shape[0]]))\n\n    # get indices of tiles/rounds/channels used\n    trc_ind = np.ix_(nbp_basic.use_tiles, nbp_basic.use_rounds, nbp_basic.use_channels)\n    tr_ind = np.ix_(nbp_basic.use_tiles, nbp_basic.use_rounds)  # needed for av_shifts as no channel index\n\n    n_matches_thresh[trc_ind] = np.clip(n_matches_thresh[trc_ind], config['matches_thresh_min'],\n                                        config['matches_thresh_max'])\n    n_matches_thresh = n_matches_thresh.astype(int)\n\n    # Initialise variables obtain from PCR algorithm. This includes spaces for tiles/rounds/channels not used\n    start_transform = transform_from_scale_shift(np.ones((nbp_basic.n_channels, 3)), initial_shift)\n    final_transform = np.zeros_like(start_transform)\n    n_matches = np.zeros_like(spot_yxz_imaging, dtype=int)\n    error = np.zeros_like(spot_yxz_imaging, dtype=float)\n    failed = np.zeros_like(spot_yxz_imaging, dtype=bool)\n    converged = np.zeros_like(spot_yxz_imaging, dtype=bool)\n    av_scaling = np.zeros((nbp_basic.n_channels, 3), dtype=float)\n    av_shifts = np.zeros_like(initial_shift)\n    transform_outliers = np.zeros_like(start_transform)\n\n    # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant.\n    reg_constant_scale = np.sqrt(0.5 * config['regularize_constant'] * config['regularize_factor'])\n    reg_constant_shift = np.sqrt(0.5 * config['regularize_constant'])\n\n    # get ICP output only for tiles/rounds/channels that we are using\n    final_transform[trc_ind], pcr_debug = \\\n        icp(spot_yxz_ref[nbp_basic.use_tiles], spot_yxz_imaging[trc_ind],\n            start_transform[trc_ind], config['n_iter'], neighb_dist_thresh,\n            n_matches_thresh[trc_ind], config['scale_dev_thresh'], config['shift_dev_thresh'],\n            reg_constant_scale, reg_constant_shift)\n\n    # save debug info at correct tile, round, channel index\n    n_matches[trc_ind] = pcr_debug['n_matches']\n    error[trc_ind] = pcr_debug['error']\n    failed[trc_ind] = pcr_debug['failed']\n    converged[trc_ind] = pcr_debug['is_converged']\n    av_scaling[nbp_basic.use_channels] = pcr_debug['av_scaling']\n    av_shifts[tr_ind] = pcr_debug['av_shifts']\n    transform_outliers[trc_ind] = pcr_debug['transforms_outlier']\n\n    # add to notebook\n    nbp.transform = final_transform\n    nbp_debug.n_matches = n_matches\n    nbp_debug.n_matches_thresh = n_matches_thresh\n    nbp_debug.error = error\n    nbp_debug.failed = failed\n    nbp_debug.converged = converged\n    nbp_debug.av_scaling = av_scaling\n    nbp_debug.av_shifts = av_shifts\n    nbp_debug.transform_outlier = transform_outliers\n\n    return nbp, nbp_debug\n</code></pre>"},{"location":"code/pipeline/register_initial/","title":"Register Initial","text":""},{"location":"code/pipeline/register_initial/#coppafish.pipeline.register_initial.register_initial","title":"<code>register_initial(config, nbp_basic, spot_details)</code>","text":"<p>This finds the shift between ref round/channel to each imaging round for each tile. These are then used as the starting point for determining the affine transforms in <code>pipeline/register.py</code>.</p> <p>See <code>'register_initial'</code> section of <code>notebook_comments.json</code> file for description of the variables in the page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'register_initial'</code> section of config file.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>spot_details</code> <code>np.ndarray</code> <p><code>int [n_spots x 7]</code>. <code>spot_details[s]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> of spot <code>s</code>. This is saved in the find_spots notebook page i.e. <code>nb.find_spots.spot_details</code>.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[register_initial]</code> - Page contains information about how shift between ref round/channel to each imaging round for each tile was found.</p> Source code in <code>coppafish/pipeline/register_initial.py</code> <pre><code>def register_initial(config: dict, nbp_basic: NotebookPage, spot_details: np.ndarray) -&gt; NotebookPage:\n\"\"\"\n    This finds the shift between ref round/channel to each imaging round for each tile.\n    These are then used as the starting point for determining the affine transforms in `pipeline/register.py`.\n\n    See `'register_initial'` section of `notebook_comments.json` file\n    for description of the variables in the page.\n\n    Args:\n        config: Dictionary obtained from `'register_initial'` section of config file.\n        nbp_basic: `basic_info` notebook page\n        spot_details: `int [n_spots x 7]`.\n            `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`.\n            This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`.\n\n    Returns:\n        `NotebookPage[register_initial]` - Page contains information about how shift between ref round/channel\n            to each imaging round for each tile was found.\n    \"\"\"\n    nbp_debug = NotebookPage(\"register_initial\")\n    if config['shift_channel'] is None:\n        config['shift_channel'] = nbp_basic.ref_channel\n    if not np.isin(config['shift_channel'], nbp_basic.use_channels):\n        raise ValueError(f\"config['shift_channel'] should be in nb.basic_info.use_channels, but value given is\\n\"\n                         f\"{config['shift_channel']} which is not in use_channels = {nbp_basic.use_channels}.\")\n    nbp_debug.shift_channel = config['shift_channel']\n\n    coords = ['y', 'x', 'z']\n    shifts = [{}]\n    start_shift_search = np.zeros((nbp_basic.n_rounds, 3, 3), dtype=int)\n    for i in range(len(coords)):\n        shifts[0][coords[i]] = np.arange(config['shift_min'][i],\n                                         config['shift_max'][i] +\n                                         config['shift_step'][i] / 2, config['shift_step'][i]).astype(int)\n        start_shift_search[nbp_basic.use_rounds, i, :] = [config['shift_min'][i], config['shift_max'][i],\n                                                          config['shift_step'][i]]\n    if not nbp_basic.is_3d:\n        config['shift_widen'][2] = 0  # so don't look for shifts in z direction\n        config['shift_max_range'][2] = 0\n        shifts[0]['z'] = np.array([0], dtype=int)\n        start_shift_search[:, 2, :2] = 0\n    shifts = shifts * nbp_basic.n_rounds  # get one set of shifts for each round\n\n    shift = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds, 3), dtype=int)\n    shift_score = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds), dtype=float)\n    shift_score_thresh = np.zeros((nbp_basic.n_tiles, nbp_basic.n_rounds), dtype=float)\n\n    c_ref = nbp_basic.ref_channel\n    r_ref = nbp_basic.ref_round\n    c_imaging = config['shift_channel']\n    # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours\n    z_scale = nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy\n    with tqdm(total=len(nbp_basic.use_rounds) * len(nbp_basic.use_tiles)) as pbar:\n        pbar.set_description(f\"Finding shift from ref_round({r_ref})/ref_channel({c_ref}) to channel {c_imaging} \"\n                             f\"of all imaging rounds\")\n        for r in nbp_basic.use_rounds:\n            for t in nbp_basic.use_tiles:\n                pbar.set_postfix({'round': r, 'tile': t})\n                shift[t, r], shift_score[t, r], shift_score_thresh[t, r] = \\\n                    compute_shift(spot_yxz(spot_details, t, r_ref, c_ref), spot_yxz(spot_details, t, r, c_imaging),\n                                  config['shift_score_thresh'], config['shift_score_thresh_multiplier'],\n                                  config['shift_score_thresh_min_dist'], config['shift_score_thresh_max_dist'],\n                                  config['neighb_dist_thresh'], shifts[r]['y'], shifts[r]['x'], shifts[r]['z'],\n                                  config['shift_widen'], config['shift_max_range'], z_scale,\n                                  config['nz_collapse'], config['shift_step'][2])[:3]\n                good_shifts = shift_score[:, r] &gt; shift_score_thresh[:, r]\n                if np.sum(good_shifts) &gt;= 3:\n                    # once found shifts, refine shifts to be searched around these\n                    for i in range(len(coords)):\n                        shifts[r][coords[i]] = update_shifts(shifts[r][coords[i]], shift[good_shifts, r, i])\n                pbar.update(1)\n    pbar.close()\n\n    # amend shifts for which score fell below score_thresh\n    shift_outlier = shift.copy()\n    shift_score_outlier = shift_score.copy()\n    n_shifts = len(nbp_basic.use_tiles)\n    final_shift_search = np.zeros_like(start_shift_search)\n    final_shift_search[:, :, 2] = start_shift_search[:, :, 2]  # spacing does not change\n    for r in nbp_basic.use_rounds:\n        good_shifts = shift_score[:, r] &gt; shift_score_thresh[:, r]\n        for i in range(len(coords)):\n            # change shift search to be near good shifts found\n            # this will only do something if 3&gt;sum(good_shifts)&gt;0, otherwise will have been done in previous loop.\n            if np.sum(good_shifts) &gt; 0:\n                shifts[r][coords[i]] = update_shifts(shifts[r][coords[i]], shift[good_shifts, r, i])\n            elif good_shifts.size &gt; 0:\n                shifts[r][coords[i]] = update_shifts(shifts[r][coords[i]], shift[:, r, i])\n        final_shift_search[r, :, 0] = [np.min(shifts[r][key]) for key in shifts[r].keys()]\n        final_shift_search[r, :, 1] = [np.max(shifts[r][key]) for key in shifts[r].keys()]\n        shift_outlier[good_shifts, r] = 0  # only keep outlier information for not good shifts\n        shift_score_outlier[good_shifts, r] = 0\n        if (np.sum(good_shifts) &lt; 2 and n_shifts &gt; 4) or (np.sum(good_shifts) == 0 and n_shifts &gt; 0):\n            warnings.warn(f\"Round {r}: {n_shifts - np.sum(good_shifts)}/{n_shifts} \"\n                          f\"of shifts fell below score threshold\")\n        for t in np.where(good_shifts == False)[0]:\n            if t not in nbp_basic.use_tiles:\n                continue\n            # re-find shifts that fell below threshold by only looking at shifts near to others found\n            # score set to 0 so will find do refined search no matter what.\n            shift[t, r], shift_score[t, r] = compute_shift(spot_yxz(spot_details, t, r_ref, c_ref),\n                                                           spot_yxz(spot_details, t, r, c_imaging), 0, None, None,\n                                                           None, config['neighb_dist_thresh'], shifts[r]['y'],\n                                                           shifts[r]['x'], shifts[r]['z'], None, None, z_scale,\n                                                           config['nz_collapse'], config['shift_step'][2])[:2]\n            warnings.warn(f\"\\nShift for tile {t} to round {r} changed from\\n\"\n                          f\"{shift_outlier[t, r]} to {shift[t, r]}.\")\n\n    nbp_debug.shift = shift\n    nbp_debug.start_shift_search = start_shift_search\n    nbp_debug.final_shift_search = final_shift_search\n    nbp_debug.shift_score = shift_score\n    nbp_debug.shift_score_thresh = shift_score_thresh\n    nbp_debug.shift_outlier = shift_outlier\n    nbp_debug.shift_score_outlier = shift_score_outlier\n\n    return nbp_debug\n</code></pre>"},{"location":"code/pipeline/run/","title":"Run","text":""},{"location":"code/pipeline/run/#coppafish.pipeline.run.initialize_nb","title":"<code>initialize_nb(config_file)</code>","text":"<p>Quick function which creates a <code>Notebook</code> and adds <code>basic_info</code> page before saving. <code>file_names</code> page will be added automatically as soon as <code>basic_info</code> page is added. If <code>Notebook</code> already exists and contains these pages, it will just be returned.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to config file.</p> required <p>Returns:</p> Type Description <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>file_names</code> and <code>basic_info</code> pages.</p> Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def initialize_nb(config_file: str) -&gt; setup.Notebook:\n\"\"\"\n    Quick function which creates a `Notebook` and adds `basic_info` page before saving.\n    `file_names` page will be added automatically as soon as `basic_info` page is added.\n    If `Notebook` already exists and contains these pages, it will just be returned.\n\n    Args:\n        config_file: Path to config file.\n\n    Returns:\n        `Notebook` containing `file_names` and `basic_info` pages.\n    \"\"\"\n    nb = setup.Notebook(config_file=config_file)\n    config = nb.get_config()\n    if not nb.has_page(\"basic_info\"):\n        nbp_basic = set_basic_info(config['file_names'], config['basic_info'])\n        nb += nbp_basic\n    else:\n        warnings.warn('basic_info', utils.warnings.NotebookPageWarning)\n    return nb\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_extract","title":"<code>run_extract(nb)</code>","text":"<p>This runs the <code>extract_and_filter</code> step of the pipeline to produce the tiff files in the tile directory.</p> <p><code>extract</code> and <code>extract_debug</code> pages are added to the <code>Notebook</code> before saving.</p> <p>If <code>Notebook</code> already contains these pages, it will just be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>file_names</code> and <code>basic_info</code> pages.</p> required Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_extract(nb: setup.Notebook):\n\"\"\"\n    This runs the `extract_and_filter` step of the pipeline to produce the tiff files in the tile directory.\n\n    `extract` and `extract_debug` pages are added to the `Notebook` before saving.\n\n    If `Notebook` already contains these pages, it will just be returned.\n\n    Args:\n        nb: `Notebook` containing `file_names` and `basic_info` pages.\n\n    \"\"\"\n    if not all(nb.has_page([\"extract\", \"extract_debug\"])):\n        config = nb.get_config()\n        nbp, nbp_debug = extract_and_filter(config['extract'], nb.file_names, nb.basic_info)\n        nb += nbp\n        nb += nbp_debug\n    else:\n        warnings.warn('extract', utils.warnings.NotebookPageWarning)\n        warnings.warn('extract_debug', utils.warnings.NotebookPageWarning)\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_find_spots","title":"<code>run_find_spots(nb)</code>","text":"<p>This runs the <code>find_spots</code> step of the pipeline to produce point cloud from each tiff file in the tile directory.</p> <p><code>find_spots</code> page added to the <code>Notebook</code> before saving.</p> <p>If <code>Notebook</code> already contains this page, it will just be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>extract</code> page.</p> required Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_find_spots(nb: setup.Notebook):\n\"\"\"\n    This runs the `find_spots` step of the pipeline to produce point cloud from each tiff file in the tile directory.\n\n    `find_spots` page added to the `Notebook` before saving.\n\n    If `Notebook` already contains this page, it will just be returned.\n\n    Args:\n        nb: `Notebook` containing `extract` page.\n\n    \"\"\"\n    if not nb.has_page(\"find_spots\"):\n        config = nb.get_config()\n        nbp = find_spots(config['find_spots'], nb.file_names, nb.basic_info, nb.extract.auto_thresh)\n        nb += nbp\n        check_n_spots(nb)  # error if too few spots - may indicate tile or channel which should not be included\n    else:\n        warnings.warn('find_spots', utils.warnings.NotebookPageWarning)\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_omp","title":"<code>run_omp(nb)</code>","text":"<p>This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of spots and their gene identity. It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at every pixel. Spots are then local maxima in these gene coefficient images.</p> <p><code>omp</code> page is added to the Notebook before saving.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>call_spots</code> page.</p> required Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_omp(nb: setup.Notebook):\n\"\"\"\n    This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of\n    spots and their gene identity.\n    It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at\n    every pixel. Spots are then local maxima in these gene coefficient images.\n\n    `omp` page is added to the Notebook before saving.\n\n    Args:\n        nb: `Notebook` containing `call_spots` page.\n\n    \"\"\"\n    if not nb.has_page(\"omp\"):\n        config = nb.get_config()\n        # Use tile with most spots on to find spot shape in omp\n        tile_most_spots = int(stats.mode(nb.ref_spots.tile[quality_threshold(nb, 'ref')])[0][0])\n        nbp = call_spots_omp(config['omp'], nb.file_names, nb.basic_info, nb.call_spots,\n                             nb.stitch.tile_origin, nb.register.transform, tile_most_spots)\n        nb += nbp\n\n        # Update omp_info files after omp notebook page saved into notebook\n        # Save only non-duplicates - important spot_coefs saved first for exception at start of call_spots_omp\n        # which can deal with case where duplicates removed from spot_coefs but not spot_info.\n        # After re-saving here, spot_coefs[s] should be the coefficients for gene at nb.omp.local_yxz[s]\n        # i.e. indices should match up.\n        spot_info = np.load(nb.file_names.omp_spot_info)\n        not_duplicate = get_non_duplicate(nb.stitch.tile_origin, nb.basic_info.use_tiles, nb.basic_info.tile_centre,\n                                          spot_info[:, :3], spot_info[:, 6])\n        spot_coefs = sparse.load_npz(nb.file_names.omp_spot_coef)\n        sparse.save_npz(nb.file_names.omp_spot_coef, spot_coefs[not_duplicate])\n        np.save(nb.file_names.omp_spot_info, spot_info[not_duplicate])\n\n        # only raise error after saving to notebook if spot_colors have nan in wrong places.\n        utils.errors.check_color_nan(nbp.colors, nb.basic_info)\n    else:\n        warnings.warn('omp', utils.warnings.NotebookPageWarning)\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_pipeline","title":"<code>run_pipeline(config_file, overwrite_ref_spots=False)</code>","text":"<p>Bridge function to run every step of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to config file.</p> required <code>overwrite_ref_spots</code> <code>bool</code> <p>Only used if Notebook contains ref_spots but not call_spots page. If <code>True</code>, the variables:</p> <ul> <li><code>gene_no</code></li> <li><code>score</code></li> <li><code>score_diff</code></li> <li><code>intensity</code></li> </ul> <p>in <code>nb.ref_spots</code> will be overwritten if they exist. If this is <code>False</code>, they will only be overwritten if they are all set to <code>None</code>, otherwise an error will occur.</p> <code>False</code> <p>Returns:</p> Type Description <code>setup.Notebook</code> <p><code>Notebook</code> containing all information gathered during the pipeline.</p> Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_pipeline(config_file: str, overwrite_ref_spots: bool = False) -&gt; setup.Notebook:\n\"\"\"\n    Bridge function to run every step of the pipeline.\n\n    Args:\n        config_file: Path to config file.\n        overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page.\n            If `True`, the variables:\n\n            * `gene_no`\n            * `score`\n            * `score_diff`\n            * `intensity`\n\n            in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten\n            if they are all set to `None`, otherwise an error will occur.\n\n    Returns:\n        `Notebook` containing all information gathered during the pipeline.\n    \"\"\"\n    nb = initialize_nb(config_file)\n    run_extract(nb)\n    run_find_spots(nb)\n    run_stitch(nb)\n    run_register(nb)\n    run_reference_spots(nb, overwrite_ref_spots)\n    run_omp(nb)\n    return nb\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_reference_spots","title":"<code>run_reference_spots(nb, overwrite_ref_spots=False)</code>","text":"<p>This runs the <code>reference_spots</code> step of the pipeline to get the intensity of each spot on the reference round/channel in each imaging round/channel. The <code>call_spots</code> step of the pipeline is then run to produce the <code>bleed_matrix</code>, <code>bled_code</code> for each gene and the gene assignments of the spots on the reference round.</p> <p><code>ref_spots</code> and <code>call_spots</code> pages are added to the Notebook before saving.</p> <p>If <code>Notebook</code> already contains these pages, it will just be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>stitch</code> and <code>register</code> pages.</p> required <code>overwrite_ref_spots</code> <code>bool</code> <p>Only used if Notebook contains ref_spots but not call_spots page. If <code>True</code>, the variables:</p> <ul> <li><code>gene_no</code></li> <li><code>score</code></li> <li><code>score_diff</code></li> <li><code>intensity</code></li> </ul> <p>in <code>nb.ref_spots</code> will be overwritten if they exist. If this is <code>False</code>, they will only be overwritten if they are all set to <code>None</code>, otherwise an error will occur.</p> <code>False</code> Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_reference_spots(nb: setup.Notebook, overwrite_ref_spots: bool = False):\n\"\"\"\n    This runs the `reference_spots` step of the pipeline to get the intensity of each spot on the reference\n    round/channel in each imaging round/channel. The `call_spots` step of the pipeline is then run to produce the\n    `bleed_matrix`, `bled_code` for each gene and the gene assignments of the spots on the reference round.\n\n    `ref_spots` and `call_spots` pages are added to the Notebook before saving.\n\n    If `Notebook` already contains these pages, it will just be returned.\n\n    Args:\n        nb: `Notebook` containing `stitch` and `register` pages.\n        overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page.\n            If `True`, the variables:\n\n            * `gene_no`\n            * `score`\n            * `score_diff`\n            * `intensity`\n\n            in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten\n            if they are all set to `None`, otherwise an error will occur.\n    \"\"\"\n    if not nb.has_page('ref_spots'):\n        nbp = get_reference_spots(nb.file_names, nb.basic_info, nb.find_spots.spot_details,\n                                  nb.stitch.tile_origin, nb.register.transform)\n        nb += nbp  # save to Notebook with gene_no, score, score_diff, intensity = None.\n                   # These will be added in call_reference_spots\n    else:\n        warnings.warn('ref_spots', utils.warnings.NotebookPageWarning)\n    if not nb.has_page(\"call_spots\"):\n        config = nb.get_config()\n        nbp, nbp_ref_spots = call_reference_spots(config['call_spots'], nb.file_names, nb.basic_info, nb.ref_spots,\n                                                  nb.extract.hist_values, nb.extract.hist_counts,\n                                                  nb.register.transform, overwrite_ref_spots)\n        nb += nbp\n        # Raise errors if stitch, register_initial or register section failed\n        # Do that at this stage, so can still run viewer to see what spots look like\n        check_shifts_stitch(nb)  # error if too many bad shifts between tiles\n        check_shifts_register(nb)  # error if too many bad shifts between rounds\n        check_transforms(nb)  # error if affine transforms found have low number of matches\n        # only raise error after saving to notebook if spot_colors have nan in wrong places.\n        utils.errors.check_color_nan(nb.ref_spots.colors, nb.basic_info)\n    else:\n        warnings.warn('call_spots', utils.warnings.NotebookPageWarning)\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_register","title":"<code>run_register(nb)</code>","text":"<p>This runs the <code>register_initial</code> step of the pipeline to find shift between ref round/channel to each imaging round for each tile. It then runs the <code>register</code> step of the pipeline which uses this as a starting point to get the affine transforms to go from the ref round/channel to each imaging round/channel for every tile.</p> <p><code>register_initial</code>, <code>register</code> and <code>register_debug</code> pages are added to the <code>Notebook</code> before saving.</p> <p>If <code>Notebook</code> already contains these pages, it will just be returned.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>extract</code> page.</p> required Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_register(nb: setup.Notebook):\n\"\"\"\n    This runs the `register_initial` step of the pipeline to find shift between ref round/channel to each imaging round\n    for each tile. It then runs the `register` step of the pipeline which uses this as a starting point to get\n    the affine transforms to go from the ref round/channel to each imaging round/channel for every tile.\n\n    `register_initial`, `register` and `register_debug` pages are added to the `Notebook` before saving.\n\n    If `Notebook` already contains these pages, it will just be returned.\n\n    Args:\n        nb: `Notebook` containing `extract` page.\n\n    \"\"\"\n    config = nb.get_config()\n    if nb.has_page(\"register_initial_debug\"):\n        # deal with old notebook files where page was called \"register_initial_debug\" instead of\n        # \"register_initial\". This will trigger a save after name change too.\n        nb.change_page_name(\"register_initial_debug\", \"register_initial\")\n    if not nb.has_page(\"register_initial\"):\n        nbp_initial = register_initial(config['register_initial'], nb.basic_info,\n                                             nb.find_spots.spot_details)\n        nb += nbp_initial\n    else:\n        warnings.warn('register_initial', utils.warnings.NotebookPageWarning)\n    if not all(nb.has_page([\"register\", \"register_debug\"])):\n        nbp, nbp_debug = register(config['register'], nb.basic_info, nb.find_spots.spot_details,\n                                  nb.register_initial.shift)\n        nb += nbp\n        nb += nbp_debug\n    else:\n        warnings.warn('register', utils.warnings.NotebookPageWarning)\n        warnings.warn('register_debug', utils.warnings.NotebookPageWarning)\n</code></pre>"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_stitch","title":"<code>run_stitch(nb)</code>","text":"<p>This runs the <code>stitch</code> step of the pipeline to produce origin of each tile such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images.</p> <p><code>stitch</code> page added to the <code>Notebook</code> before saving.</p> <p>If <code>Notebook</code> already contains this page, it will just be returned. If stitched images already exist, they won't be created again.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>setup.Notebook</code> <p><code>Notebook</code> containing <code>find_spots</code> page.</p> required Source code in <code>coppafish/pipeline/run.py</code> <pre><code>def run_stitch(nb: setup.Notebook):\n\"\"\"\n    This runs the `stitch` step of the pipeline to produce origin of each tile\n    such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images.\n\n    `stitch` page added to the `Notebook` before saving.\n\n    If `Notebook` already contains this page, it will just be returned.\n    If stitched images already exist, they won't be created again.\n\n    Args:\n        nb: `Notebook` containing `find_spots` page.\n\n    \"\"\"\n    config = nb.get_config()\n    if not nb.has_page(\"stitch\"):\n        nbp_debug = stitch(config['stitch'], nb.basic_info, nb.find_spots.spot_details)\n        nb += nbp_debug\n    else:\n        warnings.warn('stitch', utils.warnings.NotebookPageWarning)\n    if nb.file_names.big_dapi_image is not None and not os.path.isfile(nb.file_names.big_dapi_image):\n        # save stitched dapi\n        # Will load in from nd2 file if nb.extract_debug.r_dapi is None i.e. if no DAPI filtering performed.\n        utils.npy.save_stitched(nb.file_names.big_dapi_image, nb.file_names, nb.basic_info,\n                                nb.stitch.tile_origin, nb.basic_info.anchor_round,\n                                nb.basic_info.dapi_channel, nb.extract_debug.r_dapi is None,\n                                config['stitch']['save_image_zero_thresh'])\n    if nb.file_names.big_anchor_image is not None and not os.path.isfile(nb.file_names.big_anchor_image):\n        # save stitched reference round/channel\n        utils.npy.save_stitched(nb.file_names.big_anchor_image, nb.file_names, nb.basic_info,\n                                nb.stitch.tile_origin, nb.basic_info.ref_round,\n                                nb.basic_info.ref_channel, False, config['stitch']['save_image_zero_thresh'])\n</code></pre>"},{"location":"code/pipeline/stitch/","title":"Stitch","text":""},{"location":"code/pipeline/stitch/#coppafish.pipeline.stitch.stitch","title":"<code>stitch(config, nbp_basic, spot_details)</code>","text":"<p>This gets the origin of each tile such that a global coordinate system can be built.</p> <p>See <code>'stitch'</code> section of <code>notebook_comments.json</code> file for description of the variables in the page.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary obtained from <code>'stitch'</code> section of config file.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>spot_details</code> <code>np.ndarray</code> <p><code>int [n_spots x 7]</code>. <code>spot_details[s]</code> is <code>[tile, round, channel, isolated, y, x, z]</code> of spot <code>s</code>. This is saved in the find_spots notebook page i.e. <code>nb.find_spots.spot_details</code>.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p><code>NotebookPage[stitch]</code> - Page contains information about how tiles were stitched together to give global coordinates.</p> Source code in <code>coppafish/pipeline/stitch.py</code> <pre><code>def stitch(config: dict, nbp_basic: NotebookPage, spot_details: np.ndarray) -&gt; NotebookPage:\n\"\"\"\n    This gets the origin of each tile such that a global coordinate system can be built.\n\n    See `'stitch'` section of `notebook_comments.json` file\n    for description of the variables in the page.\n\n    Args:\n        config: Dictionary obtained from `'stitch'` section of config file.\n        nbp_basic: `basic_info` notebook page\n        spot_details: `int [n_spots x 7]`.\n            `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`.\n            This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`.\n\n    Returns:\n        `NotebookPage[stitch]` - Page contains information about how tiles were stitched together to give\n            global coordinates.\n    \"\"\"\n    nbp_debug = NotebookPage(\"stitch\")\n    # NOTE that directions should actually be 'north' and 'east'\n    directions = ['south', 'west']\n    coords = ['y', 'x', 'z']\n    shifts = get_shifts_to_search(config, nbp_basic, nbp_debug)\n\n    if not nbp_basic.is_3d:\n        config['nz_collapse'] = None\n        config['shift_widen'][2] = 0  # so don't look for shifts in z direction\n        config['shift_max_range'][2] = 0\n\n    # initialise variables to store shift info\n    shift_info = {'south': {}, 'west': {}}\n    for j in directions:\n        shift_info[j]['pairs'] = np.zeros((0, 2), dtype=int)\n        shift_info[j]['shifts'] = np.zeros((0, 3), dtype=int)\n        shift_info[j]['score'] = np.zeros((0, 1), dtype=float)\n        shift_info[j]['score_thresh'] = np.zeros((0, 1), dtype=float)\n\n    # find shifts between overlapping tiles\n    c = nbp_basic.ref_channel\n    r = nbp_basic.ref_round\n    t_neighb = {'south': [], 'west': []}\n    # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours\n    z_scale = nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy\n    with tqdm(total=2 * len(nbp_basic.use_tiles)) as pbar:\n        pbar.set_description(f\"Finding overlap between tiles in round {r} (ref_round)\")\n        for t in nbp_basic.use_tiles:\n            # align to south neighbour followed by west neighbour\n            t_neighb['south'] = np.where(np.sum(nbp_basic.tilepos_yx == nbp_basic.tilepos_yx[t, :] + [1, 0],\n                                                axis=1) == 2)[0]\n            t_neighb['west'] = np.where(np.sum(nbp_basic.tilepos_yx == nbp_basic.tilepos_yx[t, :] + [0, 1],\n                                               axis=1) == 2)[0]\n            for j in directions:\n                pbar.set_postfix({'tile': t, 'direction': j})\n                if t_neighb[j] in nbp_basic.use_tiles:\n                    shift, score, score_thresh = compute_shift(spot_yxz(spot_details, t, r, c),\n                                                               spot_yxz(spot_details, t_neighb[j][0], r, c),\n                                                               config['shift_score_thresh'],\n                                                               config['shift_score_thresh_multiplier'],\n                                                               config['shift_score_thresh_min_dist'],\n                                                               config['shift_score_thresh_max_dist'],\n                                                               config['neighb_dist_thresh'], shifts[j]['y'],\n                                                               shifts[j]['x'], shifts[j]['z'],\n                                                               config['shift_widen'], config['shift_max_range'],\n                                                               z_scale, config['nz_collapse'],\n                                                               config['shift_step'][2])[:3]\n                    shift_info[j]['pairs'] = np.append(shift_info[j]['pairs'],\n                                                       np.array([t, t_neighb[j][0]]).reshape(1, 2), axis=0)\n                    shift_info[j]['shifts'] = np.append(shift_info[j]['shifts'], np.array(shift).reshape(1, 3), axis=0)\n                    shift_info[j]['score'] = np.append(shift_info[j]['score'], np.array(score).reshape(1, 1), axis=0)\n                    shift_info[j]['score_thresh'] = np.append(shift_info[j]['score_thresh'],\n                                                              np.array(score_thresh).reshape(1, 1), axis=0)\n                    good_shifts = (shift_info[j]['score'] &gt; shift_info[j]['score_thresh']).flatten()\n                    if np.sum(good_shifts) &gt;= 3:\n                        # once found shifts, refine shifts to be searched around these\n                        for i in range(len(coords)):\n                            shifts[j][coords[i]] = update_shifts(shifts[j][coords[i]],\n                                                                 shift_info[j]['shifts'][good_shifts, i])\n                pbar.update(1)\n    pbar.close()\n\n    # amend shifts for which score fell below score_thresh\n    for j in directions:\n        good_shifts = (shift_info[j]['score'] &gt; shift_info[j]['score_thresh']).flatten()\n        for i in range(len(coords)):\n            # change shift search to be near good shifts found\n            # this will only do something if 3&gt;sum(good_shifts)&gt;0, otherwise will have been done in previous loop.\n            if np.sum(good_shifts) &gt; 0:\n                shifts[j][coords[i]] = update_shifts(shifts[j][coords[i]], shift_info[j]['shifts'][good_shifts, i])\n            elif good_shifts.size &gt; 0:\n                shifts[j][coords[i]] = update_shifts(shifts[j][coords[i]], shift_info[j]['shifts'][:, i])\n        # add outlier variable to shift_info to keep track of those shifts which are changed.\n        shift_info[j]['outlier_shifts'] = shift_info[j]['shifts'].copy()\n        shift_info[j]['outlier_score'] = shift_info[j]['score'].copy()\n        shift_info[j]['outlier_shifts'][good_shifts, :] = 0\n        shift_info[j]['outlier_score'][good_shifts, :] = 0\n        if (np.sum(good_shifts) &lt; 2 and len(good_shifts) &gt; 4) or (np.sum(good_shifts) == 0 and len(good_shifts) &gt; 0):\n            warnings.warn(f\"{len(good_shifts) - np.sum(good_shifts)}/{len(good_shifts)}\"\n                          f\" of shifts fell below score threshold\")\n        for i in np.where(good_shifts == False)[0]:\n            t = shift_info[j]['pairs'][i, 0]\n            t_neighb = shift_info[j]['pairs'][i, 1]\n            # re-find shifts that fell below threshold by only looking at shifts near to others found\n            # Don't allow any widening so shift found must be in this range.\n            # score_thresh given is 0, so it is not re-computed.\n            shift_info[j]['shifts'][i], shift_info[j]['score'][i] = \\\n                compute_shift(spot_yxz(spot_details, t, r, c), spot_yxz(spot_details, t_neighb, r, c), 0, None, None,\n                              None, config['neighb_dist_thresh'], shifts[j]['y'], shifts[j]['x'], shifts[j]['z'],\n                              None, None, z_scale, config['nz_collapse'], config['shift_step'][2])[:2]\n            warnings.warn(f\"\\nShift from tile {t} to tile {t_neighb} changed from\\n\"\n                          f\"{shift_info[j]['outlier_shifts'][i]} to {shift_info[j]['shifts'][i]}.\")\n\n    # get tile origins in global coordinates.\n    # global coordinates are built about central tile so found this first\n    tile_dist_to_centre = np.linalg.norm(nbp_basic.tilepos_yx[nbp_basic.use_tiles] -\n                                         np.mean(nbp_basic.tilepos_yx, axis=0), axis=1)\n    centre_tile = nbp_basic.use_tiles[tile_dist_to_centre.argmin()]\n    tile_origin = get_tile_origin(shift_info['south']['pairs'], shift_info['south']['shifts'],\n                                  shift_info['west']['pairs'], shift_info['west']['shifts'],\n                                  nbp_basic.n_tiles, centre_tile)\n    if nbp_basic.is_3d is False:\n        tile_origin[:, 2] = 0  # set z coordinate to 0 for all tiles if 2d\n\n    # add tile origin to debugging notebook so don't have whole page for one variable,\n    # and need to add other rounds to it in registration stage anyway.\n    nbp_debug.tile_origin = tile_origin\n    # save all shift info to debugging page\n    for j in directions:\n        nbp_debug.__setattr__(j + '_' + 'final_shift_search', np.zeros((3, 3), dtype=int))\n        nbp_debug.__getattribute__(j + '_' + 'final_shift_search')[:, 0] = \\\n            [np.min(shifts[j][key]) for key in shifts[j].keys()]\n        nbp_debug.__getattribute__(j + '_' + 'final_shift_search')[:, 1] = [np.max(shifts[j][key]) for key in\n                                                                            shifts[j].keys()]\n        nbp_debug.__getattribute__(j + '_' + 'final_shift_search')[:, 2] = \\\n            nbp_debug.__getattribute__(j + '_' + 'start_shift_search')[:, 2]\n        for var in shift_info[j].keys():\n            nbp_debug.__setattr__(j + '_' + var, shift_info[j][var])\n    return nbp_debug\n</code></pre>"},{"location":"code/plot/call_spots/","title":"Call Spots","text":""},{"location":"code/plot/call_spots/#bleed-matrix","title":"Bleed Matrix","text":""},{"location":"code/plot/call_spots/#view_bleed_matrix","title":"<code>view_bleed_matrix</code>","text":"<p>Diagnostic to plot <code>bleed_matrix</code>. If <code>config['call_spots']['bleed_matrix_method']</code> is <code>'single'</code>, a single <code>bleed_matrix</code> will be plotted. If it is <code>'separate'</code>, one will be shown for each round.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required Source code in <code>coppafish/plot/call_spots/bleed_matrix.py</code> <pre><code>def __init__(self, nb: Notebook):\n\"\"\"\n    Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`,\n    a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)]\n    n_use_rounds, n_use_channels = color_norm.shape\n    single_bm = (color_norm == color_norm[0]).all()\n    if single_bm:\n        bleed_matrix = [nb.call_spots.bleed_matrix[0][np.ix_(nb.basic_info.use_channels,\n                                                             nb.basic_info.use_dyes)]]\n        subplot_row_columns = [1, 1]\n        subplot_adjust = [0.07, 0.775, 0.095, 0.94]\n        fig_size = (9, 5)\n    else:\n        bleed_matrix = [nb.call_spots.bleed_matrix[r][np.ix_(nb.basic_info.use_channels,\n                                                             nb.basic_info.use_dyes)]\n                        for r in range(n_use_rounds)]\n        if n_use_rounds &lt;= 3:\n            subplot_row_columns = [n_use_rounds, 1]\n        else:\n            n_cols = int(np.ceil(n_use_rounds / 4))  # at most 4 rows\n            subplot_row_columns = [int(np.ceil(n_use_rounds / n_cols)), n_cols]\n        subplot_adjust = [0.07, 0.775, 0.095, 0.92]\n        fig_size = (12, 7)\n    n_use_dyes = bleed_matrix[0].shape[1]\n    # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes\n    # i.e. normalisation just affected by channel not by dye.\n    color_norm = [np.expand_dims(color_norm[r], 1) for r in range(n_use_rounds)]\n    super().__init__(bleed_matrix, color_norm, subplot_row_columns, subplot_adjust=subplot_adjust,\n                     fig_size=fig_size)\n    self.ax[0].set_yticks(ticks=np.arange(n_use_channels), labels=nb.basic_info.use_channels)\n    if nb.basic_info.dye_names is None:\n        self.ax[-1].set_xticks(ticks=np.arange(n_use_dyes), labels=nb.basic_info.use_dyes)\n    else:\n        self.fig.subplots_adjust(bottom=0.15)\n        self.ax[-1].set_xticks(ticks=np.arange(n_use_dyes),\n                               labels=np.asarray(nb.basic_info.dye_names)[nb.basic_info.use_dyes], rotation=45)\n    if single_bm:\n        self.ax[0].set_title('Bleed Matrix')\n        self.ax[0].set_ylabel('Color Channel')\n        self.ax[0].set_xlabel('Dyes')\n    else:\n        for i in range(n_use_rounds):\n            self.ax[i].set_title(f'Round {nb.basic_info.use_rounds[i]}', size=8)\n            plt.suptitle(\"Bleed Matrices\", size=12, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n        self.fig.supylabel('Color Channel', size=12)\n        self.fig.supxlabel('Dyes', size=12, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n    self.change_norm()  # initialise with method = 'norm'\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#view_bled_codes","title":"<code>view_bled_codes</code>","text":"<p>Diagnostic to plot <code>bleed_matrix</code>. If <code>config['call_spots']['bleed_matrix_method']</code> is <code>'single'</code>, a single <code>bleed_matrix</code> will be plotted. If it is <code>'separate'</code>, one will be shown for each round.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required Source code in <code>coppafish/plot/call_spots/bleed_matrix.py</code> <pre><code>def __init__(self, nb: Notebook):\n\"\"\"\n    Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`,\n    a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)]\n    n_use_rounds, n_use_channels = color_norm.shape\n    single_bm = (color_norm == color_norm[0]).all()\n    if single_bm:\n        bleed_matrix = [nb.call_spots.bleed_matrix[0][np.ix_(nb.basic_info.use_channels,\n                                                             nb.basic_info.use_dyes)]]\n        subplot_row_columns = [1, 1]\n        subplot_adjust = [0.07, 0.775, 0.095, 0.94]\n        fig_size = (9, 5)\n    else:\n        bleed_matrix = [nb.call_spots.bleed_matrix[r][np.ix_(nb.basic_info.use_channels,\n                                                             nb.basic_info.use_dyes)]\n                        for r in range(n_use_rounds)]\n        if n_use_rounds &lt;= 3:\n            subplot_row_columns = [n_use_rounds, 1]\n        else:\n            n_cols = int(np.ceil(n_use_rounds / 4))  # at most 4 rows\n            subplot_row_columns = [int(np.ceil(n_use_rounds / n_cols)), n_cols]\n        subplot_adjust = [0.07, 0.775, 0.095, 0.92]\n        fig_size = (12, 7)\n    n_use_dyes = bleed_matrix[0].shape[1]\n    # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes\n    # i.e. normalisation just affected by channel not by dye.\n    color_norm = [np.expand_dims(color_norm[r], 1) for r in range(n_use_rounds)]\n    super().__init__(bleed_matrix, color_norm, subplot_row_columns, subplot_adjust=subplot_adjust,\n                     fig_size=fig_size)\n    self.ax[0].set_yticks(ticks=np.arange(n_use_channels), labels=nb.basic_info.use_channels)\n    if nb.basic_info.dye_names is None:\n        self.ax[-1].set_xticks(ticks=np.arange(n_use_dyes), labels=nb.basic_info.use_dyes)\n    else:\n        self.fig.subplots_adjust(bottom=0.15)\n        self.ax[-1].set_xticks(ticks=np.arange(n_use_dyes),\n                               labels=np.asarray(nb.basic_info.dye_names)[nb.basic_info.use_dyes], rotation=45)\n    if single_bm:\n        self.ax[0].set_title('Bleed Matrix')\n        self.ax[0].set_ylabel('Color Channel')\n        self.ax[0].set_xlabel('Dyes')\n    else:\n        for i in range(n_use_rounds):\n            self.ax[i].set_title(f'Round {nb.basic_info.use_rounds[i]}', size=8)\n            plt.suptitle(\"Bleed Matrices\", size=12, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n        self.fig.supylabel('Color Channel', size=12)\n        self.fig.supxlabel('Dyes', size=12, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n    self.change_norm()  # initialise with method = 'norm'\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#spot-colors","title":"Spot Colors","text":""},{"location":"code/plot/call_spots/#view_codes","title":"<code>view_codes</code>","text":"<p>Diagnostic to compare <code>spot_color</code> to <code>bled_code</code> of predicted gene.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'anchor'</code> Source code in <code>coppafish/plot/call_spots/spot_colors.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'anchor'):\n\"\"\"\n    Diagnostic to compare `spot_color` to `bled_code` of predicted gene.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)].transpose()\n    if method.lower() == 'omp':\n        page_name = 'omp'\n        config = nb.get_config()['thresholds']\n        spot_score = omp_spot_score(nb.omp, config['score_omp_multiplier'], spot_no)\n    else:\n        page_name = 'ref_spots'\n        spot_score = nb.ref_spots.score[spot_no]\n    self.spot_color = nb.__getattribute__(page_name).colors[spot_no][\n                          np.ix_(nb.basic_info.use_rounds, nb.basic_info.use_channels)].transpose() / color_norm\n    # Get spot color after background fitting\n    self.background_removed = False\n    self.spot_color_pb = fit_background(self.spot_color.T[np.newaxis],\n                                        nb.call_spots.background_weight_shift)[0][0].T\n    gene_no = nb.__getattribute__(page_name).gene_no[spot_no]\n\n    gene_name = nb.call_spots.gene_names[gene_no]\n    gene_color = nb.call_spots.bled_codes_ge[gene_no][np.ix_(nb.basic_info.use_rounds,\n                                                             nb.basic_info.use_channels)].transpose()\n    super().__init__([self.spot_color, gene_color], color_norm, slider_pos=[0.85, 0.2, 0.01, 0.75],\n                     cbar_pos=[0.9, 0.2, 0.03, 0.75])\n    self.ax[0].set_title(f'Spot {spot_no}: match {str(np.around(spot_score, 2))} '\n                         f'to {gene_name}')\n    self.ax[1].set_title(f'Predicted code for Gene {gene_no}: {gene_name}')\n    self.ax[0].set_yticks(ticks=np.arange(self.im_data[0].shape[0]), labels=nb.basic_info.use_channels)\n    self.ax[1].set_xticks(ticks=np.arange(self.im_data[0].shape[1]))\n    self.ax[1].set_xticklabels(['{:.0f} ({:.2f})'.format(r, nb.call_spots.gene_efficiency[gene_no, r])\n                                for r in nb.basic_info.use_rounds])\n    self.ax[1].set_xlabel('Round (Gene Efficiency)')\n    self.fig.supylabel('Color Channel')\n    intense_gene_cr = np.where(gene_color &gt; self.intense_gene_thresh)\n    for i in range(len(intense_gene_cr[0])):\n        for j in range(2):\n            # can't add rectangle to multiple axes hence second for loop\n            rectangle = plt.Rectangle((intense_gene_cr[1][i]-0.5, intense_gene_cr[0][i]-0.5), 1, 1,\n                                      fill=False, ec=\"lime\", linestyle=':', lw=2)\n            self.ax[j].add_patch(rectangle)\n\n    self.background_button_ax = self.fig.add_axes([0.85, 0.1, 0.1, 0.05])\n    self.background_button = Button(self.background_button_ax, 'Background', hovercolor='0.275')\n    self.background_button.label.set_color(self.norm_button_color)\n    self.background_button.on_clicked(self.change_background)\n\n    self.change_norm()  # initialise with method = 'norm'\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#view_spot","title":"<code>view_spot</code>","text":"<p>Diagnostic to show intensity of each color channel / round in neighbourhood of spot. Will show a grid of <code>n_use_channels x n_use_rounds</code> subplots.</p> <p>Requires access to <code>nb.file_names.tile_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'anchor'</code> <code>im_size</code> <code>int</code> <p>Radius of image to be plotted for each channel/round.</p> <code>8</code> Source code in <code>coppafish/plot/call_spots/spot_colors.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'anchor', im_size: int = 8):\n\"\"\"\n    Diagnostic to show intensity of each color channel / round in neighbourhood of spot.\n    Will show a grid of `n_use_channels x n_use_rounds` subplots.\n\n    !!! warning \"Requires access to `nb.file_names.tile_dir`\"\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        im_size: Radius of image to be plotted for each channel/round.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)].transpose()\n    if method.lower() == 'omp':\n        config = nb.get_config()['thresholds']\n        page_name = 'omp'\n        spot_score = omp_spot_score(nb.omp, config['score_omp_multiplier'], spot_no)\n    else:\n        page_name = 'ref_spots'\n        spot_score = nb.ref_spots.score[spot_no]\n    gene_no = nb.__getattribute__(page_name).gene_no[spot_no]\n    t = nb.__getattribute__(page_name).tile[spot_no]\n    spot_yxz = nb.__getattribute__(page_name).local_yxz[spot_no]\n\n    gene_name = nb.call_spots.gene_names[gene_no]\n    gene_color = nb.call_spots.bled_codes_ge[gene_no][np.ix_(nb.basic_info.use_rounds,\n                                                             nb.basic_info.use_channels)].transpose().flatten()\n    n_use_channels, n_use_rounds = color_norm.shape\n    color_norm = [val for val in color_norm.flatten()]\n    spot_yxz_global = spot_yxz + nb.stitch.tile_origin[t]\n    im_size = [im_size, im_size]  # Useful for debugging to have different im_size_y, im_size_x.\n    # Subtlety here, may have y-axis flipped, but I think it is correct:\n    # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct.\n    # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5])\n    # will show the value 1 at max_y, min_x+1.\n    im_yxz = np.array(np.meshgrid(np.arange(spot_yxz[0]-im_size[0], spot_yxz[0]+im_size[0]+1)[::-1],\n                                  np.arange(spot_yxz[1]-im_size[1], spot_yxz[1]+im_size[1]+1), spot_yxz[2]),\n                      dtype=np.int16).T.reshape(-1, 3)\n    im_diameter = [2*im_size[0]+1, 2*im_size[1]+1]\n    spot_colors = get_spot_colors(im_yxz, t, nb.register.transform, nb.file_names, nb.basic_info)\n    spot_colors = np.moveaxis(spot_colors, 1, 2)  # put round as the last axis to match color_norm\n    spot_colors = spot_colors.reshape(im_yxz.shape[0], -1)\n    # reshape\n    cr_images = [spot_colors[:, i].reshape(im_diameter[0], im_diameter[1]) / color_norm[i]\n                 for i in range(spot_colors.shape[1])]\n    subplot_adjust = [0.07, 0.775, 0.075, 0.92]\n    super().__init__(cr_images, color_norm, subplot_row_columns=[n_use_channels, n_use_rounds],\n                     subplot_adjust=subplot_adjust, fig_size=(13, 8))\n    # set x, y coordinates to be those of the global coordinate system\n    plot_extent = [im_yxz[:, 1].min()-0.5+nb.stitch.tile_origin[t, 1],\n                   im_yxz[:, 1].max()+0.5+nb.stitch.tile_origin[t, 1],\n                   im_yxz[:, 0].min()-0.5+nb.stitch.tile_origin[t, 0],\n                   im_yxz[:, 0].max()+0.5+nb.stitch.tile_origin[t, 0]]\n    for i in range(self.n_images):\n        # Add cross-hair\n        if gene_color[i] &gt; self.intense_gene_thresh:\n            cross_hair_color = 'lime'  # different color if expected large intensity\n            linestyle = '--'\n            self.ax[i].tick_params(color='lime', labelcolor='lime')\n            for spine in self.ax[i].spines.values():\n                spine.set_edgecolor('lime')\n        else:\n            cross_hair_color = 'k'\n            linestyle = ':'\n        self.ax[i].axes.plot([spot_yxz_global[1], spot_yxz_global[1]], [plot_extent[2], plot_extent[3]],\n                             cross_hair_color, linestyle=linestyle, lw=1)\n        self.ax[i].axes.plot([plot_extent[0], plot_extent[1]], [spot_yxz_global[0], spot_yxz_global[0]],\n                             cross_hair_color, linestyle=linestyle, lw=1)\n        self.im[i].set_extent(plot_extent)\n        self.ax[i].tick_params(labelbottom=False, labelleft=False)\n        # Add axis labels to subplots of far left column or bottom row\n        if i % n_use_rounds == 0:\n            self.ax[i].set_ylabel(f'{nb.basic_info.use_channels[int(i/n_use_rounds)]}')\n        if i &gt;= self.n_images - n_use_rounds:\n            r = nb.basic_info.use_rounds[i-(self.n_images - n_use_rounds)]\n            self.ax[i].set_xlabel('{:.0f} ({:.2f})'.format(r, nb.call_spots.gene_efficiency[gene_no, r]))\n\n\n    self.ax[0].set_xticks([spot_yxz_global[1]])\n    self.ax[0].set_yticks([spot_yxz_global[0]])\n    self.fig.supylabel('Color Channel', size=14)\n    self.fig.supxlabel('Round (Gene Efficiency)', size=14, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n    plt.suptitle(f'Spot {spot_no}: match {str(np.around(spot_score, decimals=2))} '\n                 f'to {gene_name}', x=(subplot_adjust[0] + subplot_adjust[1]) / 2, size=16)\n    self.change_norm()\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#view_intensity","title":"<code>view_intensity</code>","text":"<p>Diagnostic to show how intensity is computed from <code>spot_color</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'anchor'</code> Source code in <code>coppafish/plot/call_spots/spot_colors.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'anchor'):\n\"\"\"\n    Diagnostic to show how intensity is computed from `spot_color`.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)].transpose()\n    if method.lower() == 'omp':\n        page_name = 'omp'\n        config = nb.get_config()['thresholds']\n    else:\n        page_name = 'ref_spots'\n    intensity_saved = nb.__getattribute__(page_name).intensity[spot_no]\n    intensity_thresh = get_intensity_thresh(nb)\n    spot_color = nb.__getattribute__(page_name).colors[spot_no][\n                     np.ix_(nb.basic_info.use_rounds, nb.basic_info.use_channels)].transpose() / color_norm\n    subplot_adjust = [0.07, 0.775, 0.1, 0.91]\n    super().__init__([spot_color], color_norm, subplot_adjust=subplot_adjust)\n    if intensity_saved &gt; intensity_thresh:\n        color = 'w'\n    else:\n        color = 'r'\n    spot_color_symbol = r\"$\\mathbf{\\zeta_s}$\"\n    intensity_symbol = r\"$\\chi_s$, (median of $\\max_c\\zeta_{s_{rc}}$ indicated in green)\"\n    self.ax[0].set_title(f'Spot Color, {spot_color_symbol}, for spot {spot_no}\\n'\n                         f'Intensity, {intensity_symbol} = {str(np.around(intensity_saved, 3))}', color=color)\n    self.ax[0].set_yticks(ticks=np.arange(self.im_data[0].shape[0]), labels=nb.basic_info.use_channels)\n    self.ax[0].set_xticks(ticks=np.arange(self.im_data[0].shape[1]), labels=nb.basic_info.use_rounds)\n    self.ax[0].set_xlabel('Round')\n    self.fig.supylabel('Color Channel')\n    # Highlight max channel in each round which contributes to intensity\n    max_channels = np.argmax(self.im_data[0], axis=0)\n    for r in range(len(nb.basic_info.use_rounds)):\n        # can't add rectangle to multiple axes hence second for loop\n        rectangle = plt.Rectangle((r-0.5, max_channels[r]-0.5), 1, 1,\n                                  fill=False, ec='lime', linestyle=':', lw=4)\n        self.ax[0].add_patch(rectangle)\n    self.change_norm()  # initialise with method = 'norm'\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#colorplotbase","title":"<code>ColorPlotBase</code>","text":"<p>This is the base class for plots with multiple subplots and with a slider to change the color axis and a button to change the normalisation. After initialising, the function <code>change_norm()</code> should be run to plot normalised images. This will change <code>self.method</code> from <code>'raw'</code> to <code>'norm'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List</code> <p><code>float [n_images]</code> Each image is <code>n_y x n_x (x n_z)</code>. This is the normalised image. There will be a subplot for each image and if it is 3D, the first z-plane will be set as the starting data, <code>self.im_data</code> while the full 3d data will be saved as <code>self.im_data_3d</code>.</p> required <code>norm_factor</code> <code>Optional[Union[np.ndarray, List]]</code> <p><code>float [n_images]</code> <code>norm_factor[i]</code> is the value to multiply <code>images[i]</code> to give raw image. <code>norm_factor[i]</code> is either an integer or an array of same dimensions as <code>image[i]</code>. If a single <code>norm_factor</code> given, assume same for each image.</p> required <code>subplot_row_columns</code> <code>Optional[List]</code> <p><code>[n_rows, n_columns]</code> The subplots will be arranged into <code>n_rows</code> and <code>n_columns</code>. If not given, <code>n_columns</code> will be 1.</p> <code>None</code> <code>fig_size</code> <code>Optional[Tuple]</code> <p><code>[width, height]</code> Size of figure to plot in inches. If not given, will be set to <code>(9, 5)</code>.</p> <code>None</code> <code>subplot_adjust</code> <code>Optional[List]</code> <p><code>[left, right, bottom, top]</code> The position of the sides of the subplots in the figure. I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that. If not given, will be set to <code>[0.07, 0.775, 0.095, 0.94]</code>.</p> <code>None</code> <code>cbar_pos</code> <code>Optional[List]</code> <p><code>[left, bottom, width, height]</code> Position of color axis. If not given, will be set to <code>[0.9, 0.15, 0.03, 0.8]</code>.</p> <code>None</code> <code>slider_pos</code> <code>Optional[List]</code> <p><code>[left, bottom, width, height]</code> Position of slider that controls color axis. If not given, will be set to <code>[0.85, 0.15, 0.01, 0.8]</code>.</p> <code>None</code> <code>button_pos</code> <code>Optional[List]</code> <p><code>[left, bottom, width, height]</code> Position of button which triggers change of normalisation. If not given, will be set to <code>[0.85, 0.02, 0.1, 0.05]</code>.</p> <code>None</code> Source code in <code>coppafish/plot/call_spots/spot_colors.py</code> <pre><code>def __init__(self, images: List, norm_factor: Optional[Union[np.ndarray, List]],\n             subplot_row_columns: Optional[List] = None,\n             fig_size: Optional[Tuple] = None, subplot_adjust: Optional[List] = None,\n             cbar_pos: Optional[List] = None, slider_pos: Optional[List] = None,\n             button_pos: Optional[List] = None):\n\"\"\"\n    This is the base class for plots with multiple subplots and with a slider to change the color axis and a button\n    to change the normalisation.\n    After initialising, the function `change_norm()` should be run to plot normalised images.\n    This will change `self.method` from `'raw'` to `'norm'`.\n\n    Args:\n        images: `float [n_images]`\n            Each image is `n_y x n_x (x n_z)`. This is the normalised image.\n            There will be a subplot for each image and if it is 3D, the first z-plane will be set as the\n            starting data, `self.im_data` while the full 3d data will be saved as `self.im_data_3d`.\n        norm_factor: `float [n_images]`\n            `norm_factor[i]` is the value to multiply `images[i]` to give raw image.\n            `norm_factor[i]` is either an integer or an array of same dimensions as `image[i]`.\n            If a single `norm_factor` given, assume same for each image.\n        subplot_row_columns: `[n_rows, n_columns]`\n            The subplots will be arranged into `n_rows` and `n_columns`.\n            If not given, `n_columns` will be 1.\n        fig_size: `[width, height]`\n            Size of figure to plot in inches.\n            If not given, will be set to `(9, 5)`.\n        subplot_adjust: `[left, right, bottom, top]`\n            The position of the sides of the subplots in the figure.\n            I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that.\n            If not given, will be set to `[0.07, 0.775, 0.095, 0.94]`.\n        cbar_pos: `[left, bottom, width, height]`\n            Position of color axis.\n            If not given, will be set to `[0.9, 0.15, 0.03, 0.8]`.\n        slider_pos: `[left, bottom, width, height]`\n            Position of slider that controls color axis.\n            If not given, will be set to `[0.85, 0.15, 0.01, 0.8]`.\n        button_pos: `[left, bottom, width, height]`\n            Position of button which triggers change of normalisation.\n            If not given, will be set to `[0.85, 0.02, 0.1, 0.05]`.\n    \"\"\"\n    # When bled code of a gene more than this, that particular round/channel will be highlighted in plots\n    self.intense_gene_thresh = 0.2\n    self.n_images = len(images)\n    if subplot_row_columns is None:\n        subplot_row_columns = [self.n_images, 1]\n    # Default positions\n    if fig_size is None:\n        fig_size = (9, 5)\n    if subplot_adjust is None:\n        subplot_adjust = [0.07, 0.775, 0.095, 0.94]\n    if cbar_pos is None:\n        cbar_pos = [0.9, 0.15, 0.03, 0.8]\n    if slider_pos is None:\n        self.slider_pos = [0.85, 0.15, 0.01, 0.8]\n    else:\n        self.slider_pos = slider_pos\n    if button_pos is None:\n        button_pos = [0.85, 0.02, 0.1, 0.05]\n    if not isinstance(norm_factor, list):\n        # allow for different norm for each image\n        if norm_factor is None:\n            self.color_norm = None\n        else:\n            self.color_norm = [norm_factor, ] * self.n_images\n    else:\n        self.color_norm = norm_factor\n    self.im_data = [val for val in images]  # put in order channels, rounds\n    self.method = 'raw' if self.color_norm is not None else 'norm'\n    if self.color_norm is None:\n        self.caxis_info = {'norm': {}}\n    else:\n        self.caxis_info = {'norm': {}, 'raw': {}}\n    for key in self.caxis_info:\n        if key == 'norm':\n            im_data = self.im_data\n            self.caxis_info[key]['format'] = '%.2f'\n        else:\n            im_data = [self.im_data[i] * self.color_norm[i] for i in range(self.n_images)]\n            self.caxis_info[key]['format'] = '%.0f'\n        self.caxis_info[key]['min'] = np.min([im.min() for im in im_data] + [-1e-20])\n        self.caxis_info[key]['max'] = np.max([im.max() for im in im_data] + [1e-20])\n        self.caxis_info[key]['max'] = np.max([self.caxis_info[key]['max'], -self.caxis_info[key]['min']])\n        # have equal either side of zero so small negatives don't look large\n        self.caxis_info[key]['min'] = -self.caxis_info[key]['max']\n        self.caxis_info[key]['clims'] = [self.caxis_info[key]['min'], self.caxis_info[key]['max']]\n        # cmap_norm is so cmap is white at 0.\n        self.caxis_info[key]['cmap_norm'] = \\\n            matplotlib.colors.TwoSlopeNorm(vmin=self.caxis_info[key]['min'],\n                                           vcenter=0, vmax=self.caxis_info[key]['max'])\n\n    self.fig, self.ax = plt.subplots(subplot_row_columns[0], subplot_row_columns[1], figsize=fig_size,\n                                     sharex=True, sharey=True)\n    if self.n_images == 1:\n        self.ax = [self.ax]  # need it to be a list\n    elif subplot_row_columns[0] &gt; 1 and subplot_row_columns[1] &gt; 1:\n        self.ax = self.ax.flatten()  # only have 1 ax index\n    oob_axes = np.arange(self.n_images, subplot_row_columns[0] * subplot_row_columns[1])\n    if oob_axes.size &gt; 0:\n        for i in oob_axes:\n            self.fig.delaxes(self.ax[i])  # delete excess subplots\n        self.ax = self.ax[:self.n_images]\n    self.fig.subplots_adjust(left=subplot_adjust[0], right=subplot_adjust[1], bottom=subplot_adjust[2],\n                             top=subplot_adjust[3])\n    self.im = [None] * self.n_images\n    if self.im_data[0].ndim == 3:\n        # For 3D data, start by showing just the first plane\n        self.im_data_3d = self.im_data.copy()\n        self.im_data = [val[:, :, 0] for val in self.im_data_3d]\n        if self.color_norm is not None:\n            self.color_norm_3d = self.color_norm.copy()\n            self.color_norm = [val[:, :, 0] for val in self.color_norm_3d]\n    else:\n        self.im_data_3d = None\n        self.color_norm_3d = None\n    # initialise plots with a zero array\n    for i in range(self.n_images):\n        self.im[i] = self.ax[i].imshow(np.zeros(self.im_data[0].shape[:2]), cmap=\"seismic\", aspect='auto',\n                                       norm=self.caxis_info[self.method]['cmap_norm'])\n    cbar_ax = self.fig.add_axes(cbar_pos)  # left, bottom, width, height\n    self.fig.colorbar(self.im[0], cax=cbar_ax)\n\n    self.slider_ax = self.fig.add_axes(self.slider_pos)\n    self.color_slider = None\n    if self.color_norm is not None:\n        self.norm_button_color = 'white'\n        self.norm_button_color_press = 'red'\n        if self.method == 'raw':\n            current_color = self.norm_button_color_press\n        else:\n            current_color = self.norm_button_color\n        self.norm_button_ax = self.fig.add_axes(button_pos)\n        self.norm_button = Button(self.norm_button_ax, 'Norm', hovercolor='0.275')\n        self.norm_button.label.set_color(current_color)\n        self.norm_button.on_clicked(self.change_norm)\n</code></pre>"},{"location":"code/plot/call_spots/#dot-product","title":"Dot Product","text":""},{"location":"code/plot/call_spots/#view_score","title":"<code>view_score</code>","text":"<p>This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically the normalised inverse variance).</p> <p>The bottom row shows the contribution from background and genes to the variance.</p> <p>The iteration as well as the alpha and beta parameters used to compute the weight can be changed through the text boxes.</p> <p>If the weight plot is clicked on, the <code>view_weight</code> plot will open for the current iteration.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the call_spots and ref_spots pages.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>g</code> <code>Optional[int]</code> <p>Gene to view dot product calculation for. If left as <code>None</code>, will show the gene with the largest dot product score.</p> <code>None</code> <code>iter</code> <code>int</code> <p>Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (<code>iter=0</code> will have only background fit, <code>iter=1</code> will have background + 1 gene etc.). The score saved as <code>nb.ref_spots.score</code> can be viewed with <code>iter=0</code>.</p> <code>0</code> <code>omp_fit_info</code> <code>Optional[List]</code> <p>This is a list containing <code>[track_info, bled_codes, dp_thresh]</code>. It is only ever used to call this function from <code>view_omp_fit</code>.</p> <code>None</code> <code>check_weight</code> <code>bool</code> <p>When this is <code>True</code>, we raise an error if weight computed here is different to that computed with <code>get_track_info</code>.</p> <code>False</code> Source code in <code>coppafish/plot/call_spots/dot_product.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp', g: Optional[int] = None,\n             iter: int = 0, omp_fit_info: Optional[List] = None, check_weight: bool = False):\n\"\"\"\n    This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically\n    the normalised inverse variance).\n\n    The bottom row shows the contribution from background and genes to the variance.\n\n    The iteration as well as the alpha and beta parameters used to compute the weight can be changed through\n    the text boxes.\n\n    If the weight plot is clicked on, the `view_weight` plot will open for the current iteration.\n\n    Args:\n        nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        g: Gene to view dot product calculation for.\n            If left as `None`, will show the gene with the largest dot product score.\n        iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes\n            which have already been fitted (`iter=0` will have only background fit,\n            `iter=1` will have background + 1 gene etc.).\n            The score saved as `nb.ref_spots.score` can be viewed with `iter=0`.\n        omp_fit_info: This is a list containing `[track_info, bled_codes, dp_thresh]`.\n            It is only ever used to call this function from `view_omp_fit`.\n        check_weight: When this is `True`, we raise an error if weight computed here is different\n            to that computed with `get_track_info`.\n    \"\"\"\n    self.spot_no = spot_no\n    if omp_fit_info is None:\n        self.track_info, self.bled_codes, self.dp_thresh = get_track_info(nb, spot_no, method)\n    else:\n        self.track_info, self.bled_codes, self.dp_thresh = omp_fit_info\n    self.n_genes, self.n_rounds_use, self.n_channels_use = self.bled_codes.shape\n    # allow to view dot product with background\n    self.bled_codes = np.append(self.bled_codes, self.track_info['background_codes'], axis=0)\n    self.n_genes_all = self.bled_codes.shape[0]\n    self.spot_color = self.track_info['residual'][0]\n\n    # Get saved values if anchor method\n    if method.lower() != 'omp':\n        self.g_saved = nb.ref_spots.gene_no[spot_no]\n        if self.track_info['gene_added'][2] &lt; self.n_genes:\n            # Possibility best gene will be background here, but impossible for saved best gene to be background\n            if self.track_info['gene_added'][2] != self.g_saved and check_weight:\n                raise ValueError(f\"\\nBest gene saved was {self.g_saved} but with parameters used here, it \"\n                                 f\"was {self.track_info['gene_added'][2]}.\\nEnsure that alpha and beta in \"\n                                 f\"config['call_spots'] have not been changed.\\n\"\n                                 f\"Set check_weight=False to skip this error.\")\n        self.dp_val_saved = nb.ref_spots.score[spot_no]\n        config_name = 'call_spots'\n    else:\n        self.g_saved = None\n        self.dp_val_saved = None\n        config_name = 'omp'\n\n    # Get default dot product params\n    config = nb.get_config()\n    self.alpha = config[config_name]['alpha']\n    self.beta = config[config_name]['beta']\n    self.dp_norm_shift = nb.call_spots.dp_norm_shift\n    self.check_weight = check_weight\n\n    self.n_iter = self.track_info['residual'].shape[0] - 2  # first two indices in track is not added gene\n    if iter &gt;= self.n_iter or iter &lt; 0:\n        warnings.warn(f\"Only {self.n_iter} iterations for this pixel but iter={iter}, \"\n                      f\"setting iter = {self.n_iter - 1}.\")\n        iter = self.n_iter - 1\n    self.iter = iter\n    if g is None:\n        g = self.track_info['gene_added'][2 + iter]\n    self.g = g\n    self.gene_names = list(nb.call_spots.gene_names) + [f'BG{i}' for i in nb.basic_info.use_channels]\n    self.use_channels = nb.basic_info.use_channels\n    self.use_rounds = nb.basic_info.use_rounds\n\n    # Initialize data\n    self.dp_val = None\n    self.dp_weight_val = None\n    self.im_data = None\n    self.update_data()\n\n    # Initialize plot\n    self.title = None\n    self.vmax = None\n    self.get_cax_lim()\n    self.fig = plt.figure(figsize=(16, 7))\n    ax1 = self.fig.add_subplot(2, 4, 1)\n    ax2 = self.fig.add_subplot(2, 4, 5)\n    ax3 = self.fig.add_subplot(2, 4, 2)\n    ax4 = self.fig.add_subplot(2, 4, 6)\n    ax5 = self.fig.add_subplot(1, 4, 3)\n    ax6 = self.fig.add_subplot(2, 4, 4)\n    ax7 = self.fig.add_subplot(2, 4, 8)\n    self.ax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7]\n    self.ax[0].get_shared_x_axes().join(self.ax[0], *self.ax[1:])\n    self.ax[0].get_shared_y_axes().join(self.ax[0], *self.ax[1:])\n    self.subplot_adjust = [0.05, 0.87, 0.05, 0.9]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.im = [None] * len(self.ax)\n    self.set_up_plots()\n    self.set_titles()\n    self.add_rectangles()\n\n    # Text boxes to change parameters\n    text_box_labels = ['Gene', 'Iteration', r'$\\alpha$', r'$\\beta$', r'dp_shift, $\\lambda_d$']\n    text_box_values = [self.g, self.iter, self.alpha, self.beta, self.dp_norm_shift]\n    text_box_funcs = [self.update_g, self.update_iter, self.update_alpha, self.update_beta,\n                      self.update_dp_norm_shift]\n    self.text_boxes = [None] * len(text_box_labels)\n    for i in range(len(text_box_labels)):\n        text_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.05, self.subplot_adjust[3] - 0.15 * (i+1),\n                                     0.05, 0.04])\n        self.text_boxes[i] = TextBox(text_ax, text_box_labels[i], text_box_values[i], color='k',\n                                     hovercolor=[0.2, 0.2, 0.2])\n        self.text_boxes[i].cursor.set_color('r')\n        # change text box title to be above not to the left of box\n        label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n        label.set_position([0.5, 1.75])  # [x,y] - change here to set the position\n        # centering the text\n        label.set_verticalalignment('top')\n        label.set_horizontalalignment('center')\n        self.text_boxes[i].on_submit(text_box_funcs[i])\n\n    # Make so if click on weight plot, it opens view_weight\n    self.nb = nb\n    self.method = method\n    self.fig.canvas.mpl_connect('button_press_event', self.show_weight)\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#weight","title":"Weight","text":""},{"location":"code/plot/call_spots/#view_weight","title":"<code>view_weight</code>","text":"<p>This produces at least 5 plots which show how the weight used in the dot product score was calculated.</p> <p>The iteration as well as the alpha and beta parameters used to compute the weight can be changed with the text boxes.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the call_spots and ref_spots pages.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>iter</code> <code>int</code> <p>Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (<code>iter=0</code> will have only background fit, <code>iter=1</code> will have background + 1 gene etc.). The score saved as <code>nb.ref_spots.score</code> can be viewed with <code>iter=0</code>.</p> <code>0</code> <code>score_info</code> <code>Optional[List]</code> <p>This is a list containing <code>[track_info, bled_codes, weight_vmax]</code>. It is only ever used to call this function from <code>view_score</code>.</p> <code>None</code> <code>check_weight</code> <code>bool</code> <p>When this is <code>True</code>, we raise an error if weight computed here is different to that computed with <code>get_track_info</code>.</p> <code>True</code> Source code in <code>coppafish/plot/call_spots/weight.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp',\n             iter: int = 0, alpha: Optional[float] = None, beta: Optional[float] = None,\n             score_info: Optional[List] = None, check_weight: bool = True):\n\"\"\"\n    This produces at least 5 plots which show how the weight used in the dot product score was calculated.\n\n    The iteration as well as the alpha and beta parameters used to compute the weight can be changed\n    with the text boxes.\n\n    Args:\n        nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes\n            which have already been fitted (`iter=0` will have only background fit,\n            `iter=1` will have background + 1 gene etc.).\n            The score saved as `nb.ref_spots.score` can be viewed with `iter=0`.\n        score_info: This is a list containing `[track_info, bled_codes, weight_vmax]`.\n            It is only ever used to call this function from `view_score`.\n        check_weight: When this is `True`, we raise an error if weight computed here is different\n            to that computed with `get_track_info`.\n    \"\"\"\n    self.spot_no = spot_no\n    if score_info is None:\n        self.track_info, self.bled_codes = get_track_info(nb, spot_no, method)[:2]\n        self.weight_vmax = None\n    else:\n        self.track_info, self.bled_codes, self.weight_vmax = score_info\n    self.n_genes, self.n_rounds_use, self.n_channels_use = self.bled_codes.shape\n    # allow to view dot product with background\n    self.bled_codes = np.append(self.bled_codes, self.track_info['background_codes'], axis=0)\n    self.n_genes_all = self.bled_codes.shape[0]\n    self.spot_color = self.track_info['residual'][0]\n\n    if method.lower() == 'omp':\n        config_name = 'omp'\n    else:\n        config_name = 'call_spots'\n    # Get default params\n    config = nb.get_config()\n    if alpha is None:\n        alpha = config[config_name]['alpha']\n    if beta is None:\n        beta = config[config_name]['beta']\n    self.alpha = alpha\n    self.beta = beta\n    self.check_weight = check_weight\n\n    self.n_iter = self.track_info['residual'].shape[0] - 2  # first two indices in track is not added gene\n    if iter &gt;= self.n_iter or iter &lt; 0:\n        warnings.warn(f\"Only {self.n_iter} iterations for this pixel but iter={iter}, \"\n                      f\"setting iter = {self.n_iter - 1}.\")\n        iter = self.n_iter - 1\n    self.iter = iter\n    self.gene_names = list(nb.call_spots.gene_names) + [f'BG{i}' for i in nb.basic_info.use_channels]\n    self.use_channels = nb.basic_info.use_channels\n    self.use_rounds = nb.basic_info.use_rounds\n\n    # Initialize data\n    self.n_plots = self.n_plots_top_row + self.n_iter\n    self.update_data()\n\n    # Initialize plots\n    self.vmax = None\n    self.get_cax_lim()\n    n_rows = 2\n    n_cols = int(np.max([self.n_plots_top_row, self.n_iter]))\n    self.fig = plt.figure(figsize=(16, 7))\n    self.ax = []\n    for i in range(self.n_plots):\n        if i &gt;= self.n_plots_top_row:\n            # So goes to next row\n            self.ax += [self.fig.add_subplot(n_rows, n_cols, n_cols + i + 1 - self.n_plots_top_row)]\n        else:\n            self.ax += [self.fig.add_subplot(n_rows, n_cols, i + 1)]\n    # Y and X axis are the same for all plots hence share\n    self.ax[0].get_shared_x_axes().join(self.ax[0], *self.ax[1:])\n    self.ax[0].get_shared_y_axes().join(self.ax[0], *self.ax[1:])\n    self.subplot_adjust = [0.05, 0.87, 0.07, 0.9]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.im = [None] * self.n_plots\n    self.variance_cbar = None\n    self.set_up_plots()\n    self.set_titles()\n    self.add_rectangles()\n\n    # Text boxes to change parameters\n    text_box_labels = ['Iteration', r'$\\alpha$', r'$\\beta$']\n    text_box_values = [self.iter, self.alpha, self.beta]\n    text_box_funcs = [self.update_iter, self.update_alpha, self.update_beta]\n    self.text_boxes = [None] * len(text_box_labels)\n    for i in range(len(text_box_labels)):\n        text_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.05, self.subplot_adjust[3] - 0.15 * (i+1),\n                                     0.05, 0.04])\n        self.text_boxes[i] = TextBox(text_ax, text_box_labels[i], text_box_values[i], color='k',\n                                     hovercolor=[0.2, 0.2, 0.2])\n        self.text_boxes[i].cursor.set_color('r')\n        # change text box title to be above not to the left of box\n        label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n        label.set_position([0.5, 1.75])  # [x,y] - change here to set the position\n        # centering the text\n        label.set_verticalalignment('top')\n        label.set_horizontalalignment('center')\n        self.text_boxes[i].on_submit(text_box_funcs[i])\n\n    self.nb = nb\n    self.method = method\n    self.fig.canvas.mpl_connect('button_press_event', self.show_background)\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#background","title":"Background","text":""},{"location":"code/plot/call_spots/#view_background","title":"<code>view_background</code>","text":"<p>This shows how the background coefficients were calculated.</p> <p>The weighted dot product is equal to weight multiplied by dot product. Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c.</p> <p>Also shows residual after removing background.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the call_spots and ref_spots pages.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>check</code> <code>bool</code> <p>When this is <code>True</code>, we raise an error if background coefs computed here is different to that computed with <code>get_track_info</code>.</p> <code>True</code> <code>track_info</code> <code>Optional[List]</code> <p>To use when calling from <code>view_weight</code>.</p> <code>None</code> Source code in <code>coppafish/plot/call_spots/background.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp', check: bool = True,\n             track_info: Optional[List] = None):\n\"\"\"\n    This shows how the background coefficients were calculated.\n\n    The weighted dot product is equal to weight multiplied by dot product.\n    Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c.\n\n    Also shows residual after removing background.\n\n    Args:\n        nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        check: When this is `True`, we raise an error if background coefs computed here is different\n            to that computed with `get_track_info`.\n        track_info: To use when calling from `view_weight`.\n    \"\"\"\n    self.spot_no = spot_no\n    if track_info is None:\n        self.track_info = get_track_info(nb, spot_no, method)[0]\n        self.color_vmax = None\n    else:\n        self.track_info, self.color_vmax = track_info\n    self.n_genes_all = self.track_info['coef'][0].size\n    self.spot_color = self.track_info['residual'][0]\n    self.n_rounds_use, self.n_channels_use = self.spot_color.shape\n    self.n_genes = self.n_genes_all - self.n_channels_use\n    self.use_channels = nb.basic_info.use_channels\n    self.use_rounds = nb.basic_info.use_rounds\n\n    self.background_weight_shift = nb.call_spots.background_weight_shift\n    self.check = check\n\n    # Initialize data\n    self.im_data = None\n    self.update_data()\n    hi = 5\n\n    # Initialize plots\n    self.vmax = None\n    self.weight_dp_max_initial = None\n    self.get_cax_lim()\n    self.fig = plt.figure(figsize=(16, 7))\n    self.ax = []\n    ax1 = self.fig.add_subplot(2, 5, 1)\n    ax2 = self.fig.add_subplot(2, 5, 6)\n    ax3 = self.fig.add_subplot(1, 5, 2)\n    ax4 = self.fig.add_subplot(2, 5, 3)\n    ax5 = self.fig.add_subplot(2, 5, 8)\n    ax6 = self.fig.add_subplot(2, 5, 4)\n    ax7 = self.fig.add_subplot(2, 5, 9)\n    ax8 = self.fig.add_subplot(2, 5, 5)\n    ax9 = self.fig.add_subplot(2, 5, 10)\n    self.ax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9]\n    self.ax[0].get_shared_y_axes().join(self.ax[0], *self.ax[1:])\n    # Coef plots have different x-axis\n    self.ax[self.coef_plot_ind[0]].get_shared_x_axes().join(self.ax[self.coef_plot_ind[0]],\n                                                            self.ax[self.coef_plot_ind[1]])\n    # All other plots have same axis\n    self.ax[0].get_shared_x_axes().join(self.ax[0], *self.ax[1:self.coef_plot_ind[0]])\n    self.ax[0].get_shared_x_axes().join(self.ax[0], *self.ax[self.coef_plot_ind[1]+1:])\n    self.subplot_adjust = [0.05, 0.97, 0.07, 0.9]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.im = [None] * len(self.ax)\n    self.set_up_plots()\n    self.set_titles()\n\n    weight_plot_pos = self.ax[self.weight_plot_ind].get_position()\n    box_x = np.mean([weight_plot_pos.x0, weight_plot_pos.x1])\n    box_y = weight_plot_pos.y0\n    text_ax = self.fig.add_axes([box_x, box_y - 0.15, 0.05, 0.04])\n    self.text_box = TextBox(text_ax, r'background_shift, $\\lambda_b$', self.background_weight_shift, color='k',\n                                     hovercolor=[0.2, 0.2, 0.2])\n    self.text_box.cursor.set_color('r')\n    label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n    label.set_position([0.5, 1.75])  # [x,y] - change here to set the position\n    # centering the text\n    label.set_verticalalignment('top')\n    label.set_horizontalalignment('center')\n    self.text_box.on_submit(self.update_background_weight_shift)\n\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#gene-counts","title":"Gene Counts","text":""},{"location":"code/plot/call_spots/#gene_counts","title":"<code>gene_counts</code>","text":"<p>This shows the number of reference spots assigned to each gene which pass the quality thresholding based on the parameters <code>score_thresh</code> and <code>intensity_thresh</code>.</p> <p>If <code>nb</code> has the OMP page, then the number of omp spots will also be shown, where the quality thresholding is based on <code>score_omp_thresh</code>, <code>score_omp_multiplier</code> and <code>intensity_thresh</code>.</p> <p>There will also be a second reference spots histogram, the difference with this is that the spots were allowed to be assigned to some fake genes with <code>bled_codes</code> specified through <code>fake_bled_codes</code>.</p> <p>Note</p> <p><code>fake_bled_codes</code> have dimension <code>n_fake x  nbp_basic.n_rounds x nbp_basic.n_channels</code> not <code>n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>call_spots</code> page.</p> required <code>fake_bled_codes</code> <code>Optional[np.ndarray]</code> <p><code>float [n_fake_genes x n_rounds x n_channels]</code>. colors of fake genes to find dot product with Will find new gene assignment of anchor spots to new set of <code>bled_codes</code> which include these in addition to <code>bled_codes_ge</code>. By default, will have a fake gene for each round and channel such that it is \\(1\\) in round r, channel \\(c\\) and 0 everywhere else.</p> <code>None</code> <code>fake_gene_names</code> <code>Optional[List[str]]</code> <p><code>str [n_fake_genes]</code>. Can give name of each fake gene. If <code>None</code>, fake gene \\(i\\) will be called FAKE:\\(i\\).</p> <code>None</code> <code>score_thresh</code> <code>Optional[float]</code> <p>Threshold for score for ref_spots. Can be changed with text box.</p> <code>None</code> <code>intensity_thresh</code> <code>Optional[float]</code> <p>Threshold for intensity. Can be changed with text box.</p> <code>None</code> <code>score_omp_thresh</code> <code>Optional[float]</code> <p>Threshold for score for omp_spots. Can be changed with text box.</p> <code>None</code> <code>score_omp_multiplier</code> <code>Optional[float]</code> <p>Can specify the value of score_omp_multiplier to use to compute omp score. If <code>None</code>, will use value in config file. Can be changed with text box.</p> <code>None</code> Source code in <code>coppafish/plot/call_spots/gene_counts.py</code> <pre><code>def __init__(self, nb: Notebook, fake_bled_codes: Optional[np.ndarray] = None,\n             fake_gene_names: Optional[List[str]] = None,\n             score_thresh: Optional[float] = None, intensity_thresh: Optional[float] = None,\n             score_omp_thresh: Optional[float] = None, score_omp_multiplier: Optional[float] = None):\n\"\"\"\n    This shows the number of reference spots assigned to each gene which pass the quality thresholding based on\n    the parameters `score_thresh` and `intensity_thresh`.\n\n    If `nb` has the *OMP* page, then the number of omp spots will also be shown, where the quality thresholding is\n    based on `score_omp_thresh`, `score_omp_multiplier` and `intensity_thresh`.\n\n    There will also be a second reference spots histogram, the difference with this is that the spots\n    were allowed to be assigned to some fake genes with `bled_codes` specified through `fake_bled_codes`.\n\n    !!! note\n        `fake_bled_codes` have dimension `n_fake x  nbp_basic.n_rounds x nbp_basic.n_channels` not\n        `n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)`.\n\n    Args:\n        nb: *Notebook* containing at least `call_spots` page.\n        fake_bled_codes: `float [n_fake_genes x n_rounds x n_channels]`.\n            colors of fake genes to find dot product with\n            Will find new gene assignment of anchor spots to new set of `bled_codes` which include these in\n            addition to `bled_codes_ge`.\n            By default, will have a fake gene for each round and channel such that it is $1$ in round r,\n            channel $c$ and 0 everywhere else.\n        fake_gene_names: `str [n_fake_genes]`.\n            Can give name of each fake gene. If `None`, fake gene $i$ will be called FAKE:$i$.\n        score_thresh: Threshold for score for ref_spots. Can be changed with text box.\n        intensity_thresh: Threshold for intensity. Can be changed with text box.\n        score_omp_thresh: Threshold for score for omp_spots. Can be changed with text box.\n        score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score.\n            If `None`, will use value in config file. Can be changed with text box.\n    \"\"\"\n    # Add fake genes\n    if fake_bled_codes is None:\n        # Default have binary fake gene for each used round and channel\n        n_fake = len(nb.basic_info.use_rounds) * len(nb.basic_info.use_channels)\n        fake_bled_codes = np.zeros((n_fake, nb.basic_info.n_rounds, nb.basic_info.n_channels))\n        i = 0\n        # Cluster fake genes by channel because more likely to be a faulty channel\n        for c in nb.basic_info.use_channels:\n            for r in nb.basic_info.use_rounds:\n                fake_bled_codes[i, r, c] = 1\n                i += 1\n        fake_gene_names = [f'r{r}c{c}' for c in nb.basic_info.use_channels for r in nb.basic_info.use_rounds]\n    n_fake = fake_bled_codes.shape[0]\n    if fake_gene_names is None:\n        fake_gene_names = [f'FAKE:{i}' for i in range(n_fake)]\n    self.gene_names = nb.call_spots.gene_names.tolist() + fake_gene_names\n    self.n_genes = len(self.gene_names)\n    self.n_genes_real = self.n_genes - n_fake\n\n    # Do new gene assignment for anchor spots when fake genes are included\n    spot_colors_pb, background_var = background_fitting(nb, 'anchor')[1:]  # Fit background before dot product score\n    grc_ind = np.ix_(np.arange(self.n_genes), nb.basic_info.use_rounds, nb.basic_info.use_channels)\n    # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds\n    bled_codes = np.append(nb.call_spots.bled_codes_ge, fake_bled_codes, axis=0)\n    bled_codes = bled_codes[grc_ind]\n    # Ensure L2 norm of 1 for each gene\n    norm_factor = np.expand_dims(np.linalg.norm(bled_codes, axis=(1, 2)), (1, 2))\n    norm_factor[norm_factor == 0] = 1  # For genes with no dye in use_dye, this avoids blow up on next line\n    bled_codes = bled_codes / norm_factor\n    score, gene_no = get_dot_product_score(spot_colors_pb, bled_codes, None, nb.call_spots.dp_norm_shift,\n                                           background_var)\n\n    # Add quality thresholding info and gene assigned to for method with no fake genes and method with fake genes\n    self.intensity = [nb.ref_spots.intensity.astype(np.float16)] * 2\n    self.score = [nb.ref_spots.score.astype(np.float16), score.astype(np.float16)]\n    self.gene_no = [nb.ref_spots.gene_no, gene_no]\n\n    # Add current thresholds\n    config = nb.get_config()['thresholds']\n    if config['intensity'] is None:\n        config['intensity'] = nb.call_spots.gene_efficiency_intensity_thresh\n    if score_thresh is None:\n        score_thresh = config['score_ref']\n    if intensity_thresh is None:\n        intensity_thresh = config['intensity']\n    self.score_thresh = [score_thresh] * 2\n    self.intensity_thresh = intensity_thresh\n\n    # Add omp gene assignment if have page\n    if nb.has_page('omp'):\n        if score_omp_multiplier is None:\n            score_omp_multiplier = config['score_omp_multiplier']\n        self.score_multiplier = score_omp_multiplier\n        self.nbp_omp = nb.omp\n        if score_omp_thresh is None:\n            score_omp_thresh = config['score_omp']\n        self.score_thresh += [score_omp_thresh]\n        self.score += [omp_spot_score(self.nbp_omp, self.score_multiplier).astype(np.float16)]\n        self.intensity += [self.nbp_omp.intensity.astype(np.float16)]\n        self.gene_no += [self.nbp_omp.gene_no]\n        self.omp = True\n    else:\n        self.omp = False\n    self.n_plots = len(self.score)\n    self.use = None\n    self.update_use()\n\n    # Initialise plot\n    self.fig, self.ax = plt.subplots(1, 1, figsize=(16, 7))\n    self.subplot_adjust = [0.07, 0.85, 0.12, 0.93]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.ax.set_ylabel(r\"Number of Spots\")\n    self.ax.set_xlabel('Gene')\n    self.ax.set_title(f\"Number of Spots assigned to each Gene\")\n\n    # record min and max for textbox input\n    self.score_min = np.around(self.score[0].min(), 2)  # Min score of score[1] cannot be less than this\n    self.score_max = np.around(self.score[1].max(), 2)  # Max score of score[0] cannot be more than this\n    self.intensity_min = np.around(self.intensity[0].min(), 2)\n    self.intensity_max = np.around(self.intensity[0].max(), 2)\n    self.plots = [None] * self.n_plots\n    default_colors = plt.rcParams['axes.prop_cycle']._left\n    default_colors = default_colors[:2] + default_colors[3:4]  # default 3 is better than default 2 for omp plot\n    for i in range(self.n_plots):\n        self.plots[i], = self.ax.plot(np.arange(self.n_genes),\n                                      np.histogram(self.gene_no[i][self.use[i]],\n                                                   np.arange(self.n_genes + 1) - 0.5)[0],\n                                      color=default_colors[i]['color'])\n        if i == 1:\n            self.plots[i].set_visible(False)\n    self.ax.set_ylim(0, None)\n    self.ax.set_xticks(np.arange(self.n_genes))\n    self.ax.set_xticklabels(self.gene_names, rotation=90, size=7)\n    self.ax.set_xlim(-0.5, self.n_genes_real - 0.5)\n\n    # Add text box to change score multiplier\n    text_box_labels = [r'Score, $\\Delta_s$' + '\\nThreshold', r'Intensity, $\\chi_s$' + '\\nThreshold']\n    text_box_values = [np.around(self.score_thresh[0], 2), np.around(self.intensity_thresh, 2)]\n    text_box_funcs = [self.update_score_thresh, self.update_intensity_thresh]\n    if self.omp:\n        text_box_labels += [r'OMP Score, $\\gamma_s$' + '\\nThreshold', 'Score\\n' + r'Multiplier, $\\rho$']\n        text_box_values += [np.around(self.score_thresh[2], 2), np.around(self.score_multiplier, 2)]\n        text_box_funcs += [self.update_score_omp_thresh, self.update_score_multiplier]\n    self.text_boxes = [None] * len(text_box_labels)\n    for i in range(len(text_box_labels)):\n        text_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.05,\n                                     self.subplot_adjust[2] + 0.15 * (len(text_box_labels) - i - 1), 0.05, 0.04])\n        self.text_boxes[i] = TextBox(text_ax, text_box_labels[i], text_box_values[i], color='k',\n                                     hovercolor=[0.2, 0.2, 0.2])\n        self.text_boxes[i].cursor.set_color('r')\n        label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n        label.set_position([0.5, 2.75])\n        # centering the text\n        label.set_verticalalignment('top')\n        label.set_horizontalalignment('center')\n        self.text_boxes[i].on_submit(text_box_funcs[i])\n\n    # Add buttons to add/remove score_dp histograms\n    self.buttons_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.02, self.subplot_adjust[3] - 0.25, 0.15, 0.3])\n    plt.axis('off')\n    self.button_labels = [\"Ref Spots\",\n                          \"Ref Spots - Fake Genes\"]\n    label_checked = [True, False]\n    if self.omp:\n        self.button_labels += [\"OMP Spots\"]\n        label_checked += [True]\n    self.buttons = CheckButtons(self.buttons_ax, self.button_labels, label_checked)\n\n    for i in range(self.n_plots):\n        self.buttons.labels[i].set_fontsize(7)\n        self.buttons.labels[i].set_color(default_colors[i]['color'])\n        self.buttons.rectangles[i].set_color('w')\n    self.buttons.on_clicked(self.choose_plots)\n    plt.show()\n</code></pre>"},{"location":"code/plot/call_spots/#score-calculation","title":"Score Calculation","text":""},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.score_calc.background_fitting","title":"<code>background_fitting(nb, method)</code>","text":"<p>Computes background using parameters in config file. Then removes this from the <code>spot_colors</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing call_spots page</p> required <code>method</code> <code>str</code> <p>'omp' or 'anchor', indicating which <code>spot_colors</code> to use.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>spot_colors</code> - <code>float [n_spots x n_rounds_use x n_channels_use]</code>. <code>spot_color</code> after normalised by <code>color_norm_factor</code> but before background fit.</p> <code>np.ndarray</code> <p><code>spot_colors_pb</code> - <code>float [n_spots x n_rounds_use x n_channels_use]</code>. <code>spot_color</code> after background removed.</p> <code>np.ndarray</code> <p><code>background_var</code> - <code>float [n_spots x n_rounds_use x n_channels_use]</code>. inverse of the weighting used for dot product score calculation.</p> Source code in <code>coppafish/plot/call_spots/score_calc.py</code> <pre><code>def background_fitting(nb: Notebook, method: str) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n\"\"\"\n    Computes background using parameters in config file. Then removes this from the `spot_colors`.\n    Args:\n        nb: Notebook containing call_spots page\n        method: 'omp' or 'anchor', indicating which `spot_colors` to use.\n\n    Returns:\n        `spot_colors` - `float [n_spots x n_rounds_use x n_channels_use]`.\n            `spot_color` after normalised by `color_norm_factor` but before background fit.\n        `spot_colors_pb` - `float [n_spots x n_rounds_use x n_channels_use]`.\n            `spot_color` after background removed.\n        `background_var` - `float [n_spots x n_rounds_use x n_channels_use]`.\n            inverse of the weighting used for dot product score calculation.\n    \"\"\"\n    rc_ind = np.ix_(nb.basic_info.use_rounds, nb.basic_info.use_channels)\n    if method.lower() == 'omp':\n        spot_colors = np.moveaxis(np.moveaxis(nb.omp.colors, 0, -1)[rc_ind], -1, 0)\n        config = nb.get_config()['omp']\n    else:\n        spot_colors = np.moveaxis(np.moveaxis(nb.ref_spots.colors, 0, -1)[rc_ind], -1, 0)\n        config = nb.get_config()['call_spots']\n    alpha = config['alpha']\n    beta = config['beta']\n    spot_colors = spot_colors / nb.call_spots.color_norm_factor[rc_ind]\n    spot_colors_pb, background_coef, background_codes = \\\n        fit_background(spot_colors, nb.call_spots.background_weight_shift)\n    background_codes = background_codes.reshape(background_codes.shape[0], -1)\n    background_var = background_coef ** 2 @ background_codes ** 2 * alpha + beta ** 2\n    return spot_colors, spot_colors_pb, background_var\n</code></pre>"},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.score_calc.get_dot_product_score","title":"<code>get_dot_product_score(spot_colors, bled_codes, spot_gene_no, dp_norm_shift, background_var)</code>","text":"<p>Finds dot product score for each <code>spot_color</code> given to the gene indicated by <code>spot_gene_no</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spot_colors</code> <code>np.ndarray</code> <p><code>float [n_spots x n_rounds_use x n_channels_use]</code>. colors of spots to find score of.</p> required <code>bled_codes</code> <code>np.ndarray</code> <p><code>float [n_genes x n_rounds_use x n_channels_use]</code>. colors of genes to find dot product with.</p> required <code>spot_gene_no</code> <code>Optional[np.ndarray]</code> <p><code>int [n_spots]</code>. Gene that each spot was assigned to. If None, will set <code>spot_gene_no[s]</code> to gene for which score was largest.</p> required <code>dp_norm_shift</code> <code>float</code> <p>Normalisation constant for single round used for dot product calculation. I.e. <code>nb.call_spots.dp_norm_shift</code>.</p> required <code>background_var</code> <code>Optional[np.ndarray]</code> <p><code>float [n_spots x n_rounds_use x n_channels_use]</code>. inverse of the weighting used for dot product score calculation.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>spot_score</code> - <code>float [n_spots]</code>. Dot product score for each spot.</p> <code>np.ndarray</code> <p><code>spot_gene_no</code> - will be same as input if given, otherwise will be the best gene assigned.</p> Source code in <code>coppafish/plot/call_spots/score_calc.py</code> <pre><code>def get_dot_product_score(spot_colors: np.ndarray, bled_codes: np.ndarray, spot_gene_no: Optional[np.ndarray],\n                          dp_norm_shift: float, background_var: Optional[np.ndarray]) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Finds dot product score for each `spot_color` given to the gene indicated by `spot_gene_no`.\n\n    Args:\n        spot_colors: `float [n_spots x n_rounds_use x n_channels_use]`.\n            colors of spots to find score of.\n        bled_codes: `float [n_genes x n_rounds_use x n_channels_use]`.\n            colors of genes to find dot product with.\n        spot_gene_no: `int [n_spots]`.\n            Gene that each spot was assigned to. If None, will set `spot_gene_no[s]` to gene for which\n            score was largest.\n        dp_norm_shift: Normalisation constant for single round used for dot product calculation.\n            I.e. `nb.call_spots.dp_norm_shift`.\n        background_var: `float [n_spots x n_rounds_use x n_channels_use]`.\n            inverse of the weighting used for dot product score calculation.\n\n    Returns:\n        `spot_score` - `float [n_spots]`.\n            Dot product score for each spot.\n        `spot_gene_no` - will be same as input if given, otherwise will be the best gene assigned.\n    \"\"\"\n    n_spots, n_rounds_use = spot_colors.shape[:2]\n    n_genes = bled_codes.shape[0]\n    dp_norm_shift = dp_norm_shift * np.sqrt(n_rounds_use)\n    if background_var is None:\n        weight = None\n    else:\n        weight = 1 / background_var\n    scores = np.asarray(dot_product_score(spot_colors.reshape(n_spots, -1),\n                                          bled_codes.reshape(n_genes, -1), dp_norm_shift, weight))\n    if spot_gene_no is None:\n        spot_gene_no = np.argmax(scores, 1)\n    spot_score = scores[np.arange(n_spots), spot_gene_no]\n    return spot_score, spot_gene_no\n</code></pre>"},{"location":"code/plot/call_spots/#scaled-k-means","title":"Scaled K Means","text":""},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.scaled_k_means.view_scaled_k_means","title":"<code>view_scaled_k_means(nb, r=0, check=False)</code>","text":"<p>Plot to show how <code>scaled_k_means</code> was used to compute the bleed matrix. There will be upto 3 columns, each with 2 plots.</p> <p>The vector for dye \\(d\\) in the <code>bleed_matrix</code> is computed from all the spot round vectors whose dot product to the dye \\(d\\) vector was the highest. The boxplots in the first row show these dot product values for each dye.</p> <p>The second row then shows the bleed matrix at each stage of the computation.</p> <p>The first column shows the initial bleed matrix. The second column shows the bleed matrix after running <code>scaled_k_means</code> once with a score threshold of 0. The third column shows the final <code>bleed_matrix</code> after running <code>scaled_k_means</code> a second time with <code>score_thresh</code> for dye \\(d\\) set to the median of the scores assigned to dye \\(d\\) in the first run. Third column only present if <code>config['call_spots']['bleed_matrix_anneal']==True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>r</code> <code>int</code> <p>Round of bleed matrix to view. Only relevant if <code>config['call_spots']['bleed_matrix_method'] = 'separate'</code>.</p> <code>0</code> <code>check</code> <code>bool</code> <p>If True, will raise error if <code>bleed_matrix</code> computed here is different to that saved in notebook</p> <code>False</code> Source code in <code>coppafish/plot/call_spots/scaled_k_means.py</code> <pre><code>def view_scaled_k_means(nb: Notebook, r: int = 0, check: bool = False):\n\"\"\"\n    Plot to show how `scaled_k_means` was used to compute the bleed matrix.\n    There will be upto 3 columns, each with 2 plots.\n\n    The vector for dye $d$ in the `bleed_matrix` is computed from all the spot round vectors\n    whose dot product to the dye $d$ vector was the highest.\n    The boxplots in the first row show these dot product values for each dye.\n\n    The second row then shows the bleed matrix at each stage of the computation.\n\n    The first column shows the initial bleed matrix. The second column shows the bleed matrix after running\n    `scaled_k_means` once with a score threshold of 0. The third column shows the final `bleed_matrix` after running\n    `scaled_k_means` a second time with `score_thresh` for dye $d$ set to the median of the scores assigned to\n    dye $d$ in the first run. Third column only present if `config['call_spots']['bleed_matrix_anneal']==True`.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        r: Round of bleed matrix to view. Only relevant if `config['call_spots']['bleed_matrix_method'] = 'separate'`.\n        check: If True, will raise error if `bleed_matrix` computed here is different to that saved in notebook\n    \"\"\"\n    # Fit background to spot_colors as is done in call_reference_spots before bleed_matrix calc\n    spot_colors = background_fitting(nb, 'ref')[1]\n\n    # Get bleed matrix and plotting info\n    rcd_ind = np.ix_(nb.basic_info.use_rounds, nb.basic_info.use_channels, nb.basic_info.use_dyes)\n    initial_bleed_matrix = nb.call_spots.initial_bleed_matrix[rcd_ind]\n    config = nb.get_config()['call_spots']\n    if config['bleed_matrix_method'].lower() == 'separate':\n        r_ind = int(np.where(np.asarray(nb.basic_info.use_rounds) == r)[0])\n        title_start = f\"Bleed matrix for round {r} \"\n    else:\n        r_ind = 0\n        title_start = \"Bleed matrix \"\n    debug_info = get_bleed_matrix(spot_colors[nb.ref_spots.isolated], initial_bleed_matrix,\n                                  config['bleed_matrix_method'], config['bleed_matrix_score_thresh'],\n                                  config['bleed_matrix_min_cluster_size'], config['bleed_matrix_n_iter'],\n                                  config['bleed_matrix_anneal'], r_ind)[1]\n    if check:\n        if np.abs(nb.call_spots.bleed_matrix[rcd_ind][r_ind]-debug_info['bleed_matrix'][-1]).max() &gt; 1e-3:\n            raise ValueError(\"Bleed Matrix saved to Notebook is different from that computed here \"\n                             \"with get_bleed_matrix.\\nMake sure that the background and bleed_matrix\"\n                             \"parameters in the config file have not changed.\")\n\n    # Make it so all bleed matrices have same L2 norm as final one\n    bm_norm = np.linalg.norm(debug_info['bleed_matrix'][-1])\n    bleed_matrix = [bm * bm_norm / np.linalg.norm(bm) for bm in debug_info['bleed_matrix']]\n    vmax = np.max(bleed_matrix)\n\n    # Set up plot\n    n_plots, n_use_channels, n_use_dyes = debug_info['bleed_matrix'].shape\n    fig, ax = plt.subplots(2, n_plots, figsize=(11, 7), sharex=True)\n    ax[0, 0].get_shared_y_axes().join(ax[0, 0], ax[0, 1])\n    subplot_adjust = [0.075, 0.92, 0.08, 0.88]\n    fig.subplots_adjust(left=subplot_adjust[0], right=subplot_adjust[1], bottom=subplot_adjust[2],\n                        top=subplot_adjust[3])\n    titles = ['Initial Bleed Matrix', 'Bleed Matrix after Scaled K Means 1', 'Bleed Matrix after Scaled K Means 2']\n    if not config['bleed_matrix_anneal']:\n        titles[1] = 'Bleed Matrix after Scaled K Means'\n    for i in range(n_plots):\n        box_data = [debug_info['cluster_score'][i][debug_info['cluster_ind'][i] == d] for d in range(n_use_dyes)]\n        bp = ax[0, i].boxplot(box_data, notch=0, sym='+', patch_artist=True)\n        for d in range(n_use_dyes):\n            ax[0, i].text(d + 1, np.percentile(box_data[d], 25), \"{:.1e}\".format(len(box_data[d])),\n                          horizontalalignment='center', color=bp['medians'][d].get_color(), size=5, clip_on=True)\n            im = ax[1, i].imshow(bleed_matrix[i], extent=[0.5, n_use_dyes + 0.5, n_use_channels - 0.5, -0.5],\n                                 aspect='auto', vmin=0, vmax=vmax)\n        if nb.basic_info.dye_names is None:\n            ax[1, i].set_xticks(ticks=np.arange(1, n_use_dyes + 1), labels=nb.basic_info.use_dyes)\n        else:\n            subplot_adjust[2] = 0.15\n            fig.subplots_adjust(bottom=subplot_adjust[2])\n            ax[1, i].set_xticks(ticks=np.arange(1, n_use_dyes + 1),\n                                labels=np.asarray(nb.basic_info.dye_names)[nb.basic_info.use_dyes], rotation=45)\n        ax[1, i].set_yticks(ticks=np.arange(n_use_channels), labels=nb.basic_info.use_channels)\n        ax[0, i].set_title(titles[i], fontsize=10)\n        if i == 0:\n            ax[0, i].set_ylabel('Dot Product to Best Dye Vector')\n            ax[1, i].set_ylabel('Channel')\n    fig.supxlabel('Dye', size=12)\n    mid_point = (subplot_adjust[2] + subplot_adjust[3]) / 2\n    cbar_ax = fig.add_axes([subplot_adjust[1] + 0.01, subplot_adjust[2],\n                            0.005, mid_point - subplot_adjust[2] - 0.04])  # left, bottom, width, height\n    fig.colorbar(im, cax=cbar_ax)\n    plt.suptitle(f'{title_start}at {n_plots} different stages\\n Box plots showing the dot product of spot round '\n                 f'vectors with the dye vector they best matched to in the bleed matrix', size=11)\n    ax[1, 1].set_title('Bleed Matrix where each dye column was computed from all vectors assigned to that dye '\n                       'in the boxplot above',\n                       size=11)\n    plt.show()\n</code></pre>"},{"location":"code/plot/extract/","title":"Extract","text":""},{"location":"code/plot/extract/#viewer","title":"Viewer","text":""},{"location":"code/plot/extract/#view_filter","title":"<code>view_filter</code>","text":"<p>Function to view filtering of raw data in napari. There will be 2 scrollbars in 3D. One to change between raw/difference_of_hanning/difference_of_hanning+smoothed and one to change z-plane.</p> <p>There are also sliders to change the parameters for the filtering/smoothing. When the sliders are changed, the time taken for the new filtering/smoothing will be printed to the console.</p> <p>Note</p> <p>When <code>r_smooth</code> is set to <code>[1, 1, 1]</code>, no smoothing will be performed. When this is the case, changing the filtering radius using the slider will be quicker because it will only do filtering and not any smoothing.</p> <p>If <code>r == anchor_round</code> and <code>c == dapi_channel</code>, the filtering will be tophat filtering and no smoothing will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel.</p> <p>The current difference of hanning kernel can be viewed by pressing the 'h' key.</p> <p>Requires access to <code>nb.file_names.input_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Optional[Notebook]</code> <p>Notebook for experiment. If no Notebook exists, pass <code>config_file</code> instead.</p> <code>None</code> <code>t</code> <code>int</code> <p>npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below:</p> <p>| 2  | 1  | 0  |</p> <p>| 5  | 4  | 3  |</p> <p>| 8  | 7  | 6  |</p> <p>| 11 | 10 | 9  |</p> <code>0</code> <code>r</code> <code>int</code> <p>round to view</p> <code>0</code> <code>c</code> <code>int</code> <p>Channel to view.</p> <code>0</code> <code>use_z</code> <code>Optional[Union[int, List[int]]]</code> <p>Which z-planes to load in from raw data. If <code>None</code>, will use load all z-planes (except from first one if <code>config['basic_info']['ignore_first_z_plane'] == True</code>).</p> <code>None</code> <code>config_file</code> <code>Optional[str]</code> <p>path to config file for experiment.</p> <code>None</code> Source code in <code>coppafish/plot/extract/viewer.py</code> <pre><code>def __init__(self, nb: Optional[Notebook] = None, t: int = 0,\n             r: int = 0,\n             c: int = 0,\n             use_z: Optional[Union[int, List[int]]] = None, config_file: Optional[str] = None):\n\"\"\"\n    Function to view filtering of raw data in napari.\n    There will be 2 scrollbars in 3D.\n    One to change between *raw/difference_of_hanning/difference_of_hanning+smoothed* and one to change z-plane.\n\n    There are also sliders to change the parameters for the filtering/smoothing.\n    When the sliders are changed, the time taken for the new filtering/smoothing\n    will be printed to the console.\n\n    !!! note\n        When `r_smooth` is set to `[1, 1, 1]`, no smoothing will be performed.\n        When this is the case, changing the filtering radius using the slider will\n        be quicker because it will only do filtering and not any smoothing.\n\n    If `r == anchor_round` and `c == dapi_channel`, the filtering will be tophat filtering and no smoothing\n    will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel.\n\n    The current difference of hanning kernel can be viewed by pressing the 'h' key.\n\n    !!! warning \"Requires access to `nb.file_names.input_dir`\"\n\n    Args:\n        nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead.\n        t: npy (as opposed to nd2 fov) tile index to view.\n            For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as\n            below:\n\n            | 2  | 1  | 0  |\n\n            | 5  | 4  | 3  |\n\n            | 8  | 7  | 6  |\n\n            | 11 | 10 | 9  |\n        r: round to view\n        c: Channel to view.\n        use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first\n            one if `config['basic_info']['ignore_first_z_plane'] == True`).\n        config_file: path to config file for experiment.\n    \"\"\"\n    if nb is None:\n        nb = Notebook(config_file=config_file)\n    add_basic_info_no_save(nb)  # deal with case where there is no notebook yet\n    if use_z is None:\n        use_z = nb.basic_info.use_z\n    t, r, c, use_z = number_to_list([t, r, c, use_z])\n    self.image_raw = get_raw_images(nb, t, r, c, use_z)[0, 0, 0]\n\n    self.is_3d = nb.basic_info.is_3d\n    if not self.is_3d:\n        self.image_raw = extract.focus_stack(self.image_raw)\n        self.image_plot = np.zeros((3, nb.basic_info.tile_sz, nb.basic_info.tile_sz), dtype=np.int32)\n        self.image_plot[0] = self.image_raw\n    else:\n        self.image_plot = np.zeros((3, len(use_z), nb.basic_info.tile_sz, nb.basic_info.tile_sz), dtype=np.int32)\n        self.image_plot[0] = np.moveaxis(self.image_raw, 2, 0)  # put z axis first for plotting\n    self.raw_max = self.image_raw.max()\n\n    # find faulty columns and update self.image_raw so don't have to run strip_hack each time we filter\n    self.image_raw, self.bad_columns = extract.strip_hack(self.image_raw)\n\n    # Get default filter info\n    # label for each image in image_plot\n    self.ax0_labels = ['Raw', 'Difference of Hanning', 'Difference of Hanning and Smoothed']\n    if not self.is_3d:\n        self.ax0_labels[0] = 'Raw (Focus Stacked)'\n    config = nb.get_config()['extract']\n    if r[0] == nb.basic_info.anchor_round and c[0] == nb.basic_info.dapi_channel:\n        self.dapi = True\n        if config['r_dapi'] is None:\n            if config['r_dapi_auto_microns'] is not None:\n                config['r_dapi'] = extract.get_pixel_length(config['r_dapi_auto_microns'],\n                                                            nb.basic_info.pixel_size_xy)\n            else:\n                config['r_dapi'] = 48  # good starting value\n        self.r_filter = config['r_dapi']\n        self.image_plot = self.image_plot[:2]  # no smoothing if dapi\n        self.ax0_labels = self.ax0_labels[:2]\n        self.update_filter_image()\n        r_filter_lims = [10, 70]\n    else:\n        self.dapi = False\n        self.r_filter = config['r1']\n        r2 = config['r2']\n        if self.r_filter is None:\n            self.r_filter = extract.get_pixel_length(config['r1_auto_microns'], nb.basic_info.pixel_size_xy)\n        if r2 is None:\n            r2 = self.r_filter * 2\n        r_filter_lims = [2, 10]\n        self.update_filter_image(r2)\n        self.r_filter2 = r2\n\n        # Get default smoothing info\n        if config['r_smooth'] is None:\n            # start with no smoothing. Quicker to change filter params as no need to update smoothing too.\n            config['r_smooth'] = [1, 1, 1]\n        if not nb.basic_info.is_3d:\n            config['r_smooth'] = config['r_smooth'][:2]\n        self.r_smooth = config['r_smooth']\n        self.update_smooth_image()\n\n    self.viewer = napari.Viewer()\n    self.viewer.add_image(self.image_plot, name=f\"Tile {t[0]}, Round {r[0]}, Channel{c[0]}\")\n    # Set min image contrast to 0 for better comparison between images\n    self.viewer.layers[0].contrast_limits = [0, 0.9 * self.viewer.layers[0].contrast_limits[1]]\n    self.ax0_ind = 0\n    self.viewer.dims.set_point(0, self.ax0_ind)  # set filter type to raw initially\n    self.viewer.dims.events.current_step.connect(self.filter_type_status)\n\n    self.filter_slider = QSlider(Qt.Orientation.Horizontal)\n    self.filter_slider.setRange(r_filter_lims[0], r_filter_lims[1])\n    self.filter_slider.setValue(self.r_filter)\n    # When dragging, status will show r_filter value\n    self.filter_slider.valueChanged.connect(lambda x: self.show_filter_radius(x))\n    # On release of slider, filtered / smoothed images updated\n    self.filter_slider.sliderReleased.connect(self.filter_slider_func)\n    if self.dapi:\n        filter_slider_name = 'Tophat kernel radius'\n    else:\n        filter_slider_name = 'Difference of Hanning Radius'\n    self.viewer.window.add_dock_widget(self.filter_slider, area=\"left\", name=filter_slider_name)\n\n    if not self.dapi:\n        self.smooth_yx_slider = QSlider(Qt.Orientation.Horizontal)\n        self.smooth_yx_slider.setRange(1, 5)  # gets very slow with large values\n        self.smooth_yx_slider.setValue(self.r_smooth[0])\n        # When dragging, status will show r_smooth value\n        self.smooth_yx_slider.valueChanged.connect(lambda x: self.show_smooth_radius_yx(x))\n        # On release of slider, smoothed image updated\n        self.smooth_yx_slider.sliderReleased.connect(self.smooth_slider_func)\n        smooth_title = \"Smooth Radius\"\n        if self.is_3d:\n            smooth_title = smooth_title + \" YX\"\n        self.viewer.window.add_dock_widget(self.smooth_yx_slider, area=\"left\", name=smooth_title)\n\n    if self.is_3d and not self.dapi:\n        self.smooth_z_slider = QSlider(Qt.Orientation.Horizontal)\n        self.smooth_z_slider.setRange(1, 5)  # gets very slow with large values\n        self.smooth_z_slider.setValue(self.r_smooth[2])\n        # When dragging, status will show r_smooth value\n        self.smooth_z_slider.valueChanged.connect(lambda x: self.show_smooth_radius_z(x))\n        # On release of slider, smoothed image updated\n        self.smooth_z_slider.sliderReleased.connect(self.smooth_slider_func)\n        self.viewer.window.add_dock_widget(self.smooth_z_slider, area=\"left\", name=\"Smooth Radius Z\")\n\n    if self.is_3d:\n        self.viewer.dims.axis_labels = ['Filter Method', 'z', 'y', 'x']\n    else:\n        self.viewer.dims.axis_labels = ['Filter Method', 'y', 'x']\n\n    self.key_call_functions()\n    napari.run()\n</code></pre>"},{"location":"code/plot/extract/#diagnostics","title":"Diagnostics","text":""},{"location":"code/plot/extract/#thresh_box_plots","title":"<code>thresh_box_plots</code>","text":"<p>Function to plot distribution of auto_threshold values amongst tiles for each round and channel.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing the extract NotebookPage.</p> required Source code in <code>coppafish/plot/extract/diagnostics.py</code> <pre><code>def thresh_box_plots(nb: Notebook):\n\"\"\"\n    Function to plot distribution of auto_threshold values amongst tiles for each round and channel.\n\n    Args:\n        nb: Notebook containing the extract NotebookPage.\n    \"\"\"\n    box_data = [nb.extract.auto_thresh[:, r, c] for c in nb.basic_info.use_channels for r in nb.basic_info.use_rounds]\n    if nb.basic_info.use_anchor:\n        box_data = box_data + [nb.extract.auto_thresh[:, nb.basic_info.anchor_round, nb.basic_info.anchor_channel]]\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    fig.subplots_adjust(left=0.075, right=0.95, bottom=0.15)\n    n_use_channels = len(nb.basic_info.use_channels)\n    # different boxplot color for each channel (+ different color for anchor channel)\n    # Must be distinct from black and white\n    channel_colors = distinctipy.get_colors(n_use_channels + int(nb.basic_info.use_anchor), [(0, 0, 0), (1, 1, 1)])\n    bp = ax1.boxplot(box_data, notch=0, sym='+', patch_artist=True)\n\n    c = -1\n    tick_labels = np.tile(nb.basic_info.use_rounds, n_use_channels).tolist()\n    leg_labels = nb.basic_info.use_channels\n    if nb.basic_info.use_anchor:\n        tick_labels = tick_labels + ['Anchor']\n        leg_labels = leg_labels + [f'Anchor ({nb.basic_info.anchor_channel})']\n    ax1.set_xticklabels(tick_labels)\n    if nb.basic_info.use_anchor:\n        ticks = ax1.get_xticklabels()\n        ticks[-1].set_rotation(90)\n\n    leg_markers = []\n    for i in range(len(box_data)):\n        if i % n_use_channels == 0:\n            c += 1\n            leg_markers = leg_markers + [bp['boxes'][i]]\n        bp['boxes'][i].set_facecolor(channel_colors[c])\n    ax1.legend(leg_markers, leg_labels, title='Channel')\n    ax1.set_xlabel('Round')\n    ax1.set_ylabel('Auto Threshold')\n    ax1.set_title('Boxplots showing distribution of Auto Threshold amongst tiles for each round and channel')\n    plt.show()\n</code></pre>"},{"location":"code/plot/extract/#histogram_plots","title":"<code>histogram_plots</code>","text":"<p>Plots histograms showing distribution of intensity values combined from all tiles for each round and channel. There is also a Norm button which equalises color channels so all color channels should have most intensity values between -1 and 1.</p> <p>In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels and a long tail from around 0.1 to just beyond 1 accounting for spot pixels.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing the extract NotebookPage.</p> required Source code in <code>coppafish/plot/extract/diagnostics.py</code> <pre><code>def __init__(self, nb: Notebook):\n\"\"\"\n    Plots histograms showing distribution of intensity values combined from all tiles for each round and channel.\n    There is also a Norm button which equalises color channels so all color channels should have most intensity\n    values between -1 and 1.\n\n    In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels\n    and a long tail from around 0.1 to just beyond 1 accounting for spot pixels.\n\n    Args:\n        nb: Notebook containing the extract NotebookPage.\n    \"\"\"\n    self.use_rounds = nb.basic_info.use_rounds\n    self.use_channels = nb.basic_info.use_channels\n    self.hist_values = nb.extract.hist_values\n    self.hist_values_norm = np.arange(-10, 10.004, 0.005)\n    n_use_rounds = len(self.use_rounds)\n    n_use_channels = len(self.use_channels)\n    self.fig, self.ax1 = plt.subplots(n_use_channels, n_use_rounds, figsize=(10, 6), sharey=True, sharex=True)\n    self.ax1 = self.ax1.flatten()\n\n    # Compute color_norm_factor as it will be computed at call_spots step of pipeline\n    config = nb.get_config()['call_spots']\n    rc_ind = np.ix_(self.use_rounds, self.use_channels)\n    hist_counts_use = np.moveaxis(np.moveaxis(nb.extract.hist_counts, 0, -1)[rc_ind], -1, 0)\n    self.color_norm_factor = color_normalisation(self.hist_values, hist_counts_use,\n                                                 config['color_norm_intensities'],\n                                                 config['color_norm_probs'], config['bleed_matrix_method'])\n    self.color_norm_factor = self.color_norm_factor.T.flatten()\n\n    i = 0\n    min_value = 3  # clip hist counts to this so don't get log(0) error\n    self.plot_lines = []\n    self.hist_counts_norm = []\n    self.hist_counts = []\n    for c in self.use_channels:\n        for r in self.use_rounds:\n            # Clip histogram to stop log(0) error.\n            self.hist_counts = self.hist_counts + [np.clip(nb.extract.hist_counts[:, r, c], min_value, np.inf)]\n            self.hist_counts_norm = self.hist_counts_norm + \\\n                                    [resample_histogram(self.hist_values / self.color_norm_factor[i],\n                                                        self.hist_counts[i], self.hist_values_norm)]\n\n            # Normalise histograms to give probabilities\n            self.hist_counts[i] = self.hist_counts[i] / np.sum(nb.extract.hist_counts[:, r, c])\n            self.hist_counts_norm[i] = self.hist_counts_norm[i] / np.sum(nb.extract.hist_counts[:, r, c])\n            self.plot_lines = self.plot_lines + self.ax1[i].plot(self.hist_values, self.hist_counts[i])\n            if r == nb.basic_info.use_rounds[0]:\n                self.ax1[i].set_ylabel(c)\n            if c == nb.basic_info.use_channels[-1]:\n                self.ax1[i].set_xlabel(r)\n            i += 1\n    self.ax1[0].set_yscale('log')\n    self.fig.supylabel('Channel')\n    self.fig.supxlabel('Round')\n    plt.suptitle('Histograms showing distribution of intensity values combined from '\n                 'all tiles for each round and channel')\n\n    self.norm = False\n    self.xlims_norm = [-1, 1]\n    self.xlims = [-300, 300]\n    self.ax1[0].set_xlim(self.xlims[0], self.xlims[1])\n    self.norm_button_ax = self.fig.add_axes([0.85, 0.02, 0.1, 0.05])\n    self.norm_button = Button(self.norm_button_ax, 'Norm', hovercolor='0.275')\n    self.norm_button.on_clicked(self.change_norm)\n\n    plt.show()\n</code></pre>"},{"location":"code/plot/find_spots/","title":"Find Spots","text":""},{"location":"code/plot/find_spots/#view_find_spots","title":"<code>view_find_spots</code>","text":"<p>This viewer shows how spots are detected in an image. There are sliders to vary the parameters used for spot detection so the effect of them can be seen.</p> <p>Can also view points from <code>\u00b1z_thick</code> z-planes on current z-plane using the z thickness slider. Initially, z thickness will be 1.</p> <p>Requires access to <code>nb.file_names.input_dir</code> or <code>nb.file_names.tile_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Optional[Notebook]</code> <p>Notebook for experiment. If no Notebook exists, pass <code>config_file</code> instead. In this case, the raw images will be loaded and then filtered according to parameters in <code>config['extract']</code>.</p> <code>None</code> <code>t</code> <code>int</code> <p>npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below:</p> <p>| 2  | 1  | 0  |</p> <p>| 5  | 4  | 3  |</p> <p>| 8  | 7  | 6  |</p> <p>| 11 | 10 | 9  |</p> <code>0</code> <code>r</code> <code>int</code> <p>round to view</p> <code>0</code> <code>c</code> <code>int</code> <p>Channel to view.</p> <code>0</code> <code>show_isolated</code> <code>bool</code> <p>Spots which are identified as isolated in the anchor round/channel are used to compute the <code>bleed_matrix</code>. Can see which spots are isolated by setting this to <code>True</code>. Note, this is very slow in 3D, around 300s for a 2048 x 2048 x 50 image.</p> <code>False</code> <code>config_file</code> <code>Optional[str]</code> <p>path to config file for experiment.</p> <code>None</code> Source code in <code>coppafish/plot/find_spots/viewer.py</code> <pre><code>def __init__(self, nb: Optional[Notebook] = None, t: int = 0, r: int = 0, c: int = 0,\n             show_isolated: bool = False, config_file: Optional[str] = None):\n\"\"\"\n    This viewer shows how spots are detected in an image.\n    There are sliders to vary the parameters used for spot detection so the effect of them can be seen.\n\n    Can also view points from `\u00b1z_thick` z-planes on current z-plane using the z thickness slider. Initially,\n    z thickness will be 1.\n\n    !!! warning \"Requires access to `nb.file_names.input_dir` or `nb.file_names.tile_dir`\"\n\n    Args:\n        nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. In this case,\n            the raw images will be loaded and then filtered according to parameters in `config['extract']`.\n        t: npy (as opposed to nd2 fov) tile index to view.\n            For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as\n            below:\n\n            | 2  | 1  | 0  |\n\n            | 5  | 4  | 3  |\n\n            | 8  | 7  | 6  |\n\n            | 11 | 10 | 9  |\n        r: round to view\n        c: Channel to view.\n        show_isolated: Spots which are identified as *isolated* in the anchor round/channel are used to compute\n            the `bleed_matrix`. Can see which spots are isolated by setting this to `True`.\n            Note, this is very slow in *3D*, around 300s for a 2048 x 2048 x 50 image.\n        config_file: path to config file for experiment.\n    \"\"\"\n    if nb is None:\n        nb = Notebook(config_file=config_file)\n    add_basic_info_no_save(nb)  # deal with case where there is no notebook yet\n\n    if r == nb.basic_info.anchor_round:\n        if c != nb.basic_info.anchor_channel:\n            raise ValueError(f'No spots are found on round {r}, channel {c} in the pipeline.\\n'\n                             f'Only spots on anchor_channel = {nb.basic_info.anchor_channel} used for the '\n                             f'anchor round.')\n    if r == nb.basic_info.ref_round and c == nb.basic_info.ref_channel:\n        if show_isolated:\n            self.show_isolated = True\n        else:\n            self.show_isolated = False\n    else:\n        if show_isolated:\n            warnings.warn(f'Not showing isolated spots as slow and isolated status not used for round {r},'\n                          f' channel {c}')\n        self.show_isolated = False\n\n    self.is_3d = nb.basic_info.is_3d\n    if self.is_3d:\n        tile_file = nb.file_names.tile[t][r][c]\n    else:\n        tile_file = nb.file_names.tile[t][r]\n    if not os.path.isfile(tile_file):\n        warnings.warn(f\"The file {tile_file}\\ndoes not exist so loading raw image and filtering it\")\n        self.image = get_filtered_image(nb, t, r, c)\n    else:\n        self.image = utils.npy.load_tile(nb.file_names, nb.basic_info, t, r, c)\n        scale = 1  # Can be any value as not actually used but needed as argument in get_extract_info\n\n    # Get auto_threshold value used to detect spots\n    if nb.has_page('extract'):\n        self.auto_thresh = nb.extract.auto_thresh[t, r, c]\n    else:\n        config = nb.get_config()['extract']\n        z_info = int(np.floor(nb.basic_info.nz / 2))\n        hist_values = np.arange(-nb.basic_info.tile_pixel_value_shift,\n                                np.iinfo(np.uint16).max - nb.basic_info.tile_pixel_value_shift + 2, 1)\n        hist_bin_edges = np.concatenate((hist_values - 0.5, hist_values[-1:] + 0.5))\n        max_npy_pixel_value = np.iinfo(np.uint16).max - nb.basic_info.tile_pixel_value_shift\n        self.auto_thresh = extract.get_extract_info(self.image, config['auto_thresh_multiplier'], hist_bin_edges,\n                                                    max_npy_pixel_value, scale, z_info)[0]\n\n    config = nb.get_config()['find_spots']\n    self.r_xy = config['radius_xy']\n    if self.is_3d:\n        self.r_z = config['radius_z']\n    else:\n        self.r_z = None\n    if config['isolation_thresh'] is None:\n        config['isolation_thresh'] = self.auto_thresh * config['auto_isolation_thresh_multiplier']\n    self.isolation_thresh = config['isolation_thresh']\n    self.r_isolation_inner = config['isolation_radius_inner']\n    self.r_isolation_xy = config['isolation_radius_xy']\n    self.normal_color = np.array([1, 0, 0, 1]) # red\n    self.isolation_color = np.array([0, 1, 0, 1])  # green\n    self.neg_neighb_color = np.array([0, 0, 1, 1])  # blue\n    self.point_size = 9\n    self.z_thick = 1  # show +/- 1 plane initially\n    self.z_thick_list = np.arange(1, 1 + 15 * 2, 2)  # only odd z-thick make any difference\n    if self.is_3d:\n        self.r_isolation_z = config['isolation_radius_z']\n    else:\n        self.r_isolation_z = None\n\n    self.small = 1e-6  # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image).\n    # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly.\n    # hence when find maxima, will only get one of the pixels not both.\n    rng = np.random.default_rng(0)   # So shift is always the same.\n    # rand_shift must be larger than small to detect a single spot.\n    rand_im_shift = rng.uniform(low=self.small*2, high=0.2, size=self.image.shape)\n    self.image = self.image + rand_im_shift\n\n    self.dilate = None\n    self.spot_zyx = None\n    self.image_isolated = None\n    self.no_negative_neighbour = None\n    self.update_dilate()\n    if self.show_isolated:\n        self.update_isolated_image()\n    self.viewer = napari.Viewer()\n    name = f\"Tile {t}, Round {r}, Channel {c}\"\n    if self.is_3d:\n        self.viewer.add_image(np.moveaxis(np.rint(self.image).astype(np.int32), 2, 0), name=name)\n    else:\n        self.viewer.add_image(np.rint(self.image).astype(np.int32), name=name)\n\n    self.thresh_slider = QSlider(Qt.Orientation.Horizontal)\n    self.thresh_slider.setRange(0, 3 * self.auto_thresh)\n    self.thresh_slider.setValue(self.auto_thresh)\n    # When dragging, status will show auto_thresh value\n    self.thresh_slider.valueChanged.connect(lambda x: self.show_thresh(x))\n    # On release of slider, filtered / smoothed images updated\n    self.thresh_slider.sliderReleased.connect(self.update_spots)\n    self.viewer.window.add_dock_widget(self.thresh_slider, area=\"left\", name='Intensity Threshold')\n    if self.show_isolated:\n        self.isolation_thresh_slider = QSlider(Qt.Orientation.Horizontal)\n        self.isolation_thresh_slider.setRange(-2 * np.abs(self.isolation_thresh), 0)\n        self.isolation_thresh_slider.setValue(self.isolation_thresh)\n        # When dragging, status will show auto_thresh value\n        self.isolation_thresh_slider.valueChanged.connect(lambda x: self.show_isolation_thresh(x))\n        # On release of slider, filtered / smoothed images updated\n        self.isolation_thresh_slider.sliderReleased.connect(self.update_isolated_spots)\n    self.update_spots()\n\n    self.r_xy_slider = QSlider(Qt.Orientation.Horizontal)\n    self.r_xy_slider.setRange(2, 10)\n    self.r_xy_slider.setValue(self.r_xy)\n    # When dragging, status will show r_xy value\n    self.r_xy_slider.valueChanged.connect(lambda x: self.show_radius_xy(x))\n    # On release of slider, filtered / smoothed images updated\n    self.r_xy_slider.sliderReleased.connect(self.radius_slider_func)\n    self.viewer.window.add_dock_widget(self.r_xy_slider, area=\"left\", name='Detection Radius YX')\n\n    if self.is_3d:\n        self.r_z_slider = QSlider(Qt.Orientation.Horizontal)\n        self.r_z_slider.setRange(2, 12)\n        self.r_z_slider.setValue(self.r_xy)\n        # When dragging, status will show r_z value\n        self.r_z_slider.valueChanged.connect(lambda x: self.show_radius_z(x))\n        # On release of slider, filtered / smoothed images updated\n        self.r_z_slider.sliderReleased.connect(self.radius_slider_func)\n        self.viewer.window.add_dock_widget(self.r_z_slider, area=\"left\", name='Detection Radius Z')\n\n        self.z_thick_slider = QSlider(Qt.Orientation.Horizontal)\n        self.z_thick_slider.setRange(0, int((self.z_thick_list[-1] - 1) / 2))\n        self.z_thick_slider.setValue(self.z_thick)\n        # When dragging, status will show r_z value\n        self.z_thick_slider.valueChanged.connect(lambda x: self.change_z_thick(x))\n        self.viewer.window.add_dock_widget(self.z_thick_slider, area=\"left\", name='Z Thickness')\n\n    if self.show_isolated:\n        self.viewer.window.add_dock_widget(self.isolation_thresh_slider, area=\"left\", name='Isolation Threshold')\n    # set image as selected layer so can see intensity values in status\n    self.viewer.layers.selection.active = self.viewer.layers[0]\n    napari.run()\n</code></pre>"},{"location":"code/plot/find_spots/#coppafish.plot.find_spots.n_spots.n_spots_grid","title":"<code>n_spots_grid(nb, n_spots_thresh=None)</code>","text":"<p>Plots a grid indicating the number of spots detected on each tile, round and channel.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing <code>find_spots</code> page.</p> required <code>n_spots_thresh</code> <code>Optional[int]</code> <p>tiles/rounds/channels with fewer spots than this will be highlighted. If <code>None</code>, will use <code>n_spots_warn_fraction</code> from config file.</p> <code>None</code> Source code in <code>coppafish/plot/find_spots/n_spots.py</code> <pre><code>def n_spots_grid(nb: Notebook, n_spots_thresh: Optional[int] = None):\n\"\"\"\n    Plots a grid indicating the number of spots detected on each tile, round and channel.\n\n    Args:\n        nb: *Notebook* containing `find_spots` page.\n        n_spots_thresh: tiles/rounds/channels with fewer spots than this will be highlighted.\n            If `None`, will use `n_spots_warn_fraction` from config file.\n    \"\"\"\n    if n_spots_thresh is None:\n        config = nb.get_config()['find_spots']\n        if nb.basic_info.is_3d:\n            n_spots_thresh = config['n_spots_warn_fraction'] * config['max_spots_3d'] * nb.basic_info.nz\n        else:\n            n_spots_thresh = config['n_spots_warn_fraction'] * config['max_spots_2d']\n        n_spots_thresh = int(np.ceil(n_spots_thresh))\n    use_tiles = np.asarray(nb.basic_info.use_tiles)\n    use_rounds = np.asarray(nb.basic_info.use_rounds)  # don't consider anchor in this analysis\n    if len(use_rounds) &gt; 0:\n        use_channels = np.asarray(nb.basic_info.use_channels)\n        spot_no = nb.find_spots.spot_no[np.ix_(use_tiles, use_rounds, use_channels)]\n        spot_no = np.moveaxis(spot_no, 1, 2)  # put rounds last\n        spot_no = spot_no.reshape(len(use_tiles), -1).T  # spot_no[n_rounds, t] is now spot_no[t, r=0, c=1]\n        n_round_channels = len(use_rounds) * len(use_channels)\n        y_labels = np.tile(use_rounds, len(use_channels))\n        vmax = spot_no.max()  # clip colorbar at max of imaging rounds/channels because anchor can be a lot higher\n    else:\n        # Deal with case where only anchor round\n        use_channels = np.asarray([])\n        spot_no = np.zeros((0, len(use_tiles)), dtype=np.int32)\n        n_round_channels = 0\n        y_labels = np.zeros(0, dtype=int)\n        vmax = None\n    if nb.basic_info.use_anchor:\n        anchor_spot_no = nb.find_spots.spot_no[use_tiles, nb.basic_info.anchor_round,\n                                               nb.basic_info.anchor_channel][np.newaxis]\n        spot_no = np.append(spot_no, anchor_spot_no, axis=0)\n        y_labels = y_labels.astype(str).tolist()\n        y_labels += ['Anchor']\n        n_round_channels += 1\n        if vmax is None:\n            vmax = spot_no.max()\n    fig, ax = plt.subplots(1, 1, figsize=(np.clip(5+len(use_tiles)/2, 3, 18), 12))\n    subplot_adjust = [0.12, 1, 0.07, 0.9]\n    fig.subplots_adjust(left=subplot_adjust[0], right=subplot_adjust[1],\n                        bottom=subplot_adjust[2], top=subplot_adjust[3])\n    im = ax.imshow(spot_no, aspect='auto', vmax=vmax)\n    fig.colorbar(im, ax=ax)\n    ax.set_yticks(np.arange(n_round_channels))\n    ax.set_yticklabels(y_labels)\n    ax.set_xticks(np.arange(len(use_tiles)))\n    ax.set_xticklabels(use_tiles)\n    ax.set_xlabel('Tile')\n\n    for c in range(len(use_channels)):\n        y_ind = 1 - c * 1/(len(use_channels))\n        ax.text(-0.1, y_ind, f\"Channel {use_channels[c]}\", va=\"top\", ha=\"left\", transform=ax.transAxes,\n                rotation='vertical')\n    fig.supylabel('Channel/Round', transform=ax.transAxes, x=-0.15)\n    plt.xticks(rotation=90)\n    low_spots = np.where(spot_no&lt;n_spots_thresh)\n    for j in range(len(low_spots[0])):\n        rectangle = plt.Rectangle((low_spots[1][j] - 0.5, low_spots[0][j] - 0.5), 1, 1,\n                                  fill=False, ec=\"r\", linestyle=':', lw=2)\n        ax.add_patch(rectangle)\n    plt.suptitle(f\"Number of Spots Found on each Tile, Round and Channel\")\n    plt.show()\n</code></pre>"},{"location":"code/plot/omp/","title":"OMP","text":""},{"location":"code/plot/omp/#coefficients","title":"Coefficients","text":""},{"location":"code/plot/omp/#view_omp","title":"<code>view_omp</code>","text":"<p>Diagnostic to show omp coefficients of all genes in neighbourhood of spot. Only genes for which a significant number of pixels are non-zero will be plotted.</p> <p>Requires access to <code>nb.file_names.tile_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>im_size</code> <code>int</code> <p>Radius of image to be plotted for each gene.</p> <code>8</code> Source code in <code>coppafish/plot/omp/coefs.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp', im_size: int = 8):\n\"\"\"\n    Diagnostic to show omp coefficients of all genes in neighbourhood of spot.\n    Only genes for which a significant number of pixels are non-zero will be plotted.\n\n    !!! warning \"Requires access to `nb.file_names.tile_dir`\"\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        im_size: Radius of image to be plotted for each gene.\n    \"\"\"\n    coef_images, min_global_yxz, max_global_yxz = get_coef_images(nb, spot_no, method, [im_size, im_size, 0])\n\n    if method.lower() == 'omp':\n        page_name = 'omp'\n        config = nb.get_config()['thresholds']\n        spot_score = omp_spot_score(nb.omp, config['score_omp_multiplier'], spot_no)\n    else:\n        page_name = 'ref_spots'\n        spot_score = nb.ref_spots.score[spot_no]\n    gene_no = nb.__getattribute__(page_name).gene_no[spot_no]\n    t = nb.__getattribute__(page_name).tile[spot_no]\n    spot_yxz = nb.__getattribute__(page_name).local_yxz[spot_no]\n    gene_name = nb.call_spots.gene_names[gene_no]\n    all_gene_names = list(nb.call_spots.gene_names) + [f'BG{i}' for i in range(nb.basic_info.n_channels)]\n    spot_yxz_global = spot_yxz + nb.stitch.tile_origin[t]\n    n_genes = nb.call_spots.bled_codes_ge.shape[0]\n\n    n_nonzero_pixels_thresh = np.min([im_size, 5])  # If 5 pixels non-zero, plot that gene\n    plot_genes = np.where(np.sum(coef_images != 0, axis=(1, 2, 3)) &gt; n_nonzero_pixels_thresh)[0]\n    coef_images = coef_images[plot_genes, :, :, 0]\n    n_plot = len(plot_genes)\n    # at most n_max_rows rows\n    if n_plot &lt;= 16:\n        n_max_rows = 4\n    else:\n        n_max_rows = int(np.ceil(np.sqrt(n_plot)))\n    n_cols = int(np.ceil(n_plot / n_max_rows))\n    subplot_row_columns = [int(np.ceil(n_plot / n_cols)), n_cols]\n    fig_size = np.clip([n_cols+5, subplot_row_columns[0]+4], 3, 12)\n    subplot_adjust = [0.05, 0.775, 0.05, 0.91]\n    super().__init__(coef_images, None, subplot_row_columns, subplot_adjust=subplot_adjust, fig_size=fig_size,\n                     cbar_pos=[0.9, 0.05, 0.03, 0.86], slider_pos=[0.85, 0.05, 0.01, 0.86])\n    # set x, y coordinates to be those of the global coordinate system\n    plot_extent = [min_global_yxz[1]-0.5, max_global_yxz[1]+0.5,\n                   min_global_yxz[0]-0.5, max_global_yxz[0]+0.5]\n    for i in range(self.n_images):\n        # Add cross-hair\n        self.ax[i].axes.plot([spot_yxz_global[1], spot_yxz_global[1]], [plot_extent[2], plot_extent[3]],\n                             'k', linestyle=\":\", lw=1)\n        self.ax[i].axes.plot([plot_extent[0], plot_extent[1]], [spot_yxz_global[0], spot_yxz_global[0]],\n                             'k', linestyle=\":\", lw=1)\n        self.im[i].set_extent(plot_extent)\n        self.ax[i].tick_params(labelbottom=False, labelleft=False)\n        # Add title\n        title_text = f'{plot_genes[i]}: {all_gene_names[plot_genes[i]]}'\n        if plot_genes[i] &gt;= n_genes:\n            text_color = (0.7, 0.7, 0.7)  # If background, make grey\n            title_text = all_gene_names[plot_genes[i]]\n        elif plot_genes[i] == gene_no:\n            text_color = 'g'\n        else:\n            text_color = 'w'  # TODO: maybe make color same as used in plot for each gene\n        self.ax[i].set_title(title_text, color=text_color)\n    plt.subplots_adjust(hspace=0.32)\n    plt.suptitle(f'OMP gene coefficients for spot {spot_no} (match'\n                 f' {str(np.around(spot_score, 2))} to {gene_name})',\n                 x=(subplot_adjust[0] + subplot_adjust[1]) / 2, size=13)\n    self.change_norm()\n    plt.show()\n</code></pre>"},{"location":"code/plot/omp/#view_omp_fit","title":"<code>view_omp_fit</code>","text":"<p>Diagnostic to run omp on a single pixel and see which genes fitted at which iteration. Right-clicking on a particular bled code will cause coppafish.plot.call_spots.dot_product.view_score to run, indicating how the dot product calculation for that iteration was performed.</p> <p>Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run, indicating how the dot product calculation for performed.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>dp_thresh</code> <code>Optional[float]</code> <p>If None, will use value in omp section of config file.</p> <code>None</code> <code>max_genes</code> <code>Optional[int]</code> <p>If None, will use value in omp section of config file.</p> <code>None</code> Source code in <code>coppafish/plot/omp/coefs.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp', dp_thresh: Optional[float] = None,\n             max_genes: Optional[int] = None):\n\"\"\"\n    Diagnostic to run omp on a single pixel and see which genes fitted at which iteration.\n    Right-clicking on a particular bled code will cause *coppafish.plot.call_spots.dot_product.view_score*\n    to run, indicating how the dot product calculation for that iteration was performed.\n\n    Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run,\n    indicating how the dot product calculation for performed.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        dp_thresh: If None, will use value in omp section of config file.\n        max_genes: If None, will use value in omp section of config file.\n    \"\"\"\n    track_info, bled_codes, dp_thresh = get_track_info(nb, spot_no, method, dp_thresh, max_genes)\n    # Add info so can call view_dot_product\n    self.nb = nb\n    self.track_info = track_info\n    self.bled_codes = bled_codes\n    self.dp_thresh = dp_thresh\n    self.spot_no = spot_no\n    self.fitting_method = method\n    n_genes, n_use_rounds, n_use_channels = bled_codes.shape\n\n    n_residual_images = track_info['residual'].shape[0]\n    residual_images = [track_info['residual'][i].transpose() for i in range(n_residual_images)]\n    background_image = np.zeros((n_use_rounds, n_use_channels))\n    for c in range(n_use_channels):\n        background_image += track_info['background_codes'][c] * track_info['background_coefs'][c]\n    background_image = background_image.transpose()\n\n    # allow for possibly adding background vector\n    # TODO: Think may get index error if best gene ever was background_vector.\n    bled_codes = np.append(bled_codes, track_info['background_codes'], axis=0)\n    all_gene_names = list(nb.call_spots.gene_names) + [f'BG{i}' for i in nb.basic_info.use_channels]\n    gene_images = [bled_codes[track_info['gene_added'][i]].transpose() *\n                   track_info['coef'][i][track_info['gene_added'][i]] for i in range(2, n_residual_images)]\n    all_images = residual_images + [background_image] + gene_images\n\n    # Plot all images\n    subplot_adjust = [0.06, 0.82, 0.075, 0.9]\n    super().__init__(all_images, None, [2, n_residual_images], subplot_adjust=subplot_adjust, fig_size=(15, 7))\n\n    # label axis\n    self.ax[0].set_yticks(ticks=np.arange(self.im_data[0].shape[0]), labels=nb.basic_info.use_channels)\n    self.ax[0].set_xticks(ticks=np.arange(self.im_data[0].shape[1]), labels=nb.basic_info.use_rounds)\n    self.fig.supxlabel('Round', size=12, x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n    self.fig.supylabel('Color Channel', size=12)\n    plt.suptitle(f'Residual at each iteration of OMP for Spot {spot_no}. '+r'$\\Delta_{thresh}$ = '+f'{dp_thresh}',\n                 x=(subplot_adjust[0] + subplot_adjust[1]) / 2)\n\n    # Add titles for each subplot\n    titles = ['Initial', 'Post Background']\n    for g in track_info['gene_added'][2:]:\n        titles = titles + ['Post ' + all_gene_names[g]]\n    for i in range(n_residual_images):\n        titles[i] = titles[i] + '\\nRes = {:.2f}'.format(np.linalg.norm(residual_images[i]))\n    titles = titles + ['Background']\n    for i in range(2, n_residual_images):\n        g = track_info['gene_added'][i]\n        titles = titles + [f'{g}: {all_gene_names[g]}']\n        titles[-1] = titles[-1] + '\\n'+r'$\\Delta_{s'+f'{i-2}'+'}$ = '+'{:.2f}'.format(track_info['dot_product'][i])\n\n    # Make title red if dot product fell below dp_thresh or if best gene background\n    is_fail_thresh = False\n    for i in range(self.n_images):\n        if np.isin(i, np.arange(2, n_residual_images)):\n            # failed if bad dot product, gene added is background or gene added has previously been added\n            is_fail_thresh = np.abs(track_info['dot_product'][i]) &lt; dp_thresh or \\\n                             track_info['gene_added'][i] &gt;= n_genes or \\\n                             np.isin(track_info['gene_added'][i], track_info['gene_added'][2:i])\n            if is_fail_thresh:\n                text_color = 'r'\n            else:\n                text_color = 'w'\n        elif i == self.n_images - 1 and is_fail_thresh:\n            text_color = 'r'\n        else:\n            text_color = 'w'\n        self.ax[i].set_title(titles[i], size=8, color=text_color)\n\n    # Add rectangles where added gene is intense\n    for i in range(len(gene_images)):\n        gene_coef = track_info['coef'][i+2][track_info['gene_added'][i+2]]\n        intense_gene_cr = np.where(np.abs(gene_images[i] / gene_coef) &gt; self.intense_gene_thresh)\n        for j in range(len(intense_gene_cr[0])):\n            for k in [i+1, i+1+n_residual_images]:\n                # can't add rectangle to multiple axes hence second for loop\n                rectangle = plt.Rectangle((intense_gene_cr[1][j]-0.5, intense_gene_cr[0][j]-0.5), 1, 1,\n                                          fill=False, ec=\"g\", linestyle=':', lw=2)\n                self.ax[k].add_patch(rectangle)\n\n    self.change_norm()\n    self.fig.canvas.mpl_connect('button_press_event', self.show_calc)\n    self.track_info = track_info\n    plt.show()\n</code></pre>"},{"location":"code/plot/omp/#get_coef_images","title":"<code>get_coef_images</code>","text":"<p>Gets image of \\(yxz\\) dimension <code>(2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)</code> of the coefficients fitted by omp for each gene.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to get gene coefficient images for.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> required <code>im_size</code> <code>List[int]</code> <p>\\(yxz\\) radius of image to get for each gene.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>coef_images</code> - <code>float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)]</code>. Image for each gene, axis order is \\(gyxz\\). <code>coef_images[g, 0, 0, 0]</code> refers to coefficient of gene g at <code>global_yxz = min_global_yxz</code>. <code>coef_images[g, -1, -1, -1]</code> refers to coefficient of gene g at <code>global_yxz = max_global_yxz</code>.</p> <code>List[float]</code> <p><code>min_global_yxz</code> - <code>float [3]</code>. Min \\(yxz\\) coordinates of image in global coordinates.</p> <code>List[float]</code> <p><code>max_global_yxz</code> - <code>float [3]</code>. Max \\(yxz\\) coordinates of image in global coordinates.</p> Source code in <code>coppafish/plot/omp/coefs.py</code> <pre><code>def get_coef_images(nb: Notebook, spot_no: int, method: str, im_size: List[int]) -&gt; Tuple[np.ndarray, List[float],\n                                                                                          List[float]]:\n\"\"\"\n    Gets image of $yxz$ dimension `(2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)` of the coefficients\n    fitted by omp for each gene.\n\n    Args:\n        nb: *Notebook* containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to get gene coefficient images for.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        im_size: $yxz$ radius of image to get for each gene.\n\n    Returns:\n        `coef_images` - `float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)]`.\n            Image for each gene, axis order is $gyxz$.\n            `coef_images[g, 0, 0, 0]` refers to coefficient of gene g at `global_yxz = min_global_yxz`.\n            `coef_images[g, -1, -1, -1]` refers to coefficient of gene g at `global_yxz = max_global_yxz`.\n        `min_global_yxz` - `float [3]`. Min $yxz$ coordinates of image in global coordinates.\n        `max_global_yxz` - `float [3]`. Max $yxz$ coordinates of image in global coordinates.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)]\n\n    if method.lower() == 'omp':\n        page_name = 'omp'\n    else:\n        page_name = 'ref_spots'\n    t = nb.__getattribute__(page_name).tile[spot_no]\n    spot_yxz = nb.__getattribute__(page_name).local_yxz[spot_no]\n\n    # Subtlety here, may have y-axis flipped, but I think it is correct:\n    # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct.\n    # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5])\n    # will show the value 1 at max_y, min_x+1.\n    im_yxz = np.array(np.meshgrid(np.arange(spot_yxz[0] - im_size[0], spot_yxz[0] + im_size[0] + 1)[::-1],\n                                  np.arange(spot_yxz[1] - im_size[1], spot_yxz[1] + im_size[1] + 1),\n                                  spot_yxz[2]),\n                      dtype=np.int16).T.reshape(-1, 3)\n    z = np.arange(-im_size[2], im_size[2]+1)\n    im_yxz = np.vstack([im_yxz + [0, 0, val] for val in z])\n    im_diameter_yx = [2 * im_size[0] + 1, 2 * im_size[1] + 1]\n    spot_colors = get_spot_colors(im_yxz, t, nb.register.transform, nb.file_names, nb.basic_info) / color_norm\n\n    # Only look at pixels with high enough intensity - same as in full pipeline\n    spot_intensity = get_spot_intensity(np.abs(spot_colors))\n    config = nb.get_config()['omp']\n    if nb.has_page('omp'):\n        initial_intensity_thresh = nb.omp.initial_intensity_thresh\n    else:\n        initial_intensity_thresh = get_initial_intensity_thresh(config, nb.call_spots)\n\n    keep = spot_intensity &gt; initial_intensity_thresh\n    bled_codes = nb.call_spots.bled_codes_ge\n    n_genes = bled_codes.shape[0]\n    bled_codes = np.asarray(bled_codes[np.ix_(np.arange(n_genes),\n                                              nb.basic_info.use_rounds, nb.basic_info.use_channels)])\n    n_use_rounds = len(nb.basic_info.use_rounds)\n    dp_norm_shift = nb.call_spots.dp_norm_shift * np.sqrt(n_use_rounds)\n\n    dp_thresh = config['dp_thresh']\n    if method.lower() == 'omp':\n        alpha = config['alpha']\n        beta = config['beta']\n    else:\n        config_call_spots = nb.get_config()['call_spots']\n        alpha = config_call_spots['alpha']\n        beta = config_call_spots['beta']\n    max_genes = config['max_genes']\n    weight_coef_fit = config['weight_coef_fit']\n\n    all_coefs = np.zeros((spot_colors.shape[0], n_genes + nb.basic_info.n_channels))\n    all_coefs[np.ix_(keep, np.arange(n_genes))], \\\n    all_coefs[np.ix_(keep, np.array(nb.basic_info.use_channels) + n_genes)] = \\\n        get_all_coefs(spot_colors[keep], bled_codes, nb.call_spots.background_weight_shift, dp_norm_shift,\n                      dp_thresh, alpha, beta, max_genes, weight_coef_fit)\n\n    n_genes = all_coefs.shape[1]\n    nz = len(z)\n    coef_images = np.zeros((n_genes, len(z), im_diameter_yx[0], im_diameter_yx[1]))\n    for g in range(n_genes):\n        ind = 0\n        for z in range(nz):\n            coef_images[g, z] = all_coefs[ind:ind+np.prod(im_diameter_yx), g].reshape(im_diameter_yx[0],\n                                                                                      im_diameter_yx[1])\n            ind += np.prod(im_diameter_yx)\n    coef_images = np.moveaxis(coef_images, 1, -1)  # move z index to end\n    min_global_yxz = im_yxz.min(axis=0)+nb.stitch.tile_origin[t]\n    max_global_yxz = im_yxz.max(axis=0)+nb.stitch.tile_origin[t]\n    return coef_images.astype(np.float16), min_global_yxz, max_global_yxz\n</code></pre>"},{"location":"code/plot/omp/#score-shape","title":"Score Shape","text":""},{"location":"code/plot/omp/#view_omp_score","title":"<code>view_omp_score</code>","text":"<p>Diagnostic to show how score is computed in the omp method Hatched region in top plot shows pixels which contribute to the final score. Score is actually equal to the absolute sum of the top plots in the hatched regions.</p> <p>Can also see how <code>score_omp_multiplier</code> affects the final score. The larger this is, the more the positive pixels contribute compared to the negative.</p> <p>Requires access to <code>nb.file_names.tile_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to be plotted.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>score_multiplier</code> <code>Optional[float]</code> <p>Initial value of <code>score_omp_multiplier</code>.</p> <code>None</code> <code>check</code> <code>bool</code> <p>If <code>True</code>, will compare score found to that saved in Notebook and raise error if they differ. Will also check that absolute sum of the top plots in the hatched regions is equal to score calculated from counting the number of pixels with the correct sign.</p> <code>False</code> Source code in <code>coppafish/plot/omp/score_shape.py</code> <pre><code>def __init__(self, nb: Notebook, spot_no: int, method: str = 'omp', score_multiplier: Optional[float] = None,\n             check: bool = False):\n\"\"\"\n    Diagnostic to show how score is computed in the omp method\n    Hatched region in top plot shows pixels which contribute to the final score.\n    Score is actually equal to the absolute sum of the top plots in the hatched regions.\n\n    Can also see how `score_omp_multiplier` affects the final score. The larger this is, the more\n    the positive pixels contribute compared to the negative.\n\n    !!! warning \"Requires access to `nb.file_names.tile_dir`\"\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to be plotted.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        score_multiplier: Initial value of `score_omp_multiplier`.\n        check: If `True`, will compare score found to that saved in *Notebook* and raise error if they differ.\n            Will also check that absolute sum of the top plots in the hatched regions is equal to\n            score calculated from counting the number of pixels with the correct sign.\n\n    \"\"\"\n    # TODO: The textbox for this plot seems to be much less responsive than in the other diagnostics\n    #  for some reason.\n    if method.lower() == 'omp':\n        page_name = 'omp'\n    else:\n        page_name = 'ref_spots'\n        check = False\n    self.check = check\n    self.nbp_omp = nb.omp\n    self.spot_no = spot_no\n    self.gene_no = nb.__getattribute__(page_name).gene_no[spot_no]\n    self.gene_names = nb.call_spots.gene_names\n    self.expected_sign_image = nb.omp.spot_shape\n    self.nz = self.expected_sign_image.shape[2]\n    im_radius = ((np.asarray(self.expected_sign_image.shape) - 1) / 2).astype(int)\n    self.coef_images, min_global_yxz, max_global_yxz = get_coef_images(nb, spot_no, method, im_radius)\n    # Find where sign of coef image matches that of expected sign image\n    self.both_positive = np.asarray([self.coef_images[self.gene_no] &gt; 0, self.expected_sign_image &gt; 0]).all(axis=0)\n    self.both_negative = np.asarray([self.coef_images[self.gene_no] &lt; 0, self.expected_sign_image &lt; 0]).all(axis=0)\n    self.check_neighbours()\n\n    # Start with default multiplier\n    config = nb.get_config()['thresholds']\n    if score_multiplier is None:\n        score_multiplier = config['score_omp_multiplier']\n    self.score_multiplier = score_multiplier\n    self.score_thresh = config['score_omp']\n\n    # maximum possible value of any one pixel in expected_shape for any score_multiplier\n    self.vmax_expected = 1 / np.min([np.sum(self.expected_sign_image &gt; 0), np.sum(self.expected_sign_image &lt; 0)])\n    self.vmax_coef = np.abs(self.coef_images[self.gene_no]).max()\n\n    # Initialize plots\n    self.plot_extent = [min_global_yxz[1] - 0.5, max_global_yxz[1] + 0.5,\n                        min_global_yxz[0] - 0.5, max_global_yxz[0] + 0.5]\n    self.fig, self.ax = plt.subplots(2, self.nz, figsize=(14, 5), sharex=True, sharey=True)\n    self.subplot_adjust = [0.05, 0.88, 0.09, 0.85]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.ax = self.ax.flatten()\n    self.im = [None] * self.nz * 2\n    expected_image_plot = self.expected_image()\n    self.score = self.get_score(expected_image_plot)\n    for i in range(self.nz):\n        self.im[i] = self.ax[i].imshow(expected_image_plot[:, :, i],\n                                       vmin=-self.vmax_expected, vmax=self.vmax_expected, cmap='bwr',\n                                       extent=self.plot_extent)\n        self.im[i+self.nz] = self.ax[i + self.nz].imshow(self.coef_images[self.gene_no, :, :, i],\n                                                         vmin=-self.vmax_coef, vmax=self.vmax_coef, cmap='bwr',\n                                                         extent=self.plot_extent)\n        if i == (self.nz - 1) / 2:\n            self.ax[i].set_title(f\"Expected Coefficient Sign\\nZ={int(np.rint(min_global_yxz[2] + i))}\")\n        else:\n            self.ax[i].set_title(f\"Z={int(np.rint(min_global_yxz[2] + i))}\")\n    self.set_coef_plot_title()\n    self.add_hatching()\n\n    # Set up colorbars for each plot\n    mid_point = (self.subplot_adjust[2]+self.subplot_adjust[3])/2\n    gap_size = 0.08\n    cbar_ax = self.fig.add_axes([self.subplot_adjust[1]+0.01, mid_point+gap_size/2,\n                                 0.005, self.subplot_adjust[3]-mid_point-gap_size/2])  # left, bottom, width, height\n    self.fig.colorbar(self.im[0], cax=cbar_ax)\n    cbar_ax = self.fig.add_axes([self.subplot_adjust[1]+0.01, self.subplot_adjust[2]+gap_size/5,\n                                 0.005, mid_point-self.subplot_adjust[2]-gap_size/2])  # left, bottom, width, height\n    self.fig.colorbar(self.im[self.nz], cax=cbar_ax)\n\n    # Add titles\n    self.fig.supylabel('Y')\n    self.fig.supxlabel('X', size=12, x=(self.subplot_adjust[0] + self.subplot_adjust[1]) / 2)\n    plt.suptitle(f\"OMP Score Calculation for Spot {spot_no}, Gene {self.gene_no}: {self.gene_names[self.gene_no]}\",\n                 x=(self.subplot_adjust[0] + self.subplot_adjust[1]) / 2)\n\n    # Add text box to change score multiplier\n    text_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.062, self.subplot_adjust[2]+gap_size/5,\n                                 0.05, 0.04])\n    self.text_box = TextBox(text_ax, 'Score\\n'+r'Multiplier, $\\rho$', str(np.around(self.score_multiplier, 2)),\n                            color='k', hovercolor=[0.2, 0.2, 0.2])\n    self.text_box.cursor.set_color('r')\n    label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n    label.set_position([0.5, 2.75])  # [x,y] - change here to set the position\n    # centering the text\n    label.set_verticalalignment('top')\n    label.set_horizontalalignment('center')\n    self.text_box.on_submit(self.update)\n\n    plt.show()\n</code></pre>"},{"location":"code/plot/omp/#score-histogram","title":"Score Histogram","text":""},{"location":"code/plot/omp/#histogram_score","title":"<code>histogram_score</code>","text":"<p>If method is anchor, this will show the histogram of <code>nb.ref_spots.score</code> with the option to view the histogram of the score computed using various other configurations of <code>background</code> fitting and <code>gene_efficiency</code>. This allows one to see how the these affect the score.</p> <p>If <code>method</code> is omp, this will show the histogram of omp score, computed with <code>coppafish.call_spots.omp_spot_score</code>. There will also be the option to view the histograms shown for the anchor method. I.e. we compute the dot product score for the omp spots.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>call_spots</code> page.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> <code>'omp'</code> <code>score_omp_multiplier</code> <code>Optional[float]</code> <p>Can specify the value of score_omp_multiplier to use to compute omp score. If <code>None</code>, will use value in config file.</p> <code>None</code> <code>check</code> <code>bool</code> <p>If <code>True</code>, and <code>method='anchor'</code>, will check that scores computed here match those saved to Notebook.</p> <code>False</code> <code>hist_spacing</code> <code>float</code> <p>Initial width of bin in histogram.</p> <code>0.001</code> <code>show_plot</code> <code>bool</code> <p>Whether to run <code>plt.show()</code> or not.</p> <code>True</code> Source code in <code>coppafish/plot/omp/score_hist.py</code> <pre><code>def __init__(self, nb: Notebook, method: str = 'omp', score_omp_multiplier: Optional[float] = None,\n             check: bool = False, hist_spacing: float = 0.001, show_plot: bool = True):\n\"\"\"\n    If method is anchor, this will show the histogram of `nb.ref_spots.score` with the option to\n    view the histogram of the score computed using various other configurations of `background` fitting\n    and `gene_efficiency`. This allows one to see how the these affect the score.\n\n    If `method` is omp, this will show the histogram of omp score, computed with\n    `coppafish.call_spots.omp_spot_score`.\n    There will also be the option to view the histograms shown for the anchor method.\n    I.e. we compute the dot product score for the omp spots.\n\n    Args:\n        nb: *Notebook* containing at least `call_spots` page.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score.\n            If `None`, will use value in config file.\n        check: If `True`, and `method='anchor'`, will check that scores computed here match those saved to Notebook.\n        hist_spacing: Initial width of bin in histogram.\n        show_plot: Whether to run `plt.show()` or not.\n    \"\"\"\n    # Add data\n    if score_omp_multiplier is None:\n        config = nb.get_config()['thresholds']\n        score_omp_multiplier = config['score_omp_multiplier']\n    self.score_multiplier = score_omp_multiplier\n    self.gene_names = nb.call_spots.gene_names\n    self.n_genes = self.gene_names.size\n    # Use all genes by default\n    self.genes_use = np.arange(self.n_genes)\n\n    # For computing score_dp\n    spot_colors, spot_colors_pb, background_var = background_fitting(nb, method)\n    grc_ind = np.ix_(np.arange(self.n_genes), nb.basic_info.use_rounds, nb.basic_info.use_channels)\n    # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds\n    bled_codes = nb.call_spots.bled_codes[grc_ind]\n    bled_codes_ge = nb.call_spots.bled_codes_ge[grc_ind]\n\n    # Save score_dp for each permutation of with/without background/gene_efficiency\n    self.n_plots = 5\n    if method.lower() == 'omp':\n        self.n_plots += 1\n        self.nbp_omp = nb.omp\n        self.gene_no = self.nbp_omp.gene_no\n        self.score = np.zeros((self.gene_no.size, self.n_plots), dtype=np.float32)\n        self.score[:, -1] = omp_spot_score(self.nbp_omp, self.score_multiplier)\n        self.method = 'OMP'\n    else:\n        self.gene_no = nb.ref_spots.gene_no\n        self.score = np.zeros((self.gene_no.size, self.n_plots), dtype=np.float32)\n        self.method = 'Anchor'\n    self.use = np.isin(self.gene_no, self.genes_use)  # which spots to plot\n    # DP score\n    self.score[:, 0] = get_dot_product_score(spot_colors_pb, bled_codes_ge, self.gene_no,\n                                             nb.call_spots.dp_norm_shift, background_var)[0]\n    if method.lower() != 'omp' and check:\n        if np.max(np.abs(self.score[:, 0] - nb.ref_spots.score)) &gt; self.check_tol:\n            raise ValueError(f\"nb.ref_spots.score differs to that computed here\\n\"\n                             f\"Set check=False to get past this error\")\n\n    # DP score no weight\n    self.score[:, 1] = get_dot_product_score(spot_colors_pb, bled_codes_ge, self.gene_no,\n                                             nb.call_spots.dp_norm_shift, None)[0]\n    # DP score no background\n    self.score[:, 2] = get_dot_product_score(spot_colors, bled_codes_ge, self.gene_no,\n                                             nb.call_spots.dp_norm_shift, None)[0]\n    # DP score no gene efficiency\n    self.score[:, 3] = get_dot_product_score(spot_colors_pb, bled_codes, self.gene_no,\n                                             nb.call_spots.dp_norm_shift, background_var)[0]\n    # DP score no background or gene efficiency\n    self.score[:, 4] = get_dot_product_score(spot_colors, bled_codes, self.gene_no,\n                                             nb.call_spots.dp_norm_shift, None)[0]\n\n    # Initialise plot\n    self.fig, self.ax = plt.subplots(1, 1, figsize=(11, 5))\n    self.subplot_adjust = [0.07, 0.85, 0.1, 0.93]\n    self.fig.subplots_adjust(left=self.subplot_adjust[0], right=self.subplot_adjust[1],\n                             bottom=self.subplot_adjust[2], top=self.subplot_adjust[3])\n    self.ax.set_ylabel(r\"Number of Spots\")\n    if method.lower() == 'omp':\n        self.ax.set_xlabel(r\"Score, $\\gamma_s$ or $\\Delta_s$\")\n    else:\n        self.ax.set_xlabel(r\"Score, $\\Delta_s$\")\n    self.ax.set_title(f\"Distribution of Scores for all {self.method} spots\")\n\n    # Set lower bound based on dot product score with no GE/no background as likely to be lowest\n    self.hist_min = np.percentile(self.score[:, 4], 0.1)\n    # Set upper bound based on dot product score with GE and background because this is likely to be highest\n    self.hist_max = np.clip(np.percentile(self.score[:, 0], 99.9), 1, 2)\n    self.hist_spacing = hist_spacing\n    hist_bins = np.arange(self.hist_min, self.hist_max + self.hist_spacing / 2, self.hist_spacing)\n    self.plots = [None] * self.n_plots\n    default_colors = plt.rcParams['axes.prop_cycle']._left\n    for i in range(self.n_plots):\n        y, x = np.histogram(self.score[self.use, i], hist_bins)\n        x = x[:-1] + self.hist_spacing / 2  # so same length as x\n        self.plots[i], = self.ax.plot(x, y, color=default_colors[i]['color'])\n        if method.lower() == 'omp' and i &lt; self.n_plots - 1:\n            self.plots[i].set_visible(False)\n        elif i &gt; 0 and method.lower() != 'omp':\n            self.plots[i].set_visible(False)\n\n    self.ax.set_xlim(self.hist_min, self.hist_max)\n    self.ax.set_ylim(0, None)\n\n    # Add text box to change score multiplier\n    text_box_labels = ['Gene', 'Histogram\\nSpacing', 'Score\\n' + r'Multiplier, $\\rho$']\n    text_box_values = ['all', self.hist_spacing, np.around(self.score_multiplier, 2)]\n    text_box_funcs = [self.update_genes, self.update_hist_spacing, self.update_score_multiplier]\n    if method.lower() != 'omp':\n        text_box_labels = text_box_labels[:2]\n        text_box_values = text_box_values[:2]\n        text_box_funcs = text_box_funcs[:2]\n    self.text_boxes = [None] * len(text_box_labels)\n    for i in range(len(text_box_labels)):\n        text_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.05,\n                                     self.subplot_adjust[2] + 0.15 * (len(text_box_labels) - i - 1), 0.05, 0.04])\n        self.text_boxes[i] = TextBox(text_ax, text_box_labels[i], text_box_values[i], color='k',\n                                     hovercolor=[0.2, 0.2, 0.2])\n        self.text_boxes[i].cursor.set_color('r')\n        label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n        if i == 0:\n            label.set_position([0.5, 1.77])  # [x,y] - change here to set the position\n        else:\n            label.set_position([0.5, 2.75])\n            # centering the text\n        label.set_verticalalignment('top')\n        label.set_horizontalalignment('center')\n        self.text_boxes[i].on_submit(text_box_funcs[i])\n\n    # Add buttons to add/remove score_dp histograms\n    self.buttons_ax = self.fig.add_axes([self.subplot_adjust[1] + 0.02, self.subplot_adjust[3] - 0.45, 0.15, 0.5])\n    plt.axis('off')\n    self.button_labels = [r\"$\\Delta_s$\" + \"\\nDot Product Score\",\n                          r\"$\\Delta_s$\" + \"\\nNo Weighting\",\n                          r\"$\\Delta_s$\" + \"\\nNo Background\",\n                          r\"$\\Delta_s$\" + \"\\nNo Gene Efficiency\",\n                          r\"$\\Delta_s$\" + \"\\nNo Background\\nNo Gene Efficiency\"]\n    label_checked = [True, False, False, False, False]\n    if method.lower() == 'omp':\n        self.button_labels += [r\"$\\gamma_s$\" + \"\\nOMP Score\"]\n        label_checked += [True]\n        label_checked[0] = False\n    self.buttons = CheckButtons(self.buttons_ax, self.button_labels, label_checked)\n\n    for i in range(self.n_plots):\n        self.buttons.labels[i].set_fontsize(7)\n        self.buttons.labels[i].set_color(default_colors[i]['color'])\n        self.buttons.rectangles[i].set_color('w')\n    self.buttons.on_clicked(self.choose_plots)\n    if show_plot:\n        plt.show()\n</code></pre>"},{"location":"code/plot/omp/#histogram_2d_score","title":"<code>histogram_2d_score</code>","text":"<p>This plots the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\).</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>call_spots</code> page.</p> required <code>score_omp_multiplier</code> <code>Optional[float]</code> <p>Can specify the value of score_omp_multiplier to use to compute omp score. If <code>None</code>, will use value in config file.</p> <code>None</code> Source code in <code>coppafish/plot/omp/score_hist.py</code> <pre><code>def __init__(self, nb: Notebook, score_omp_multiplier: Optional[float] = None):\n\"\"\"\n    This plots the bivariate histogram to see the correlation between the omp spot score, $\\gamma_s$ and\n    the dot product score $\\Delta_s$.\n\n    Args:\n        nb: *Notebook* containing at least `call_spots` page.\n        score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score.\n            If `None`, will use value in config file.\n    \"\"\"\n    # large hist_spacing so quick as we change it anway\n    super().__init__(nb, 'omp', score_omp_multiplier, False, 0.5, False)\n    self.ax.clear()\n    # Get rid of buttons - only use actual dot product score\n    self.buttons_ax.clear()\n    plt.axis('off')\n    self.score = self.score[:, [0, self.n_plots - 1]]\n    self.n_plots = 2\n    del self.plots\n    hist_bins = np.arange(self.hist_min, self.hist_max + self.hist_spacing / 2, self.hist_spacing)\n    self.x_score_ind = 0\n    self.hist_spacing = 0.01\n    self.plot = self.ax.hist2d(self.score[:, self.x_score_ind], self.score[:, -1], hist_bins)[3]\n    self.cbar = self.fig.colorbar(self.plot, ax=self.ax)\n    self.ax.set_xlim(self.hist_min, self.hist_max)\n    self.ax.set_ylim(self.hist_min, self.hist_max)\n    self.text_boxes[1].set_val(self.hist_spacing)\n    self.ax.set_xlabel(self.button_labels[0].replace('\\n', ', '))\n    self.ax.set_ylabel(self.button_labels[-1].replace('\\n', ', '))\n    plt.show()\n</code></pre>"},{"location":"code/plot/omp/#track-fit","title":"Track Fit","text":""},{"location":"code/plot/omp/#coppafish.plot.omp.track_fit.get_track_info","title":"<code>get_track_info(nb, spot_no, method, dp_thresh=None, max_genes=None)</code>","text":"<p>This runs omp while tracking the residual at each stage.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing experiment details. Must have run at least as far as <code>call_reference_spots</code>.</p> required <code>spot_no</code> <code>int</code> <p>Spot of interest to get track_info for.</p> required <code>method</code> <code>str</code> <p><code>'anchor'</code> or <code>'omp'</code>. Which method of gene assignment used i.e. <code>spot_no</code> belongs to <code>ref_spots</code> or <code>omp</code> page of Notebook.</p> required <code>dp_thresh</code> <code>Optional[float]</code> <p>If None, will use value in omp section of config file.</p> <code>None</code> <code>max_genes</code> <code>Optional[int]</code> <p>If None, will use value in omp section of config file.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p><code>track_info</code> - dictionary containing info about genes added at each step returned:</p> <ul> <li><code>background_codes</code> - <code>float [n_channels x n_rounds x n_channels]</code>.     <code>background_codes[c]</code> is the background vector for channel <code>c</code> with L2 norm of 1.</li> <li><code>background_coefs</code> - <code>float [n_channels]</code>.     <code>background_coefs[c]</code> is the coefficient value for <code>background_codes[c]</code>.</li> <li><code>gene_added</code> - <code>int [n_genes_added + 2]</code>.     <code>gene_added[0]</code> and <code>gene_added[1]</code> are -1.     <code>gene_added[2+i]</code> is the <code>ith</code> gene that was added.</li> <li><code>residual</code> - <code>float [(n_genes_added + 2) x n_rounds x n_channels]</code>.     <code>residual[0]</code> is the initial <code>pixel_color</code>.     <code>residual[1]</code> is the post background <code>pixel_color</code>.     <code>residual[2+i]</code> is the <code>pixel_color</code> after removing gene <code>gene_added[2+i]</code>.</li> <li><code>coef</code> - <code>float [(n_genes_added + 2) x n_genes]</code>.     <code>coef[0]</code> and <code>coef[1]</code> are all 0.     <code>coef[2+i]</code> are the coefficients for all genes after the ith gene has been added.</li> <li><code>dot_product</code> - <code>float [n_genes_added + 2]</code>.     <code>dot_product[0]</code> and <code>dot_product[1]</code> are 0.     <code>dot_product[2+i]</code> is the dot product for the gene <code>gene_added[2+i]</code>.</li> <li><code>inverse_var</code> - <code>float [(n_genes_added + 2) x n_rounds x n_channels]</code>.     <code>inverse_var[0]</code> and <code>inverse_var[1]</code> are all 0.     <code>inverse_var[2+i]</code> is the weighting used to compute <code>dot_product[2+i]</code>,      which down-weights rounds/channels for which a gene has already been fitted.</li> </ul> <code>np.ndarray</code> <p><code>bled_codes</code> - <code>float [n_genes x n_use_rounds x n_use_channels]</code>. gene <code>bled_codes</code> used in omp with L2 norm = 1.</p> <code>float</code> <p><code>dp_thresh</code> - threshold dot product score, above which gene is fitted.</p> Source code in <code>coppafish/plot/omp/track_fit.py</code> <pre><code>def get_track_info(nb: Notebook, spot_no: int, method: str, dp_thresh: Optional[float] = None,\n                   max_genes: Optional[int] = None) -&gt; Tuple[dict, np.ndarray, float]:\n\"\"\"\n    This runs omp while tracking the residual at each stage.\n\n    Args:\n        nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`.\n        spot_no: Spot of interest to get track_info for.\n        method: `'anchor'` or `'omp'`.\n            Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook.\n        dp_thresh: If None, will use value in omp section of config file.\n        max_genes: If None, will use value in omp section of config file.\n\n    Returns:\n        `track_info` - dictionary containing info about genes added at each step returned:\n\n            - `background_codes` - `float [n_channels x n_rounds x n_channels]`.\n                `background_codes[c]` is the background vector for channel `c` with L2 norm of 1.\n            - `background_coefs` - `float [n_channels]`.\n                `background_coefs[c]` is the coefficient value for `background_codes[c]`.\n            - `gene_added` - `int [n_genes_added + 2]`.\n                `gene_added[0]` and `gene_added[1]` are -1.\n                `gene_added[2+i]` is the `ith` gene that was added.\n            - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`.\n                `residual[0]` is the initial `pixel_color`.\n                `residual[1]` is the post background `pixel_color`.\n                `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`.\n            - `coef` - `float [(n_genes_added + 2) x n_genes]`.\n                `coef[0]` and `coef[1]` are all 0.\n                `coef[2+i]` are the coefficients for all genes after the ith gene has been added.\n            - `dot_product` - `float [n_genes_added + 2]`.\n                `dot_product[0]` and `dot_product[1]` are 0.\n                `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`.\n            - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`.\n                `inverse_var[0]` and `inverse_var[1]` are all 0.\n                `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`,\n                 which down-weights rounds/channels for which a gene has already been fitted.\n        `bled_codes` - `float [n_genes x n_use_rounds x n_use_channels]`.\n            gene `bled_codes` used in omp with L2 norm = 1.\n        `dp_thresh` - threshold dot product score, above which gene is fitted.\n    \"\"\"\n    color_norm = nb.call_spots.color_norm_factor[np.ix_(nb.basic_info.use_rounds,\n                                                        nb.basic_info.use_channels)]\n    n_use_rounds, n_use_channels = color_norm.shape\n    if method.lower() == 'omp':\n        page_name = 'omp'\n        config_name = 'omp'\n    else:\n        page_name = 'ref_spots'\n        config_name = 'call_spots'\n    spot_color = nb.__getattribute__(page_name).colors[spot_no][\n                     np.ix_(nb.basic_info.use_rounds, nb.basic_info.use_channels)] / color_norm\n    n_genes = nb.call_spots.bled_codes_ge.shape[0]\n    bled_codes = np.asarray(\n        nb.call_spots.bled_codes_ge[np.ix_(np.arange(n_genes),\n                                           nb.basic_info.use_rounds, nb.basic_info.use_channels)])\n    # ensure L2 norm is 1 for bled codes\n    norm_factor = np.expand_dims(np.linalg.norm(bled_codes, axis=(1, 2)), (1, 2))\n    norm_factor[norm_factor == 0] = 1  # For genes with no dye in use_dye, this avoids blow up on next line\n    bled_codes = bled_codes / norm_factor\n\n    # Get info to run omp\n    dp_norm_shift = nb.call_spots.dp_norm_shift * np.sqrt(n_use_rounds)\n    config = nb.get_config()\n    if dp_thresh is None:\n        dp_thresh = config['omp']['dp_thresh']\n    alpha = config[config_name]['alpha']\n    beta = config[config_name]['beta']\n    if max_genes is None:\n        max_genes = config['omp']['max_genes']\n    weight_coef_fit = config['omp']['weight_coef_fit']\n\n    # Run omp with track to get residual at each stage\n    track_info = get_all_coefs(spot_color[np.newaxis], bled_codes, nb.call_spots.background_weight_shift,\n                               dp_norm_shift, dp_thresh, alpha, beta, max_genes, weight_coef_fit, True)[2]\n    return track_info, bled_codes, dp_thresh\n</code></pre>"},{"location":"code/plot/raw/","title":"Raw","text":""},{"location":"code/plot/raw/#coppafish.plot.raw.add_basic_info_no_save","title":"<code>add_basic_info_no_save(nb)</code>","text":"<p>This adds the <code>basic_info</code> page to the notebook without saving the notebook.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook with no <code>basic_info</code> page.</p> required Source code in <code>coppafish/plot/raw.py</code> <pre><code>def add_basic_info_no_save(nb: Notebook):\n\"\"\"\n    This adds the `basic_info` page to the notebook without saving the notebook.\n\n    Args:\n        nb: Notebook with no `basic_info` page.\n\n    \"\"\"\n    if not nb.has_page(\"basic_info\"):\n        nb._no_save_pages['basic_info'] = {}  # don't save if add basic_info page\n        config = nb.get_config()\n        nbp_basic = set_basic_info(config['file_names'], config['basic_info'])\n        nb += nbp_basic\n</code></pre>"},{"location":"code/plot/raw/#coppafish.plot.raw.get_raw_images","title":"<code>get_raw_images(nb, tiles, rounds, channels, use_z)</code>","text":"<p>This loads in raw images for the experiment corresponding to the Notebook.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook for experiment</p> required <code>tiles</code> <code>List[int]</code> <p>npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below:</p> <p>| 2  | 1  | 0  |</p> <p>| 5  | 4  | 3  |</p> <p>| 8  | 7  | 6  |</p> <p>| 11 | 10 | 9  |</p> required <code>rounds</code> <code>List[int]</code> <p>Rounds to view.</p> required <code>channels</code> <code>List[int]</code> <p>Channels to view.</p> required <code>use_z</code> <code>List[int]</code> <p>Which z-planes to load in from raw data.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>raw_images</code> - <code>[len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)]</code> uint16 array.</p> <code>np.ndarray</code> <p><code>raw_images[t, r, c]</code> is the <code>[n_y x n_x x len(use_z)]</code> image for tile <code>tiles[t]</code>, round <code>rounds[r]</code> and channel</p> <code>np.ndarray</code> <p><code>channels[c]</code>.</p> Source code in <code>coppafish/plot/raw.py</code> <pre><code>def get_raw_images(nb: Notebook, tiles: List[int], rounds: List[int],\n                   channels: List[int], use_z: List[int]) -&gt; np.ndarray:\n\"\"\"\n    This loads in raw images for the experiment corresponding to the *Notebook*.\n\n    Args:\n        nb: Notebook for experiment\n        tiles: npy (as opposed to nd2 fov) tile indices to view.\n            For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as\n            below:\n\n            | 2  | 1  | 0  |\n\n            | 5  | 4  | 3  |\n\n            | 8  | 7  | 6  |\n\n            | 11 | 10 | 9  |\n        rounds: Rounds to view.\n        channels: Channels to view.\n        use_z: Which z-planes to load in from raw data.\n\n    Returns:\n        `raw_images` - `[len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)]` uint16 array.\n        `raw_images[t, r, c]` is the `[n_y x n_x x len(use_z)]` image for tile `tiles[t]`, round `rounds[r]` and channel\n        `channels[c]`.\n    \"\"\"\n    n_tiles = len(tiles)\n    n_rounds = len(rounds)\n    n_channels = len(channels)\n    n_images = n_rounds * n_tiles * n_channels\n    ny = nb.basic_info.tile_sz\n    nx = ny\n    nz = len(use_z)\n\n    raw_images = np.zeros((n_tiles, n_rounds, n_channels, ny, nx, nz), dtype=np.uint16)\n    with tqdm(total=n_images) as pbar:\n        pbar.set_description(f'Loading in raw data')\n        for r in range(n_rounds):\n            round_dask_array = raw.load(nb.file_names, nb.basic_info, r=rounds[r])\n            # TODO: Can get rid of these two for loops, when round_dask_array is always a dask array.\n            #  At the moment though, is not dask array when using nd2_reader (On Mac M1).\n            for t in range(n_tiles):\n                for c in range(n_channels):\n                    pbar.set_postfix({'round': rounds[r], 'tile': tiles[t], 'channel': channels[c]})\n                    raw_images[t, r, c] = raw.load(nb.file_names, nb.basic_info, round_dask_array,\n                                                   rounds[r], tiles[t], channels[c], use_z)\n                    pbar.update(1)\n    return raw_images\n</code></pre>"},{"location":"code/plot/raw/#coppafish.plot.raw.view_raw","title":"<code>view_raw(nb=None, tiles=0, rounds=0, channels=None, use_z=None, config_file=None)</code>","text":"<p>Function to view raw data in napari. There will upto 4 scrollbars for each image to change tile, round, channel and z-plane.</p> <p>Requires access to <code>nb.file_names.input_dir</code></p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Optional[Notebook]</code> <p>Notebook for experiment. If no Notebook exists, pass <code>config_file</code> instead.</p> <code>None</code> <code>tiles</code> <code>Union[int, List[int]]</code> <p>npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below:</p> <p>| 2  | 1  | 0  |</p> <p>| 5  | 4  | 3  |</p> <p>| 8  | 7  | 6  |</p> <p>| 11 | 10 | 9  |</p> <code>0</code> <code>rounds</code> <code>Union[int, List[int]]</code> <p>rounds to view (<code>anchor</code> will be <code>nb.basic_info.n_rounds</code> i.e. the last round.)</p> <code>0</code> <code>channels</code> <code>Optional[Union[int, List[int]]]</code> <p>Channels to view. If <code>None</code>, will load all channels.</p> <code>None</code> <code>use_z</code> <code>Optional[Union[int, List[int]]]</code> <p>Which z-planes to load in from raw data. If <code>None</code>, will use load all z-planes (except from first one if <code>config['basic_info']['ignore_first_z_plane'] == True</code>).</p> <code>None</code> <code>config_file</code> <code>Optional[str]</code> <p>path to config file for experiment.</p> <code>None</code> Source code in <code>coppafish/plot/raw.py</code> <pre><code>def view_raw(nb: Optional[Notebook] = None, tiles: Union[int, List[int]] = 0, rounds: Union[int, List[int]] = 0,\n             channels: Optional[Union[int, List[int]]] = None,\n             use_z: Optional[Union[int, List[int]]] = None, config_file: Optional[str] = None):\n\"\"\"\n    Function to view raw data in napari.\n    There will upto 4 scrollbars for each image to change tile, round, channel and z-plane.\n\n    !!! warning \"Requires access to `nb.file_names.input_dir`\"\n\n    Args:\n        nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead.\n        tiles: npy (as opposed to nd2 fov) tile indices to view.\n            For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as\n            below:\n\n            | 2  | 1  | 0  |\n\n            | 5  | 4  | 3  |\n\n            | 8  | 7  | 6  |\n\n            | 11 | 10 | 9  |\n        rounds: rounds to view (`anchor` will be `nb.basic_info.n_rounds` i.e. the last round.)\n        channels: Channels to view. If `None`, will load all channels.\n        use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first\n            one if `config['basic_info']['ignore_first_z_plane'] == True`).\n        config_file: path to config file for experiment.\n    \"\"\"\n    if nb is None:\n        nb = Notebook(config_file=config_file)\n    add_basic_info_no_save(nb)  # deal with case where there is no notebook yet\n    if channels is None:\n        channels = np.arange(nb.basic_info.n_channels)\n    if use_z is None:\n        use_z = nb.basic_info.use_z\n    tiles, rounds, channels, use_z = number_to_list([tiles, rounds, channels, use_z])\n\n    raw_images = get_raw_images(nb, tiles, rounds, channels, use_z)\n    viewer = napari.Viewer()\n    viewer.add_image(np.moveaxis(raw_images, -1, 3), name='Raw Images')\n\n    @viewer.dims.events.current_step.connect\n    def update_slider(event):\n        viewer.status = f'Tile: {tiles[event.value[0]]}, Round: {rounds[event.value[1]]}, ' \\\n                        f'Channel: {channels[event.value[2]]}, Z: {use_z[event.value[3]]}'\n\n    viewer.dims.axis_labels = ['Tile', 'Round', 'Channel', 'z', 'y', 'x']\n    viewer.dims.set_point([0, 1, 2], [0, 0, 0])  # set to first tile, round and channel initially\n    napari.run()\n</code></pre>"},{"location":"code/plot/register/","title":"Register","text":""},{"location":"code/plot/register/#register-initial","title":"Register Initial","text":""},{"location":"code/plot/register/#view_register_shift_info","title":"<code>view_register_shift_info</code>","text":"<p>For all shifts to imaging rounds from the reference round computed in the <code>register_initial</code> section of the pipeline, this plots the values of the shifts found and the <code>score</code> compared to the <code>score_thresh</code>.</p> <p>For each round, there will be 3 plots:</p> <ul> <li>y shift vs x shift for all tiles</li> <li>z shift vs x shift for all tiles</li> <li><code>score</code> vs <code>score_thresh</code> for all tiles (a green score = score_thresh line is plotted in this).</li> </ul> <p>In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if <code>score &gt; score_thresh</code> and red otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>register_initial</code> page.</p> required <code>outlier</code> <code>bool</code> <p>If <code>True</code>, will plot <code>nb.register_initial.shift_outlier</code> instead of <code>nb.register_initial.shift</code>. In this case, only tiles for which the two are different are plotted for each round.</p> <code>False</code> Source code in <code>coppafish/plot/register/shift.py</code> <pre><code>def view_register_shift_info(nb: Notebook, outlier: bool = False):\n\"\"\"\n    For all shifts to imaging rounds from the reference round computed in the `register_initial` section\n    of the pipeline, this plots the values of the shifts found and the `score` compared to\n    the `score_thresh`.\n\n    For each round, there will be 3 plots:\n\n    * y shift vs x shift for all tiles\n    * z shift vs x shift for all tiles\n    * `score` vs `score_thresh` for all tiles (a green score = score_thresh line is plotted in this).\n\n    In each case, the markers in the plots are numbers.\n    These numbers indicate the tile the shift was found for.\n    The number will be blue if `score &gt; score_thresh` and red otherwise.\n\n    Args:\n        nb: Notebook containing at least the `register_initial` page.\n        outlier: If `True`, will plot `nb.register_initial.shift_outlier` instead of\n            `nb.register_initial.shift`. In this case, only tiles for which\n            the two are different are plotted for each round.\n    \"\"\"\n    shift_info = {}\n    if nb.basic_info.is_3d:\n        ndim = 3\n    else:\n        ndim = 2\n    for r in nb.basic_info.use_rounds:\n        name = f'Round {r}'\n        shift_info[name] = {}\n        shift_info[name]['tile'] = nb.basic_info.use_tiles\n        if outlier:\n            shift_info[name]['shift'] = nb.register_initial.shift_outlier[nb.basic_info.use_tiles, r, :ndim]\n            shift_info[name]['score'] = nb.register_initial.shift_score_outlier[nb.basic_info.use_tiles, r]\n        else:\n            shift_info[name]['shift'] = nb.register_initial.shift[nb.basic_info.use_tiles, r, :ndim]\n            shift_info[name]['score'] = nb.register_initial.shift_score[nb.basic_info.use_tiles, r]\n        shift_info[name]['score_thresh'] = nb.register_initial.shift_score_thresh[nb.basic_info.use_tiles, r]\n\n    if outlier:\n        title_start = \"Outlier \"\n    else:\n        title_start = \"\"\n    shift_info_plot(shift_info, f\"{title_start}Shifts found in register_initial part of pipeline \"\n                                f\"from round {nb.basic_info.ref_round}, channel \"\n                                f\"{nb.basic_info.ref_channel} to channel \"\n                                f\"{nb.register_initial.shift_channel} for each round and tile\")\n</code></pre>"},{"location":"code/plot/register/#view_register_search","title":"<code>view_register_search</code>","text":"<p>Function to plot results of exhaustive search to find shift between <code>ref_round/ref_channel</code> and round <code>r</code>, channel <code>c</code> for tile <code>t</code>. This shift will then be used as the starting point when running point cloud registration to find affine transform. Useful for debugging the <code>register_initial</code> section of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing results of the experiment. Must contain <code>find_spots</code> page.</p> required <code>t</code> <code>int</code> <p>tile interested in.</p> required <code>r</code> <code>int</code> <p>Want to find the shift between the reference round and this round.</p> required <code>c</code> <code>Optional[int]</code> <p>Want to find the shift between the reference channel and this channel. If <code>None</code>, <code>config['shift_channel']</code> will be used, as it is in the pipeline.</p> <code>None</code> <code>return_shift</code> <code>bool</code> <p>If True, will return shift found and will not call plt.show() otherwise will return None.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[np.ndarray]</code> <p><code>best_shift</code> - <code>float [shift_y, shift_x, shift_z]</code>. Best shift found. <code>shift_z</code> is in units of z-pixels.</p> Source code in <code>coppafish/plot/register/shift.py</code> <pre><code>def view_register_search(nb: Notebook, t: int, r: int, c: Optional[int] = None,\n                         return_shift: bool = False) -&gt; Optional[np.ndarray]:\n\"\"\"\n    Function to plot results of exhaustive search to find shift between `ref_round/ref_channel` and\n    round `r`, channel `c` for tile `t`. This shift will then be used as the starting point when running point cloud\n    registration to find affine transform.\n    Useful for debugging the `register_initial` section of the pipeline.\n\n    Args:\n        nb: Notebook containing results of the experiment. Must contain `find_spots` page.\n        t: tile interested in.\n        r: Want to find the shift between the reference round and this round.\n        c: Want to find the shift between the reference channel and this channel. If `None`, `config['shift_channel']`\n            will be used, as it is in the pipeline.\n        return_shift: If True, will return shift found and will not call plt.show() otherwise will return None.\n\n    Returns:\n        `best_shift` - `float [shift_y, shift_x, shift_z]`.\n            Best shift found. `shift_z` is in units of z-pixels.\n    \"\"\"\n    config = nb.get_config()['register_initial']\n    if c is None:\n        c = config['shift_channel']\n        if c is None:\n            c = nb.basic_info.ref_channel\n    if not np.isin(c, nb.basic_info.use_channels):\n        raise ValueError(f\"c should be in nb.basic_info.use_channels, but value given is\\n\"\n                         f\"{c} which is not in use_channels = {nb.basic_info.use_channels}.\")\n\n    coords = ['y', 'x', 'z']\n    shifts = [{}]\n    for i in range(len(coords)):\n        shifts[0][coords[i]] = np.arange(config['shift_min'][i],\n                                         config['shift_max'][i] +\n                                         config['shift_step'][i] / 2, config['shift_step'][i]).astype(int)\n    if not nb.basic_info.is_3d:\n        config['nz_collapse'] = None\n        config['shift_widen'][2] = 0  # so don't look for shifts in z direction\n        config['shift_max_range'][2] = 0\n        shifts[0]['z'] = np.array([0], dtype=int)\n    shifts = shifts * nb.basic_info.n_rounds  # get one set of shifts for each round\n    c_ref = nb.basic_info.ref_channel\n    r_ref = nb.basic_info.ref_round\n    # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours\n    z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy\n    print(f'Finding shift between round {r_ref}, channel {c_ref} to round {r}, channel {c} for tile {t}')\n    shift, shift_score, shift_score_thresh, debug_info = \\\n        compute_shift(spot_yxz(nb.find_spots.spot_details, t, r_ref, c_ref),\n                      spot_yxz(nb.find_spots.spot_details, t, r, c),\n                      config['shift_score_thresh'], config['shift_score_thresh_multiplier'],\n                      config['shift_score_thresh_min_dist'], config['shift_score_thresh_max_dist'],\n                      config['neighb_dist_thresh'], shifts[r]['y'], shifts[r]['x'], shifts[r]['z'],\n                      config['shift_widen'], config['shift_max_range'], z_scale,\n                      config['nz_collapse'], config['shift_step'][2])\n    title = f'Shift between r={r_ref}, c={c_ref} and r={r}, c={c} for tile {t}. YXZ Shift = {shift}.'\n    if return_shift:\n        show = False\n    else:\n        show = True\n    view_shifts(debug_info['shifts_2d'], debug_info['scores_2d'], debug_info['shifts_3d'],\n                debug_info['scores_3d'], shift, debug_info['min_score_2d'], debug_info['shift_2d_initial'],\n                shift_score_thresh, debug_info['shift_thresh'], config['shift_score_thresh_min_dist'],\n                config['shift_score_thresh_max_dist'], title, show)\n    if return_shift:\n        return shift\n</code></pre>"},{"location":"code/plot/register/#register","title":"Register","text":""},{"location":"code/plot/register/#scale_box_plots","title":"<code>scale_box_plots</code>","text":"<p>Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel. Want very similar values for a given channel across all tiles and rounds for each dimension. Also expect \\(y\\) and \\(x\\) scaling to be very similar. \\(z\\) scaling different due to unit conversion.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing the <code>register</code> and <code>register_debug</code> NotebookPages.</p> required Source code in <code>coppafish/plot/register/diagnostics.py</code> <pre><code>def scale_box_plots(nb: Notebook):\n\"\"\"\n    Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel.\n    Want very similar values for a given channel across all tiles and rounds for each dimension.\n    Also expect $y$ and $x$ scaling to be very similar. $z$ scaling different due to unit conversion.\n\n    Args:\n        nb: *Notebook* containing the `register` and `register_debug` *NotebookPages*.\n    \"\"\"\n    if nb.basic_info.is_3d:\n        ndim = 3\n        if np.ptp(nb.register.transform[np.ix_(nb.basic_info.use_tiles, nb.basic_info.use_rounds,\n                                               nb.basic_info.use_channels)][:, :, :, 2, 2]) &lt; 1e-5:\n            ndim = 2\n            warnings.warn(\"Not showing z-scaling as all are the same\")\n    else:\n        ndim = 2\n\n    fig, ax = plt.subplots(ndim, figsize=(10, 6), sharex=True)\n    ax[0].get_shared_y_axes().join(ax[0], ax[1])\n    fig.subplots_adjust(left=0.075, right=0.95, bottom=0.15)\n    y_titles = [\"Scaling - Y\", \"Scaling - X\", \"Scaling - Z\"]\n    n_use_channels = len(nb.basic_info.use_channels)\n    # different boxplot color for each channel\n    # Must be distinct from black and white\n    channel_colors = distinctipy.get_colors(n_use_channels, [(0, 0, 0), (1, 1, 1)])\n    for i in range(ndim):\n        box_data = [nb.register.transform[nb.basic_info.use_tiles, r, c, i, i] for c in nb.basic_info.use_channels\n                    for r in nb.basic_info.use_rounds]\n        bp = ax[i].boxplot(box_data, notch=0, sym='+', patch_artist=True)\n        leg_markers = []\n        c = -1\n        for j in range(len(box_data)):\n            if j % n_use_channels == 0:\n                c += 1\n                leg_markers = leg_markers + [bp['boxes'][j]]\n            bp['boxes'][j].set_facecolor(channel_colors[c])\n        ax[i].set_ylabel(y_titles[i])\n\n        if i == ndim-1:\n            tick_labels = np.tile(nb.basic_info.use_rounds, n_use_channels).tolist()\n            leg_labels = nb.basic_info.use_channels\n            ax[i].set_xticks(np.arange(len(tick_labels)))\n            ax[i].set_xticklabels(tick_labels)\n            ax[i].legend(leg_markers, leg_labels, title='Channel')\n            ax[i].set_xlabel('Round')\n    ax[0].set_title('Boxplots showing distribution of scalings due to\\nchromatic aberration amongst tiles for each '\n                    'round and channel')\n    plt.show()\n</code></pre>"},{"location":"code/plot/register/#view_affine_shift_info","title":"<code>view_affine_shift_info</code>","text":"<p>For all affine transforms to imaging rounds/channels from the reference round computed in the <code>register</code> section of the pipeline, this plots the values of the shifts, <code>n_matches</code> (number of neighbours found) and <code>error</code> (average distance between neighbours).</p> <p>For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots:</p> <ul> <li>y shift vs x shift for all tiles</li> <li>z shift vs x shift for all tiles</li> <li><code>n_matches</code> vs <code>error</code> for all tiles</li> </ul> <p>In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if <code>nb.register_debug.n_matches &gt; nb.register_debug.n_matches_thresh</code> and red otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>register</code> page.</p> required <code>c</code> <code>Optional[int]</code> <p>If None, will give option to scroll with mouse to change channel. If specify c, will show just one channel with no scrolling.</p> <code>None</code> <code>outlier</code> <code>bool</code> <p>If <code>True</code>, will plot shifts from <code>nb.register_debug.transform_outlier</code> instead of <code>nb.register.transform</code>. In this case, only tiles for which <code>nb.register_debug.failed == True</code> are plotted for each round/channel.</p> <code>False</code> Source code in <code>coppafish/plot/register/diagnostics.py</code> <pre><code>def __init__(self, nb: Notebook, c: Optional[int] = None, outlier: bool = False):\n\"\"\"\n    For all affine transforms to imaging rounds/channels from the reference round computed in the `register` section\n    of the pipeline, this plots the values of the shifts, `n_matches` (number of neighbours found) and\n    `error` (average distance between neighbours).\n\n    For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots:\n\n    * y shift vs x shift for all tiles\n    * z shift vs x shift for all tiles\n    * `n_matches` vs `error` for all tiles\n\n    In each case, the markers in the plots are numbers.\n    These numbers indicate the tile the shift was found for.\n    The number will be blue if `nb.register_debug.n_matches &gt; nb.register_debug.n_matches_thresh` and red otherwise.\n\n    Args:\n        nb: Notebook containing at least the `register` page.\n        c: If None, will give option to scroll with mouse to change channel. If specify c, will show just\n            one channel with no scrolling.\n        outlier: If `True`, will plot shifts from `nb.register_debug.transform_outlier` instead of\n            `nb.register.transform`. In this case, only tiles for which\n            `nb.register_debug.failed == True` are plotted for each round/channel.\n    \"\"\"\n    self.outlier = outlier\n    self.nb = nb\n    if c is None:\n        if self.outlier:\n            # only show channels for which there is an outlier shift\n            self.channels = np.sort(np.unique(np.where(nb.register_debug.failed)[2]))\n            if len(self.channels) == 0:\n                raise ValueError(f\"No outlier transforms were computed\")\n        else:\n            self.channels = np.asarray(nb.basic_info.use_channels)\n    else:\n        self.channels = [c]\n    self.n_channels = len(self.channels)\n    self.c_ind = 0\n    self.c = self.channels[self.c_ind]\n\n    n_cols = len(nb.basic_info.use_rounds)\n    if nb.basic_info.is_3d:\n        n_rows = 3\n    else:\n        n_rows = 2\n    self.fig, self.ax = plt.subplots(n_rows, n_cols, figsize=(15, 7))\n    self.fig.subplots_adjust(hspace=0.4, bottom=0.08, left=0.06, right=0.97, top=0.9)\n    self.shift_info = self.get_ax_lims(self.nb, self.channels, self.outlier)\n    self.update()\n    if self.n_channels &gt; 1:\n        self.fig.canvas.mpl_connect('scroll_event', self.z_scroll)\n    plt.show()\n</code></pre>"},{"location":"code/plot/register/#icp","title":"ICP","text":""},{"location":"code/plot/register/#view_icp","title":"<code>view_icp</code>","text":"<p>Function to plot results of iterative closest point to find affine transform between <code>ref_round/ref_channel</code> and round <code>r</code>, channel <code>c</code> for tile <code>t</code>. Useful for debugging the <code>register</code> section of the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing results of the experiment. Must contain <code>find_spots</code> page. If contains <code>register_initial</code> and/or <code>register</code> pages, then transform from these will be used.</p> required <code>t</code> <code>int</code> <p>tile interested in.</p> required <code>r</code> <code>int</code> <p>Want to find the transform between the reference round and this round.</p> required <code>c</code> <code>int</code> <p>Want to find the transform between the reference channel and this channel.</p> required Source code in <code>coppafish/plot/register/icp.py</code> <pre><code>def view_icp(nb: Notebook, t: int, r: int, c: int):\n\"\"\"\n    Function to plot results of iterative closest point to find affine transform between\n    `ref_round/ref_channel` and round `r`, channel `c` for tile `t`.\n    Useful for debugging the `register` section of the pipeline.\n\n    Args:\n        nb: Notebook containing results of the experiment. Must contain `find_spots` page.\n            If contains `register_initial` and/or `register` pages, then transform from these will be used.\n        t: tile interested in.\n        r: Want to find the transform between the reference round and this round.\n        c: Want to find the transform between the reference channel and this channel.\n    \"\"\"\n    config = nb.get_config()['register']\n    if nb.basic_info.is_3d:\n        neighb_dist_thresh = config['neighb_dist_thresh_3d']\n    else:\n        neighb_dist_thresh = config['neighb_dist_thresh_2d']\n    z_scale = [1, 1, nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy]\n    point_clouds = []\n    # 1st point cloud is imaging one as does not change\n    point_clouds = point_clouds + [spot_yxz(nb.find_spots.spot_details, t, r, c)]\n    # only keep isolated spots, those whose second neighbour is far away\n    # Do this only for imaging point cloud as that is what is done in pipeline/register\n    isolated = get_isolated_points(point_clouds[0] * z_scale, 2 * neighb_dist_thresh)\n    point_clouds[0] = point_clouds[0][isolated]\n\n    # 2nd is untransformed reference point cloud\n    r_ref = nb.basic_info.ref_round\n    c_ref = nb.basic_info.ref_channel\n    point_clouds = point_clouds + [spot_yxz(nb.find_spots.spot_details, t, r_ref, c_ref)]\n    z_scale = z_scale[2]\n\n    # Add shifted reference point cloud\n    if nb.has_page('register_initial'):\n        shift = nb.register_initial.shift[t, r]\n    else:\n        shift = view_register_search(nb, t, r, return_shift=True)\n    point_clouds = point_clouds + [point_clouds[1] + shift]\n\n    # Add reference point cloud transformed by an affine transform\n    transform_outlier = None\n    if nb.has_page('register'):\n        transform = nb.register.transform[t, r, c]\n        if nb.has_page('register_debug'):\n            # If particular tile/round/channel was found by regularised least squares\n            transform_outlier = nb.register_debug.transform_outlier[t, r, c]\n            if np.abs(transform_outlier).max() == 0:\n                transform_outlier = None\n    else:\n        start_transform = np.eye(4, 3)  # no scaling just shift to start off icp\n        start_transform[3] = shift * [1, 1, z_scale]\n        transform = get_single_affine_transform(point_clouds[1], point_clouds[0], z_scale, z_scale,\n                                                start_transform, neighb_dist_thresh, nb.basic_info.tile_centre,\n                                                config['n_iter'])[0]\n\n    if not nb.basic_info.is_3d:\n        # use numpy not jax.numpy as reading in tiff is done in numpy.\n        tile_sz = np.array([nb.basic_info.tile_sz, nb.basic_info.tile_sz, 1], dtype=np.int16)\n    else:\n        tile_sz = np.array([nb.basic_info.tile_sz, nb.basic_info.tile_sz, nb.basic_info.nz], dtype=np.int16)\n\n    if transform_outlier is not None:\n        point_clouds = point_clouds + [apply_transform(point_clouds[1], transform_outlier, nb.basic_info.tile_centre,\n                                                       z_scale, tile_sz)[0]]\n\n    point_clouds = point_clouds + [apply_transform(point_clouds[1], transform, nb.basic_info.tile_centre, z_scale,\n                                                   tile_sz)[0]]\n    pc_labels = [f'Imaging: r{r}, c{c}', f'Reference: r{r_ref}, c{c_ref}', f'Reference: r{r_ref}, c{c_ref} - Shift',\n                 f'Reference: r{r_ref}, c{c_ref} - Affine']\n    if transform_outlier is not None:\n        pc_labels = pc_labels + [f'Reference: r{r_ref}, c{c_ref} - Regularized']\n    view_point_clouds(point_clouds, pc_labels, neighb_dist_thresh, z_scale,\n                      f'Transform of tile {t} to round {r}, channel {c}')\n    plt.show()\n</code></pre>"},{"location":"code/plot/register/#view_icp_reg","title":"<code>view_icp_reg</code>","text":"<p>Function to plot how regularisation changes the affine transform found through iterative closest point between <code>ref_round/ref_channel</code> and round \\(r\\), channel \\(c\\) for tile \\(t\\).</p> <p>Useful for finding suitable values for <code>config['register']['regularize_constant']</code> and <code>config['register']['regularize_factor']</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing results of the experiment. Must contain <code>find_spots</code> page. Must contain <code>register_initial</code>/<code>register_debug</code> pages if <code>start_transform</code>/<code>reg_transform</code> not specified.</p> required <code>t</code> <code>int</code> <p>tile interested in.</p> required <code>r</code> <code>int</code> <p>Want to find the transform between the reference round and this round.</p> required <code>c</code> <code>int</code> <p>Want to find the transform between the reference channel and this channel.</p> required <code>reg_constant</code> <code>Optional[List]</code> <p><code>int [n_reg]</code> Constant used when doing regularized least squares. Will be a point cloud produced for affine transformed produced with each of these parameters. Value will be indicated by \\(\\lambda\\) in legend/buttons. If <code>None</code>, will use <code>config['register']['regularize_constant']</code>.</p> <code>None</code> <code>reg_factor</code> <code>Optional[List]</code> <p><code>float [n_reg]</code> Factor to boost rotation/scaling term in regularized least squares. Must be same length as <code>reg_constant</code>. Value will be indicated by \\(\\mu\\) in legend/buttons. If <code>None</code>, will use <code>config['register']['regularize_factor']</code>.</p> <p>The regularized term in the loss function for finding the transform is:</p> <p>\\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\)</p> <p>Where:</p> <ul> <li>\\(D_{scale}^2\\) is the squared distance between <code>transform[:3, :]</code> and <code>reg_transform[:3, :]</code>. I.e. the squared difference of the scaling/rotation part of the transform from the target.</li> <li>\\(D_{shift}^2\\) is the squared distance between <code>transform[3]</code> and <code>reg_transform[3]</code>. I.e. the squared difference of the shift part of the transform from the target.</li> <li>\\(\\lambda\\) is <code>reg_constant</code> - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\).</li> <li>\\(\\mu\\) is <code>reg_factor</code> - the larger it is, the smaller \\(D_{scale}^2\\).</li> </ul> <code>None</code> <code>reg_transform</code> <code>Optional[np.ndarray]</code> <p>Transform to regularize towards i.e. the expected transform. If not specified, will use average transformation based on <code>nb.register_debug.av_scaling[c]</code> and <code>nb.register_debug.av_shifts[t, r]</code> with no rotation. This is the same as what is used in <code>coppafish.register.icp</code>.</p> <code>None</code> <code>start_transform</code> <code>Optional[np.ndarray]</code> <p>Initial transform to use as starting point for ICP. If <code>None</code>, will use <code>nb.register_initial.shift[t, r]</code> for the non-regularized case and <code>reg_transform</code> for the regularized case to match method used in <code>coppafish.register.icp</code>.</p> <code>None</code> <code>plot_residual</code> <code>bool</code> <p>If <code>True</code>, will run plot_reg_residual as well.</p> <code>False</code> Source code in <code>coppafish/plot/register/icp.py</code> <pre><code>def view_icp_reg(nb: Notebook, t: int, r: int, c: int, reg_constant: Optional[List] = None,\n                 reg_factor: Optional[List] = None, reg_transform: Optional[np.ndarray] = None,\n                 start_transform: Optional[np.ndarray] = None, plot_residual: bool = False):\n\"\"\"\n    Function to plot how regularisation changes the affine transform found through iterative closest point between\n    `ref_round/ref_channel` and round $r$, channel $c$ for tile $t$.\n\n    Useful for finding suitable values for `config['register']['regularize_constant']`\n    and `config['register']['regularize_factor']`.\n\n\n    Args:\n        nb: Notebook containing results of the experiment. Must contain `find_spots` page.\n            Must contain `register_initial`/`register_debug` pages if `start_transform`/`reg_transform` not specified.\n        t: tile interested in.\n        r: Want to find the transform between the reference round and this round.\n        c: Want to find the transform between the reference channel and this channel.\n        reg_constant: `int [n_reg]`\n            Constant used when doing regularized least squares.\n            Will be a point cloud produced for affine transformed produced with each of these parameters.\n            Value will be indicated by $\\lambda$ in legend/buttons.\n            If `None`, will use `config['register']['regularize_constant']`.\n        reg_factor: `float [n_reg]`\n            Factor to boost rotation/scaling term in regularized least squares.\n            Must be same length as `reg_constant`.\n            Value will be indicated by $\\mu$ in legend/buttons.\n            If `None`, will use `config['register']['regularize_factor']`.\n\n            The regularized term in the loss function for finding the transform is:\n\n            $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$\n\n            Where:\n\n            * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`.\n            I.e. the squared difference of the scaling/rotation part of the transform from the target.\n            * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`.\n            I.e. the squared difference of the shift part of the transform from the target.\n            * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$.\n            * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$.\n        reg_transform: Transform to regularize towards i.e. the expected transform.\n            If not specified, will use average transformation based on `nb.register_debug.av_scaling[c]`\n            and `nb.register_debug.av_shifts[t, r]` with no rotation.\n            This is the same as what is used in `coppafish.register.icp`.\n        start_transform: Initial transform to use as starting point for ICP.\n            If `None`, will use `nb.register_initial.shift[t, r]` for the non-regularized case\n            and `reg_transform` for the regularized case to match method used in `coppafish.register.icp`.\n        plot_residual: If `True`, will run plot_reg_residual as well.\n    \"\"\"\n    config = nb.get_config()['register']\n    n_iter = config['n_iter']\n    if nb.basic_info.is_3d:\n        neighb_dist_thresh = config['neighb_dist_thresh_3d']\n    else:\n        neighb_dist_thresh = config['neighb_dist_thresh_2d']\n    z_scale = [1, 1, nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy]\n    if not nb.basic_info.is_3d:\n        # use numpy not jax.numpy as reading in tiff is done in numpy.\n        tile_sz = np.array([nb.basic_info.tile_sz, nb.basic_info.tile_sz, 1], dtype=np.int16)\n    else:\n        tile_sz = np.array([nb.basic_info.tile_sz, nb.basic_info.tile_sz, nb.basic_info.nz], dtype=np.int16)\n    point_clouds = []\n    # 1st point cloud is imaging one as does not change\n    point_clouds = point_clouds + [spot_yxz(nb.find_spots.spot_details, t, r, c)]\n    # only keep isolated spots, those whose second neighbour is far away\n    # Do this only for imaging point cloud as that is what is done in pipeline/register\n    isolated = get_isolated_points(point_clouds[0] * z_scale, 2 * neighb_dist_thresh)\n    point_clouds[0] = point_clouds[0][isolated]\n\n    # 2nd is untransformed reference point cloud\n    r_ref = nb.basic_info.ref_round\n    c_ref = nb.basic_info.ref_channel\n    point_clouds = point_clouds + [spot_yxz(nb.find_spots.spot_details, t, r_ref, c_ref)]\n    z_scale = z_scale[2]\n\n    # 3rd is ref point cloud transformed according to affine transform with no regularisation\n    if start_transform is None:\n        # no scaling just shift to start off icp as used in pipeline if no start_transform given\n        shift = nb.register_initial.shift[t, r]\n        start_transform = np.eye(4, 3)\n        start_transform[3] = shift * [1, 1, z_scale]\n    transform_no_reg = get_single_affine_transform(point_clouds[1], point_clouds[0], z_scale, z_scale,\n                                                   start_transform, neighb_dist_thresh, nb.basic_info.tile_centre,\n                                                   n_iter)[0]\n    point_clouds = point_clouds + [apply_transform(point_clouds[1], transform_no_reg, nb.basic_info.tile_centre, z_scale,\n                                                   tile_sz)[0]]\n\n    # 4th is ref point cloud transformed according to target reg transform\n    if reg_transform is None:\n        # If reg transform not specified, use av scaling for c and av shift for t, r.\n        # Also set start_transform to reg_transform if not specified\n        # This is same as what is done in `coppafish.register.base.icp` for t/r/c where regularisation required.\n        reg_transform = np.eye(4, 3) * nb.register_debug.av_scaling[c]\n        reg_transform[3] = nb.register_debug.av_shifts[t, r]\n        start_transform = reg_transform.copy()\n    point_clouds = point_clouds + [apply_transform(point_clouds[1], reg_transform, nb.basic_info.tile_centre, z_scale,\n                                                   tile_sz)[0]]\n    pc_labels = [f'Imaging: r{r}, c{c}', f'Reference: r{r_ref}, c{c_ref}',\n                 r'$\\lambda=0$', r'$\\lambda=\\infty$']\n\n    # Now add ref point cloud transformed according to reg transform found with all reg params given.\n    # If None given, use default params.\n    if reg_constant is None:\n        reg_constant = [config['regularize_constant']]\n    if reg_factor is None:\n        reg_factor = [config['regularize_factor']]\n\n    n_reg = len(reg_constant)\n    if n_reg != len(reg_factor):\n        raise ValueError(f\"reg_constant and reg_factor need to be same size but they \"\n                         f\"have {n_reg} and {len(reg_factor)} values respectively\")\n    transforms = np.zeros((n_reg, 4, 3))\n    for i in range(n_reg):\n        # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant.\n        reg_constant_scale = np.sqrt(0.5 * reg_constant[i] * reg_factor[i])\n        reg_constant_shift = np.sqrt(0.5 * reg_constant[i])\n        transforms[i] = \\\n            get_single_affine_transform(point_clouds[1], point_clouds[0], z_scale, z_scale, start_transform,\n                                        neighb_dist_thresh, nb.basic_info.tile_centre, n_iter,\n                                        reg_constant_scale, reg_constant_shift, reg_transform)[0]\n        point_clouds += [apply_transform(point_clouds[1], transforms[i], nb.basic_info.tile_centre,\n                                         z_scale, tile_sz)[0]]\n        if reg_constant[i] &gt; 1000:\n            pc_labels += [r'$\\lambda={:.0E},\\mu={:.0E}$'.format(int(reg_constant[i]), int(reg_factor[i]))]\n        else:\n            pc_labels += [r'$\\lambda={:.0f},\\mu={:.0E}$'.format(int(reg_constant[i]), int(reg_factor[i]))]\n\n    vpc = view_point_clouds(point_clouds, pc_labels, neighb_dist_thresh, z_scale,\n                            f'Regularized transform of tile {t} to round {r}, channel {c}')\n    n_matches_reg_target = np.sum(vpc.neighb[3] &gt;= 0)\n\n    if plot_residual and n_reg &gt; 1:\n        plot_reg_residual(reg_transform, transforms, reg_constant, reg_factor, transform_no_reg, n_matches_reg_target)\n    else:\n        plt.show()\n</code></pre>"},{"location":"code/plot/register/#plot_reg_residual","title":"<code>plot_reg_residual</code>","text":"<p>This shows how changing the regularization parameters affect how close the affine transform is to that which it was being regularized towards. E.g. it should show that the larger <code>reg_constant</code>, the smaller the difference (y-axis values in the plots).</p> <p>There will be up to 4 plots, in each, the different colors refer to the different <code>reg_constant</code>/<code>reg_factor</code> combinations and the smaller the y-axis value, the closer the transform is to <code>reg_transform</code>. The different axis variables in the plot are explained in the <code>reg_factor</code> variable description below.</p> <p>Parameters:</p> Name Type Description Default <code>reg_transform</code> <code>np.ndarray</code> <p><code>float [4 x 3]</code> Transform which was regularized towards i.e. the expected transform.</p> required <code>transforms_plot</code> <code>List[np.ndarray]</code> <p><code>[n_reg]</code>. <code>transforms_plot[i]</code> is the <code>[4 x 3]</code> transform found with regularization parameters <code>reg_constant[i]</code> and <code>reg_factor[i]</code>.</p> required <code>reg_constant</code> <code>List</code> <p><code>int [n_reg]</code> Constant used when doing regularized least squares. Value will be indicated by \\(\\lambda\\) on x-axis.</p> required <code>reg_factor</code> <code>List</code> <p><code>float [n_reg]</code> Factor to boost rotation/scaling term in regularized least squares. Must be same length as <code>reg_constant</code>. Value will be indicated by \\(\\mu\\) on x-axis.</p> <p>The regularized term in the loss function for finding the transform is:</p> <p>\\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\)</p> <p>Where:</p> <ul> <li>\\(D_{scale}^2\\) is the squared distance between <code>transform[:3, :]</code> and <code>reg_transform[:3, :]</code>. I.e. the squared difference of the scaling/rotation part of the transform from the target.</li> <li>\\(D_{shift}^2\\) is the squared distance between <code>transform[3]</code> and <code>reg_transform[3]</code>. I.e. the squared difference of the shift part of the transform from the target.</li> <li>\\(\\lambda\\) is <code>reg_constant</code> - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\).</li> <li>\\(\\mu\\) is <code>reg_factor</code> - the larger it is, the smaller \\(D_{scale}^2\\).</li> </ul> required <code>transform_no_reg</code> <code>Optional[np.ndarray]</code> <p><code>float [4 x 3]</code>. Transform found with no regularization. If given, will show white line labelled by \\(\\lambda=0\\) with y-axis value indicating value with no regularization.</p> <code>None</code> <code>n_matches</code> <code>Optional[int]</code> <p>Number of nearest neighbours found for <code>reg_transform</code>. If given, will show white line labelled by \\(n_{matches}\\) with x-axis value equal to this on the plots where \\(\\lambda\\) is the x-axis variable. This is because we expect the regularization should have more of an effect if <code>reg_constant &gt; n_matches</code>, i.e. the y-axis variable should go towards zero at x-axis values beyond this line.</p> <code>None</code> Source code in <code>coppafish/plot/register/icp.py</code> <pre><code>def plot_reg_residual(reg_transform: np.ndarray, transforms_plot: List[np.ndarray],\n                      reg_constant: List, reg_factor: List, transform_no_reg: Optional[np.ndarray] = None,\n                      n_matches: Optional[int] = None):\n\"\"\"\n    This shows how changing the regularization parameters affect how close the affine transform\n    is to that which it was being regularized towards. E.g. it should show that\n    the larger `reg_constant`, the smaller the difference (y-axis values in the plots).\n\n    There will be up to 4 plots, in each, the different colors refer to the different\n    `reg_constant`/`reg_factor` combinations and the smaller the y-axis value,\n    the closer the transform is to `reg_transform`. The different axis variables in the plot are\n    explained in the `reg_factor` variable description below.\n\n    Args:\n        reg_transform: `float [4 x 3]`\n            Transform which was regularized towards i.e. the expected transform.\n        transforms_plot: `[n_reg]`.\n            `transforms_plot[i]` is the `[4 x 3]` transform found with regularization parameters\n            `reg_constant[i]` and `reg_factor[i]`.\n        reg_constant: `int [n_reg]`\n            Constant used when doing regularized least squares.\n            Value will be indicated by $\\lambda$ on x-axis.\n        reg_factor: `float [n_reg]`\n            Factor to boost rotation/scaling term in regularized least squares.\n            Must be same length as `reg_constant`.\n            Value will be indicated by $\\mu$ on x-axis.\n\n            The regularized term in the loss function for finding the transform is:\n\n            $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$\n\n            Where:\n\n            * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`.\n            I.e. the squared difference of the scaling/rotation part of the transform from the target.\n            * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`.\n            I.e. the squared difference of the shift part of the transform from the target.\n            * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$.\n            * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$.\n        transform_no_reg: `float [4 x 3]`.\n            Transform found with no regularization. If given, will show white line\n            labelled by $\\lambda=0$ with y-axis value indicating value with no regularization.\n        n_matches: Number of nearest neighbours found for `reg_transform`.\n            If given, will show white line labelled by $n_{matches}$ with x-axis value\n            equal to this on the plots where $\\lambda$ is the x-axis variable.\n            This is because we expect the regularization should have more of an effect if `reg_constant &gt; n_matches`,\n            i.e. the y-axis variable should go towards zero at x-axis values beyond this line.\n    \"\"\"\n    n_reg = len(reg_constant)\n    col_info = {}\n    col_ind = 0\n    if len(np.unique(reg_constant)) &gt; 1:\n        col_info[col_ind] = {}\n        if len(np.unique(reg_factor)) == 1:\n            col_info[col_ind]['title'] = r\"Varying $\\lambda$ ($\\mu = {:.0E}$)\".format(int(reg_factor[0]))\n        else:\n            col_info[col_ind]['title'] = f\"Varying $\\lambda$\"\n        col_info[col_ind]['x_label'] = r\"$\\lambda$\"\n        col_info[col_ind]['x'] = reg_constant\n        col_info[col_ind]['x_lims'] = [int(0.9 * np.min(reg_constant)), int(1.1 * np.max(reg_constant))]\n        if n_matches is not None:\n            col_info[col_ind]['x_lims'] = [int(0.9 * np.min(list(reg_constant) + [n_matches])),\n                                           int(1.1 * np.max(list(reg_constant) + [n_matches]))]\n        col_info[col_ind]['log_x'] = np.ptp(col_info[col_ind]['x_lims']) &gt; 100\n        col_info[col_ind]['n_matches'] = n_matches\n        col_ind += 1\n    if len(np.unique(reg_factor)) &gt; 1:\n        col_info[col_ind] = {}\n        if len(np.unique(reg_constant)) == 1:\n            col_info[col_ind]['title'] = rf\"Varying $\\mu$ ($\\lambda = {int(reg_constant[0])}$)\"\n        else:\n            col_info[col_ind]['title'] = f\"Varying $\\mu$\"\n        col_info[col_ind]['x_label'] = r\"$\\mu$\"\n        col_info[col_ind]['x'] = reg_factor\n        col_info[col_ind]['x_lims'] = [int(0.9 * np.min(reg_factor)), int(1.1 * np.max(reg_factor))]\n        col_info[col_ind]['log_x'] = np.ptp(col_info[col_ind]['x_lims']) &gt; 100\n        col_info[col_ind]['n_matches'] = None\n\n    if n_reg != len(reg_factor):\n        raise ValueError(f\"reg_constant and reg_factor need to be same size but they \"\n                         f\"have {n_reg} and {len(reg_factor)} values respectively\")\n    if n_reg != len(transforms_plot):\n        raise ValueError(f\"reg_constant and transforms_plot need to be same size but they \"\n                         f\"have {n_reg} and {len(transforms_plot)} values respectively\")\n    if len(col_info) == 0:\n        raise ValueError(\"Not enough data to plot\")\n\n    y0 = [np.linalg.norm(transforms_plot[i][:3, :]-reg_transform[:3, :]) for i in range(n_reg)]\n    y1 = [np.linalg.norm(transforms_plot[i][3] - reg_transform[3]) for i in range(n_reg)]\n    if transform_no_reg is not None:\n        y0_no_reg = np.linalg.norm(transform_no_reg[:3, :]-reg_transform[:3,:])\n        y1_no_reg = np.linalg.norm(transform_no_reg[3] - reg_transform[3])\n        y0_lim = [np.min(y0 + [y0_no_reg]) - 0.0002, np.max(y0 + [y0_no_reg]) + 0.0002]\n        y1_lim = [np.min(y1 + [y1_no_reg]) - 0.2, np.max(y1 + [y1_no_reg]) + 0.2]\n    else:\n        y0_lim = [np.min(y0) - 0.0002, np.max(y0) + 0.0002]\n        y1_lim = [np.min(y1) - 0.2, np.max(y1) + 0.2]\n    y0_lim = np.clip(y0_lim, 0, np.inf)\n    y1_lim = np.clip(y1_lim, 0, np.inf)\n    colors = distinctipy.get_colors(n_reg, [(0, 0, 0), (1, 1, 1)])\n    fig, ax = plt.subplots(2, len(col_info), figsize=(15, 7))\n    if len(col_info) == 1:\n        ax = ax[:, np.newaxis]\n    for i in range(len(col_info)):\n        if transform_no_reg is not None:\n            ax[0, i].plot(col_info[i]['x_lims'], [y0_no_reg, y0_no_reg], 'w:')\n            ax[0, i].text(np.mean(col_info[i]['x_lims']), y0_no_reg, r\"$\\lambda = 0$\",\n                          va='bottom', ha=\"center\", color='w')\n            ax[1, i].plot(col_info[i]['x_lims'], [y1_no_reg, y1_no_reg], 'w:')\n            ax[1, i].text(np.mean(col_info[i]['x_lims']), y1_no_reg, r\"$\\lambda = 0$\",\n                          va='bottom', ha=\"center\", color='w')\n        if col_info[i]['n_matches'] is not None:\n            ax[0, i].plot([n_matches, n_matches], y0_lim, 'w:')\n            ax[0, i].text(n_matches, np.percentile(y0_lim, 10), r\"$n_{matches}$\",\n                          va='bottom', ha=\"right\", color='w', rotation=90)\n            ax[1, i].plot([n_matches, n_matches], y1_lim, 'w:')\n            ax[1, i].text(n_matches, np.percentile(y1_lim, 10), r\"$n_{matches}$\",\n                          va='bottom', ha=\"right\", color='w', rotation=90)\n        ax[0, i].scatter(col_info[i]['x'], y0, color=colors)\n        ax[1, i].scatter(col_info[i]['x'], y1, color=colors)\n        ax[0, i].get_shared_x_axes().join(ax[0, i], ax[1, i])\n        ax[1, i].set_xlabel(col_info[i]['x_label'])\n        ax[0, i].set_title(col_info[i]['title'])\n        if col_info[i]['log_x']:\n            ax[1, i].set_xscale('log')\n        ax[1, i].set_xlim(col_info[i]['x_lims'])\n        if i == 1:\n            ax[0, 0].get_shared_y_axes().join(ax[0, 0], ax[0, 1])\n            ax[1, 0].get_shared_y_axes().join(ax[1, 0], ax[1, 1])\n        ax[0, 0].set_ylim(y0_lim)\n        ax[0, 0].set_ylabel(\"$D_{scale}$\")\n        ax[1, 0].set_ylim(y1_lim)\n        ax[1, 0].set_ylabel(\"$D_{shift}$\")\n    fig.suptitle(\"How varying regularization parameters affects how similar transform found is to target transform\")\n    plt.show()\n</code></pre>"},{"location":"code/plot/stitch/","title":"Stitch","text":""},{"location":"code/plot/stitch/#shift","title":"Shift","text":""},{"location":"code/plot/stitch/#view_stitch_search","title":"<code>view_stitch_search</code>","text":"<p>Function to plot results of exhaustive search to find overlap between tile <code>t</code> and its neighbours. Useful for debugging the <code>stitch</code> section of the pipeline. White in the color plot refers to the value of <code>score_thresh</code> for this search.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing results of the experiment. Must contain <code>find_spots</code> page.</p> required <code>t</code> <code>int</code> <p>Want to look at overlap between tile <code>t</code> and its north/east neighbour.</p> required <code>direction</code> <code>Optional[str]</code> <p>Direction of overlap interested in - either <code>'south'</code>/<code>'north'</code> or <code>'west'</code>/<code>'east'</code>. If <code>None</code>, then will look at both directions.</p> <code>None</code> Source code in <code>coppafish/plot/stitch/shift.py</code> <pre><code>def view_stitch_search(nb: Notebook, t: int, direction: Optional[str] = None):\n\"\"\"\n    Function to plot results of exhaustive search to find overlap between tile `t` and its neighbours.\n    Useful for debugging the `stitch` section of the pipeline.\n    White in the color plot refers to the value of `score_thresh` for this search.\n\n    Args:\n        nb: Notebook containing results of the experiment. Must contain `find_spots` page.\n        t: Want to look at overlap between tile `t` and its north/east neighbour.\n        direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`.\n            If `None`, then will look at both directions.\n    \"\"\"\n    # NOTE that directions should actually be 'north' and 'east'\n    if direction is None:\n        directions = ['south', 'west']\n    elif direction.lower() == 'south' or direction.lower() == 'west':\n        directions = [direction.lower()]\n    elif direction.lower() == 'north':\n        directions = ['south']\n    elif direction.lower() == 'east':\n        directions = ['west']\n    else:\n        raise ValueError(f\"direction must be either 'south' or 'west' but {direction} given.\")\n    direction_label = {'south': 'north', 'west': 'east'}  # label refers to actual direction\n\n    config = nb.get_config()['stitch']\n    # determine shifts to search over\n    shifts = get_shifts_to_search(config, nb.basic_info)\n    if not nb.basic_info.is_3d:\n        config['nz_collapse'] = None\n        config['shift_widen'][2] = 0  # so don't look for shifts in z direction\n        config['shift_max_range'][2] = 0\n\n    # find shifts between overlapping tiles\n    c = nb.basic_info.ref_channel\n    r = nb.basic_info.ref_round\n    t_neighb = {'south': [], 'west': []}\n    z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy\n\n    # align to south neighbour followed by west neighbour\n    t_neighb['south'] = np.where(np.sum(nb.basic_info.tilepos_yx == nb.basic_info.tilepos_yx[t, :] + [1, 0],\n                                        axis=1) == 2)[0]\n    t_neighb['west'] = np.where(np.sum(nb.basic_info.tilepos_yx == nb.basic_info.tilepos_yx[t, :] + [0, 1],\n                                       axis=1) == 2)[0]\n    fig = []\n    for j in directions:\n        if t_neighb[j] in nb.basic_info.use_tiles:\n            print(f'Finding shift between tiles {t} and {t_neighb[j][0]} ({direction_label[j]} overlap)')\n            shift, score, score_thresh, debug_info = \\\n                compute_shift(spot_yxz(nb.find_spots.spot_details, t, r, c),\n                              spot_yxz(nb.find_spots.spot_details, t_neighb[j][0], r, c),\n                              config['shift_score_thresh'],\n                              config['shift_score_thresh_multiplier'],\n                              config['shift_score_thresh_min_dist'],\n                              config['shift_score_thresh_max_dist'],\n                              config['neighb_dist_thresh'], shifts[j]['y'],\n                              shifts[j]['x'], shifts[j]['z'],\n                              config['shift_widen'], config['shift_max_range'],\n                              z_scale, config['nz_collapse'],\n                              config['shift_step'][2])\n            title = f'Overlap between t={t} and neighbor in {direction_label[j]} (t={t_neighb[j][0]}). ' \\\n                    f'YXZ Shift = {shift}.'\n            fig = fig + [view_shifts(debug_info['shifts_2d'], debug_info['scores_2d'], debug_info['shifts_3d'],\n                                     debug_info['scores_3d'], shift, debug_info['min_score_2d'],\n                                     debug_info['shift_2d_initial'],\n                                     score_thresh, debug_info['shift_thresh'], config['shift_score_thresh_min_dist'],\n                                     config['shift_score_thresh_max_dist'], title, False)]\n    if len(fig) &gt; 0:\n        plt.show()\n    else:\n        warnings.warn(f\"Tile {t} has no overlapping tiles in nb.basic_info.use_tiles.\")\n</code></pre>"},{"location":"code/plot/stitch/#view_shifts","title":"<code>view_shifts</code>","text":"<p>Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts applied to one of them. I.e. you can use this to view the output from <code>coppafish/stitch/shift/compute_shift</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>shifts_2d</code> <code>np.ndarray</code> <p><code>int [n_shifts_2d x 2]</code>. <code>shifts_2d[i]</code> is the yx shift which achieved <code>scores_2d[i]</code> when considering just yx shift between point clouds. I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then applying a yx shift to these planes.</p> required <code>scores_2d</code> <code>np.ndarray</code> <p><code>float [n_shifts_2d]</code>. <code>scores_2d[i]</code> is the score corresponding to <code>shifts_2d[i]</code>. It is approximately the number of neighbours between the two point clouds after the shift was applied.</p> required <code>shifts_3d</code> <code>Optional[np.ndarray]</code> <p><code>int [n_shifts_3d x 3]</code>. <code>shifts_3d[i]</code> is the yxz shift which achieved <code>scores_3d[i]</code> when considering the yxz shift between point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels. If None, only 2D image plotted.</p> <code>None</code> <code>scores_3d</code> <code>Optional[np.ndarray]</code> <p><code>float [n_shifts_3d]</code>. <code>scores_3d[i]</code> is the score corresponding to <code>shifts_3d[i]</code>. It is approximately the number of neighbours between the two point clouds after the shift was applied.</p> <code>None</code> <code>best_shift</code> <code>Optional[np.ndarray]</code> <p><code>int [y_shift, x_shift, z_shift]</code>. Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels. Will be plotted as black cross on image if provided.</p> <code>None</code> <code>score_thresh_2d</code> <code>Optional[float]</code> <p>Threshold returned by <code>compute_shift</code> function for 2d calculation, if <code>score</code> is above this, it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding to <code>score_thresh_2d</code> in the 2D plot</p> <code>None</code> <code>best_shift_initial</code> <code>Optional[np.ndarray]</code> <p><code>int [y_shift, x_shift]</code>. Best yx shift found by in first search of algorithm. I.e. <code>score_thresh</code> computation based on this. Will show as green x if given.</p> <code>None</code> <code>score_thresh_3d</code> <code>Optional[float]</code> <p>Threshold returned by <code>compute_shift</code> function for 3d calculation. If given, a red-white-blue colorbar will be used with white corresponding to <code>score_thresh_3d</code> in the 3D plots.</p> <code>None</code> <code>thresh_shift</code> <code>Optional[np.ndarray]</code> <p><code>int [y_shift, x_shift, z_shift]</code>. yx shift corresponding to <code>score_thresh</code>. Will show as green + in both 2D and 3D plots if given.</p> <code>None</code> <code>thresh_min_dist</code> <code>Optional[int]</code> <p><code>shift_thresh</code> is the shift with the max score in an annulus a distance between <code>thresh_min_dist</code> and <code>thresh_max_dist</code> away from <code>best_shift_initial</code>. Annulus will be shown in green if given.</p> <code>None</code> <code>thresh_max_dist</code> <code>Optional[int]</code> <p><code>shift_thresh</code> is the shift with the max score in an annulus a distance between <code>thresh_min_dist</code> and <code>thresh_max_dist</code> away from <code>best_shift_initial</code>. Annulus will be shown in green if given.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>Title to show.</p> <code>None</code> <code>show</code> <code>bool</code> <p>If <code>True</code>, will call <code>plt.show()</code>, else will <code>return fig</code>.</p> <code>True</code> Source code in <code>coppafish/plot/stitch/shift.py</code> <pre><code>def view_shifts(shifts_2d: np.ndarray, scores_2d: np.ndarray, shifts_3d: Optional[np.ndarray] = None,\n                scores_3d: Optional[np.ndarray] = None, best_shift: Optional[np.ndarray] = None,\n                score_thresh_2d: Optional[float] = None, best_shift_initial: Optional[np.ndarray] = None,\n                score_thresh_3d: Optional[float] = None, thresh_shift: Optional[np.ndarray] = None,\n                thresh_min_dist: Optional[int] = None,\n                thresh_max_dist: Optional[int] = None, title: Optional[str] = None, show: bool = True):\n\"\"\"\n    Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts\n    applied to one of them. I.e. you can use this to view the output from\n    `coppafish/stitch/shift/compute_shift` function.\n\n    Args:\n        shifts_2d: `int [n_shifts_2d x 2]`.\n            `shifts_2d[i]` is the yx shift which achieved `scores_2d[i]` when considering just yx shift between\n            point clouds.\n            I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then\n            applying a yx shift to these planes.\n        scores_2d: `float [n_shifts_2d]`.\n            `scores_2d[i]` is the score corresponding to `shifts_2d[i]`. It is approximately the number of neighbours\n            between the two point clouds after the shift was applied.\n        shifts_3d: `int [n_shifts_3d x 3]`.\n            `shifts_3d[i]` is the yxz shift which achieved `scores_3d[i]` when considering the yxz shift between\n            point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels.\n            If None, only 2D image plotted.\n        scores_3d: `float [n_shifts_3d]`.\n            `scores_3d[i]` is the score corresponding to `shifts_3d[i]`. It is approximately the number of neighbours\n            between the two point clouds after the shift was applied.\n        best_shift: `int [y_shift, x_shift, z_shift]`.\n            Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels.\n            Will be plotted as black cross on image if provided.\n        score_thresh_2d: Threshold returned by `compute_shift` function for 2d calculation, if `score` is above this,\n            it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding\n            to `score_thresh_2d` in the 2D plot\n        best_shift_initial: `int [y_shift, x_shift]`.\n            Best yx shift found by in first search of algorithm. I.e. `score_thresh` computation based on this.\n            Will show as green x if given.\n        score_thresh_3d: Threshold returned by `compute_shift` function for 3d calculation. If given, a red-white-blue\n            colorbar will be used with white corresponding to `score_thresh_3d` in the 3D plots.\n        thresh_shift: `int [y_shift, x_shift, z_shift]`.\n            yx shift corresponding to `score_thresh`. Will show as green + in both 2D and 3D plots if given.\n        thresh_min_dist: `shift_thresh` is the shift with the max score in an annulus a distance between\n            `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`.\n            Annulus will be shown in green if given.\n        thresh_max_dist: `shift_thresh` is the shift with the max score in an annulus a distance between\n            `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`.\n            Annulus will be shown in green if given.\n        title: Title to show.\n        show: If `True`, will call `plt.show()`, else will `return fig`.\n    \"\"\"\n    image_2d, extent_2d = get_plot_images_from_shifts(np.rint(shifts_2d).astype(int), scores_2d)\n    image_2d = interpolate_array(image_2d, 0)  # replace 0 with nearest neighbor value\n    fig = plt.figure(figsize=(12, 8))\n    if score_thresh_2d is None:\n        score_thresh_2d = (image_2d.min() + image_2d.max()) / 2\n        cmap_2d = 'virids'\n    else:\n        cmap_2d = 'bwr'\n    v_max = np.max([image_2d.max(), 1.2 * score_thresh_2d])\n    v_min = image_2d.min()\n    if cmap_2d == 'bwr':\n        cmap_extent = np.max([v_max - score_thresh_2d, score_thresh_2d - v_min])\n        # Have equal range above and below score_thresh to not skew colormap\n        v_min = score_thresh_2d - cmap_extent\n        v_max = score_thresh_2d + cmap_extent\n    cmap_norm = matplotlib.colors.TwoSlopeNorm(vmin=v_min, vcenter=score_thresh_2d, vmax=v_max)\n    if shifts_3d is not None:\n        images_3d, extent_3d = get_plot_images_from_shifts(np.rint(shifts_3d).astype(int), scores_3d)\n        images_3d = interpolate_array(images_3d, 0)  # replace 0 with nearest neighbor value\n        if score_thresh_3d is None:\n            score_thresh_3d = (images_3d.min() + images_3d.max()) / 2\n            cmap_3d = 'virids'\n        else:\n            cmap_3d = 'bwr'\n        v_max_3d = np.max([images_3d.max(), 1.2 * score_thresh_3d])\n        v_min_3d = images_3d.min()\n        if cmap_3d == 'bwr':\n            cmap_extent = np.max([v_max_3d - score_thresh_3d, score_thresh_3d - v_min_3d])\n            # Have equal range above and below score_thresh to not skew colormap\n            v_min_3d = score_thresh_3d - cmap_extent\n            v_max_3d = score_thresh_3d + cmap_extent\n        cmap_norm_3d = matplotlib.colors.TwoSlopeNorm(vmin=v_min_3d, vcenter=score_thresh_3d, vmax=v_max_3d)\n        n_cols = images_3d.shape[2]\n        if n_cols &gt; 13:\n            # If loads of z-planes, just show the 13 with the largest score\n            n_cols = 13\n            max_score_z = images_3d.max(axis=(0, 1))\n            use_z = np.sort(np.argsort(max_score_z)[::-1][:n_cols])\n        else:\n            use_z = np.arange(n_cols)\n\n        plot_3d_height = int(np.ceil(n_cols / 4))\n        plot_2d_height = n_cols - plot_3d_height\n        ax_2d = plt.subplot2grid(shape=(n_cols, n_cols), loc=(0, 0), colspan=n_cols, rowspan=plot_2d_height)\n        ax_3d = [plt.subplot2grid(shape=(n_cols, n_cols), loc=(plot_2d_height + 1, i), rowspan=plot_3d_height) for i in\n                 range(n_cols)]\n        for i in range(n_cols):\n            # share axes for 3D plots\n            ax_3d[i].get_shared_y_axes().join(ax_3d[i], *ax_3d)\n            ax_3d[i].get_shared_x_axes().join(ax_3d[i], *ax_3d)\n            im_3d = ax_3d[i].imshow(images_3d[:, :, use_z[i]], extent=extent_3d[:4], aspect='auto', cmap=cmap_3d,\n                                    norm=cmap_norm_3d)\n            z_plane = int(np.rint(extent_3d[4] + use_z[i] + 0.5))\n            ax_3d[i].set_title(f'Z = {z_plane}')\n            if thresh_shift is not None and z_plane == thresh_shift[2]:\n                # Indicate threshold shift on correct 3d plot\n                ax_3d[i].plot(thresh_shift[1], thresh_shift[0], '+', color='lime', label='Thresh shift')\n            if i &gt; 0:\n                ax_3d[i].tick_params(labelbottom=False, labelleft=False)\n            if best_shift is not None:\n                if z_plane == best_shift[2]:\n                    ax_3d[i].plot(best_shift[1], best_shift[0], 'kx')\n\n        fig.supxlabel('X')\n        fig.supylabel('Y')\n        ax_3d[0].invert_yaxis()\n        cbar_gap = 0.05\n        cbar_3d_height = plot_3d_height / (plot_3d_height + plot_2d_height)\n        cbar_2d_height = plot_2d_height / (plot_3d_height + plot_2d_height)\n        cbar_ax = fig.add_axes([0.9, 0.07, 0.03, cbar_3d_height - 2*cbar_gap])  # left, bottom, width, height\n        fig.colorbar(im_3d, cax=cbar_ax)\n        cbar_ax.set_ylim(np.clip(v_min_3d, 0, v_max_3d), v_max_3d)\n    else:\n        n_cols = 1\n        ax_2d = plt.subplot2grid(shape=(n_cols, n_cols), loc=(0, 0))\n        ax_2d.set_xlabel('X')\n        ax_2d.set_ylabel('Y')\n\n    im_2d = ax_2d.imshow(image_2d, extent=extent_2d, aspect='auto', cmap=cmap_2d, norm=cmap_norm)\n    if best_shift is not None:\n        ax_2d.plot(best_shift[1], best_shift[0], 'kx', label='Best shift')\n    if best_shift_initial is not None and best_shift_initial != best_shift:\n        ax_2d.plot(best_shift_initial[1], best_shift_initial[0], 'x', color='lime', label='Best shift initial')\n    if thresh_shift is not None:\n        ax_2d.plot(thresh_shift[1], thresh_shift[0], '+', color='lime', label='Thresh shift')\n    if thresh_min_dist is not None and best_shift_initial is not None:\n        ax_2d.add_patch(plt.Circle((best_shift_initial[1], best_shift_initial[0]), thresh_min_dist, color='lime',\n                                   fill=False))\n    if thresh_max_dist is not None and best_shift_initial is not None:\n        ax_2d.add_patch(plt.Circle((best_shift_initial[1], best_shift_initial[0]), thresh_max_dist, color='lime',\n                                   fill=False))\n    ax_2d.invert_yaxis()\n    if title is None:\n        title = 'Approx number of neighbours found for all shifts'\n    ax_2d.set_title(title)\n    ax_2d.legend(facecolor='b')\n    fig.subplots_adjust(left=0.07, right=0.85, bottom=0.07, top=0.95)\n    if shifts_3d is None:\n        cbar_ax = fig.add_axes([0.9, 0.07, 0.03, 0.9])  # left, bottom, width, height\n    else:\n        cbar_ax = fig.add_axes([0.9, 0.07 + cbar_3d_height, 0.03, cbar_2d_height - 2.5 * cbar_gap])\n    fig.colorbar(im_2d, cax=cbar_ax)\n    cbar_ax.set_ylim(np.clip(v_min, 0, v_max), v_max)\n    if show:\n        plt.show()\n    else:\n        return fig\n</code></pre>"},{"location":"code/plot/stitch/#point-clouds","title":"Point Clouds","text":""},{"location":"code/plot/stitch/#view_stitch_overlap","title":"<code>view_stitch_overlap</code>","text":"<p>This plots point clouds of neighbouring tiles with:</p> <ul> <li>No overlap</li> <li>Initial guess at shift using <code>config['stitch']['expected_overlap']</code></li> <li>Overlap determined in stitch stage of the pipeline (using <code>nb.stitch.south_shifts</code> or <code>nb.stitch.west_shifts</code>)</li> <li>Their final global coordinate system positions (using <code>nb.stitch.tile_origin</code>)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>stitch</code> page.</p> required <code>t</code> <code>int</code> <p>Want to look at overlap between tile <code>t</code> and its north or east neighbour.</p> required <code>direction</code> <code>str</code> <p>Direction of overlap interested in - either <code>'south'</code>/<code>'north'</code> or <code>'west'</code>/<code>'east'</code>.</p> <code>'south'</code> Source code in <code>coppafish/plot/stitch/point_clouds.py</code> <pre><code>def view_stitch_overlap(nb: Notebook, t: int, direction: str = 'south'):\n\"\"\"\n    This plots point clouds of neighbouring tiles with:\n\n    * No overlap\n    * Initial guess at shift using `config['stitch']['expected_overlap']`\n    * Overlap determined in stitch stage of the pipeline (using `nb.stitch.south_shifts` or `nb.stitch.west_shifts`)\n    * Their final global coordinate system positions (using `nb.stitch.tile_origin`)\n\n    Args:\n        nb: *Notebook* containing at least `stitch` page.\n        t: Want to look at overlap between tile `t` and its north or east neighbour.\n        direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`.\n    \"\"\"\n    # NOTE that directions should actually be 'north' and 'east'\n    if direction.lower() == 'south' or direction.lower() == 'west':\n        direction = direction.lower()\n    elif direction.lower() == 'north':\n        direction = 'south'\n    elif direction.lower() == 'east':\n        direction = 'west'\n    else:\n        raise ValueError(f\"direction must be either 'south' or 'west' but {direction} given.\")\n\n    direction_label = {'south': 'north', 'west': 'east'}  # label refers to actual direction\n    if direction == 'south':\n        t_neighb = np.where(np.sum(nb.basic_info.tilepos_yx == nb.basic_info.tilepos_yx[t, :] + [1, 0],\n                                   axis=1) == 2)[0]\n        if t_neighb not in nb.basic_info.use_tiles:\n            warnings.warn(f\"Tile {t} has no overlapping tiles in the south direction so changing to west.\")\n            direction = 'west'\n        else:\n            no_overlap_shift = np.array([-nb.basic_info.tile_sz, 0, 0])  # assuming no overlap between tiles\n            found_shift = nb.stitch.south_shifts[np.where(nb.stitch.south_pairs[:,0] == t)[0]][0]\n    if direction == 'west':\n        t_neighb = np.where(np.sum(nb.basic_info.tilepos_yx == nb.basic_info.tilepos_yx[t, :] + [0, 1],\n                                   axis=1) == 2)[0]\n        if t_neighb not in nb.basic_info.use_tiles:\n            raise ValueError(f\"Tile {t} has no overlapping tiles in the west direction.\")\n        no_overlap_shift = np.array([0, -nb.basic_info.tile_sz, 0])  # assuming no overlap between tiles\n        found_shift = nb.stitch.west_shifts[np.where(nb.stitch.west_pairs[:, 0] == t)[0]][0]\n\n    config = nb.get_config()['stitch']\n    t_neighb = t_neighb[0]\n    r = nb.basic_info.ref_round\n    c = nb.basic_info.ref_channel\n    point_clouds = []\n    # add global coordinates of neighbour tile as point cloud that is always present.\n    point_clouds = point_clouds + [spot_yxz(nb.find_spots.spot_details, t_neighb, r, c) +\n                                   nb.stitch.tile_origin[t_neighb]]\n\n    local_yxz_t = spot_yxz(nb.find_spots.spot_details, t, r, c)\n    # Add point cloud for tile t assuming no overlap\n    point_clouds = point_clouds + [local_yxz_t + nb.stitch.tile_origin[t_neighb] + no_overlap_shift]\n    # Add point cloud assuming expected overlap\n    initial_shift = (1-config['expected_overlap']) * no_overlap_shift\n    point_clouds = point_clouds + [local_yxz_t + nb.stitch.tile_origin[t_neighb] + initial_shift]\n    # Add point cloud for tile t with found shift\n    point_clouds = point_clouds + [local_yxz_t + nb.stitch.tile_origin[t_neighb] + found_shift]\n    # Add point cloud for tile t in global coordinate system\n    point_clouds = point_clouds + [local_yxz_t + nb.stitch.tile_origin[t]]\n\n    neighb_dist_thresh = config['neighb_dist_thresh']\n    z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy\n    pc_labels = [f'Tile {t_neighb}', f'Tile {t} - No overlap',\n                 f\"Tile {t} - {int(config['expected_overlap']*100)}% overlap\", f'Tile {t} - Shift', f'Tile {t} - Final']\n    view_point_clouds(point_clouds, pc_labels, neighb_dist_thresh, z_scale,\n                      f'Overlap between tile {t} and tile {t_neighb} in the {direction_label[direction]}')\n    plt.show()\n</code></pre>"},{"location":"code/plot/stitch/#view_stitch","title":"<code>view_stitch</code>","text":"<p>This plots all the reference spots found (<code>ref_round</code>/<code>ref_channel</code>) in the global coordinate system created in the <code>stitch</code> stage of the pipeline.</p> <p>It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre they are closest to). These will be removed in the <code>get_reference_spots</code> step of the pipeline so we don't double count the same spot.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>stitch</code> page.</p> required Source code in <code>coppafish/plot/stitch/point_clouds.py</code> <pre><code>def view_stitch(nb: Notebook):\n\"\"\"\n    This plots all the reference spots found (`ref_round`/`ref_channel`) in the global coordinate system created\n    in the `stitch` stage of the pipeline.\n\n    It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre\n    they are closest to).\n    These will be removed in the `get_reference_spots` step of the pipeline so we don't double count the same spot.\n\n    Args:\n        nb: *Notebook* containing at least `stitch` page.\n    \"\"\"\n    is_ref = np.all((nb.find_spots.spot_details[:, 1] == nb.basic_info.ref_round,\n                     nb.find_spots.spot_details[:, 2] == nb.basic_info.ref_channel), axis=0)\n    local_yxz = nb.find_spots.spot_details[is_ref, -3:]\n    tile = nb.find_spots.spot_details[is_ref, 0]\n    local_yxz = local_yxz[np.isin(tile, nb.basic_info.use_tiles)]\n    tile = tile[np.isin(tile, nb.basic_info.use_tiles)]\n\n    # find duplicate spots as those detected on a tile which is not tile centre they are closest to\n    tile_origin = nb.stitch.tile_origin\n    not_duplicate = get_non_duplicate(tile_origin, nb.basic_info.use_tiles, nb.basic_info.tile_centre,\n                                      local_yxz, tile)\n\n    global_yxz = local_yxz + nb.stitch.tile_origin[tile]\n    global_yxz[:, 2] = np.rint(global_yxz[:, 2])  # make z coordinate an integer\n    config = nb.get_config()['stitch']\n    neighb_dist_thresh = config['neighb_dist_thresh']\n    z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy\n\n    point_clouds = [global_yxz[not_duplicate], global_yxz[np.invert(not_duplicate)]]\n    pc_labels = ['Not Duplicate', 'Duplicate']\n    if nb.has_page('ref_spots'):\n        # Add point cloud indicating those spots that were not saved in ref_spots page\n        # because they were shifted outside the tile bounds on at least one round/channel\n        # and thus spot_color could not be found.\n        local_yxz = local_yxz[not_duplicate]  # Want to find those which were not duplicates but still removed\n        tile = tile[not_duplicate]\n        local_yxz_saved = nb.ref_spots.local_yxz\n        tile_saved = nb.ref_spots.tile\n        local_yxz_saved = local_yxz_saved[np.isin(tile_saved, nb.basic_info.use_tiles)]\n        tile_saved = tile_saved[np.isin(tile_saved, nb.basic_info.use_tiles)]\n        global_yxz_ns = np.zeros((0, 3)) # not saved in ref_spots spots\n        for t in nb.basic_info.use_tiles:\n            missing_ind = -100\n            local_yxz_t = local_yxz[tile == t]\n            removed_ind = np.where(numpy_indexed.indices(local_yxz_saved[tile_saved == t], local_yxz_t,\n                                                         missing=missing_ind) == missing_ind)[0]\n            global_yxz_ns = np.append(global_yxz_ns, local_yxz_t[removed_ind] + nb.stitch.tile_origin[t], axis=0)\n        global_yxz_ns[:, 2] = np.rint(global_yxz_ns[:, 2])\n        point_clouds += [global_yxz_ns]\n        pc_labels += [\"No Spot Color\"]\n\n    # Sometimes can be empty point cloud, so remove these\n    use_pc = [len(pc) &gt; 0 for pc in point_clouds]\n    pc_labels = [pc_labels[i] for i in range(len(use_pc)) if use_pc[i]]\n    point_clouds= [point_clouds[i] for i in range(len(use_pc)) if use_pc[i]]\n    vpc = view_point_clouds(point_clouds, pc_labels, neighb_dist_thresh, z_scale,\n                            \"Reference Spots in the Global Coordinate System\")\n\n    tile_sz = nb.basic_info.tile_sz\n    for t in nb.basic_info.use_tiles:\n        rect = matplotlib.patches.Rectangle((tile_origin[t, 1], tile_origin[t, 0]), tile_sz, tile_sz,\n                                            linewidth=1, edgecolor='w', facecolor='none', linestyle=':')\n        vpc.ax.add_patch(rect)\n        vpc.ax.text(tile_origin[t, 1] + 20, tile_origin[t, 0] + 20, f\"Tile {t}\",\n                    size=6, color='w', ha='left', weight='light')\n    plt.show()\n</code></pre>"},{"location":"code/plot/stitch/#view_point_clouds","title":"<code>view_point_clouds</code>","text":"<p>Plots two point clouds. <code>point_clouds[0]</code> always plotted but you can change the second point cloud using radio buttons.</p> <p>Parameters:</p> Name Type Description Default <code>point_clouds</code> <code>List</code> <p>List of point clouds, each of which is yxz coordinates <code>float [n_points x 3]</code>. <code>point_clouds[0]</code> is always plotted. YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels. Radio buttons used to select other point_cloud plotted.</p> required <code>pc_labels</code> <code>List</code> <p>List of labels to appear in legend/radio-buttons for each point cloud. Must provide one for each <code>point_cloud</code>.</p> required <code>neighb_dist_thresh</code> <code>float</code> <p>If distance between neighbours is less than this, a white line will connect them.</p> <code>5</code> <code>z_scale</code> <code>float</code> <p><code>pixel_size_z / pixel_size_y</code> i.e. used to convert z coordinates from z-pixels to yx pixels.</p> <code>1</code> <code>super_title</code> <code>Optional[str]</code> <p>Optional title for plot</p> <code>None</code> Source code in <code>coppafish/plot/stitch/point_clouds.py</code> <pre><code>def __init__(self, point_clouds: List, pc_labels: List, neighb_dist_thresh: float = 5, z_scale: float = 1,\n             super_title: Optional[str] = None):\n\"\"\"\n    Plots two point clouds. `point_clouds[0]` always plotted but you can change the second point cloud using\n    radio buttons.\n\n    Args:\n        point_clouds: List of point clouds, each of which is yxz coordinates `float [n_points x 3]`.\n            `point_clouds[0]` is always plotted.\n            YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels.\n            Radio buttons used to select other point_cloud plotted.\n        pc_labels: List of labels to appear in legend/radio-buttons for each point cloud.\n            Must provide one for each `point_cloud`.\n        neighb_dist_thresh: If distance between neighbours is less than this, a white line will connect them.\n        z_scale: `pixel_size_z / pixel_size_y` i.e. used to convert z coordinates from z-pixels to yx pixels.\n        super_title: Optional title for plot\n    \"\"\"\n    n_point_clouds = len(point_clouds)\n    if len(point_clouds) != len(pc_labels):\n        raise ValueError(f'There are {n_point_clouds} point clouds but {len(pc_labels)} labels')\n    self.fig, self.ax = plt.subplots(1, 1, figsize=(15, 8))\n    subplots_adjust = [0.07, 0.775, 0.095, 0.89]\n    self.fig.subplots_adjust(left=subplots_adjust[0], right=subplots_adjust[1], bottom=subplots_adjust[2],\n                             top=subplots_adjust[3])\n\n    # Find neighbours between all point clouds and the first.\n    self.neighb = [None]\n    n_matches_str = 'Number Of Matches'\n    for i in range(1, n_point_clouds):\n        tree = KDTree(point_clouds[i] * [1, 1, z_scale])\n        dist, neighb = tree.query(point_clouds[0] * [1, 1, z_scale],\n                                  distance_upper_bound=neighb_dist_thresh * 3)\n        neighb[dist &gt; neighb_dist_thresh] = -1  # set too large distance neighb_ind to -1.\n        n_matches_str = n_matches_str + f'\\n- {pc_labels[i]}: {np.sum(neighb &gt;= 0)}'\n        self.neighb = self.neighb + [neighb]\n\n    # Add text box to indicate number of matches between first point cloud and each of the others.\n    n_matches_ax = self.fig.add_axes([0.8, subplots_adjust[3] - 0.75, 0.15, 0.2])\n    plt.axis('off')\n    n_matches_ax.text(0.05, 0.95, n_matches_str)\n\n    #  round z to closest z-plane for plotting\n    for i in range(n_point_clouds):\n        point_clouds[i][:, 2] = np.rint(point_clouds[i][:, 2])\n\n    # get yxz axis limits taking account all point clouds\n    pc_min_lims = np.zeros((n_point_clouds, 3))\n    pc_max_lims = np.zeros_like(pc_min_lims)\n    for i in range(n_point_clouds):\n        pc_min_lims[i] = np.min(point_clouds[i], axis=0)\n        pc_max_lims[i] = np.max(point_clouds[i], axis=0)\n\n    self.z_planes = np.arange(int(np.min(pc_min_lims[:, 2])), int(np.max(pc_max_lims[:, 2]) + 1))\n    self.nz = len(self.z_planes)\n    self.z_ind = 0\n    self.z = self.z_planes[self.z_ind]\n    self.z_thick = 0\n    self.active_pc = [0, 1]\n    self.in_z = [np.array([val[:, 2] &gt;= self.z - self.z_thick, val[:, 2] &lt;= self.z + self.z_thick]).all(axis=0)\n                 for val in point_clouds]\n    self.point_clouds = point_clouds\n    self.pc_labels = np.array(pc_labels)\n    self.pc_shapes = ['rx', 'bo']\n    alpha = [1, 0.7]\n    self.pc_plots = [self.ax.plot(point_clouds[self.active_pc[i]][self.in_z[i], 1],\n                                  point_clouds[self.active_pc[i]][self.in_z[i], 0],\n                                  self.pc_shapes[i], label=self.pc_labels[i], alpha=alpha[i])[0]\n                     for i in range(2)]\n\n    self.neighb_yx = None\n    self.neighb_plot = None\n    self.update_neighb_lines()\n\n    self.ax.legend(loc='upper right')\n    self.ax.set_ylabel('Y')\n    self.ax.set_xlabel('X')\n    self.ax.set_ylim(np.min(pc_min_lims[:, 0]), np.max(pc_max_lims[:, 0]))\n    self.ax.set_xlim(np.min(pc_min_lims[:, 1]), np.max(pc_max_lims[:, 1]))\n\n    if self.nz &gt; 1:\n        # If 3D, add text box to change number of z-planes collapsed onto single plane\n        # and add scrolling to change z-plane\n        self.ax.set_title(f'Z = {int(self.z)}', size=10)\n        self.fig.canvas.mpl_connect('scroll_event', self.z_scroll)\n        text_ax = self.fig.add_axes([0.8, 0.095, 0.15, 0.04])\n    else:\n        # For some reason in 2D, still need the text box otherwise buttons don't do work\n        # But shift it off-screen and make small\n        text_ax = self.fig.add_axes([40, 40, 0.00001, 0.00001])\n    self.text_box = TextBox(text_ax, 'Z-Thick', self.z_thick, color='k', hovercolor=[0.2, 0.2, 0.2])\n    self.text_box.cursor.set_color('r')\n    # change text box title to be above not to the left of box\n    label = text_ax.get_children()[0]  # label is a child of the TextBox axis\n    if self.nz == 1:\n        label.set_position([40, 40])  # shift label off-screen in 2D\n    else:\n        label.set_position([0.5, 2])  # [x,y] - change here to set the position\n    # centering the text\n    label.set_verticalalignment('top')\n    label.set_horizontalalignment('center')\n    self.text_box.on_submit(self.text_update)\n\n    if n_point_clouds &gt;= 3:\n        # If 3 or more point clouds, add radio button to change the second point cloud shown.\n        buttons_ax = self.fig.add_axes([0.8, subplots_adjust[3] - 0.2, 0.15, 0.2])\n        plt.axis('off')\n        self.buttons = RadioButtons(buttons_ax, self.pc_labels[1:], 0, activecolor='w')\n        for i in range(n_point_clouds-1):\n            self.buttons.circles[i].set_color('w')\n            self.buttons.circles[i].set_color('w')\n        self.buttons.set_active(0)\n        self.buttons.on_clicked(self.button_update)\n    if super_title is not None:\n        plt.suptitle(super_title, x=(0.07 + 0.775) / 2)\n</code></pre>"},{"location":"code/plot/stitch/#diagnostics","title":"Diagnostics","text":""},{"location":"code/plot/stitch/#view_stitch_shift_info","title":"<code>view_stitch_shift_info</code>","text":"<p>For all north/south and east/west shifts computed in the <code>stitch</code> section of the pipeline, this plots the values of the shifts found and the <code>score</code> compared to the <code>score_thresh</code>.</p> <p>For each direction, there will be 3 plots:</p> <ul> <li>y shift vs x shift for all pairs of neighbouring tiles</li> <li>z shift vs x shift for all pairs of neighbouring tiles</li> <li><code>score</code> vs <code>score_thresh</code> for all pairs of neighbouring tiles (a green score = score_thresh line is plotted in this).</li> </ul> <p>In each case, the markers in the plots are numbers. These numbers indicate the tile, the shift was applied to, to take it to its north or east neighbour i.e. <code>nb.stitch.south_pairs[:, 0]</code> or <code>nb.stitch.west_pairs[:, 0]</code>. The number will be blue if <code>score &gt; score_thresh</code> and red otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>stitch</code> page.</p> required <code>outlier</code> <code>bool</code> <p>If <code>True</code>, will plot <code>nb.stitch.south_shift_outlier</code> instead of <code>nb.stitch.south_shift</code>. In this case, only tiles for which the two are different are plotted for each round.</p> <code>False</code> Source code in <code>coppafish/plot/stitch/diagnostics.py</code> <pre><code>def view_stitch_shift_info(nb: Notebook, outlier: bool = False):\n\"\"\"\n    For all north/south and east/west shifts computed in the `stitch` section\n    of the pipeline, this plots the values of the shifts found and the `score` compared to\n    the `score_thresh`.\n\n    For each direction, there will be 3 plots:\n\n    * y shift vs x shift for all pairs of neighbouring tiles\n    * z shift vs x shift for all pairs of neighbouring tiles\n    * `score` vs `score_thresh` for all pairs of neighbouring tiles\n    (a green score = score_thresh line is plotted in this).\n\n    In each case, the markers in the plots are numbers.\n    These numbers indicate the tile, the shift was applied to,\n    to take it to its north or east neighbour i.e. `nb.stitch.south_pairs[:, 0]`\n    or `nb.stitch.west_pairs[:, 0]`.\n    The number will be blue if `score &gt; score_thresh` and red otherwise.\n\n    Args:\n        nb: Notebook containing at least the `stitch` page.\n        outlier: If `True`, will plot `nb.stitch.south_shift_outlier` instead of\n            `nb.stitch.south_shift`. In this case, only tiles for which\n            the two are different are plotted for each round.\n    \"\"\"\n    if nb.basic_info.is_3d:\n        ndim = 3\n    else:\n        ndim = 2\n    shift_info = {}\n    if len(nb.stitch.south_shifts) &gt; 0:\n        shift_info['South'] = {}\n        shift_info['South']['tile'] = nb.stitch.south_pairs[:, 0]\n        shift_info['South']['score_thresh'] = nb.stitch.south_score_thresh\n        if outlier:\n            shift_info['South']['shift'] = nb.stitch.south_outlier_shifts[:, :ndim]\n            shift_info['South']['score'] = nb.stitch.south_outlier_score\n        else:\n            shift_info['South']['shift'] = nb.stitch.south_shifts[:, :ndim]\n            shift_info['South']['score'] = nb.stitch.south_score\n    if len(nb.stitch.west_shifts) &gt; 0:\n        shift_info['West'] = {}\n        shift_info['West']['tile'] = nb.stitch.west_pairs[:, 0]\n        shift_info['West']['score_thresh'] = nb.stitch.west_score_thresh\n        if outlier:\n            shift_info['West']['shift'] = nb.stitch.west_outlier_shifts[:, :ndim]\n            shift_info['West']['score'] = nb.stitch.west_outlier_score\n        else:\n            shift_info['West']['shift'] = nb.stitch.west_shifts[:, :ndim]\n            shift_info['West']['score'] = nb.stitch.west_score\n    if outlier:\n        title_start = \"Outlier \"\n    else:\n        title_start = \"\"\n    shift_info_plot(shift_info, f\"{title_start}Shifts found in stitch part of pipeline between each tile and the \"\n                                f\"neighbouring tile in the direction specified\")\n</code></pre>"},{"location":"code/plot/stitch/#shift_info_plot","title":"<code>shift_info_plot</code>","text":"<p>If <code>shift_info</code> contains \\(n\\) keys, this will produce an \\(n\\) column x 3 row grid of subplots. For each key in <code>shift_info</code> dictionary, there are 3 plots:</p> <ul> <li>y shift vs x shift</li> <li>z shift vs x shift</li> <li><code>score</code> vs <code>score_thresh</code> or <code>n_matches</code> vs <code>error</code></li> </ul> <p>In each case, the markers in the plots are numbers. These numbers are given by <code>shift_info[key][tile]</code>. The number will be blue if <code>score &gt; score_thresh</code> and red otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>shift_info</code> <code>dict</code> <p>Dictionary containing \\(n\\) dictionaries. Each of these dictionaries contains (either (<code>score</code> and <code>score_thresh</code>) or (<code>n_matches</code>, <code>n_matches_thresh</code> and <code>error</code>)):</p> <ul> <li>shift - <code>float [n_tiles x 3]</code>. \\(yxz\\) shifts for each tile.</li> <li>tile - <code>int [n_tiles]</code>. Indicates tile each shift was found for.</li> <li>score - <code>float [n_tiles]</code>. Indicates <code>score</code> found for each shift (approx number of matches between     point clouds).</li> <li>score_thresh - <code>float [n_tiles]</code>. If <code>score&lt;score_thresh</code>, it will be shown in red.</li> <li>n_matches - <code>int [n_tiles]</code>. Indicates <code>score</code> found for each shift (approx number of matches between     point clouds).</li> <li>n_matches_thresh - <code>int [n_tiles]</code>. If <code>n_matches&lt;n_matches_thresh</code>, it will be shown in red.</li> <li>error - <code>float [n_tiles]</code>. Average distance between neighbours.</li> <li>x_lim - <code>float [n_plots x 2]</code>. Can optionally specify number axis limits for each plot.</li> <li>y_lim - <code>float [n_plots x 2]</code>. Can optionally specify number axis limits for each plot.</li> </ul> required <code>title</code> <code>Optional[str]</code> <p>Overall title for the plot.</p> <code>None</code> <code>score_plot_thresh</code> <code>int</code> <p>Only shifts with <code>score</code> (or <code>n_matches</code>) &gt; <code>score_plot_thresh</code> are shown.</p> <code>0</code> <code>fig</code> <code>Optional[plt.Figure]</code> <p>Can provide previous figure to plot on.</p> <code>None</code> <code>ax</code> <code>Optional[np.ndarray]</code> <p>Can provide array of plt.Axes to plot on.</p> <code>None</code> <code>return_ax</code> <code>bool</code> <p>If <code>True</code>, ax will be returned and <code>plt.show()</code> will not be run.</p> <code>False</code> Source code in <code>coppafish/plot/stitch/diagnostics.py</code> <pre><code>def shift_info_plot(shift_info: dict, title: Optional[str] = None, score_plot_thresh: int = 0,\n                    fig: Optional[plt.Figure] = None, ax: Optional[np.ndarray] = None, return_ax: bool = False):\n\"\"\"\n    If `shift_info` contains $n$ keys, this will produce an $n$ column x 3 row grid of subplots.\n    For each key in `shift_info` dictionary, there are 3 plots:\n\n    * y shift vs x shift\n    * z shift vs x shift\n    * `score` vs `score_thresh` or `n_matches` vs `error`\n\n    In each case, the markers in the plots are numbers.\n    These numbers are given by `shift_info[key][tile]`.\n    The number will be blue if `score &gt; score_thresh` and red otherwise.\n\n    Args:\n        shift_info: Dictionary containing $n$ dictionaries.\n            Each of these dictionaries contains (either (`score` and `score_thresh`) or (`n_matches`,\n            `n_matches_thresh` and `error`)):\n\n            * shift - `float [n_tiles x 3]`. $yxz$ shifts for each tile.\n            * tile - `int [n_tiles]`. Indicates tile each shift was found for.\n            * score - `float [n_tiles]`. Indicates `score` found for each shift (approx number of matches between\n                point clouds).\n            * score_thresh - `float [n_tiles]`. If `score&lt;score_thresh`, it will be shown in red.\n            * n_matches - `int [n_tiles]`. Indicates `score` found for each shift (approx number of matches between\n                point clouds).\n            * n_matches_thresh - `int [n_tiles]`. If `n_matches&lt;n_matches_thresh`, it will be shown in red.\n            * error - `float [n_tiles]`. Average distance between neighbours.\n            * x_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot.\n            * y_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot.\n        title: Overall title for the plot.\n        score_plot_thresh: Only shifts with `score` (or `n_matches`) &gt; `score_plot_thresh` are shown.\n        fig: Can provide previous figure to plot on.\n        ax: Can provide array of plt.Axes to plot on.\n        return_ax: If `True`, ax will be returned and `plt.show()` will not be run.\n    \"\"\"\n    n_cols = len(shift_info)\n    col_titles = list(shift_info.keys())\n    n_rows = len(shift_info[col_titles[0]]['shift'][0])  # 2 if 2D shift, 3 if 3D.\n    if fig is None:\n        fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 7))\n        fig.subplots_adjust(hspace=0.4, bottom=0.08, left=0.06, right=0.97, top=0.9)\n    if n_cols == 1 and len(ax.shape) == 1:\n        ax = ax[:, np.newaxis]\n    for i in range(n_cols):\n        shift_info_i = shift_info[col_titles[i]]\n        # good tiles are blue, bad tiles are red\n        n_tiles = len(shift_info[col_titles[i]]['tile'])\n        tile_color = np.full(n_tiles, 'b')\n        if 'score' in shift_info_i:\n            tile_color[(shift_info_i['score'] &lt; shift_info_i['score_thresh']).flatten()] = 'r'\n            skip_tile = shift_info_i['score'] &lt;= score_plot_thresh  # don't plot if score = 0\n        elif 'n_matches' in shift_info_i:\n            tile_color[(shift_info_i['n_matches'] &lt; shift_info_i['n_matches_thresh']).flatten()] = 'r'\n            skip_tile = shift_info_i['n_matches'] &lt;= score_plot_thresh  # don't plot if n_matches = 0\n        else:\n            raise ValueError(f\"shift_info must contain either a 'score' or 'n_matches' key\")\n        for t in range(n_tiles):\n            if skip_tile[t]:\n                continue\n            ax[0, i].text(shift_info_i['shift'][t, 1], shift_info_i['shift'][t, 0],\n                          str(shift_info_i['tile'][t]), color=tile_color[t], fontsize=12,\n                          ha='center', va='center')\n        if 'x_lim' in shift_info_i:\n            ax[0, i].set_xlim(shift_info_i['x_lim'][0])\n        else:\n            ax[0, i].set_xlim([np.min(shift_info_i['shift'][:, 1]) - 3, np.max(shift_info_i['shift'][:, 1]) + 3])\n        if 'y_lim' in shift_info_i:\n            ax[0, i].set_ylim(shift_info_i['y_lim'][0])\n        else:\n            ax[0, i].set_ylim([np.min(shift_info_i['shift'][:, 0]) - 3, np.max(shift_info_i['shift'][:, 0]) + 3])\n        if i == int(np.ceil(n_cols/2)-1):\n            ax[0, i].set_xlabel('X Shift')\n        if i == 0:\n            ax[0, i].set_ylabel('Y Shift')\n        ax[0, i].set_title(col_titles[i])\n\n        row_ind = 1\n        if n_rows == 3:\n            for t in range(n_tiles):\n                if skip_tile[t]:\n                    continue\n                ax[row_ind, i].text(shift_info_i['shift'][t, 1], shift_info_i['shift'][t, 2],\n                                    str(shift_info_i['tile'][t]), color=tile_color[t], fontsize=12,\n                                    ha='center', va='center')\n            if 'x_lim' in shift_info_i:\n                ax[row_ind, i].set_xlim(shift_info_i['x_lim'][row_ind])\n            else:\n                ax[row_ind, i].set_xlim([np.min(shift_info_i['shift'][:, 1]) - 3,\n                                         np.max(shift_info_i['shift'][:, 1]) + 3])\n            if 'y_lim' in shift_info_i:\n                ax[row_ind, i].set_ylim(shift_info_i['y_lim'][row_ind])\n            else:\n                ax[row_ind, i].set_ylim([np.min(shift_info_i['shift'][:, 2]) - 1,\n                                         np.max(shift_info_i['shift'][:, 2]) + 1])\n            if i == int(np.ceil(n_cols/2)-1):\n                ax[row_ind, i].set_xlabel('X Shift')\n            if i == 0:\n                ax[row_ind, i].set_ylabel('Z Shift')\n            row_ind += 1\n\n        if 'score' in shift_info_i:\n            # Plot line so that all with score &gt; score_thresh are above it\n            if 'x_lim' in shift_info_i and 'y_lim' in shift_info_i:\n                score_min = np.min(shift_info_i['y_lim'][row_ind, 0], shift_info_i['x_lim'][row_ind, 0]) - 5\n                score_max = np.max(shift_info_i['y_lim'][row_ind, 1], shift_info_i['x_lim'][row_ind, 1]) + 5\n            else:\n                score_min = np.min(np.vstack([shift_info_i['score_thresh'], shift_info_i['score']])) - 5\n                score_max = np.max(np.vstack([shift_info_i['score_thresh'], shift_info_i['score']])) + 5\n            ax[row_ind, i].set_xlim([score_min, score_max])\n            ax[row_ind, i].set_ylim([score_min, score_max])\n            ax[row_ind, i].plot([score_min, score_max], [score_min, score_max], 'lime', linestyle=':', linewidth=2,\n                                alpha=0.5)\n            for t in range(n_tiles):\n                if skip_tile[t]:\n                    continue\n                ax[row_ind, i].text(shift_info_i['score_thresh'][t], shift_info_i['score'][t],\n                                    str(shift_info_i['tile'][t]), color=tile_color[t], fontsize=12,\n                                    ha='center', va='center')\n            if i == int(np.ceil(n_cols/2)-1):\n                ax[row_ind, i].set_xlabel('Score Threshold')\n            if i == 0:\n                ax[row_ind, i].set_ylabel('Score')\n        elif 'n_matches' in shift_info_i:\n            if 'x_lim' in shift_info_i:\n                ax[row_ind, i].set_xlim(shift_info_i['x_lim'][row_ind])\n            else:\n                ax[row_ind, i].set_xlim([np.min(shift_info_i['error']) - 0.1, np.max(shift_info_i['error']) + 0.1])\n            if 'y_lim' in shift_info_i:\n                ax[row_ind, i].set_ylim(shift_info_i['y_lim'][row_ind])\n            else:\n                ax[row_ind, i].set_ylim([np.min(shift_info_i['n_matches']) - 100,\n                                         np.max(shift_info_i['n_matches']) + 100])\n            for t in range(n_tiles):\n                if skip_tile[t]:\n                    continue\n                ax[row_ind, i].text(shift_info_i['error'][t], shift_info_i['n_matches'][t],\n                                    str(shift_info_i['tile'][t]), color=tile_color[t], fontsize=12,\n                                    ha='center', va='center')\n            if i == int(np.ceil(n_cols/2)-1):\n                ax[row_ind, i].set_xlabel('Error')\n            if i == 0:\n                ax[row_ind, i].set_ylabel(r'$n_{matches}$')\n    if title is not None:\n        plt.suptitle(title)\n    if return_ax:\n        return ax\n    else:\n        plt.show()\n</code></pre>"},{"location":"code/plot/viewer/","title":"Viewer","text":""},{"location":"code/plot/viewer/#viewer","title":"Viewer","text":"<p>This is the function to view the results of the pipeline i.e. the spots found and which genes they were assigned to.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least the <code>ref_spots</code> page.</p> required <code>background_image</code> <code>Optional[Union[str, np.ndarray]]</code> <p>Optional file_name or image that will be plotted as the background image. If image, z dimension needs to be first i.e. <code>n_z x n_y x n_x</code> if 3D or <code>n_y x n_x</code> if 2D. If pass 2D image for 3D data, will show same image as background on each z-plane.</p> <code>'dapi'</code> <code>gene_marker_file</code> <code>Optional[str]</code> <p>Path to csv file containing marker and color for each gene. There must be 6 columns in the csv file with the following headers:</p> <ul> <li>GeneNames - str, name of gene with first letter capital</li> <li>ColorR - float, Rgb color for plotting</li> <li>ColorG - float, rGb color for plotting</li> <li>ColorB - float, rgB color for plotting</li> <li>napari_symbol - str, symbol used to plot in napari</li> <li>mpl_symbol - str, equivalent of napari symbol in matplotlib.</li> </ul> <p>If it is not provided, then the default file coppafish/plot/results_viewer/legend.gene_color.csv will be used.</p> <code>None</code> Source code in <code>coppafish/plot/results_viewer/base.py</code> <pre><code>def __init__(self, nb: Notebook, background_image: Optional[Union[str, np.ndarray]] = 'dapi',\n             gene_marker_file: Optional[str] = None):\n\"\"\"\n    This is the function to view the results of the pipeline\n    i.e. the spots found and which genes they were assigned to.\n\n    Args:\n        nb: Notebook containing at least the `ref_spots` page.\n        background_image: Optional file_name or image that will be plotted as the background image.\n            If image, z dimension needs to be first i.e. `n_z x n_y x n_x` if 3D or `n_y x n_x` if 2D.\n            If pass *2D* image for *3D* data, will show same image as background on each z-plane.\n        gene_marker_file: Path to csv file containing marker and color for each gene. There must be 6 columns\n            in the csv file with the following headers:\n\n            * GeneNames - str, name of gene with first letter capital\n            * ColorR - float, Rgb color for plotting\n            * ColorG - float, rGb color for plotting\n            * ColorB - float, rgB color for plotting\n            * napari_symbol - str, symbol used to plot in napari\n            * mpl_symbol - str, equivalent of napari symbol in matplotlib.\n\n            If it is not provided, then the default file *coppafish/plot/results_viewer/legend.gene_color.csv*\n            will be used.\n    \"\"\"\n    # TODO: flip y axis so origin bottom left\n    self.nb = nb\n    if gene_marker_file is None:\n        gene_marker_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'gene_color.csv')\n    gene_legend_info = pd.read_csv(gene_marker_file)\n\n    # indices of genes in notebook to gene_color data - quicker to look up integers than names\n    # in change_threshold\n    n_legend_genes = len(gene_legend_info['GeneNames'])\n    self.legend_gene_symbol = np.asarray(gene_legend_info['mpl_symbol'])\n    self.legend_gene_no = np.ones(n_legend_genes, dtype=int) * -1\n    self.call_spots_gene_names = np.asarray([n if n in gene_legend_info['GeneNames'].tolist() else \"other\" for n in self.nb.call_spots.gene_names])\n    for i in range(n_legend_genes):\n        # TODO: maybe guard against different cases in this comparison\n        gene_ind = np.where(self.call_spots_gene_names == gene_legend_info['GeneNames'][i])[0]\n        if len(gene_ind) &gt; 0:\n            self.legend_gene_no[i] = gene_ind[0]\n\n    # concatenate anchor and omp spots so can use button to switch between them.\n    self.omp_0_ind = self.nb.ref_spots.tile.size  # number of anchor spots\n    if self.nb.has_page('omp'):\n        self.n_spots = self.omp_0_ind + self.nb.omp.tile.size  # number of anchor + number of omp spots\n    else:\n        self.n_spots = self.omp_0_ind\n    spot_zyx = np.zeros((self.n_spots, 3))\n    spot_zyx[:self.omp_0_ind] = (self.nb.ref_spots.local_yxz + self.nb.stitch.tile_origin[self.nb.ref_spots.tile]\n                                 )[:, [2, 0, 1]]\n    if self.nb.has_page('omp'):\n        spot_zyx[self.omp_0_ind:] = (self.nb.omp.local_yxz + self.nb.stitch.tile_origin[self.nb.omp.tile]\n                                     )[:, [2, 0, 1]]\n    if not self.nb.basic_info.is_3d:\n        spot_zyx = spot_zyx[:, 1:]\n\n    # indicate spots shown when plot first opened - omp if exists, else anchor\n    if self.nb.has_page('omp'):\n        show_spots = np.zeros(self.n_spots, dtype=bool)\n        show_spots[self.omp_0_ind:] = quality_threshold(self.nb, 'omp')\n    else:\n        show_spots = quality_threshold(self.nb, 'anchor')\n\n    # color to plot for all genes in the notebook\n    gene_color = np.ones((len(self.call_spots_gene_names), 3))\n    for i in range(n_legend_genes):\n        gene_color[self.legend_gene_no[i]] = [gene_legend_info.loc[i, 'ColorR'],\n                                              gene_legend_info.loc[i, 'ColorG'],\n                                              gene_legend_info.loc[i, 'ColorB']]\n\n    self.viewer = napari.Viewer()\n    self.viewer.window.qt_viewer.dockLayerList.setVisible(False)\n    self.viewer.window.qt_viewer.dockLayerControls.setVisible(False)\n\n    # Add background image if given\n    self.diagnostic_layer_ind = 0\n    self.image_layer_ind = None\n    if background_image is not None:\n        if isinstance(background_image, str):\n            if background_image.lower() == 'dapi':\n                file_name = nb.file_names.big_dapi_image\n            elif background_image.lower() == 'anchor':\n                file_name = nb.file_names.big_anchor_image\n            else:\n                file_name = background_image\n            if file_name is not None and os.path.isfile(file_name):\n                background_image = np.load(file_name)\n                if file_name.endswith('.npz'):\n                    # Assume image is first array if .npz file\n                    background_image = background_image[background_image._files[0]]\n            else:\n                background_image = None\n                warnings.warn(f'No file exists with address =\\n{file_name}\\nso plotting with no background.')\n        if background_image is not None:\n            self.viewer.add_image(background_image)\n            self.diagnostic_layer_ind = 1\n            self.image_layer_ind = 0\n            self.viewer.layers[self.image_layer_ind].contrast_limits_range = [background_image.min(),\n                                                                              background_image.max()]\n            self.image_contrast_slider = QRangeSlider(Qt.Orientation.Horizontal)  # Slider to change score_thresh\n            self.image_contrast_slider.setRange(background_image.min(), background_image.max())\n            # Make starting lower bound contrast the 95th percentile value so most appears black\n            # Use mid_z to quicken up calculation\n            mid_z = int(background_image.shape[0]/2)\n            start_contrast = np.percentile(background_image[mid_z], [95, 99.99]).astype(int).tolist()\n            self.image_contrast_slider.setValue(start_contrast)\n            self.change_image_contrast()\n            # When dragging, status will show contrast values.\n            self.image_contrast_slider.valueChanged.connect(lambda x: self.show_image_contrast(x[0], x[1]))\n            # On release of slider, genes shown will change\n            self.image_contrast_slider.sliderReleased.connect(self.change_image_contrast)\n\n    # Add legend indicating genes plotted\n    self.legend = {'fig': None, 'ax': None}\n    self.legend['fig'], self.legend['ax'], n_gene_label_letters = \\\n        add_legend(gene_legend_info=gene_legend_info, genes=self.call_spots_gene_names)\n    # xy is position of each symbol in legend, need to see which gene clicked on.\n    self.legend['xy'] = np.zeros((len(self.legend['ax'].collections), 2), dtype=float)\n    self.legend['gene_no'] = np.zeros(len(self.legend['ax'].collections), dtype=int)\n    # In legend, each gene name label has at most n_gene_label_letters letters so need to crop\n    # gene_names in notebook when looking for corresponding gene in legend.\n    gene_names_crop = np.asarray([gene_name[:n_gene_label_letters] for gene_name in self.call_spots_gene_names])\n    for i in range(self.legend['xy'].shape[0]):\n        # Position of label for each gene in legend window\n        self.legend['xy'][i] = np.asarray(self.legend['ax'].collections[i].get_offsets())\n        # gene no in notebook that each position in the legend corresponds to\n        self.legend['gene_no'][i] = \\\n            np.where(gene_names_crop == self.legend['ax'].texts[i].get_text())[0][0]\n    self.legend['fig'].mpl_connect('button_press_event', self.update_genes)\n    self.viewer.window.add_dock_widget(self.legend['fig'], area='left', name='Genes')\n    self.active_genes = np.arange(len(self.call_spots_gene_names))  # start with all genes shown\n\n    if background_image is not None:\n        # Slider to change background image contrast\n        self.viewer.window.add_dock_widget(self.image_contrast_slider, area=\"left\", name='Image Contrast')\n\n    # Add all spots in layer as transparent white spots.\n    point_size = 10  # with size=4, spots are too small to see\n    self.viewer.add_points(spot_zyx, name='Diagnostic', face_color='w', size=point_size + 2, opacity=0,\n                           shown=show_spots)\n\n    # Add gene spots with coppafish color code - different layer for each symbol\n    if self.nb.has_page('omp'):\n        self.spot_gene_no = np.hstack((self.nb.ref_spots.gene_no, self.nb.omp.gene_no))\n    else:\n        self.spot_gene_no = self.nb.ref_spots.gene_no\n    self.label_prefix = 'Gene Symbol:'  # prefix of label for layers showing spots\n    for s in np.unique(self.legend_gene_symbol):\n        spots_correct_gene = np.isin(self.spot_gene_no, self.legend_gene_no[self.legend_gene_symbol == s])\n        if spots_correct_gene.any():\n            coords_to_plot = spot_zyx[spots_correct_gene]\n            spotcolor_to_plot = gene_color[self.spot_gene_no[spots_correct_gene]]\n            symb_to_plot = np.unique(gene_legend_info[self.legend_gene_symbol == s]['napari_symbol'])[0]\n            self.viewer.add_points(coords_to_plot, face_color=spotcolor_to_plot, symbol=symb_to_plot,\n                                   name=f'{self.label_prefix}{s}', size=point_size,\n                                   shown=show_spots[spots_correct_gene])\n            # TODO: showing multiple z-planes at once is possible using out_of_slice_display=True,\n            #  but at the moment cannot use at same time as show.\n            #  When this works, can change n_z shown by changing z-dimension of point_size i.e.\n            #  point_size = [z_size, 10, 10] and z_size changes.\n            #  On next napari, release should be able to change thickness of spots in z-direction i.e. control what\n            #  number of z-planes can be seen at any one time:\n            #  https://github.com/napari/napari/issues/4816#issuecomment-1186600574.\n            #  Then see find_spots viewer for how I added a z-thick slider\n\n    self.viewer.layers.selection.active = self.viewer.layers[self.diagnostic_layer_ind]\n    # so indicates when a spot is selected in viewer status\n    # It is needed because layer is transparent so can't see when select spot.\n    self.viewer_status_on_select()\n\n    config = self.nb.get_config()['thresholds']\n    self.score_omp_multiplier = config['score_omp_multiplier']\n    self.score_thresh_slider = QDoubleRangeSlider(Qt.Orientation.Horizontal)  # Slider to change score_thresh\n    # Scores for anchor/omp are different so reset score range when change method\n    # Max possible score is that found for ref_spots, as this can be more than 1.\n    # Max possible omp score is 1.\n    max_score = np.around(round_any(nb.ref_spots.score.max(), 0.1, 'ceil'), 2)\n    max_score = float(np.clip(max_score, 1, np.inf))\n    self.score_range = {'anchor': [config['score_ref'], max_score]}\n    if self.nb.has_page('omp'):\n        self.score_range['omp'] = [config['score_omp'], max_score]\n        self.score_thresh_slider.setValue(self.score_range['omp'])\n    else:\n        self.score_thresh_slider.setValue(self.score_range['anchor'])\n    self.score_thresh_slider.setRange(0, max_score)\n    # When dragging, status will show thresh.\n    self.score_thresh_slider.valueChanged.connect(lambda x: self.show_score_thresh(x[0], x[1]))\n    # On release of slider, genes shown will change\n    self.score_thresh_slider.sliderReleased.connect(self.update_plot)\n    self.viewer.window.add_dock_widget(self.score_thresh_slider, area=\"left\", name='Score Range')\n\n    # OMP Score Multiplier Slider\n    self.omp_score_multiplier_slider = QDoubleSlider(Qt.Orientation.Horizontal)\n    self.omp_score_multiplier_slider.setValue(self.score_omp_multiplier)\n    self.omp_score_multiplier_slider.setRange(0, 50)\n    self.omp_score_multiplier_slider.valueChanged.connect(lambda x: self.show_omp_score_multiplier(x))\n    self.omp_score_multiplier_slider.sliderReleased.connect(self.update_plot)\n\n    # intensity is calculated same way for anchor / omp method so do not reset intensity threshold\n    # when change method.\n    self.intensity_thresh_slider = QDoubleSlider(Qt.Orientation.Horizontal)\n    self.intensity_thresh_slider.setRange(0, 1)\n    intensity_thresh = get_intensity_thresh(nb)\n    self.intensity_thresh_slider.setValue(intensity_thresh)\n    # When dragging, status will show thresh.\n    self.intensity_thresh_slider.valueChanged.connect(lambda x: self.show_intensity_thresh(x))\n    # On release of slider, genes shown will change\n    self.intensity_thresh_slider.sliderReleased.connect(self.update_plot)\n    self.viewer.window.add_dock_widget(self.intensity_thresh_slider, area=\"left\", name='Intensity Threshold')\n\n    if self.nb.has_page('omp'):\n        self.method_buttons = ButtonMethodWindow('OMP')  # Buttons to change between Anchor and OMP spots showing.\n    else:\n        self.method_buttons = ButtonMethodWindow('Anchor')\n    self.method_buttons.button_anchor.clicked.connect(self.button_anchor_clicked)\n    self.method_buttons.button_omp.clicked.connect(self.button_omp_clicked)\n    if self.nb.has_page('omp'):\n        self.viewer.window.add_dock_widget(self.omp_score_multiplier_slider, area=\"left\",\n                                           name='OMP Score Multiplier')\n        # Only have button to change method if have omp page too.\n        self.viewer.window.add_dock_widget(self.method_buttons, area=\"left\", name='Method')\n\n    self.key_call_functions()\n    if self.nb.basic_info.is_3d:\n        self.viewer.dims.axis_labels = ['z', 'y', 'x']\n    else:\n        self.viewer.dims.axis_labels = ['y', 'x']\n\n    napari.run()\n</code></pre>"},{"location":"code/register/base/","title":"Base","text":""},{"location":"code/register/base/#coppafish.register.base.get_average_transform","title":"<code>get_average_transform(transforms, n_matches, matches_thresh, scale_thresh, shift_thresh)</code>","text":"<p>This finds all transforms which pass some thresholds and computes the average transform using them. <code>av_transforms[t, r, c]</code> is the average transform for tile <code>t</code>, round <code>r</code>, channel <code>c</code> and has:</p> <ul> <li>Zero rotation.</li> <li>Scaling given by median for channel <code>c</code> over all tiles and rounds.     I.e. <code>median(av_transforms[:, :, c, 0, 0])</code> for y scaling.</li> <li>shift given by median for tile <code>t</code>, round <code>r</code> over all channels.     I.e. <code>median(av_transforms[t, r, _, 4, 0])</code> for y shift if <code>dim=3</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>. <code>transforms[t, r, c]</code> is the affine transform for tile <code>t</code> from the reference image to  round <code>r</code>, channel <code>c</code>.</p> required <code>n_matches</code> <code>np.ndarray</code> <p><code>int [n_tiles x n_rounds x n_channels]</code>. Number of matches found by point cloud registration.</p> required <code>matches_thresh</code> <code>Union[int, np.ndarray]</code> <p><code>int [n_tiles x n_rounds x n_channels]</code> or single <code>int</code>. <code>n_matches</code> for a particular transform must exceed this to be used when computing <code>av_transforms</code>. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: <code>200</code>.</p> required <code>scale_thresh</code> <code>np.ndarray</code> <p><code>float [n_dim]</code>. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: <code>0.01</code>.</p> required <code>shift_thresh</code> <code>np.ndarray</code> <p><code>float [n_dim]</code>. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: <code>10</code> xy pixels in xy direction, <code>2</code> z pixels in z direction (normalised to have same units as <code>yx_pixels</code>).</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>av_transforms</code> - <code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>. <code>av_transforms[t, r, c]</code> is the average transform for tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</li> </ul> <code>np.ndarray</code> <ul> <li><code>av_scaling</code> - <code>float [n_channels x dim]</code>. <code>av_scaling[c, d]</code> is the median scaling for channel <code>c</code>, dimension <code>d</code>, over all tiles and rounds.</li> </ul> <code>np.ndarray</code> <ul> <li><code>av_shifts</code> - <code>float [n_tiles x n_rounds x dim]</code>. <code>av_shifts[t, r, d]</code> is the median scaling for tile <code>t</code>, round <code>r</code>, dimension <code>d</code>, over all channels.</li> </ul> <code>np.ndarray</code> <ul> <li><code>failed</code> - <code>bool [n_tiles x n_rounds x n_channels]</code>. Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating <code>av_transforms</code>.</li> </ul> <code>np.ndarray</code> <ul> <li><code>failed_non_matches</code> - <code>bool [n_tiles x n_rounds x n_channels]</code>. Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift or scaling in one or more directions.</li> </ul> Source code in <code>coppafish/register/base.py</code> <pre><code>def get_average_transform(transforms: np.ndarray, n_matches: np.ndarray, matches_thresh: Union[int, np.ndarray],\n                          scale_thresh: np.ndarray,\n                          shift_thresh: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray,\n                                                             np.ndarray]:\n\"\"\"\n    This finds all transforms which pass some thresholds and computes the average transform using them.\n    `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c` and has:\n\n    - Zero rotation.\n    - Scaling given by median for channel `c` over all tiles and rounds.\n        I.e. `median(av_transforms[:, :, c, 0, 0])` for y scaling.\n    - shift given by median for tile `t`, round `r` over all channels.\n        I.e. `median(av_transforms[t, r, _, 4, 0])` for y shift if `dim=3`.\n\n    Args:\n        transforms: ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```.\n            ```transforms[t, r, c]``` is the affine transform for tile ```t``` from the reference image to\n             round ```r```, channel ```c```.\n        n_matches: ```int [n_tiles x n_rounds x n_channels]```.\n            Number of matches found by point cloud registration.\n        matches_thresh: ```int [n_tiles x n_rounds x n_channels]``` or single ```int```.\n            ```n_matches``` for a particular transform must exceed this to be used when computing ```av_transforms```.\n            Can specify a single threshold for all transforms or a different threshold for each.\n            E.g. you may give a lower threshold if that tile/round/channel has fewer spots.\n            Typical: ```200```.\n        scale_thresh: ```float [n_dim]```.\n            Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension.\n            Typically, this threshold will be the same in all dimensions as expect\n            chromatic aberration to be same in each.\n            Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes\n            get a lot of matches.\n            Typical: `0.01`.\n        shift_thresh: `float [n_dim]`.\n            Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension.\n            Typically, this threshold will be the same in y and x but different in z.\n            Typical: `10` xy pixels in xy direction, `2` z pixels in z direction\n            (normalised to have same units as `yx_pixels`).\n\n    Returns:\n        - `av_transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`.\n            `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c`.\n        - `av_scaling` - `float [n_channels x dim]`.\n            `av_scaling[c, d]` is the median scaling for channel `c`, dimension `d`, over all tiles and rounds.\n        - `av_shifts` - `float [n_tiles x n_rounds x dim]`.\n            `av_shifts[t, r, d]` is the median scaling for tile `t`, round `r`, dimension `d`, over all channels.\n        - `failed` - `bool [n_tiles x n_rounds x n_channels]`.\n            Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous\n            compared to median. These were not included when calculating `av_transforms`.\n        - `failed_non_matches` - `bool [n_tiles x n_rounds x n_channels]`.\n            Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift\n            or scaling in one or more directions.\n    \"\"\"\n    dim = transforms.shape[-1]\n    failed_matches = n_matches &lt; matches_thresh\n    failed = failed_matches.copy()\n\n    # Assume scaling is the same for particular channel across all tile and rounds\n    scaling = transforms[:, :, :, np.arange(dim), np.arange(dim)]\n    scaling = np.moveaxis(scaling, -1, 0)\n    av_scaling = mod_median(scaling, np.expand_dims(failed, 0).repeat(dim, 0), axis=[1, 2])\n    diff_to_av_scaling = np.abs(scaling - np.expand_dims(av_scaling, [1, 2]))\n    failed_scale = np.max(diff_to_av_scaling - np.array(scale_thresh).reshape(dim, 1, 1, 1) &gt; 0, axis=0)\n    failed = np.logical_or(failed, failed_scale)\n\n    # Assume shift the same for particular tile and round across all channels\n    shifts = np.moveaxis(transforms[:, :, :, 3], -1, 0)\n    av_shifts = mod_median(shifts, np.expand_dims(failed, 0).repeat(dim, 0), axis=3)\n    diff_to_av_shift = np.abs(shifts - np.expand_dims(av_shifts, 3))\n    failed_shift = np.max(diff_to_av_shift - np.array(shift_thresh).reshape(dim, 1, 1, 1), axis=0) &gt; 0\n    failed = np.logical_or(failed, failed_shift)\n\n    # find average shifts and scaling again using final failed array\n    av_scaling = mod_median(scaling, np.expand_dims(failed, 0).repeat(dim, 0), axis=[1, 2])\n    av_shifts = mod_median(shifts, np.expand_dims(failed, 0).repeat(dim, 0), axis=3)\n    all_failed_scale_c = np.unique(np.argwhere(np.isnan(av_scaling))[:, 1:], axis=0)\n    n_failed = len(all_failed_scale_c)\n    if n_failed &gt; 0:\n        # to compute median scale to particular channel, at least one good tile/round.\n        raise ValueError(f\"\\nNo suitable scales found for the following channels across all tiles/rounds\\n\"\n                         f\"{[all_failed_scale_c[i][0] for i in range(n_failed)]}\\n\"\n                         f\"Consider removing these from use_channels.\")\n    all_failed_shifts_tr = np.unique(np.argwhere(np.isnan(av_shifts))[:, 1:], axis=0)\n    n_failed = len(all_failed_shifts_tr[:, 0])\n    if n_failed &gt; 0:\n        # to compute median shift to particular tile/round, at least one good channel is required.\n        raise ValueError(f\"\\nNo suitable shifts found for the following tile/round combinations\"\n                         f\" across all colour channels\\n\"\n                         f\"t: {[all_failed_shifts_tr[i, 0] for i in range(n_failed)]}\\n\"\n                         f\"r: {[all_failed_shifts_tr[i, 1] for i in range(n_failed)]}\\n\"\n                         f\"Look at the following diagnostics to see why registration has few matches for these:\\n\"\n                         f\"coppafish.plot.view_register_shift_info\\ncoppafish.plot.view_register_search\\n\"\n                         f\"coppafish.plot.view_icp\\n\"\n                         f\"If it seems to be a single tile/round that is the problem, maybe remove from \"\n                         f\"use_tiles/use_rounds and re-run.\")\n\n    av_scaling = np.moveaxis(av_scaling, 0, -1)  # so get in order channel,dim\n    av_shifts = np.moveaxis(av_shifts, 0, -1)  # so get in order tile,round,dim\n    av_transforms = transform_from_scale_shift(av_scaling, av_shifts)\n    # indicates tiles/rounds/channels which have anomalous transform compared to median independent of number of matches\n    failed_non_matches = np.logical_or(failed_scale, failed_shift)\n    return av_transforms, av_scaling, av_shifts, failed, failed_non_matches\n</code></pre>"},{"location":"code/register/base/#coppafish.register.base.get_single_affine_transform","title":"<code>get_single_affine_transform(spot_yxz_base, spot_yxz_transform, z_scale_base, z_scale_transform, start_transform, neighb_dist_thresh, tile_centre, n_iter=100, reg_constant_scale=None, reg_constant_shift=None, reg_transform=None)</code>","text":"<p>Finds the affine transform taking <code>spot_yxz_base</code> to <code>spot_yxz_transform</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spot_yxz_base</code> <code>np.ndarray</code> <p>Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels.</p> required <code>spot_yxz_transform</code> <code>np.ndarray</code> <p>Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels.</p> required <code>z_scale_base</code> <code>float</code> <p>Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.</p> required <code>z_scale_transform</code> <code>float</code> <p>Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.</p> required <code>start_transform</code> <code>np.ndarray</code> <p><code>float [4 x 3]</code>. Start affine transform for iterative closest point. Typically, <code>start_transform[:3, :]</code> is identity matrix and <code>start_transform[3]</code> is approx yxz shift (z shift in units of xy pixels).</p> required <code>neighb_dist_thresh</code> <code>float</code> <p>Distance between 2 points must be less than this to be constituted a match.</p> required <code>tile_centre</code> <code>np.ndarray</code> <p>int [3]. yxz coordinates of centre of image where spot_yxz found on.</p> required <code>n_iter</code> <code>int</code> <p>Max number of iterations to perform of ICP.</p> <code>100</code> <code>reg_constant_scale</code> <code>Optional[float]</code> <p>Constant used for scaling and rotation when doing regularized least squares. <code>None</code> means no regularized least squares performed. Typical = <code>5e8</code>.</p> <code>None</code> <code>reg_constant_shift</code> <code>Optional[float]</code> <p>Constant used for shift when doing regularized least squares. <code>None</code> means no regularized least squares performed. Typical = <code>500</code></p> <code>None</code> <code>reg_transform</code> <code>Optional[np.ndarray]</code> <p><code>float [4 x 3]</code>. Transform to regularize to when doing regularized least squares. <code>None</code> means no regularized least squares performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>transform</code> - <code>float [4 x 3]</code>. <code>transform</code> is the final affine transform found.</li> </ul> <code>int</code> <ul> <li><code>n_matches</code> - Number of matches found for each transform.</li> </ul> <code>float</code> <ul> <li><code>error</code> - Average distance between neighbours below <code>neighb_dist_thresh</code>.</li> </ul> <code>bool</code> <ul> <li><code>is_converged</code> - <code>False</code> if max iterations reached before transform converged.</li> </ul> Source code in <code>coppafish/register/base.py</code> <pre><code>def get_single_affine_transform(spot_yxz_base: np.ndarray, spot_yxz_transform: np.ndarray, z_scale_base: float,\n                                z_scale_transform: float, start_transform: np.ndarray,\n                                neighb_dist_thresh: float, tile_centre: np.ndarray, n_iter: int = 100,\n                                reg_constant_scale: Optional[float] = None, reg_constant_shift: Optional[float] = None,\n                                reg_transform: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, int, float, bool]:\n\"\"\"\n    Finds the affine transform taking `spot_yxz_base` to `spot_yxz_transform`.\n\n    Args:\n        spot_yxz_base: Point cloud want to find the shift from.\n            spot_yxz_base[:, 2] is the z coordinate in units of z-pixels.\n        spot_yxz_transform: Point cloud want to find the shift to.\n            spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels.\n        z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.\n        z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base.\n        start_transform: `float [4 x 3]`.\n            Start affine transform for iterative closest point.\n            Typically, `start_transform[:3, :]` is identity matrix and\n            `start_transform[3]` is approx yxz shift (z shift in units of xy pixels).\n        neighb_dist_thresh: Distance between 2 points must be less than this to be constituted a match.\n        tile_centre: int [3].\n            yxz coordinates of centre of image where spot_yxz found on.\n        n_iter: Max number of iterations to perform of ICP.\n        reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares.\n            `None` means no regularized least squares performed.\n            Typical = `5e8`.\n        reg_constant_shift: Constant used for shift when doing regularized least squares.\n            `None` means no regularized least squares performed.\n            Typical = `500`\n        reg_transform: `float [4 x 3]`.\n            Transform to regularize to when doing regularized least squares.\n            `None` means no regularized least squares performed.\n\n    Returns:\n        - `transform` - `float [4 x 3]`.\n            `transform` is the final affine transform found.\n        - `n_matches` - Number of matches found for each transform.\n        - `error` - Average distance between neighbours below `neighb_dist_thresh`.\n        - `is_converged` - `False` if max iterations reached before transform converged.\n    \"\"\"\n\n    spot_yxz_base = (spot_yxz_base - tile_centre) * [1, 1, z_scale_base]\n    spot_yxz_transform = (spot_yxz_transform - tile_centre) * [1, 1, z_scale_transform]\n    tree_transform = KDTree(spot_yxz_transform)\n    neighbour = np.zeros(spot_yxz_base.shape[0], dtype=int)\n    transform = start_transform.copy()\n    for i in range(n_iter):\n        neighbour_last = neighbour.copy()\n        transform, neighbour, n_matches, error = \\\n            get_transform(spot_yxz_base, transform, spot_yxz_transform, neighb_dist_thresh,\n                          tree_transform, reg_constant_scale, reg_constant_shift, reg_transform)\n\n        is_converged = bool(np.abs(neighbour - neighbour_last).max() == 0)\n        if is_converged:\n            break\n    return transform, n_matches, error, is_converged\n</code></pre>"},{"location":"code/register/base/#coppafish.register.base.get_transform","title":"<code>get_transform(yxz_base, transform_old, yxz_target, dist_thresh, yxz_target_tree=None, reg_constant_scale=30000, reg_constant_shift=9, reg_transform=None)</code>","text":"<p>This finds the affine transform that transforms <code>yxz_base</code> such that the distances between the neighbours with <code>yxz_target</code> are minimised.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>float [n_base_spots x 3]</code>. Coordinates of spots you want to transform.</p> required <code>transform_old</code> <code>np.ndarray</code> <p><code>float [4 x 3]</code>. Affine transform found for previous iteration of PCR algorithm.</p> required <code>yxz_target</code> <code>np.ndarray</code> <p><code>float [n_target_spots x 3]</code>. Coordinates of spots in image that you want to transform <code>yxz_base</code> to.</p> required <code>dist_thresh</code> <code>float</code> <p>If neighbours closer than this, they are used to compute the new transform. Typical: <code>3</code>.</p> required <code>yxz_target_tree</code> <code>Optional[KDTree]</code> <p>KDTree produced from <code>yxz_target</code>. If <code>None</code>, it will be computed.</p> <code>None</code> <code>reg_constant_scale</code> <code>float</code> <p>Constant used for scaling and rotation when doing regularized least squares.</p> <code>30000</code> <code>reg_constant_shift</code> <code>float</code> <p>Constant used for shift when doing regularized least squares.</p> <code>9</code> <code>reg_transform</code> <code>Optional[np.ndarray]</code> <p><code>float [4 x 3]</code>. Affine transform which we want final transform to be near when doing regularized least squares. If <code>None</code>, then no regularization is performed.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>transform</code> - <code>float [4 x 3]</code>. Updated affine transform.</li> </ul> <code>np.ndarray</code> <ul> <li><code>neighbour</code> - <code>int [n_base_spots]</code>. <code>neighbour[i]</code> is index of coordinate in <code>yxz_target</code> to which transformation of <code>yxz_base[i]</code> is closest.</li> </ul> <code>int</code> <ul> <li><code>n_matches</code> - <code>int</code>. Number of neighbours which have distance less than <code>dist_thresh</code>.</li> </ul> <code>float</code> <ul> <li><code>error</code> - <code>float</code>. Average distance between <code>neighbours</code> below <code>dist_thresh</code>.</li> </ul> Source code in <code>coppafish/register/base.py</code> <pre><code>def get_transform(yxz_base: np.ndarray, transform_old: np.ndarray, yxz_target: np.ndarray, dist_thresh: float,\n                  yxz_target_tree: Optional[KDTree] = None, reg_constant_scale: float = 30000,\n                  reg_constant_shift: float = 9,\n                  reg_transform: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, np.ndarray, int, float]:\n\"\"\"\n    This finds the affine transform that transforms ```yxz_base``` such that the distances between the neighbours\n    with ```yxz_target``` are minimised.\n\n    Args:\n        yxz_base: ```float [n_base_spots x 3]```.\n            Coordinates of spots you want to transform.\n        transform_old: ```float [4 x 3]```.\n            Affine transform found for previous iteration of PCR algorithm.\n        yxz_target: ```float [n_target_spots x 3]```.\n            Coordinates of spots in image that you want to transform ```yxz_base``` to.\n        dist_thresh: If neighbours closer than this, they are used to compute the new transform.\n            Typical: ```3```.\n        yxz_target_tree: KDTree produced from ```yxz_target```.\n            If ```None```, it will be computed.\n        reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares.\n        reg_constant_shift: Constant used for shift when doing regularized least squares.\n        reg_transform: ```float [4 x 3]```.\n            Affine transform which we want final transform to be near when doing regularized least squares.\n            If ```None```, then no regularization is performed.\n\n    Returns:\n        - ```transform``` - ```float [4 x 3]```.\n            Updated affine transform.\n        - ```neighbour``` - ```int [n_base_spots]```.\n            ```neighbour[i]``` is index of coordinate in ```yxz_target``` to which transformation of\n            ```yxz_base[i]``` is closest.\n        - ```n_matches``` - ```int```.\n            Number of neighbours which have distance less than ```dist_thresh```.\n        - ```error``` - ```float```.\n            Average distance between ```neighbours``` below ```dist_thresh```.\n    \"\"\"\n    if yxz_target_tree is None:\n        yxz_target_tree = KDTree(yxz_target)\n    yxz_base_pad = np.pad(yxz_base, [(0, 0), (0, 1)], constant_values=1)\n    yxz_transform = yxz_base_pad @ transform_old\n    distances, neighbour = yxz_target_tree.query(yxz_transform, distance_upper_bound=dist_thresh)\n    neighbour = neighbour.flatten()\n    distances = distances.flatten()\n    use = distances &lt; dist_thresh\n    n_matches = int(np.sum(use))\n    error = float(np.sqrt(np.mean(distances[use] ** 2)))\n    if reg_transform is None:\n        transform = np.linalg.lstsq(yxz_base_pad[use, :], yxz_target[neighbour[use], :], rcond=None)[0]\n    else:\n        scale = np.array([reg_constant_scale, reg_constant_scale, reg_constant_scale, reg_constant_shift]).reshape(4, 1)\n        yxz_base_regularised = np.concatenate((yxz_base_pad[use, :], np.eye(4) * scale), axis=0)\n        yxz_target_regularised = np.concatenate((yxz_target[neighbour[use], :], reg_transform * scale), axis=0)\n        transform = np.linalg.lstsq(yxz_base_regularised, yxz_target_regularised, rcond=None)[0]\n    if np.sum(transform[2, :] == 0) == 3:\n        transform[2, 2] = 1  # if 2d transform, set scaling of z to 1 still\n    return transform, neighbour, n_matches, error\n</code></pre>"},{"location":"code/register/base/#coppafish.register.base.icp","title":"<code>icp(yxz_base, yxz_target, transforms_initial, n_iter, dist_thresh, matches_thresh, scale_dev_thresh, shift_dev_thresh, reg_constant_scale=None, reg_constant_shift=None)</code>","text":"<p>This gets the affine <code>transforms</code> from <code>yxz_base</code> to <code>yxz_target</code> using iterative closest point until all iterations used or convergence. For <code>transforms</code> that have matches below <code>matches_thresh</code> or are anomalous compared to <code>av_transform</code>, the <code>transforms</code> are recomputed using regularized least squares to ensure they are close to the <code>av_transform</code>. If either <code>reg_constant_rot = None</code> or <code>reg_constant_shift = None</code> then this is not done.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>object [n_tiles]</code>. <code>yxz_base[t]</code> is a numpy <code>float</code> array <code>[n_base_spots x dim]</code>. coordinates of spots on reference round of tile <code>t</code>.</p> required <code>yxz_target</code> <code>np.ndarray</code> <p><code>object [n_tiles x n_rounds x n_channels]</code>. <code>yxz_target[t, r, c]</code> is a numpy <code>float</code> array <code>[n_target_spots x 3]</code>. coordinates of spots in tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</p> required <code>transforms_initial</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>. <code>transforms_initial[t, r, c]</code> is the starting affine transform for tile <code>t</code> from the reference image to round <code>r</code>, channel <code>c</code>. <code>transforms_initial[t, r, c, :dim, :dim]</code> is probably going to be the identity matrix. <code>transforms_initial[t, r, c, dim, :]</code> is the shift which needs to be pre-computed somehow to get a good result.</p> required <code>n_iter</code> <code>int</code> <p>Max number of iterations to perform of ICP.</p> required <code>dist_thresh</code> <code>float</code> <p>If neighbours closer than this, they are used to compute the new transform. Typical: <code>3</code>.</p> required <code>matches_thresh</code> <code>Union[int, np.ndarray]</code> <p><code>int [n_tiles x n_rounds x n_channels]</code> or single <code>int</code>. <code>n_matches</code> for a particular transform must exceed this to be used when computing <code>av_transform</code>. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: <code>200</code>.</p> required <code>scale_dev_thresh</code> <code>np.ndarray</code> <p><code>float [n_dim]</code>. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: <code>0.01</code>.</p> required <code>shift_dev_thresh</code> <code>np.ndarray</code> <p><code>float [n_dim]</code>. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: <code>10</code> xy pixels in xy direction, <code>2</code> z pixels in z direction (normalised to have same units as <code>yx_pixels</code>).</p> required <code>reg_constant_scale</code> <code>Optional[float]</code> <p>Constant used for scaling and rotation when doing regularized least squares. <code>None</code> means no regularized least squares performed. Typical = <code>5e8</code>.</p> <code>None</code> <code>reg_constant_shift</code> <code>Optional[float]</code> <p>Constant used for shift when doing regularized least squares. <code>None</code> means no regularized least squares performed. Typical = <code>500</code></p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>transforms</code> - <code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>. <code>transforms[t, r, c]</code> is the final affine transform found for tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</li> </ul> <code>dict</code> <ul> <li> <p><code>debug_info</code> - <code>dict</code> containing 7 <code>np.ndarray</code> -</p> </li> <li> <p><code>n_matches</code> - <code>int [n_tiles x n_rounds x n_channels]</code>.     Number of matches found for each transform.</p> </li> <li><code>error</code> - <code>float [n_tiles x n_rounds x n_channels]</code>.     Average distance between neighbours below <code>dist_thresh</code>.</li> <li><code>failed</code> - <code>bool [n_tiles x n_rounds x n_channels]</code>.     Indicates tiles/rounds/channels to which transform had too few matches or transform was     anomalous compared to median. These were not included when calculating <code>av_scalings</code> or <code>av_shifts</code>.</li> <li><code>is_converged</code> - <code>bool [n_tiles x n_rounds x n_channels]</code>.     <code>False</code> if max iterations reached before transform converged.</li> <li><code>av_scaling</code> - <code>float [n_channels x n_dim]</code>.     Chromatic aberration scaling factor to each channel from reference channel.     Made using all rounds and tiles.</li> <li><code>av_shift</code> - <code>float [n_tiles x n_rounds x dim]</code>.     <code>av_shift[t, r]</code> is the average shift from reference round to round <code>r</code> for tile <code>t</code> across all     colour channels.</li> <li><code>transforms_outlier</code> - <code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>.     <code>transforms_outlier[t, r, c]</code> is the final affine transform found for tile <code>t</code>, round <code>r</code>, channel <code>c</code>     without regularization for <code>t</code>, <code>r</code>, <code>c</code> indicated by failed otherwise it is <code>0</code>.</li> </ul> Source code in <code>coppafish/register/base.py</code> <pre><code>def icp(yxz_base: np.ndarray, yxz_target: np.ndarray, transforms_initial: np.ndarray, n_iter: int,\n        dist_thresh: float, matches_thresh: Union[int, np.ndarray], scale_dev_thresh: np.ndarray,\n        shift_dev_thresh: np.ndarray, reg_constant_scale: Optional[float] = None,\n        reg_constant_shift: Optional[float] = None) -&gt; Tuple[np.ndarray, dict]:\n\"\"\"\n    This gets the affine `transforms` from `yxz_base` to `yxz_target` using iterative closest point until\n    all iterations used or convergence.\n    For `transforms` that have matches below `matches_thresh` or are anomalous compared to `av_transform`,\n    the `transforms` are recomputed using regularized least squares to ensure they are close to the `av_transform`.\n    If either `reg_constant_rot = None` or `reg_constant_shift = None` then this is not done.\n\n    Args:\n        yxz_base: `object [n_tiles]`.\n            `yxz_base[t]` is a numpy `float` array `[n_base_spots x dim]`.\n            coordinates of spots on reference round of tile `t`.\n        yxz_target: `object [n_tiles x n_rounds x n_channels]`.\n            `yxz_target[t, r, c]` is a numpy `float` array `[n_target_spots x 3]`.\n            coordinates of spots in tile `t`, round `r`, channel `c`.\n        transforms_initial: `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`.\n            `transforms_initial[t, r, c]` is the starting affine transform for tile `t`\n            from the reference image to round `r`, channel `c`.\n            `transforms_initial[t, r, c, :dim, :dim]` is probably going to be the identity matrix.\n            `transforms_initial[t, r, c, dim, :]` is the shift which needs to be pre-computed somehow to get a\n            good result.\n        n_iter: Max number of iterations to perform of ICP.\n        dist_thresh: If neighbours closer than this, they are used to compute the new transform.\n            Typical: `3`.\n        matches_thresh: `int [n_tiles x n_rounds x n_channels]` or single `int`.\n            `n_matches` for a particular transform must exceed this to be used when computing `av_transform`.\n            Can specify a single threshold for all transforms or a different threshold for each.\n            E.g. you may give a lower threshold if that tile/round/channel has fewer spots.\n            Typical: `200`.\n        scale_dev_thresh: `float [n_dim]`.\n            Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension.\n            Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be\n            same in each.\n            Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get\n            a lot of matches.\n            Typical: `0.01`.\n        shift_dev_thresh: `float [n_dim]`.\n            Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension.\n            Typically, this threshold will be the same in y and x but different in z.\n            Typical: `10` xy pixels in xy direction, `2` z pixels in z direction\n            (normalised to have same units as `yx_pixels`).\n        reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares.\n            `None` means no regularized least squares performed.\n            Typical = `5e8`.\n        reg_constant_shift: Constant used for shift when doing regularized least squares.\n            `None` means no regularized least squares performed.\n            Typical = `500`\n\n    Returns:\n        - `transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`.\n            `transforms[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c`.\n        - `debug_info` - `dict` containing 7 `np.ndarray` -\n\n            - `n_matches` - `int [n_tiles x n_rounds x n_channels]`.\n                Number of matches found for each transform.\n            - `error` - `float [n_tiles x n_rounds x n_channels]`.\n                Average distance between neighbours below `dist_thresh`.\n            - `failed` - `bool [n_tiles x n_rounds x n_channels]`.\n                Indicates tiles/rounds/channels to which transform had too few matches or transform was\n                anomalous compared to median. These were not included when calculating `av_scalings` or `av_shifts`.\n            - `is_converged` - `bool [n_tiles x n_rounds x n_channels]`.\n                `False` if max iterations reached before transform converged.\n            - `av_scaling` - `float [n_channels x n_dim]`.\n                Chromatic aberration scaling factor to each channel from reference channel.\n                Made using all rounds and tiles.\n            - `av_shift` - `float [n_tiles x n_rounds x dim]`.\n                `av_shift[t, r]` is the average shift from reference round to round `r` for tile `t` across all\n                colour channels.\n            - `transforms_outlier` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`.\n                `transforms_outlier[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c`\n                without regularization for `t`, `r`, `c` indicated by failed otherwise it is `0`.\n    \"\"\"\n    n_tiles, n_rounds, n_channels = yxz_target.shape\n    if not utils.errors.check_shape(yxz_base, [n_tiles]):\n        raise utils.errors.ShapeError(\"yxz_base\", yxz_base.shape, (n_tiles,))\n    tree_target = np.zeros_like(yxz_target)\n    for t in range(n_tiles):\n        for r in range(n_rounds):\n            for c in range(n_channels):\n                tree_target[t, r, c] = KDTree(yxz_target[t, r, c])\n\n    n_matches = np.zeros_like(yxz_target, dtype=int)\n    error = np.zeros_like(yxz_target, dtype=float)\n    neighbour = np.zeros_like(yxz_target)\n    is_converged = np.zeros_like(yxz_target, dtype=bool)\n    transforms = transforms_initial.copy().astype(float)\n    transforms_outlier = np.zeros_like(transforms)\n    finished_good_images = False\n    av_transforms = None\n    i_finished_good = 0\n    i = 0\n    with tqdm(total=n_tiles * n_rounds * n_channels) as pbar:\n        pbar.set_description(f\"Iterative Closest Point to find affine transforms\")\n        while i &lt; n_iter:\n            pbar.set_postfix({'iter': f'{i + 1}/{n_iter}', 'regularized': str(finished_good_images)})\n            neighbour_last = neighbour.copy()\n            for t in range(n_tiles):\n                for r in range(n_rounds):\n                    for c in range(n_channels):\n                        if is_converged[t, r, c]:\n                            continue\n                        if finished_good_images:\n                            reg_transform = av_transforms[t, r, c]\n                        else:\n                            reg_transform = None\n                        transforms[t, r, c], neighbour[t, r, c], n_matches[t, r, c], error[t, r, c] = \\\n                            get_transform(yxz_base[t], transforms[t, r, c], yxz_target[t, r, c], dist_thresh,\n                                          tree_target[t, r, c], reg_constant_scale, reg_constant_shift, reg_transform)\n                        if i &gt; i_finished_good:\n                            is_converged[t, r, c] = np.abs(neighbour[t, r, c] - neighbour_last[t, r, c]).max() == 0\n                            if is_converged[t, r, c]:\n                                pbar.update(1)\n            if (is_converged.all() and not finished_good_images) or (i == n_iter - 1 and not finished_good_images):\n                av_transforms, av_scaling, av_shifts, failed, failed_non_matches = \\\n                    get_average_transform(transforms, n_matches, matches_thresh, scale_dev_thresh, shift_dev_thresh)\n                if reg_constant_scale is not None and reg_constant_shift is not None:\n                    # reset transforms of those that failed to average transform as starting point for\n                    # regularised fitting\n                    transforms_outlier[failed, :, :] = transforms[failed, :, :].copy()\n                    transforms[failed, :, :] = av_transforms[failed, :, :]\n                    is_converged[failed] = False\n                    i = -1  # Allow n_iter to find regularized best transforms as well.\n                    finished_good_images = True\n                    pbar.update(-np.sum(failed.flatten()))\n            i += 1\n            if is_converged.all():\n                break\n    pbar.close()\n    if i == n_iter:\n        warnings.warn(f\"Max number of iterations, {n_iter} reached but only \"\n                      f\"{np.sum(is_converged)}/{np.sum(is_converged&gt;=0)} transforms converged\")\n\n    debug_info = {'n_matches': n_matches, 'error': error, 'failed': failed, 'is_converged': is_converged,\n                  'av_scaling': av_scaling, 'av_shifts': av_shifts, 'transforms_outlier': transforms_outlier}\n    return transforms, debug_info\n</code></pre>"},{"location":"code/register/base/#coppafish.register.base.mod_median","title":"<code>mod_median(array, ignore, axis=0)</code>","text":"<p>This computes the median ignoring values indicated by <code>ignore</code>.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>np.ndarray</code> <p><code>float [n_dim_1 x n_dim_2 x ... x n_dim_N]</code>. array to compute median from.</p> required <code>ignore</code> <code>np.ndarray</code> <p><code>bool [n_dim_1 x n_dim_2 x ... x n_dim_N]</code>. True for values in array that should not be used to compute median.</p> required <code>axis</code> <code>Union[int, List[int]]</code> <p><code>int [n_axis_av]</code>. Which axis to average over.</p> <code>0</code> <p>Returns:</p> Type Description <code>Union[float, np.ndarray]</code> <p>Median value without using those values indicated by <code>ignore</code>.</p> Source code in <code>coppafish/register/base.py</code> <pre><code>def mod_median(array: np.ndarray, ignore: np.ndarray, axis: Union[int, List[int]] = 0) -&gt; Union[float, np.ndarray]:\n\"\"\"\n    This computes the median ignoring values indicated by ```ignore```.\n\n    Args:\n        array: ```float [n_dim_1 x n_dim_2 x ... x n_dim_N]```.\n            array to compute median from.\n        ignore: ```bool [n_dim_1 x n_dim_2 x ... x n_dim_N]```.\n            True for values in array that should not be used to compute median.\n        axis: ```int [n_axis_av]```.\n            Which axis to average over.\n\n    Returns:\n        Median value without using those values indicated by ```ignore```.\n    \"\"\"\n    mod_array = array.copy()\n    mod_array[ignore] = np.nan\n    return np.nanmedian(mod_array, axis=axis)\n</code></pre>"},{"location":"code/register/base/#coppafish.register.base.transform_from_scale_shift","title":"<code>transform_from_scale_shift(scale, shift)</code>","text":"<p>Gets <code>[dim+1 x dim]</code> affine transform from scale for each channel and shift for each tile/round.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>np.ndarray</code> <p><code>float [n_channels x n_dims]</code>. <code>scale[c, d]</code> is the scaling to account for chromatic aberration from reference channel to channel <code>c</code> for dimension <code>d</code>. Typically, as an initial guess all values in scale will be <code>1</code>.</p> required <code>shift</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_dims]</code>. <code>shift[t, r, d]</code> is the shift to account for the shift between the reference round for tile <code>t</code> and round <code>r</code> for tile <code>t</code> in dimension <code>d</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x dim+1 x dim]</code>. <code>[t, r, c]</code> is the affine transform for tile <code>t</code>, round <code>r</code>, channel <code>c</code> computed from <code>scale[c]</code> and <code>shift[t, r]</code>.</p> Source code in <code>coppafish/register/base.py</code> <pre><code>def transform_from_scale_shift(scale: np.ndarray, shift: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Gets ```[dim+1 x dim]``` affine transform from scale for each channel and shift for each tile/round.\n\n    Args:\n        scale: ```float [n_channels x n_dims]```.\n            ```scale[c, d]``` is the scaling to account for chromatic aberration from reference channel\n            to channel ```c``` for dimension ```d```.\n            Typically, as an initial guess all values in scale will be ```1```.\n        shift: ```float [n_tiles x n_rounds x n_dims]```.\n            ```shift[t, r, d]``` is the shift to account for the shift between the reference round for tile ```t``` and\n            round ```r``` for tile ```t``` in dimension ```d```.\n\n    Returns:\n        ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```.\n            ```[t, r, c]``` is the affine transform for tile ```t```, round ```r```, channel ```c``` computed from\n            ```scale[c]``` and ```shift[t, r]```.\n    \"\"\"\n    n_channels = scale.shape[0]\n    n_tiles, n_rounds, dim = shift.shape\n    transforms = np.zeros((n_tiles, n_rounds, n_channels, dim + 1, dim))\n    for t in range(n_tiles):\n        for r in range(n_rounds):\n            for c in range(n_channels):\n                transforms[t, r, c, :dim, :, ] = np.eye(dim) * scale[c]\n                transforms[t, r, c, dim, :] = shift[t, r]\n    return transforms\n</code></pre>"},{"location":"code/register/check_transforms/","title":"Check Transforms","text":""},{"location":"code/register/check_transforms/#coppafish.register.check_transforms.check_transforms","title":"<code>check_transforms(nb)</code>","text":"<p>This checks that a decent number of affine transforms computed in the register stage of the pipeline are acceptable (<code>n_matches &gt; n_matches_thresh</code>).</p> <p>If for any of the following, the fraction of transforms with <code>n_matches &lt; n_matches_thresh</code> exceeds <code>config['register']['n_transforms_error_fraction']</code> an error will be raised.</p> <ul> <li>Each channel across all rounds and tiles.</li> <li>Each tile across all rounds and channels.</li> <li>Each round across all tile and channels.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing <code>find_spots</code> page.</p> required Source code in <code>coppafish/register/check_transforms.py</code> <pre><code>def check_transforms(nb: Notebook):\n\"\"\"\n    This checks that a decent number of affine transforms computed in the register stage of the pipeline\n    are acceptable (`n_matches &gt; n_matches_thresh`).\n\n    If for any of the following, the fraction of transforms with\n    `n_matches &lt; n_matches_thresh` exceeds `config['register']['n_transforms_error_fraction']`\n    an error will be raised.\n\n    * Each channel across all rounds and tiles.\n    * Each tile across all rounds and channels.\n    * Each round across all tile and channels.\n\n    Args:\n        nb: *Notebook* containing `find_spots` page.\n\n    \"\"\"\n    config = nb.get_config()['register']\n    use_tiles = np.asarray(nb.basic_info.use_tiles)\n    use_rounds = np.asarray(nb.basic_info.use_rounds)\n    use_channels = np.asarray(nb.basic_info.use_channels)\n    n_matches = nb.register_debug.n_matches[np.ix_(use_tiles, use_rounds, use_channels)]\n    n_matches_thresh = nb.register_debug.n_matches_thresh[np.ix_(use_tiles, use_rounds, use_channels)]\n    error_message = \"\"\n\n    # Consider bad channels first as most likely to have consistently failed transform\n    n_transforms = len(use_tiles) * len(use_rounds)\n    n_transforms_error = int(np.clip(np.floor(n_transforms * config['n_transforms_error_fraction']), 1, np.inf))\n    n_transforms_fail = np.zeros(len(use_channels), dtype=int)\n    for c in range(len(use_channels)):\n        failed = np.vstack(np.where(n_matches[:, :, c] &lt; n_matches_thresh[:, :, c])).T\n        n_transforms_fail[c] = failed.shape[0]\n        if n_transforms_fail[c] &gt; 0:\n            failed_info = np.zeros((n_transforms_fail[c], 4), dtype=int)\n            failed_info[:, 0] = use_tiles[failed[:, 0]]\n            failed_info[:, 1] = use_rounds[failed[:, 1]]\n            failed_info[:, 2] = n_matches[failed[:,0], failed[:,1], c]\n            failed_info[:, 3] = n_matches_thresh[failed[:, 0], failed[:, 1], c]\n            warnings.warn(f\"\\nChannel {use_channels[c]} - {n_transforms_fail[c]} tiles/rounds with n_matches &lt; \"\n                          f\"n_matches_thresh:\\nInformation for failed transforms\\n\"\n                          f\"Tile, Round, n_matches, n_matches_thresh:\\n{failed_info}\")\n\n    fail_inds = np.where(n_transforms_fail &gt;= n_transforms_error)[0]\n    if len(fail_inds) &gt; 0:\n        error_message = error_message + f\"\\nChannels that failed: {use_channels[fail_inds]}\\n\" \\\n                                        f\"This is because out of {n_transforms} tiles/rounds, these channels had \" \\\n                                        f\"respectively:\\n{n_transforms_fail[fail_inds]}\\ntiles/rounds with \" \\\n                                        f\"n_matches &lt; n_matches_thresh. \" \\\n                                        f\"These are all more than the error threshold of \" \\\n                                        f\"{n_transforms_error}.\\nConsider removing these from use_channels.\"\n        # don't consider failed channels for subsequent warnings/errors\n        use_channels = np.setdiff1d(use_channels, use_channels[fail_inds])\n        n_matches = nb.register_debug.n_matches[np.ix_(use_tiles, use_rounds, use_channels)]\n        n_matches_thresh = nb.register_debug.n_matches_thresh[np.ix_(use_tiles, use_rounds, use_channels)]\n\n    # Consider bad tiles next as second most likely to have consistently low spot counts in a tile\n    n_transforms = len(use_channels) * len(use_rounds)\n    n_transforms_error = int(np.clip(np.floor(n_transforms * config['n_transforms_error_fraction']), 1, np.inf))\n    n_transforms_fail = np.zeros(len(use_tiles), dtype=int)\n    for t in range(len(use_tiles)):\n        failed = np.vstack(np.where(n_matches[t] &lt; n_matches_thresh[t])).T\n        n_transforms_fail[t] = failed.shape[0]\n    fail_inds = np.where(n_transforms_fail &gt;= n_transforms_error)[0]\n    if len(fail_inds) &gt; 0:\n        error_message = error_message + f\"\\nTiles that failed: {use_tiles[fail_inds]}\\n\" \\\n                                        f\"This is because out of {n_transforms} rounds/channels, these tiles had \" \\\n                                        f\"respectively:\\n{n_transforms_fail[fail_inds]}\\nrounds/channels with \" \\\n                                        f\"n_matches &lt; n_matches_thresh. \" \\\n                                        f\"These are all more than the error threshold of \" \\\n                                        f\"{n_transforms_error}.\\nConsider removing these from use_tiles.\"\n        # don't consider failed channels for subsequent warnings/errors\n        use_tiles = np.setdiff1d(use_tiles, use_tiles[fail_inds])\n        n_matches = nb.register_debug.n_matches[np.ix_(use_tiles, use_rounds, use_channels)]\n        n_matches_thresh = nb.register_debug.n_matches_thresh[np.ix_(use_tiles, use_rounds, use_channels)]\n\n    # Consider bad rounds last as least likely to have consistently low spot counts in a round\n    n_transforms = len(use_channels) * len(use_tiles)\n    n_transforms_error = int(np.clip(np.floor(n_transforms * config['n_transforms_error_fraction']), 1, np.inf))\n    n_transforms_fail = np.zeros(len(use_rounds), dtype=int)\n    for r in range(len(use_rounds)):\n        failed = np.vstack(np.where(n_matches[:, r] &lt; n_matches_thresh[:, r])).T\n        n_transforms_fail[r] = failed.shape[0]\n    fail_inds = np.where(n_transforms_fail &gt;= n_transforms_error)[0]\n    if len(fail_inds) &gt; 0:\n        error_message = error_message + f\"\\nRounds that failed: {use_rounds[fail_inds]}\\n\" \\\n                                        f\"This is because out of {n_transforms} tiles/channels, these rounds had \" \\\n                                        f\"respectively:\\n{n_transforms_fail[fail_inds]}\\ntiles/channels with \" \\\n                                        f\"n_matches &lt; n_matches_thresh. \" \\\n                                        f\"These are all more than the error threshold of \" \\\n                                        f\"{n_transforms_error}.\\nConsider removing these from use_rounds.\"\n\n    if len(error_message) &gt; 0:\n        error_message = error_message + f\"\\nLook at the following diagnostics to decide if transforms \" \\\n                                        f\"are acceptable to continue:\\n\" \\\n                                        f\"coppafish.plot.scale_box_plots\\ncoppafish.plot.view_affine_shift_info\\n\" \\\n                                        f\"coppafish.plot.view_icp\"\n        raise ValueError(error_message)\n</code></pre>"},{"location":"code/setup/file_names/","title":"File Names","text":""},{"location":"code/setup/file_names/#coppafish.setup.file_names.set_file_names","title":"<code>set_file_names(nb, nbp)</code>","text":"<p>Function to set add <code>file_names</code> page to notebook. It requires notebook to be able to access a config file containing a <code>file_names</code> section and also the notebook to contain a <code>basic_info</code> page.</p> <p>Note</p> <p>This will be called every time the notebook is loaded to deal will case when <code>file_names</code> section of config file changed.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <p>Notebook containing at least the <code>basic_info</code> page.</p> required <code>nbp</code> <p>NotebookPage with no variables added. This is just to avoid initialising it within the function which would cause a circular import.</p> required Source code in <code>coppafish/setup/file_names.py</code> <pre><code>def set_file_names(nb, nbp):\n\"\"\"\n    Function to set add `file_names` page to notebook. It requires notebook to be able to access a\n    config file containing a `file_names` section and also the notebook to contain a `basic_info` page.\n\n    !!! note\n        This will be called every time the notebook is loaded to deal will case when `file_names` section of\n        config file changed.\n\n    Args:\n        nb: *Notebook* containing at least the `basic_info` page.\n        nbp: *NotebookPage* with no variables added.\n            This is just to avoid initialising it within the function which would cause a circular import.\n\n    \"\"\"\n    config = nb.get_config()['file_names']\n    nbp.name = 'file_names'  # make sure name is correct\n    # Copy some variables that are in config to page.\n    nbp.input_dir = config['input_dir']\n    nbp.output_dir = config['output_dir']\n    nbp.tile_dir = config['tile_dir']\n\n    # remove file extension from round and anchor file names if it is present\n    if config['round'] is None:\n        if config['anchor'] is None:\n            raise ValueError(f'Neither imaging rounds nor anchor_round provided')\n        config['round'] = []  # Sometimes the case where just want to run the anchor round.\n    config['round'] = [r.replace(config['raw_extension'], '') for r in config['round']]\n    nbp.round = config['round']\n\n    if config['anchor'] is not None:\n        config['anchor'] = config['anchor'].replace(config['raw_extension'], '')\n    nbp.anchor = config['anchor']\n    nbp.raw_extension = config['raw_extension']\n    nbp.raw_metadata = config['raw_metadata']\n\n    if config['dye_camera_laser'] is None:\n        # Default information is project\n        config['dye_camera_laser'] = os.path.join(os.path.dirname(__file__), 'dye_camera_laser_raw_intensity.csv')\n    nbp.dye_camera_laser = config['dye_camera_laser']\n    config['code_book'] = config['code_book'].replace('.txt', '')\n    nbp.code_book = config['code_book'] + '.txt'\n\n    # where to save scale and scale_anchor values used in extract step.\n    config['scale'] = config['scale'].replace('.txt', '')\n    nbp.scale = os.path.join(config['tile_dir'], config['scale'] + '.txt')\n\n    # where to save psf, indicating average spot shape in raw image. Only ever needed in 3D.\n    if nb.basic_info.is_3d:\n        config['psf'] = config['psf'].replace('.npy', '')\n        nbp.psf = os.path.join(config['output_dir'], config['psf'] + '.npy')\n    else:\n        nbp.psf = None\n\n    # where to save omp_spot_shape, indicating average spot shape in omp coefficient sign images.\n    config['omp_spot_shape'] = config['omp_spot_shape'].replace('.npy', '')\n    omp_spot_shape_file = os.path.join(config['output_dir'], config['omp_spot_shape'] + '.npy')\n    nbp.omp_spot_shape = omp_spot_shape_file\n\n    # Add files so save omp results after each tile as security if hit any bugs\n    config['omp_spot_info'] = config['omp_spot_info'].replace('.npy', '')\n    nbp.omp_spot_info = os.path.join(config['output_dir'], config['omp_spot_info'] + '.npy')\n    config['omp_spot_coef'] = config['omp_spot_coef'].replace('.npz', '')\n    nbp.omp_spot_coef = os.path.join(config['output_dir'], config['omp_spot_coef'] + '.npz')\n\n    # Add files so save plotting information for pciseq\n    config['pciseq'] = [val.replace('.csv', '') for val in config['pciseq']]\n    nbp.pciseq = [os.path.join(config['output_dir'], val + '.csv') for val in config['pciseq']]\n\n    # add dapi channel and anchor channel to notebook even if set to None.\n    if config['big_dapi_image'] is None:\n        nbp.big_dapi_image = None\n    else:\n        config['big_dapi_image'] = config['big_dapi_image'].replace('.npz', '')\n        if nb.basic_info.dapi_channel is None:\n            nbp.big_dapi_image = None\n        else:\n            nbp.big_dapi_image = os.path.join(config['output_dir'], config['big_dapi_image'] + '.npz')\n    if config['big_anchor_image'] is None:\n        nbp.big_anchor_image = None\n    else:\n        config['big_anchor_image'] = config['big_anchor_image'].replace('.npz', '')\n        nbp.big_anchor_image = os.path.join(config['output_dir'], config['big_anchor_image'] + '.npz')\n\n    if config['anchor'] is not None:\n        round_files = config['round'] + [config['anchor']]\n    else:\n        round_files = config['round']\n\n    if nb.basic_info.is_3d:\n        tile_names = get_tile_file_names(config['tile_dir'], round_files, nb.basic_info.n_tiles,\n                                         nb.basic_info.n_channels)\n    else:\n        tile_names = get_tile_file_names(config['tile_dir'], round_files, nb.basic_info.n_tiles)\n\n    nbp.tile = tile_names.tolist()  # npy tile file paths list [n_tiles x n_rounds (x n_channels if 3D)]\n    nb += nbp\n</code></pre>"},{"location":"code/setup/notebook/","title":"Notebook","text":""},{"location":"code/setup/notebook/#notebook","title":"Notebook","text":"<p>A write-only file-synchronized class to keep track of coppaFISH results.</p> <p>The <code>Notebook</code> object stores all of the outputs of the script.  Almost all information saved in the <code>Notebook</code> is encapsulated within <code>\"pages\"</code>, from the <code>NotebookPage</code> object.  To add a <code>NotebookPage</code> object to a <code>Notebook</code>, use the <code>\"add_page\"</code> method. In addition to saving pages, it also saves the contents of the config file, and the time at which the notebook and each page was created.</p> <p>To create a <code>Notebook</code>, pass it the path to the file where the <code>Notebook</code> is to be stored (<code>notebook_file</code>), and optionally, the path to the configuration file (<code>config_file</code>).  If <code>notebook_file</code> already exists, the notebook located at this path will be loaded.  If not, a new file will be created as soon as the first data is written to the <code>Notebook</code>.</p> <p>Example</p> With config_fileNo config_file <pre><code>nb = Notebook(\"nbfile.npz\", \"config_file.ini\")\nnbp = NotebookPage(\"pagename\")\nnbp.var = 1\nnb.add_page(nbp) or nb += nbp or nb.pagename = nbp\nassert nb.pagename.var == 1\n</code></pre> <pre><code>nb = Notebook(\"nbfile.npz\")\nnbp = NotebookPage(\"pagename\")\nnbp.var = 1\nnb.add_page(nbp) or nb += nbp or nb.pagename = nbp\nassert nb.pagename.var == 1\n</code></pre> <p>Because it is automatically saved to the disk, you can close Python, reopen it, and do the following (Once <code>config_file</code>, added to notebook there is no need to load it again unless it has been changed): <pre><code>nb2 = Notebook(\"nbfile.npz\")\nassert nb2.pagename.var == 1\n</code></pre></p> <p>If you create a notebook without specifying <code>notebook_file</code>, i.e. <code>nb = Notebook(config_file=\"config_file.ini\")</code>, the <code>notebook_file</code> will be set to: <pre><code>notebook_file = config['file_names']['output_dir'] + config['file_names']['notebook_name'])\n</code></pre></p> <p>On using config_file</p> <p>When running the coppafish pipeline, the <code>Notebook</code> requires a <code>config_file</code> to access information required for the different stages of the pipeline through <code>nb.get_config()</code>. But if using the <code>Notebook</code> to store information not in coppafish pipeline, it is not needed.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>class Notebook:\n\"\"\"A write-only file-synchronized class to keep track of *coppaFISH* results.\n\n    The `Notebook` object stores all of the outputs of the script.  Almost all\n    information saved in the `Notebook` is encapsulated within `\"pages\"`, from the\n    `NotebookPage` object.  To add a `NotebookPage` object to a `Notebook`, use the\n    `\"add_page\"` method.\n    In addition to saving pages, it also saves the contents of the\n    config file, and the time at which the notebook and each page was created.\n\n    To create a `Notebook`, pass it the path to the file where the `Notebook` is to\n    be stored (`notebook_file`), and optionally, the path to the configuration file\n    (`config_file`).  If `notebook_file` already exists, the notebook located\n    at this path will be loaded.  If not, a new file will be created as soon as\n    the first data is written to the `Notebook`.\n\n    !!!example\n        === \"With config_file\"\n\n            ``` python\n            nb = Notebook(\"nbfile.npz\", \"config_file.ini\")\n            nbp = NotebookPage(\"pagename\")\n            nbp.var = 1\n            nb.add_page(nbp) or nb += nbp or nb.pagename = nbp\n            assert nb.pagename.var == 1\n            ```\n\n        === \"No config_file\"\n\n            ``` python\n            nb = Notebook(\"nbfile.npz\")\n            nbp = NotebookPage(\"pagename\")\n            nbp.var = 1\n            nb.add_page(nbp) or nb += nbp or nb.pagename = nbp\n            assert nb.pagename.var == 1\n            ```\n\n    Because it is automatically saved to the disk, you can close Python, reopen\n    it, and do the following (Once `config_file`, added to notebook there is no need to load it again unless it has\n    been changed):\n    ```python\n    nb2 = Notebook(\"nbfile.npz\")\n    assert nb2.pagename.var == 1\n    ```\n\n    If you create a notebook without specifying `notebook_file`, i.e.\n    ```nb = Notebook(config_file=\"config_file.ini\")```, the `notebook_file` will be set to:\n    ```python\n    notebook_file = config['file_names']['output_dir'] + config['file_names']['notebook_name'])\n    ```\n\n    !!!note \"On using config_file\"\n        When running the coppafish pipeline, the `Notebook` requires a `config_file` to access information required for\n        the different stages of the pipeline through `nb.get_config()`.\n        But if using the `Notebook` to store information not in coppafish pipeline, it is not needed.\n    \"\"\"\n    _SEP = \"_-_\"  # Separator between notebook page name and item name when saving to file\n    _ADDEDMETA = \"TIME_CREATED\"  # Key for notebook created time\n    _CONFIGMETA = \"CONFIGFILE\"  # Key for config string\n    _NBMETA = \"NOTEBOOKMETA\"  # Key for metadata about the entire notebook\n    # If these sections of config files are different, will not raise error.\n    _no_compare_config_sections = ['file_names']\n\n    # When the pages corresponding to the keys are added, a save will not be triggered.\n    # When save does happen, these pages won't be saved, but made on loading using\n    # the corresponding function, load_func, if the notebook contains the pages indicated by\n    # load_func_req.\n    # load_func must only take notebook and page_name as input and has no output but page will be added to notebook.\n    # When last of pages in load_func_req have been added, the page will automatically be added.\n    _no_save_pages = {'file_names': {'load_func': load_file_names, 'load_func_req': ['basic_info']}}\n\n    def __init__(self, notebook_file: Optional[str] = None, config_file: Optional[str] = None):\n        # Give option to load with config_file as None so don't have to supply ini_file location every time if\n        # already initialised.\n        # Also, can provide config_file if file_names section changed.\n        # Don't need to provide notebook_file as can determine this from config_file as:\n        # config['file_names']['output_dir'] + config['file_names']['notebook_name']\n\n        # numpy isn't compatible with npz files which do not end in the suffix\n        # .npz.  If one isn't there, it will add the extension automatically.\n        # We do the same thing here.\n        object.__setattr__(self, '_page_times', {})\n        if notebook_file is None:\n            if config_file is None:\n                raise ValueError('Both notebook_file and config_file are None')\n            else:\n                config_file_names = get_config(config_file)['file_names']\n                notebook_file = os.path.join(config_file_names['output_dir'], config_file_names['notebook_name'])\n                if not os.path.isdir(config_file_names['output_dir']):\n                    raise ValueError(f\"\\nconfig['file_names']['output_dir'] = {config_file_names['output_dir']}\\n\"\n                                     f\"is not a valid directory.\")\n        if not notebook_file.endswith(\".npz\"):\n            notebook_file = notebook_file + \".npz\"\n        # Note that the ordering of _pages may change across saves and loads,\n        # but the order will always correspond to the order of _pages_times\n        self._file = notebook_file\n        self._config_file = config_file\n        # Read the config file, but don't assign anything yet.  Here, we just\n        # save a copy of the config file.  This isn't the main place the config\n        # file should be read from.\n        if config_file is not None:\n            if os.path.isfile(str(config_file)):\n                with open(config_file, 'r') as f:\n                    read_config = f.read()\n            else:\n                raise ValueError(f'Config file given is not valid: {config_file}')\n        else:\n            read_config = None\n        # If the file already exists, initialize the Notebook object from this\n        # file.  Otherwise, initialize it empty.\n        if os.path.isfile(self._file):\n            pages, self._page_times, self._created_time, self._config = self.from_file(self._file)\n            for page in pages:\n                object.__setattr__(self, page.name, page)  # don't want to set page_time hence use object setattr\n            if read_config is not None:\n                if not self.compare_config(get_config(read_config)):\n                    raise SystemError(\"Passed config file is not the same as the saved config file\")\n                self._config = read_config  # update config to new one - only difference will be in file_names section\n            self.add_no_save_pages()  # add file_names page with new config\n        else:\n            warnings.warn(\"Notebook file not found, creating a new notebook.\")\n            if read_config is None:\n                warnings.warn(\"Have not passed a config_file so Notebook.get_config() won't work.\")\n            self._created_time = time.time()\n            self._config = read_config\n\n    def __repr__(self):\n        # This means that print(nb) gives file location of notebook and\n        # pages in the notebook sorted by time added to the notebook.\n        sort_page_names = sorted(self._page_times.items(), key=lambda x: x[1])  # sort by time added to notebook\n        page_names = [name[0] for name in sort_page_names]\n        n_names_per_line = 4\n        i = n_names_per_line - 1\n        while i &lt; len(page_names) - n_names_per_line / 2:\n            page_names[i + 1] = \"\\n\" + page_names[i + 1]\n            i = i + n_names_per_line\n        page_names = \", \".join(page_names)\n        return f\"File: {self._file}\\nPages: {page_names}\"\n\n    def get_config(self):\n\"\"\"\n        Returns config as dictionary.\n        \"\"\"\n        if self._config is not None:\n            return get_config(self._config)\n        else:\n            raise ValueError('Notebook does not contain config parameter.')\n\n    def compare_config(self, config_2: dict) -&gt; bool:\n\"\"\"\n        Compares whether `config_2` is equal to the config file saved in the notebook.\n        Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook\n        will be checked.\n\n        Args:\n            config_2: Dictionary with keys corresponding to sections where a section\n                is also a dictionary containing parameters.\n                E.g. `config_2['basic_info]['param1'] = 5`.\n\n        Returns:\n            `True` if config dictionaries are equal in required sections.\n\n        \"\"\"\n        # TODO: issue here that if default settings file changed, the equality here would still be true.\n        config = self.get_config()\n        is_equal = True\n        if config.keys() != config_2.keys():\n            warnings.warn('The config files have different sections.')\n            is_equal = False\n        else:\n            sort_page_names = sorted(self._page_times.items(), key=lambda x: x[1])  # sort by time added to notebook\n            # page names are either same as config sections or with _debug suffix\n            page_names = [name[0].replace('_debug', '') for name in sort_page_names]\n            for section in config.keys():\n                # Only compare sections for which there is a corresponding page in the notebook.\n                if section not in self._no_compare_config_sections and section in page_names:\n                    if config[section] != config_2[section]:\n                        warnings.warn(f\"The {section} section of the two config files differ.\")\n                        is_equal = False\n        return is_equal\n\n    def describe(self, key=None):\n\"\"\"\n        `describe(var)` will print comments for variables called `var` in each `NotebookPage`.\n        \"\"\"\n        if key is None:\n            print(self.__repr__())\n        elif len(self._page_times) == 0:\n            print(f\"No pages so cannot search for variable {key}\")\n        else:\n            sort_page_names = sorted(self._page_times.items(), key=lambda x: x[1])  # sort by time added to notebook\n            page_names = [name[0] for name in sort_page_names]\n            first_page = self.__getattribute__(page_names[0])\n            with open(first_page._comments_file) as f:\n                json_comments = json.load(f)\n            if self._config is not None:\n                config = self.get_config()\n            n_times_appeared = 0\n            for page_name in page_names:\n                # if in comments file, then print the comment\n                if key in json_comments[page_name]:\n                    print(f\"{key} in {page_name}:\")\n                    self.__getattribute__(page_name).describe(key)\n                    print(\"\")\n                    n_times_appeared += 1\n\n                elif self._config is not None:\n                    # if in config file, then print the comment\n                    # find sections in config file with matching name to current page\n                    config_sections_with_name = [page_name.find(list(config.keys())[i]) for i in\n                                                 range(len(config.keys()))]\n                    config_sections = np.array(list(config.keys()))[np.array(config_sections_with_name) != -1]\n                    for section in config_sections:\n                        for param in config[section].keys():\n                            if param.lower() == key.lower():\n                                print(f\"No variable named {key} in the {page_name} page.\\n\"\n                                      f\"But it is in the {section} section of the config file and has value:\\n\"\n                                      f\"{config[section][param]}\\n\")\n                                n_times_appeared += 1\n            if n_times_appeared == 0:\n                print(f\"{key} is not in any of the pages in this notebook.\")\n\n    def __eq__(self, other):\n        # Test if two `Notebooks` are identical\n        #\n        # For two `Notebooks` to be identical, all aspects must be the same,\n        # excluding the ordering of the pages, and the filename.  All timestamps\n        # must also be identical.\n\n        if self._created_time != other._created_time:\n            return False\n        if self._config != other._config:\n            return False\n        if len(self._page_times) != len(other._page_times):\n            return False\n        for k in self._page_times.keys():\n            if k not in other._page_times or getattr(self, k) != getattr(other, k):\n                return False\n        for k in other._page_times.keys():\n            if k not in self._page_times or getattr(other, k) != getattr(self, k):\n                return False\n        for k, v in self._page_times.items():\n            if k not in other._page_times or v != other._page_times[k]:\n                return False\n        return True\n\n    def __len__(self):\n        # Return the number of pages in the Notebook\n        return len(self._page_times)\n\n    def __setattr__(self, key, value):\n        # Deals with the syntax `nb.key = value`\n        # automatically triggers save if `NotebookPage` is added.\n        # If adding something other than a `NotebookPage`, this syntax does exactly as it is for other classes.\n        if isinstance(value, NotebookPage):\n            if self._SEP in key:\n                raise NameError(f\"The separator {self._SEP} may not be in the page's name\")\n            if value.finalized:\n                raise ValueError(\"Page already added to a Notebook, cannot add twice\")\n            if key in self._page_times.keys():\n                raise ValueError(\"Cannot add two pages with the same name\")\n            if value.name != key:\n                raise ValueError(f\"Page name is {value.name} but key given is {key}\")\n\n            # ensure all the variables in the comments file are included\n            with open(value._comments_file) as f:\n                json_comments = json.load(f)\n            if value.name in json_comments:\n                for var in json_comments[value.name]:\n                    if var not in value._times and var != \"DESCRIPTION\":\n                        raise InvalidNotebookPageError(None, var, value.name)\n                # ensure all variables in page are in comments file\n                for var in value._times:\n                    if var not in json_comments[value.name]:\n                        raise InvalidNotebookPageError(var, None, value.name)\n\n            value.finalized = True\n            object.__setattr__(self, key, value)\n            self._page_times[key] = time.time()\n            if value.name not in self._no_save_pages.keys():\n                self.save()\n            self.add_no_save_pages()\n        elif key in self._page_times.keys():\n            raise ValueError(f\"Page with name {key} in notebook so can't add variable with this name.\")\n        else:\n            object.__setattr__(self, key, value)\n\n    def __delattr__(self, name):\n        # Method to delete a page or attribute. Deals with del nb.name.\n        object.__delattr__(self, name)\n        if name in self._page_times:\n            # extra bit if page\n            del self._page_times[name]\n\n    def add_page(self, page):\n\"\"\"Insert the page `page` into the `Notebook`.\n\n        This function automatically triggers a save.\n        \"\"\"\n        if not isinstance(page, NotebookPage):\n            raise ValueError(\"Only NotebookPage objects may be added to a notebook.\")\n        self.__setattr__(page.name, page)\n\n    def has_page(self, page_name):\n\"\"\"A check to see if notebook includes a page called page_name.\n        If page_name is a list, a boolean list of equal size will be\n        returned indicating whether each page is present.\"\"\"\n        if isinstance(page_name, str):\n            output = any(page_name == p for p in self._page_times)\n        elif isinstance(page_name, list):\n            output = [any(page_name[i] == p for p in self._page_times) for i in range(len(page_name))]\n        else:\n            raise ValueError(f\"page_name given was {page_name}. This is not a list or a string.\")\n        return output\n\n    def __iadd__(self, other):\n        # Syntactic sugar for the add_page method\n        self.add_page(other)\n        return self\n\n    def add_no_save_pages(self):\n\"\"\"\n        This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if\n        the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']`\n        by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`.\n\n        At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page\n        has been added.\n        \"\"\"\n        for page_name in self._no_save_pages.keys():\n            if self.has_page(page_name):\n                continue\n            if all(self.has_page(self._no_save_pages[page_name]['load_func_req'])):\n                # If contains all required pages to run load_func, then add the page\n                self._no_save_pages[page_name]['load_func'](self, page_name)\n\n    def change_page_name(self, old_name: str, new_name: str):\n\"\"\"\n        This changes the name of the page `old_name` to `new_name`. It will trigger two saves,\n        one after changing the new and one after changing the time the page was added to be the time\n        the initial page was added.\n\n        Args:\n            old_name:\n            new_name:\n        \"\"\"\n        nbp = self.__getattribute__(old_name)\n        warnings.warn(f\"Changing name of {old_name} page to {new_name}\")\n        time_added = self._page_times[old_name]\n        nbp.finalized = False\n        nbp.name = new_name\n        self.__delattr__(old_name)\n        self.add_page(nbp)\n        self._page_times[new_name] = time_added  # set time to time page initially added\n        self.save()\n\n    def version_hash(self):\n        # A short string representing the file version.\n        #\n        # Since there are many possible page names and entry names within those\n        # pages, that means there are many, many possible file versions based on\n        # different versions of the code.  Rather than try to keep track of these\n        # versions and appropriately increment some centralized counter, we\n        # generate a short string which is a hash of the page names and the names\n        # of the entries in that page.  This way, it is possible to see if two\n        # notebooks were generated using the same version of the software.  (Of\n        # course, it assumes that no fields are ever set conditionally.)\n\n        s = \"\"\n        for p_name in self._page_times:\n            s += p_name + \"\\n\\n\"\n            page = getattr(self, p_name)\n            s += \"\\n\".join(sorted(page._times.keys()))\n        return hashlib.md5(bytes(s, \"utf8\")).hexdigest()\n\n    def save(self, file: Optional[str] = None):\n\"\"\"\n        Saves Notebook as a npz file at the path indicated by `file`.\n        Args:\n            file: Where to save *Notebook*. If `None`, will use `self._file`.\n\n        \"\"\"\n\"\"\"Save the Notebook to a file\"\"\"\n        if file is not None:\n            if not file.endswith(\".npz\"):\n                file = file + \".npz\"\n            self._file = file\n        d = {}\n        # Diagnostic information about how long the save took.  We can probably\n        # take this out, or else set it at a higher debug level via warnings\n        # module.\n        save_start_time = time.time()\n        for p_name in self._page_times.keys():\n            if p_name in self._no_save_pages.keys():\n                continue\n            p = getattr(self, p_name)\n            pd = p.to_serial_dict()\n            for k, v in pd.items():\n                if v is None:\n                    # save None objects as string then convert back to None on loading\n                    v = str(v)\n                d[p_name + self._SEP + k] = v\n            d[p_name + self._SEP + self._ADDEDMETA] = self._page_times[p_name]\n        d[self._NBMETA + self._SEP + self._ADDEDMETA] = self._created_time\n        if self._config is not None:\n            d[self._NBMETA + self._SEP + self._CONFIGMETA] = self._config\n        np.savez_compressed(self._file, **d)\n        # Finishing the diagnostics described above\n        #print(f\"Notebook saved: took {time.time() - save_start_time} seconds\")\n\n    def from_file(self, fn: str) -&gt; Tuple[List, dict, float, str]:\n\"\"\"\n        Read a `Notebook` from a file\n\n        Args:\n            fn: Filename of the saved `Notebook` to load.\n\n        Returns:\n            A list of `NotebookPage` objects\n            A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and\n                keys are `page.name`\n            A timestamp for the time the `Notebook` was created.\n            A string of the config file\n        \"\"\"\n        # Right now we won't use lazy loading.  One problem with lazy loading\n        # is that we must keep the file handle open.  We would rather not do\n        # this, because if we write to the file, it will get screwed up, and if\n        # there is a network issue, it will also mess things up.  I can't\n        # imagine that loading the notebook will be a performance bottleneck,\n        # but if it is, we can rethink this decision.  It should be pretty easy\n        # to lazy load the pages, but eager load everything in the page.\n        f = np.load(fn)\n        keys = list(f.keys())\n        page_items = {}\n        page_times = {}\n        created_time = None\n        config_str = None  # If no config saved, will stay as None. Otherwise, will be the config in str form.\n        for pk in keys:\n            p, k = pk.split(self._SEP, 1)\n            if p in self._no_save_pages.keys():\n                # This is to deal with the legacy case from old code where a no_save_page has been saved.\n                # If this is the case, don't load in this page.\n                continue\n            if p == self._NBMETA:\n                if k == self._ADDEDMETA:\n                    created_time = float(f[pk])\n                    continue\n                if k == self._CONFIGMETA:\n                    config_str = str(f[pk])\n                    continue\n            if k == self._ADDEDMETA:\n                page_times[p] = float(f[pk])\n                continue\n            if p not in page_items.keys():\n                page_items[p] = {}\n            page_items[p][k] = f[pk]\n        pages = [NotebookPage.from_serial_dict(page_items[d]) for d in sorted(page_items.keys())]\n        for page in pages:\n            page.finalized = True  # if loading from file, then all pages are final\n        assert len(pages) == len(page_times), \"Invalid file, lengths don't match\"\n        assert created_time is not None, \"Invalid file, invalid created date\"\n        return pages, page_times, created_time, config_str\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.add_no_save_pages","title":"<code>add_no_save_pages()</code>","text":"<p>This adds the page <code>page_name</code> listed in <code>nb._no_save_pages</code> to the notebook if the notebook already contains the pages listed in <code>nb._no_save_pages['page_name']['load_func_req']</code> by running the function <code>nb._no_save_pages['page_name']['load_func'](nb, 'page_name')</code>.</p> <p>At the moment, this is only used to add the <code>file_names</code> page to the notebook as soon as the <code>basic_info</code> page has been added.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def add_no_save_pages(self):\n\"\"\"\n    This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if\n    the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']`\n    by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`.\n\n    At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page\n    has been added.\n    \"\"\"\n    for page_name in self._no_save_pages.keys():\n        if self.has_page(page_name):\n            continue\n        if all(self.has_page(self._no_save_pages[page_name]['load_func_req'])):\n            # If contains all required pages to run load_func, then add the page\n            self._no_save_pages[page_name]['load_func'](self, page_name)\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.add_page","title":"<code>add_page(page)</code>","text":"<p>Insert the page <code>page</code> into the <code>Notebook</code>.</p> <p>This function automatically triggers a save.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def add_page(self, page):\n\"\"\"Insert the page `page` into the `Notebook`.\n\n    This function automatically triggers a save.\n    \"\"\"\n    if not isinstance(page, NotebookPage):\n        raise ValueError(\"Only NotebookPage objects may be added to a notebook.\")\n    self.__setattr__(page.name, page)\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.change_page_name","title":"<code>change_page_name(old_name, new_name)</code>","text":"<p>This changes the name of the page <code>old_name</code> to <code>new_name</code>. It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added.</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> required <code>new_name</code> <code>str</code> required Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def change_page_name(self, old_name: str, new_name: str):\n\"\"\"\n    This changes the name of the page `old_name` to `new_name`. It will trigger two saves,\n    one after changing the new and one after changing the time the page was added to be the time\n    the initial page was added.\n\n    Args:\n        old_name:\n        new_name:\n    \"\"\"\n    nbp = self.__getattribute__(old_name)\n    warnings.warn(f\"Changing name of {old_name} page to {new_name}\")\n    time_added = self._page_times[old_name]\n    nbp.finalized = False\n    nbp.name = new_name\n    self.__delattr__(old_name)\n    self.add_page(nbp)\n    self._page_times[new_name] = time_added  # set time to time page initially added\n    self.save()\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.compare_config","title":"<code>compare_config(config_2)</code>","text":"<p>Compares whether <code>config_2</code> is equal to the config file saved in the notebook. Only sections not in <code>_no_compare_config_sections</code> and with a corresponding page saved to the notebook will be checked.</p> <p>Parameters:</p> Name Type Description Default <code>config_2</code> <code>dict</code> <p>Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. <code>config_2['basic_info]['param1'] = 5</code>.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if config dictionaries are equal in required sections.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def compare_config(self, config_2: dict) -&gt; bool:\n\"\"\"\n    Compares whether `config_2` is equal to the config file saved in the notebook.\n    Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook\n    will be checked.\n\n    Args:\n        config_2: Dictionary with keys corresponding to sections where a section\n            is also a dictionary containing parameters.\n            E.g. `config_2['basic_info]['param1'] = 5`.\n\n    Returns:\n        `True` if config dictionaries are equal in required sections.\n\n    \"\"\"\n    # TODO: issue here that if default settings file changed, the equality here would still be true.\n    config = self.get_config()\n    is_equal = True\n    if config.keys() != config_2.keys():\n        warnings.warn('The config files have different sections.')\n        is_equal = False\n    else:\n        sort_page_names = sorted(self._page_times.items(), key=lambda x: x[1])  # sort by time added to notebook\n        # page names are either same as config sections or with _debug suffix\n        page_names = [name[0].replace('_debug', '') for name in sort_page_names]\n        for section in config.keys():\n            # Only compare sections for which there is a corresponding page in the notebook.\n            if section not in self._no_compare_config_sections and section in page_names:\n                if config[section] != config_2[section]:\n                    warnings.warn(f\"The {section} section of the two config files differ.\")\n                    is_equal = False\n    return is_equal\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.describe","title":"<code>describe(key=None)</code>","text":"<p><code>describe(var)</code> will print comments for variables called <code>var</code> in each <code>NotebookPage</code>.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def describe(self, key=None):\n\"\"\"\n    `describe(var)` will print comments for variables called `var` in each `NotebookPage`.\n    \"\"\"\n    if key is None:\n        print(self.__repr__())\n    elif len(self._page_times) == 0:\n        print(f\"No pages so cannot search for variable {key}\")\n    else:\n        sort_page_names = sorted(self._page_times.items(), key=lambda x: x[1])  # sort by time added to notebook\n        page_names = [name[0] for name in sort_page_names]\n        first_page = self.__getattribute__(page_names[0])\n        with open(first_page._comments_file) as f:\n            json_comments = json.load(f)\n        if self._config is not None:\n            config = self.get_config()\n        n_times_appeared = 0\n        for page_name in page_names:\n            # if in comments file, then print the comment\n            if key in json_comments[page_name]:\n                print(f\"{key} in {page_name}:\")\n                self.__getattribute__(page_name).describe(key)\n                print(\"\")\n                n_times_appeared += 1\n\n            elif self._config is not None:\n                # if in config file, then print the comment\n                # find sections in config file with matching name to current page\n                config_sections_with_name = [page_name.find(list(config.keys())[i]) for i in\n                                             range(len(config.keys()))]\n                config_sections = np.array(list(config.keys()))[np.array(config_sections_with_name) != -1]\n                for section in config_sections:\n                    for param in config[section].keys():\n                        if param.lower() == key.lower():\n                            print(f\"No variable named {key} in the {page_name} page.\\n\"\n                                  f\"But it is in the {section} section of the config file and has value:\\n\"\n                                  f\"{config[section][param]}\\n\")\n                            n_times_appeared += 1\n        if n_times_appeared == 0:\n            print(f\"{key} is not in any of the pages in this notebook.\")\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.from_file","title":"<code>from_file(fn)</code>","text":"<p>Read a <code>Notebook</code> from a file</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>str</code> <p>Filename of the saved <code>Notebook</code> to load.</p> required <p>Returns:</p> Type Description <code>List</code> <p>A list of <code>NotebookPage</code> objects</p> <code>dict</code> <p>A dictionary of timestamps, of identical length to the list of <code>NotebookPage</code> objects and keys are <code>page.name</code></p> <code>float</code> <p>A timestamp for the time the <code>Notebook</code> was created.</p> <code>str</code> <p>A string of the config file</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def from_file(self, fn: str) -&gt; Tuple[List, dict, float, str]:\n\"\"\"\n    Read a `Notebook` from a file\n\n    Args:\n        fn: Filename of the saved `Notebook` to load.\n\n    Returns:\n        A list of `NotebookPage` objects\n        A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and\n            keys are `page.name`\n        A timestamp for the time the `Notebook` was created.\n        A string of the config file\n    \"\"\"\n    # Right now we won't use lazy loading.  One problem with lazy loading\n    # is that we must keep the file handle open.  We would rather not do\n    # this, because if we write to the file, it will get screwed up, and if\n    # there is a network issue, it will also mess things up.  I can't\n    # imagine that loading the notebook will be a performance bottleneck,\n    # but if it is, we can rethink this decision.  It should be pretty easy\n    # to lazy load the pages, but eager load everything in the page.\n    f = np.load(fn)\n    keys = list(f.keys())\n    page_items = {}\n    page_times = {}\n    created_time = None\n    config_str = None  # If no config saved, will stay as None. Otherwise, will be the config in str form.\n    for pk in keys:\n        p, k = pk.split(self._SEP, 1)\n        if p in self._no_save_pages.keys():\n            # This is to deal with the legacy case from old code where a no_save_page has been saved.\n            # If this is the case, don't load in this page.\n            continue\n        if p == self._NBMETA:\n            if k == self._ADDEDMETA:\n                created_time = float(f[pk])\n                continue\n            if k == self._CONFIGMETA:\n                config_str = str(f[pk])\n                continue\n        if k == self._ADDEDMETA:\n            page_times[p] = float(f[pk])\n            continue\n        if p not in page_items.keys():\n            page_items[p] = {}\n        page_items[p][k] = f[pk]\n    pages = [NotebookPage.from_serial_dict(page_items[d]) for d in sorted(page_items.keys())]\n    for page in pages:\n        page.finalized = True  # if loading from file, then all pages are final\n    assert len(pages) == len(page_times), \"Invalid file, lengths don't match\"\n    assert created_time is not None, \"Invalid file, invalid created date\"\n    return pages, page_times, created_time, config_str\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.get_config","title":"<code>get_config()</code>","text":"<p>Returns config as dictionary.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def get_config(self):\n\"\"\"\n    Returns config as dictionary.\n    \"\"\"\n    if self._config is not None:\n        return get_config(self._config)\n    else:\n        raise ValueError('Notebook does not contain config parameter.')\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.has_page","title":"<code>has_page(page_name)</code>","text":"<p>A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def has_page(self, page_name):\n\"\"\"A check to see if notebook includes a page called page_name.\n    If page_name is a list, a boolean list of equal size will be\n    returned indicating whether each page is present.\"\"\"\n    if isinstance(page_name, str):\n        output = any(page_name == p for p in self._page_times)\n    elif isinstance(page_name, list):\n        output = [any(page_name[i] == p for p in self._page_times) for i in range(len(page_name))]\n    else:\n        raise ValueError(f\"page_name given was {page_name}. This is not a list or a string.\")\n    return output\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.save","title":"<code>save(file=None)</code>","text":"<p>Saves Notebook as a npz file at the path indicated by <code>file</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Optional[str]</code> <p>Where to save Notebook. If <code>None</code>, will use <code>self._file</code>.</p> <code>None</code> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def save(self, file: Optional[str] = None):\n\"\"\"\n    Saves Notebook as a npz file at the path indicated by `file`.\n    Args:\n        file: Where to save *Notebook*. If `None`, will use `self._file`.\n\n    \"\"\"\n\"\"\"Save the Notebook to a file\"\"\"\n    if file is not None:\n        if not file.endswith(\".npz\"):\n            file = file + \".npz\"\n        self._file = file\n    d = {}\n    # Diagnostic information about how long the save took.  We can probably\n    # take this out, or else set it at a higher debug level via warnings\n    # module.\n    save_start_time = time.time()\n    for p_name in self._page_times.keys():\n        if p_name in self._no_save_pages.keys():\n            continue\n        p = getattr(self, p_name)\n        pd = p.to_serial_dict()\n        for k, v in pd.items():\n            if v is None:\n                # save None objects as string then convert back to None on loading\n                v = str(v)\n            d[p_name + self._SEP + k] = v\n        d[p_name + self._SEP + self._ADDEDMETA] = self._page_times[p_name]\n    d[self._NBMETA + self._SEP + self._ADDEDMETA] = self._created_time\n    if self._config is not None:\n        d[self._NBMETA + self._SEP + self._CONFIGMETA] = self._config\n    np.savez_compressed(self._file, **d)\n</code></pre>"},{"location":"code/setup/notebook/#notebook-page","title":"Notebook Page","text":"<p>A page, to be added to a <code>Notebook</code> object</p> <p>Expected usage is for a <code>NotebookPage</code> to be created at the beginning of a large step in the analysis pipeline.  The name of the page should reflect its function, and it will be used as the indexing key when it is added to a Notebook.  The <code>NotebookPage</code> should be created at the beginning of the step in the pipeline, because then the timestamp will be more meaningful.  As results are computed, they should be added.  This will provide a timestamp for each of the results as well.  Then, at the end, the pipeline step should return a <code>NotebookPage</code>, which can then be added to the <code>Notebook</code>.</p> <p>Example</p> <pre><code>    nbp = NotebookPage(\"extract_and_filter\")\n    nbp.scale_factor = 10\n    ...\n    return nbp\n</code></pre> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>class NotebookPage:\n\"\"\"A page, to be added to a `Notebook` object\n\n    Expected usage is for a `NotebookPage` to be created at the beginning of a\n    large step in the analysis pipeline.  The name of the page should reflect\n    its function, and it will be used as the indexing key when it is added to a\n    Notebook.  The `NotebookPage` should be created at the beginning of the step\n    in the pipeline, because then the timestamp will be more meaningful.  As\n    results are computed, they should be added.  This will provide a timestamp\n    for each of the results as well.  Then, at the end, the pipeline step should return\n    a `NotebookPage`, which can then be added to the `Notebook`.\n\n    !!!example\n        ```python\n            nbp = NotebookPage(\"extract_and_filter\")\n            nbp.scale_factor = 10\n            ...\n            return nbp\n        ```\n    \"\"\"\n    _PAGEMETA = \"PAGEINFO\"  # Filename for metadata about the page\n    _TIMEMETA = \"___TIME\"  # Filename suffix for timestamp information\n    _TYPEMETA = \"___TYPE\"  # Filename suffix for type information\n    _NON_RESULT_KEYS = ['name', 'finalized']\n    _comments_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'notebook_comments.json')\n\n    def __init__(self, name, input_dict=None):\n        self.finalized = False  # Set to true when added to a Notebook\n        self._times = {}\n        self.name = name\n        self._time_created = time.time()\n        if isinstance(input_dict, dict):\n            self.from_dict(input_dict)\n\n    def __eq__(self, other):\n        # Test for equality using the == syntax.\n        # To be honest, I don't know why you would ever need this, but it is very\n        # useful for writing unit tests, so here it is.\n        if not isinstance(other, self.__class__):\n            return False\n        if self.name != other.name:\n            return False\n        if self._time_created != other._time_created:\n            return False\n        for k in self._times.keys():\n            if k not in other._times or not np.array_equal(getattr(self, k), getattr(other, k)):\n                # second condition in case failed first because of nan == nan is False.\n                # Need first condition as well because equal_nan=True gives error for strings.\n                if k not in other._times or not np.array_equal(getattr(self, k), getattr(other, k), equal_nan=True):\n                    return False\n        for k in other._times.keys():\n            if k not in self._times or not np.array_equal(getattr(other, k), getattr(self, k)):\n                # second condition in case failed first because of nan == nan is False.\n                # Need first condition as well because equal_nan=True gives error for strings.\n                if k not in self._times or not np.array_equal(getattr(other, k), getattr(self, k), equal_nan=True):\n                    return False\n        for k, v in self._times.items():\n            if k not in other._times or v != other._times[k]:\n                return False\n        return True\n\n    def __len__(self):\n        # Return the number of results in the NotebookPage\n        return len(self._times)\n\n    def _is_result_key(self, key):\n        # Whether key is a result variable or part of the metadata\n        if key in self._NON_RESULT_KEYS or key[0] == '_':\n            return False\n        else:\n            return True\n\n    def __repr__(self):\n        # This means that print(nbp) gives description of page if available or name and time created if not.\n        json_comments = json.load(open(self._comments_file))\n        if self.name in json_comments:\n            return \"\\n\".join(json_comments[self.name]['DESCRIPTION'])\n        else:\n            time_created = time.strftime('%d-%m-%Y- %H:%M:%S', time.localtime(self._time_created))\n            return f\"{self.name} page created at {time_created}\"\n\n    def describe(self, key: Optional[str] = None):\n\"\"\"\n        Prints a description of the variable indicated by `key`.\n\n        Args:\n            key: name of variable to describe that must be in `self._times.keys()`.\n                If not specified, will describe the whole page.\n\n        \"\"\"\n        if key is None:\n            print(self.__repr__())  # describe whole page if no key given\n        else:\n            if key not in self._times.keys():\n                print(f\"No variable named {key} in the {self.name} page.\")\n            else:\n                json_comments = json.load(open(self._comments_file))\n                if self.name in json_comments:\n                    # Remove empty lines\n                    while '' in json_comments[self.name][key]: json_comments[self.name][key].remove('')\n                    # replace below removes markdown code indicators\n                    print(\"\\n\".join(json_comments[self.name][key]).replace('`', ''))\n                else:\n                    print(f\"No comments available for page called {self.name}.\")\n\n    def __setattr__(self, key, value):\n        # Add an item to the notebook page.\n        #\n        # For a `NotebookPage` object `nbp`, this handles the syntax `nbp.key = value`.\n        # It checks the key and value for validity, and then adds them to the\n        # notebook.  Specifically, it implements a write-once mechanism.\n        if self._is_result_key(key):\n            if self.finalized:\n                raise ValueError(\"This NotebookPage has already been added to a Notebook, no more values can be added.\")\n            assert isinstance(key, str), f\"NotebookPage key {key!r} must be a string, not {type(key)}\"\n            _get_type(key, value)\n            if key in self.__dict__.keys():\n                raise ValueError(f\"Cannot assign {key} = {value!r} to the notebook page, key already exists\")\n            with open(self._comments_file) as f:\n                json_comments = json.load(f)\n            if self.name in json_comments:\n                if key not in json_comments[self.name]:\n                    raise InvalidNotebookPageError(key, None, self.name)\n                if key == 'DESCRIPTION':\n                    raise InvalidNotebookPageError(key, None, self.name)\n            self._times[key] = time.time()\n        object.__setattr__(self, key, value)\n\n    def __delattr__(self, name):\n        # Method to delete a result or attribute. Deals with del nbp.name.\n        # Can only delete attribute if page has not been finalized.\n        if self.finalized:\n            raise ValueError(\"This NotebookPage has already been added to a Notebook, no values can be deleted.\")\n        object.__delattr__(self, name)\n        if name in self._times:\n            # extra bit if _is_result_key\n            del self._times[name]\n\n    def has_item(self, key):\n\"\"\"Check to see whether page has attribute `key`\"\"\"\n        return key in self._times.keys()\n\n    def from_dict(self, d):\n\"\"\"\n        Adds all string keys of dictionary d to page.\n        Keys whose value is None will be ignored.\n        \"\"\"\n        for key, value in d.items():\n            if isinstance(key, (str, np.str_)):\n                if value is not None:\n                    self.__setattr__(key, value)\n\n    def to_serial_dict(self):\n\"\"\"Convert to a dictionary which can be written to a file.\n\n        In general, this function shouldn't need to be called other than within\n        a `Notebook` object.\n        \"\"\"\n        keys = {}\n        keys[self._PAGEMETA] = self.name\n        keys[self._PAGEMETA + self._TIMEMETA] = self._time_created\n        for rn in self._times.keys():\n            r = getattr(self, rn)\n            keys[rn] = r\n            keys[rn + self._TIMEMETA] = self._times[rn]\n            keys[rn + self._TYPEMETA] = _get_type(rn, r)\n        return keys\n\n    @classmethod\n    def from_serial_dict(cls, d):\n\"\"\"Convert from a dictionary to a `NotebookPage` object\n\n        In general, this function shouldn't need to be called other than within\n        a `Notebook` object.\n        \"\"\"\n        # Note that this method will need to be updated if you update the\n        # constructor.\n        name = str(d[cls._PAGEMETA][()])\n        n = cls(name)\n        n._time_created = float(d[cls._PAGEMETA + cls._TIMEMETA])\n        # n.finalized = d[cls._FINALIZEDMETA]\n        for k in d.keys():\n            # If we've already dealt with the key, skip it.\n            if k.startswith(cls._PAGEMETA): continue\n            # Each key has an associated \"time\" and \"type\" key.  We deal with\n            # the time and type keys separately when dealing with the main key.\n            if k.endswith(cls._TIMEMETA): continue\n            if k.endswith(cls._TYPEMETA): continue\n            # Now that we have a real key, add it to the page.\n            object.__setattr__(n, k, _decode_type(k, d[k], str(d[k + cls._TYPEMETA][()])))\n            n._times[k] = float(d[k + cls._TIMEMETA])\n        return n\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.describe","title":"<code>describe(key=None)</code>","text":"<p>Prints a description of the variable indicated by <code>key</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Optional[str]</code> <p>name of variable to describe that must be in <code>self._times.keys()</code>. If not specified, will describe the whole page.</p> <code>None</code> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def describe(self, key: Optional[str] = None):\n\"\"\"\n    Prints a description of the variable indicated by `key`.\n\n    Args:\n        key: name of variable to describe that must be in `self._times.keys()`.\n            If not specified, will describe the whole page.\n\n    \"\"\"\n    if key is None:\n        print(self.__repr__())  # describe whole page if no key given\n    else:\n        if key not in self._times.keys():\n            print(f\"No variable named {key} in the {self.name} page.\")\n        else:\n            json_comments = json.load(open(self._comments_file))\n            if self.name in json_comments:\n                # Remove empty lines\n                while '' in json_comments[self.name][key]: json_comments[self.name][key].remove('')\n                # replace below removes markdown code indicators\n                print(\"\\n\".join(json_comments[self.name][key]).replace('`', ''))\n            else:\n                print(f\"No comments available for page called {self.name}.\")\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.from_dict","title":"<code>from_dict(d)</code>","text":"<p>Adds all string keys of dictionary d to page. Keys whose value is None will be ignored.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def from_dict(self, d):\n\"\"\"\n    Adds all string keys of dictionary d to page.\n    Keys whose value is None will be ignored.\n    \"\"\"\n    for key, value in d.items():\n        if isinstance(key, (str, np.str_)):\n            if value is not None:\n                self.__setattr__(key, value)\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.from_serial_dict","title":"<code>from_serial_dict(d)</code>  <code>classmethod</code>","text":"<p>Convert from a dictionary to a <code>NotebookPage</code> object</p> <p>In general, this function shouldn't need to be called other than within a <code>Notebook</code> object.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>@classmethod\ndef from_serial_dict(cls, d):\n\"\"\"Convert from a dictionary to a `NotebookPage` object\n\n    In general, this function shouldn't need to be called other than within\n    a `Notebook` object.\n    \"\"\"\n    # Note that this method will need to be updated if you update the\n    # constructor.\n    name = str(d[cls._PAGEMETA][()])\n    n = cls(name)\n    n._time_created = float(d[cls._PAGEMETA + cls._TIMEMETA])\n    # n.finalized = d[cls._FINALIZEDMETA]\n    for k in d.keys():\n        # If we've already dealt with the key, skip it.\n        if k.startswith(cls._PAGEMETA): continue\n        # Each key has an associated \"time\" and \"type\" key.  We deal with\n        # the time and type keys separately when dealing with the main key.\n        if k.endswith(cls._TIMEMETA): continue\n        if k.endswith(cls._TYPEMETA): continue\n        # Now that we have a real key, add it to the page.\n        object.__setattr__(n, k, _decode_type(k, d[k], str(d[k + cls._TYPEMETA][()])))\n        n._times[k] = float(d[k + cls._TIMEMETA])\n    return n\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.has_item","title":"<code>has_item(key)</code>","text":"<p>Check to see whether page has attribute <code>key</code></p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def has_item(self, key):\n\"\"\"Check to see whether page has attribute `key`\"\"\"\n    return key in self._times.keys()\n</code></pre>"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.to_serial_dict","title":"<code>to_serial_dict()</code>","text":"<p>Convert to a dictionary which can be written to a file.</p> <p>In general, this function shouldn't need to be called other than within a <code>Notebook</code> object.</p> Source code in <code>coppafish/setup/notebook.py</code> <pre><code>def to_serial_dict(self):\n\"\"\"Convert to a dictionary which can be written to a file.\n\n    In general, this function shouldn't need to be called other than within\n    a `Notebook` object.\n    \"\"\"\n    keys = {}\n    keys[self._PAGEMETA] = self.name\n    keys[self._PAGEMETA + self._TIMEMETA] = self._time_created\n    for rn in self._times.keys():\n        r = getattr(self, rn)\n        keys[rn] = r\n        keys[rn + self._TIMEMETA] = self._times[rn]\n        keys[rn + self._TYPEMETA] = _get_type(rn, r)\n    return keys\n</code></pre>"},{"location":"code/setup/tile_details/","title":"Tile Detail","text":""},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tile_file_names","title":"<code>get_tile_file_names(tile_directory, file_base, n_tiles, n_channels=0)</code>","text":"<p>Gets array of all tile file paths which will be saved in tile directory.</p> <p>Parameters:</p> Name Type Description Default <code>tile_directory</code> <code>str</code> <p>Path to folder where tiles npy files saved.</p> required <code>file_base</code> <code>List[str]</code> <p><code>str [n_rounds]</code>. <code>file_base[r]</code> is identifier for round <code>r</code>.</p> required <code>n_tiles</code> <code>int</code> <p>Number of tiles in data set.</p> required <code>n_channels</code> <code>int</code> <p>Total number of imaging channels if using 3D. <code>0</code> if using 2D pipeline as all channels saved in same file.</p> <code>0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>object [n_tiles x n_rounds (x n_channels)]</code>.</p> <code>np.ndarray</code> <p><code>tile_files</code> such that</p> <code>np.ndarray</code> <ul> <li>If 2D so <code>n_channels = 0</code>, <code>tile_files[t, r]</code> is the full path to npy file containing all channels of tile <code>t</code>, round <code>r</code>.</li> </ul> <code>np.ndarray</code> <ul> <li>If 3D so <code>n_channels &gt; 0</code>, <code>tile_files[t, r]</code> is the full path to npy file containing all z-planes of</li> </ul> <code>np.ndarray</code> <p>tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</p> Source code in <code>coppafish/setup/tile_details.py</code> <pre><code>def get_tile_file_names(tile_directory: str, file_base: List[str], n_tiles: int, n_channels: int = 0) -&gt; np.ndarray:\n\"\"\"\n    Gets array of all tile file paths which will be saved in tile directory.\n\n    Args:\n        tile_directory: Path to folder where tiles npy files saved.\n        file_base: `str [n_rounds]`.\n            `file_base[r]` is identifier for round `r`.\n        n_tiles: Number of tiles in data set.\n        n_channels: Total number of imaging channels if using 3D.\n            `0` if using 2D pipeline as all channels saved in same file.\n\n    Returns:\n        `object [n_tiles x n_rounds (x n_channels)]`.\n        `tile_files` such that\n\n        - If 2D so `n_channels = 0`, `tile_files[t, r]` is the full path to npy file containing all channels of\n            tile `t`, round `r`.\n        - If 3D so `n_channels &gt; 0`, `tile_files[t, r]` is the full path to npy file containing all z-planes of\n        tile `t`, round `r`, channel `c`.\n    \"\"\"\n    n_rounds = len(file_base)\n    if n_channels == 0:\n        # 2D\n        tile_files = np.zeros((n_tiles, n_rounds), dtype=object)\n        for r in range(n_rounds):\n            for t in range(n_tiles):\n                tile_files[t, r] = \\\n                    get_tile_name(tile_directory, file_base, r, t)\n    else:\n        # 3D\n        tile_files = np.zeros((n_tiles, n_rounds, n_channels), dtype=object)\n        for r in range(n_rounds):\n            for t in range(n_tiles):\n                for c in range(n_channels):\n                    tile_files[t, r, c] = \\\n                        get_tile_name(tile_directory, file_base, r, t, c)\n    return tile_files\n</code></pre>"},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tile_name","title":"<code>get_tile_name(tile_directory, file_base, r, t, c=None)</code>","text":"<p>Finds the full path to tile, <code>t</code>, of particular round, <code>r</code>, and channel, <code>c</code>, in <code>tile_directory</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tile_directory</code> <code>str</code> <p>Path to folder where tiles npy files saved.</p> required <code>file_base</code> <code>List[str]</code> <p><code>str [n_rounds]</code>. <code>file_base[r]</code> is identifier for round <code>r</code>.</p> required <code>r</code> <code>int</code> <p>Round of desired npy image.</p> required <code>t</code> <code>int</code> <p>Tile of desired npy image.</p> required <code>c</code> <code>Optional[int]</code> <p>Channel of desired npy image.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Full path of tile npy file.</p> Source code in <code>coppafish/setup/tile_details.py</code> <pre><code>def get_tile_name(tile_directory: str, file_base: List[str], r: int, t: int, c: Optional[int] = None) -&gt; str:\n\"\"\"\n    Finds the full path to tile, `t`, of particular round, `r`, and channel, `c`, in `tile_directory`.\n\n    Args:\n        tile_directory: Path to folder where tiles npy files saved.\n        file_base: `str [n_rounds]`.\n            `file_base[r]` is identifier for round `r`.\n        r: Round of desired npy image.\n        t: Tile of desired npy image.\n        c: Channel of desired npy image.\n\n    Returns:\n        Full path of tile npy file.\n    \"\"\"\n    if c is None:\n        tile_name = os.path.join(tile_directory, '{}_t{}.npy'.format(file_base[r], t))\n    else:\n        tile_name = os.path.join(tile_directory, '{}_t{}c{}.npy'.format(file_base[r], t, c))\n    return tile_name\n</code></pre>"},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tilepos","title":"<code>get_tilepos(xy_pos, tile_sz)</code>","text":"<p>Using <code>xy_pos</code> from nd2 metadata, this obtains the yx position of each tile. I.e. how tiles are arranged with respect to each other. Note that this is indexed differently in nd2 file and npy files in the tile directory.</p> <p>Parameters:</p> Name Type Description Default <code>xy_pos</code> <code>np.ndarray</code> <p><code>float [n_tiles x 2]</code>. xy position of tiles in pixels. Obtained from nd2 metadata.</p> required <code>tile_sz</code> <code>int</code> <p>xy dimension of tile in pixels.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>tilepos_yx_nd2</code> - <code>int [n_tiles x 2]</code>. <code>tilepos_yx_nd2[i]</code> is yx index of tile with fov index <code>i</code> in nd2 file. Index 0 refers to <code>YX = [0, 0]</code>. Index 1 refers to <code>YX = [0, 1] if MaxX &gt; 0</code>.</li> </ul> <code>np.ndarray</code> <ul> <li><code>tilepos_yx_npy</code> - <code>int [n_tiles x 2]</code>. <code>tilepos_yx_npy[i, 0]</code> is yx index of tile with tile directory (npy files) index <code>i</code>. Index 0 refers to <code>YX = [MaxY, MaxX]</code>. Index 1 refers to <code>YX = [MaxY, MaxX - 1] if MaxX &gt; 0</code>.</li> </ul> Source code in <code>coppafish/setup/tile_details.py</code> <pre><code>def get_tilepos(xy_pos: np.ndarray, tile_sz: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    Using `xy_pos` from nd2 metadata, this obtains the yx position of each tile.\n    I.e. how tiles are arranged with respect to each other.\n    Note that this is indexed differently in nd2 file and npy files in the tile directory.\n\n    Args:\n        xy_pos: `float [n_tiles x 2]`.\n            xy position of tiles in pixels. Obtained from nd2 metadata.\n        tile_sz: xy dimension of tile in pixels.\n\n    Returns:\n        - `tilepos_yx_nd2` - `int [n_tiles x 2]`.\n            `tilepos_yx_nd2[i]` is yx index of tile with fov index `i` in nd2 file.\n            Index 0 refers to ```YX = [0, 0]```.\n            Index 1 refers to ```YX = [0, 1] if MaxX &gt; 0```.\n        - `tilepos_yx_npy` - `int [n_tiles x 2]`.\n            `tilepos_yx_npy[i, 0]` is yx index of tile with tile directory (npy files) index `i`.\n            Index 0 refers to ```YX = [MaxY, MaxX]```.\n            Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX &gt; 0```.\n    \"\"\"\n    tilepos_yx_nd2 = np.zeros_like(xy_pos, dtype=int)\n    tilepos_yx_npy = np.zeros_like(xy_pos, dtype=int)\n    if np.shape(xy_pos)[0] != 1:\n        # say y coordinate changes when successive tiles have pixel separation of more than tile_sz/2\n        change_y_coord = np.abs(np.ediff1d(xy_pos[:, 1])) &gt; tile_sz / 2\n        if False in change_y_coord:\n            # sometimes get faulty first xy_pos\n            # know that if there are more than one y coordinates, then the first\n            # and second tile must have the same y coordinate.\n            change_y_coord[0] = False\n        ny = sum(change_y_coord) + 1\n        nx = np.shape(xy_pos)[0] / ny\n        if round(nx) != nx:\n            raise ValueError('nx is not an integer')\n        tilepos_yx_nd2[:, 0] = np.arange(ny).repeat(nx)\n        tilepos_yx_nd2[:, 1] = np.tile(np.concatenate((np.arange(nx), np.flip(np.arange(nx)))),\n                                       np.ceil(ny / 2).astype(int))[:np.shape(xy_pos)[0]]\n        tilepos_yx_npy[:, 0] = np.flip(np.arange(ny).repeat(nx))\n        tilepos_yx_npy[:, 1] = np.tile(np.concatenate((np.flip(np.arange(nx)), np.flip(np.arange(nx)))),\n                                        np.ceil(ny / 2).astype(int))[:np.shape(xy_pos)[0]]\n    return tilepos_yx_nd2, tilepos_yx_npy\n</code></pre>"},{"location":"code/spot_colors/base/","title":"Base","text":""},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.all_pixel_yxz","title":"<code>all_pixel_yxz(y_size, x_size, z_planes)</code>","text":"<p>Returns the yxz coordinates of all pixels on the indicated z-planes of an image.</p> <p>Parameters:</p> Name Type Description Default <code>y_size</code> <code>int</code> <p>number of pixels in y direction of image.</p> required <code>x_size</code> <code>int</code> <p>number of pixels in x direction of image.</p> required <code>z_planes</code> <code>Union[List, int, np.ndarray]</code> <p><code>int [n_z_planes]</code> z_planes, coordinates are desired for.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int16 [y_size * x_size * n_z_planes, 3]</code> yxz coordinates of all pixels on <code>z_planes</code>.</p> Source code in <code>coppafish/spot_colors/base.py</code> <pre><code>def all_pixel_yxz(y_size: int, x_size: int, z_planes: Union[List, int, np.ndarray]) -&gt; np.ndarray:\n\"\"\"\n    Returns the yxz coordinates of all pixels on the indicated z-planes of an image.\n\n    Args:\n        y_size: number of pixels in y direction of image.\n        x_size: number of pixels in x direction of image.\n        z_planes: `int [n_z_planes]` z_planes, coordinates are desired for.\n\n    Returns:\n        `int16 [y_size * x_size * n_z_planes, 3]`\n            yxz coordinates of all pixels on `z_planes`.\n    \"\"\"\n    if isinstance(z_planes, int):\n        z_planes = np.array([z_planes])\n    elif isinstance(z_planes, list):\n        z_planes = np.array(z_planes)\n    return np.array(np.meshgrid(np.arange(y_size), np.arange(x_size), z_planes), dtype=np.int16).T.reshape(-1, 3)\n</code></pre>"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.apply_transform","title":"<code>apply_transform(yxz, transform, tile_centre, z_scale, tile_sz)</code>","text":"<p>This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel.</p> <p>Parameters:</p> Name Type Description Default <code>yxz</code> <code>np.ndarray</code> <p><code>int [n_spots x 3]</code>. <code>yxz[i, :2]</code> are the non-centered yx coordinates in <code>yx_pixels</code> for spot <code>i</code>. <code>yxz[i, 2]</code> is the non-centered z coordinate in <code>z_pixels</code> for spot <code>i</code>. E.g. these are the coordinates stored in <code>nb['find_spots']['spot_details']</code>.</p> required <code>transform</code> <code>np.ndarray</code> <p><code>float [4 x 3]</code>. Affine transform to apply to <code>yxz</code>, once centered and z units changed to <code>yx_pixels</code>. <code>transform[3, 2]</code> is approximately the z shift in units of <code>yx_pixels</code>. E.g. this is one of the transforms stored in <code>nb['register']['transform']</code>.</p> required <code>tile_centre</code> <code>np.ndarray</code> <p><code>float [3]</code>. <code>tile_centre[:2]</code> are yx coordinates in <code>yx_pixels</code> of the centre of the tile that spots in <code>yxz</code> were found on. <code>tile_centre[2]</code> is the z coordinate in <code>z_pixels</code> of the centre of the tile. E.g. for tile of <code>yxz</code> dimensions <code>[2048, 2048, 51]</code>, <code>tile_centre = [1023.5, 1023.5, 25]</code> Each entry in <code>tile_centre</code> must be an integer multiple of <code>0.5</code>.</p> required <code>z_scale</code> <code>float</code> <p>Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. <code>z_scale = pixel_size_z / pixel_size_yx</code> where both are measured in microns. typically, <code>z_scale &gt; 1</code> because <code>z_pixels</code> are larger than the <code>yx_pixels</code>.</p> required <code>tile_sz</code> <code>np.ndarray</code> <p><code>int16 [3]</code>. YXZ dimensions of tile</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [n_spots x 3]</code>. <code>yxz_transform</code> such that <code>yxz_transform[i, [1,2]]</code> are the transformed non-centered yx coordinates in <code>yx_pixels</code> for spot <code>i</code>. <code>yxz_transform[i, 2]</code> is the transformed non-centered z coordinate in <code>z_pixels</code> for spot <code>i</code>.</p> <code>np.ndarray</code> <ul> <li><code>in_range</code> - <code>bool [n_spots]</code>. Whether spot s was in the bounds of the tile when transformed to round <code>r</code>, channel <code>c</code>.</li> </ul> Source code in <code>coppafish/spot_colors/base.py</code> <pre><code>def apply_transform(yxz: np.ndarray, transform: np.ndarray, tile_centre: np.ndarray, z_scale: float,\n                    tile_sz: np.ndarray) -&gt; Tuple[np.ndarray, np.ndarray]:\n\"\"\"\n    This transforms the coordinates yxz based on an affine transform.\n    E.g. to find coordinates of spots on the same tile but on a different round and channel.\n\n    Args:\n        yxz: ```int [n_spots x 3]```.\n            ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```.\n            ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```.\n            E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```.\n        transform: ```float [4 x 3]```.\n            Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```.\n            ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```.\n            E.g. this is one of the transforms stored in ```nb['register']['transform']```.\n        tile_centre: ```float [3]```.\n            ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in\n            ```yxz``` were found on.\n            ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile.\n            E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]```\n            Each entry in ```tile_centre``` must be an integer multiple of ```0.5```.\n        z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels.\n            I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns.\n            typically, ```z_scale &gt; 1``` because ```z_pixels``` are larger than the ```yx_pixels```.\n        tile_sz: ```int16 [3]```.\n            YXZ dimensions of tile\n\n    Returns:\n        ```int [n_spots x 3]```.\n            ```yxz_transform``` such that\n            ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels```\n            for spot ```i```.\n            ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```.\n        - ```in_range``` - ```bool [n_spots]```.\n            Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`.\n    \"\"\"\n    if (utils.round_any(tile_centre, 0.5) == tile_centre).min() == False:\n        raise ValueError(f\"tile_centre given, {tile_centre}, is not a multiple of 0.5 in each dimension.\")\n    yxz_pad = np.pad((yxz - tile_centre) * [1, 1, z_scale], [(0, 0), (0, 1)], constant_values=1)\n    yxz_transform = yxz_pad @ transform\n    yxz_transform = np.round((yxz_transform / [1, 1, z_scale]) + tile_centre).astype(np.int16)\n    in_range = np.logical_and((yxz_transform &gt;= np.array([0, 0, 0])).all(axis=1),\n                              (yxz_transform &lt; tile_sz).all(axis=1))  # set color to nan if out range\n    return yxz_transform, in_range\n</code></pre>"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.get_spot_colors","title":"<code>get_spot_colors(yxz_base, t, transforms, nbp_file, nbp_basic, use_rounds=None, use_channels=None, return_in_bounds=False)</code>","text":"<p>Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on <code>nbp_basic.use_rounds</code> and <code>nbp_basic.use_channels</code>.</p> <p>Note</p> <p>Returned spot colors have dimension <code>n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)</code> not <code>n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels</code>.</p> <p>Note</p> <p><code>invalid_value = -nbp_basic.tile_pixel_value_shift</code> is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence, I use this as integer nan. It will be <code>invalid_value</code> if the registered coordinate of spot <code>s</code> is outside the tile in round <code>r</code>, channel <code>c</code>.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>int16 [n_spots x 3]</code>. Local yxz coordinates of spots found in the reference round/reference channel of tile <code>t</code> yx coordinates are in units of <code>yx_pixels</code>. z coordinates are in units of <code>z_pixels</code>.</p> required <code>t</code> <code>int</code> <p>Tile that spots were found on.</p> required <code>transforms</code> <code>np.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x 4 x 3]</code>. <code>transforms[t, r, c]</code> is the affine transform to get from tile <code>t</code>, <code>ref_round</code>, <code>ref_channel</code> to tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>use_rounds</code> <code>Optional[List[int]]</code> <p><code>int [n_use_rounds]</code>. Rounds you would like to find the <code>spot_color</code> for. Error will raise if transform is zero for particular round. If <code>None</code>, all rounds in <code>nbp_basic.use_rounds</code> used.</p> <code>None</code> <code>use_channels</code> <code>Optional[List[int]]</code> <p><code>int [n_use_channels]</code>. Channels you would like to find the <code>spot_color</code> for. Error will raise if transform is zero for particular channel. If <code>None</code>, all channels in <code>nbp_basic.use_channels</code> used.</p> <code>None</code> <code>return_in_bounds</code> <code>bool</code> <p>if <code>True</code>, then only <code>spot_colors</code> which are within the tile bounds in all <code>use_rounds</code> / <code>use_channels</code> will be returned. The corresponding <code>yxz_base</code> coordinates will also be returned in this case. Otherwise, <code>spot_colors</code> will be returned for all the given <code>yxz_base</code> but if spot <code>s</code> is out of bounds on round <code>r</code>, channel <code>c</code>, then <code>spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift</code>. This is the only scenario for which <code>spot_colors = invalid_value</code> due to clipping in the extract step.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li><code>spot_colors</code> - <code>int32 [n_spots x n_rounds_use x n_channels_use]</code> or <code>int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]</code>. <code>spot_colors[s, r, c]</code> is the spot color for spot <code>s</code> in round <code>use_rounds[r]</code>, channel <code>use_channels[c]</code>.</li> </ul> <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <ul> <li><code>yxz_base</code> - <code>int16 [n_spots_in_bounds x 3]</code>. If <code>return_in_bounds</code>, the <code>yxz_base</code> corresponding to spots in bounds for all <code>use_rounds</code> / <code>use_channels</code> will be returned. It is likely that <code>n_spots_in_bounds</code> won't be the same as <code>n_spots</code>.</li> </ul> Source code in <code>coppafish/spot_colors/base.py</code> <pre><code>def get_spot_colors(yxz_base: np.ndarray, t: int, transforms: np.ndarray, nbp_file: NotebookPage,\n                    nbp_basic: NotebookPage, use_rounds: Optional[List[int]] = None,\n                    use_channels: Optional[List[int]] = None,\n                    return_in_bounds: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n\"\"\"\n    Takes some spots found on the reference round, and computes the corresponding spot intensity\n    in specified imaging rounds/channels.\n    By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`.\n\n    !!! note\n        Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not\n        `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`.\n\n    !!! note\n        `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file\n        minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this.\n        Hence, I use this as integer nan. It will be `invalid_value` if the registered coordinate of\n        spot `s` is outside the tile in round `r`, channel `c`.\n\n    Args:\n        yxz_base: `int16 [n_spots x 3]`.\n            Local yxz coordinates of spots found in the reference round/reference channel of tile `t`\n            yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`.\n        t: Tile that spots were found on.\n        transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`.\n            `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to\n            tile `t`, round `r`, channel `c`.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        use_rounds: `int [n_use_rounds]`.\n            Rounds you would like to find the `spot_color` for.\n            Error will raise if transform is zero for particular round.\n            If `None`, all rounds in `nbp_basic.use_rounds` used.\n        use_channels: `int [n_use_channels]`.\n            Channels you would like to find the `spot_color` for.\n            Error will raise if transform is zero for particular channel.\n            If `None`, all channels in `nbp_basic.use_channels` used.\n        return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all\n            `use_rounds` / `use_channels` will be returned.\n            The corresponding `yxz_base` coordinates will also be returned in this case.\n            Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on\n            round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`.\n            This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step.\n\n\n    Returns:\n        - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or\n            `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`.\n            `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`.\n        - `yxz_base` - `int16 [n_spots_in_bounds x 3]`.\n            If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels`\n            will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`.\n    \"\"\"\n    if use_rounds is None:\n        use_rounds = nbp_basic.use_rounds\n    if use_channels is None:\n        use_channels = nbp_basic.use_channels\n    z_scale = nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy\n\n    n_spots = yxz_base.shape[0]\n    no_verbose = n_spots &lt; 10000\n    # note using nan means can't use integer even though data is integer\n    n_use_rounds = len(use_rounds)\n    n_use_channels = len(use_channels)\n    # spots outside tile bounds on particular r/c will initially be set to 0.\n    spot_colors = np.zeros((n_spots, n_use_rounds, n_use_channels), dtype=np.int32)\n    tile_centre = np.array(nbp_basic.tile_centre)\n    if not nbp_basic.is_3d:\n        # use numpy not jax.numpy as reading in tiff is done in numpy.\n        tile_sz = np.array([nbp_basic.tile_sz, nbp_basic.tile_sz, 1], dtype=np.int16)\n    else:\n        tile_sz = np.array([nbp_basic.tile_sz, nbp_basic.tile_sz, nbp_basic.nz], dtype=np.int16)\n\n    with tqdm(total=n_use_rounds * n_use_channels, disable=no_verbose) as pbar:\n        pbar.set_description(f\"Reading {n_spots} spot_colors found on tile {t} from npy files\")\n        for r in range(n_use_rounds):\n            if not nbp_basic.is_3d:\n                # If 2D, load in all channels first\n                image_all_channels = np.load(nbp_file.tile[t][use_rounds[r]], mmap_mode='r')\n            for c in range(n_use_channels):\n                transform_rc = transforms[t, use_rounds[r], use_channels[c]]\n                pbar.set_postfix({'round': use_rounds[r], 'channel': use_channels[c]})\n                if transform_rc[0, 0] == 0:\n                    raise ValueError(\n                        f\"Transform for tile {t}, round {use_rounds[r]}, channel {use_channels[c]} is zero:\"\n                        f\"\\n{transform_rc}\")\n                yxz_transform, in_range = apply_transform(yxz_base, transform_rc, tile_centre, z_scale, tile_sz)\n                yxz_transform = yxz_transform[in_range]\n                if yxz_transform.shape[0] &gt; 0:\n                    # Read in the shifted uint16 colors here, and remove shift later.\n                    if nbp_basic.is_3d:\n                        spot_colors[in_range, r, c] = utils.npy.load_tile(nbp_file, nbp_basic, t, use_rounds[r],\n                                                                          use_channels[c], yxz_transform,\n                                                                          apply_shift=False)\n                    else:\n                        spot_colors[in_range, r, c] = image_all_channels[use_channels[c]][\n                            tuple(np.asarray(yxz_transform[:, i]) for i in range(2))]\n                pbar.update(1)\n    # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value.\n    # It is impossible for any actual spot color to be this due to clipping at the extract stage.\n    spot_colors = spot_colors - nbp_basic.tile_pixel_value_shift\n    invalid_value = -nbp_basic.tile_pixel_value_shift\n    if return_in_bounds:\n        good = ~np.any(spot_colors == invalid_value, axis=(1, 2))\n        return spot_colors[good], yxz_base[good]\n    else:\n        return spot_colors\n</code></pre>"},{"location":"code/spot_colors/base/#optimised","title":"Optimised","text":""},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.all_pixel_yxz","title":"<code>all_pixel_yxz(y_size, x_size, z_planes)</code>","text":"<p>Returns the yxz coordinates of all pixels on the indicated z-planes of an image.</p> <p>Parameters:</p> Name Type Description Default <code>y_size</code> <code>int</code> <p>number of pixels in y direction of image.</p> required <code>x_size</code> <code>int</code> <p>number of pixels in x direction of image.</p> required <code>z_planes</code> <code>Union[List, int, np.ndarray]</code> <p><code>int [n_z_planes]</code> z_planes, coordinates are desired for.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>int16 [y_size * x_size * n_z_planes, 3]</code> yxz coordinates of all pixels on <code>z_planes</code>.</p> Source code in <code>coppafish/spot_colors/base_optimised.py</code> <pre><code>def all_pixel_yxz(y_size: int, x_size: int, z_planes: Union[List, int, np.ndarray]) -&gt; jnp.ndarray:\n\"\"\"\n    Returns the yxz coordinates of all pixels on the indicated z-planes of an image.\n\n    Args:\n        y_size: number of pixels in y direction of image.\n        x_size: number of pixels in x direction of image.\n        z_planes: `int [n_z_planes]` z_planes, coordinates are desired for.\n\n    Returns:\n        `int16 [y_size * x_size * n_z_planes, 3]`\n            yxz coordinates of all pixels on `z_planes`.\n    \"\"\"\n    if isinstance(z_planes, int):\n        z_planes = jnp.array([z_planes])\n    elif isinstance(z_planes, list):\n        z_planes = jnp.array(z_planes)\n    return jnp.array(jnp.meshgrid(jnp.arange(y_size), jnp.arange(x_size), z_planes), dtype=jnp.int16).T.reshape(-1, 3)\n</code></pre>"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.apply_transform","title":"<code>apply_transform(yxz, transform, tile_centre, z_scale, tile_sz)</code>","text":"<p>This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel.</p> <p>Parameters:</p> Name Type Description Default <code>yxz</code> <code>jnp.ndarray</code> <p><code>int16 [n_spots x 3]</code>. <code>yxz[i, :2]</code> are the non-centered yx coordinates in <code>yx_pixels</code> for spot <code>i</code>. <code>yxz[i, 2]</code> is the non-centered z coordinate in <code>z_pixels</code> for spot <code>i</code>. E.g. these are the coordinates stored in <code>nb['find_spots']['spot_details']</code>.</p> required <code>transform</code> <code>jnp.ndarray</code> <p><code>float [4 x 3]</code>. Affine transform to apply to <code>yxz</code>, once centered and z units changed to <code>yx_pixels</code>. <code>transform[3, 2]</code> is approximately the z shift in units of <code>yx_pixels</code>. E.g. this is one of the transforms stored in <code>nb['register']['transform']</code>.</p> required <code>tile_centre</code> <code>jnp.ndarray</code> <p><code>float [3]</code>. <code>tile_centre[:2]</code> are yx coordinates in <code>yx_pixels</code> of the centre of the tile that spots in <code>yxz</code> were found on. <code>tile_centre[2]</code> is the z coordinate in <code>z_pixels</code> of the centre of the tile. E.g. for tile of <code>yxz</code> dimensions <code>[2048, 2048, 51]</code>, <code>tile_centre = [1023.5, 1023.5, 25]</code> Each entry in <code>tile_centre</code> must be an integer multiple of <code>0.5</code>.</p> required <code>z_scale</code> <code>float</code> <p>Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. <code>z_scale = pixel_size_z / pixel_size_yx</code> where both are measured in microns. typically, <code>z_scale &gt; 1</code> because <code>z_pixels</code> are larger than the <code>yx_pixels</code>.</p> required <code>tile_sz</code> <code>jnp.ndarray</code> <p><code>int16 [3]</code>. YXZ dimensions of tile</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li><code>yxz_transform</code> - <code>int [n_spots x 3]</code>. <code>yxz_transform</code> such that <code>yxz_transform[i, [1,2]]</code> are the transformed non-centered yx coordinates in <code>yx_pixels</code> for spot <code>i</code>. <code>yxz_transform[i, 2]</code> is the transformed non-centered z coordinate in <code>z_pixels</code> for spot <code>i</code>.</li> </ul> <code>jnp.ndarray</code> <ul> <li><code>in_range</code> - <code>bool [n_spots]</code>. Whether spot s was in the bounds of the tile when transformed to round <code>r</code>, channel <code>c</code>.</li> </ul> Source code in <code>coppafish/spot_colors/base_optimised.py</code> <pre><code>@partial(jax.jit, static_argnums=3)\ndef apply_transform(yxz: jnp.ndarray, transform: jnp.ndarray, tile_centre: jnp.ndarray,\n                    z_scale: float, tile_sz: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    This transforms the coordinates yxz based on an affine transform.\n    E.g. to find coordinates of spots on the same tile but on a different round and channel.\n    Args:\n        yxz: ```int16 [n_spots x 3]```.\n            ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```.\n            ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```.\n            E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```.\n        transform: ```float [4 x 3]```.\n            Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```.\n            ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```.\n            E.g. this is one of the transforms stored in ```nb['register']['transform']```.\n        tile_centre: ```float [3]```.\n            ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in\n            ```yxz``` were found on.\n            ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile.\n            E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]```\n            Each entry in ```tile_centre``` must be an integer multiple of ```0.5```.\n        z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels.\n            I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns.\n            typically, ```z_scale &gt; 1``` because ```z_pixels``` are larger than the ```yx_pixels```.\n        tile_sz: ```int16 [3]```.\n            YXZ dimensions of tile\n\n    Returns:\n        - `yxz_transform` - ```int [n_spots x 3]```.\n            ```yxz_transform``` such that\n            ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels```\n            for spot ```i```.\n            ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```.\n        - ```in_range``` - ```bool [n_spots]```.\n            Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`.\n    \"\"\"\n    return jax.vmap(apply_transform_single, in_axes=(0, None, None, None, None),\n                    out_axes=(0, 0))(yxz, transform, tile_centre, z_scale, tile_sz)\n</code></pre>"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.get_spot_colors","title":"<code>get_spot_colors(yxz_base, t, transforms, nbp_file, nbp_basic, use_rounds=None, use_channels=None, return_in_bounds=False)</code>","text":"<p>Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on <code>nbp_basic.use_rounds</code> and <code>nbp_basic.use_channels</code>.</p> <p>Note</p> <p>Returned spot colors have dimension <code>n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)</code> not <code>n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels</code>.</p> <p>Note</p> <p><code>invalid_value = -nbp_basic.tile_pixel_value_shift</code> is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence I use this as integer nan. It will be <code>invalid_value</code> if the registered coordinate of spot <code>s</code> is outside the tile in round <code>r</code>, channel <code>c</code>.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>jnp.ndarray</code> <p><code>int16 [n_spots x 3]</code>. Local yxz coordinates of spots found in the reference round/reference channel of tile <code>t</code> yx coordinates are in units of <code>yx_pixels</code>. z coordinates are in units of <code>z_pixels</code>.</p> required <code>t</code> <code>int</code> <p>Tile that spots were found on.</p> required <code>transforms</code> <code>jnp.ndarray</code> <p><code>float [n_tiles x n_rounds x n_channels x 4 x 3]</code>. <code>transforms[t, r, c]</code> is the affine transform to get from tile <code>t</code>, <code>ref_round</code>, <code>ref_channel</code> to tile <code>t</code>, round <code>r</code>, channel <code>c</code>.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>use_rounds</code> <code>Optional[List[int]]</code> <p><code>int [n_use_rounds]</code>. Rounds you would like to find the <code>spot_color</code> for. Error will raise if transform is zero for particular round. If <code>None</code>, all rounds in <code>nbp_basic.use_rounds</code> used.</p> <code>None</code> <code>use_channels</code> <code>Optional[List[int]]</code> <p><code>int [n_use_channels]</code>. Channels you would like to find the <code>spot_color</code> for. Error will raise if transform is zero for particular channel. If <code>None</code>, all channels in <code>nbp_basic.use_channels</code> used.</p> <code>None</code> <code>return_in_bounds</code> <code>bool</code> <p>if <code>True</code>, then only <code>spot_colors</code> which are within the tile bounds in all <code>use_rounds</code> / <code>use_channels</code> will be returned. The corresponding <code>yxz_base</code> coordinates will also be returned in this case. Otherwise, <code>spot_colors</code> will be returned for all the given <code>yxz_base</code> but if spot <code>s</code> is out of bounds on round <code>r</code>, channel <code>c</code>, then <code>spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift</code>. This is the only scenario for which <code>spot_colors = invalid_value</code> due to clipping in the extract step.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, jnp.ndarray]]</code> <ul> <li><code>spot_colors</code> - <code>int32 [n_spots x n_rounds_use x n_channels_use]</code> or <code>int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]</code>. <code>spot_colors[s, r, c]</code> is the spot color for spot <code>s</code> in round <code>use_rounds[r]</code>, channel <code>use_channels[c]</code>.</li> </ul> <code>Union[np.ndarray, Tuple[np.ndarray, jnp.ndarray]]</code> <ul> <li><code>yxz_base</code> - <code>int16 [n_spots_in_bounds x 3]</code>. If <code>return_in_bounds</code>, the <code>yxz_base</code> corresponding to spots in bounds for all <code>use_rounds</code> / <code>use_channels</code> will be returned. It is likely that <code>n_spots_in_bounds</code> won't be the same as <code>n_spots</code>.</li> </ul> Source code in <code>coppafish/spot_colors/base_optimised.py</code> <pre><code>def get_spot_colors(yxz_base: jnp.ndarray, t: int, transforms: jnp.ndarray, nbp_file: NotebookPage,\n                    nbp_basic: NotebookPage, use_rounds: Optional[List[int]] = None,\n                    use_channels: Optional[List[int]] = None,\n                    return_in_bounds: bool = False) -&gt; Union[np.ndarray, Tuple[np.ndarray, jnp.ndarray]]:\n\"\"\"\n    Takes some spots found on the reference round, and computes the corresponding spot intensity\n    in specified imaging rounds/channels.\n    By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`.\n\n    !!! note\n        Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not\n        `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`.\n\n    !!! note\n        `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file\n        minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this.\n        Hence I use this as integer nan. It will be `invalid_value` if the registered coordinate of\n        spot `s` is outside the tile in round `r`, channel `c`.\n\n    Args:\n        yxz_base: `int16 [n_spots x 3]`.\n            Local yxz coordinates of spots found in the reference round/reference channel of tile `t`\n            yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`.\n        t: Tile that spots were found on.\n        transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`.\n            `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to\n            tile `t`, round `r`, channel `c`.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        use_rounds: `int [n_use_rounds]`.\n            Rounds you would like to find the `spot_color` for.\n            Error will raise if transform is zero for particular round.\n            If `None`, all rounds in `nbp_basic.use_rounds` used.\n        use_channels: `int [n_use_channels]`.\n            Channels you would like to find the `spot_color` for.\n            Error will raise if transform is zero for particular channel.\n            If `None`, all channels in `nbp_basic.use_channels` used.\n        return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all\n            `use_rounds` / `use_channels` will be returned.\n            The corresponding `yxz_base` coordinates will also be returned in this case.\n            Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on\n            round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`.\n            This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step.\n\n\n    Returns:\n        - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or\n            `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`.\n            `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`.\n        - `yxz_base` - `int16 [n_spots_in_bounds x 3]`.\n            If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels`\n            will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`.\n    \"\"\"\n    if use_rounds is None:\n        use_rounds = nbp_basic.use_rounds\n    if use_channels is None:\n        use_channels = nbp_basic.use_channels\n    z_scale = nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy\n\n    n_spots = yxz_base.shape[0]\n    no_verbose = n_spots &lt; 10000\n    # note using nan means can't use integer even though data is integer\n    n_use_rounds = len(use_rounds)\n    n_use_channels = len(use_channels)\n    # spots outside tile bounds on particular r/c will initially be set to 0.\n    spot_colors = np.zeros((n_spots, n_use_rounds, n_use_channels), dtype=np.int32)\n    tile_centre = jnp.array(nbp_basic.tile_centre)\n    if not nbp_basic.is_3d:\n        # use numpy not jax.numpy as reading in tiff is done in numpy.\n        tile_sz = jnp.array([nbp_basic.tile_sz, nbp_basic.tile_sz, 1], dtype=jnp.int16)\n    else:\n        tile_sz = jnp.array([nbp_basic.tile_sz, nbp_basic.tile_sz, nbp_basic.nz], dtype=jnp.int16)\n\n    with tqdm(total=n_use_rounds * n_use_channels, disable=no_verbose) as pbar:\n        pbar.set_description(f\"Reading {n_spots} spot_colors found on tile {t} from npy files\")\n        for r in range(n_use_rounds):\n            if not nbp_basic.is_3d:\n                # If 2D, load in all channels first\n                image_all_channels = np.load(nbp_file.tile[t][use_rounds[r]], mmap_mode='r')\n            for c in range(n_use_channels):\n                transform_rc = transforms[t, use_rounds[r], use_channels[c]]\n                pbar.set_postfix({'round': use_rounds[r], 'channel': use_channels[c]})\n                if transform_rc[0, 0] == 0:\n                    raise ValueError(\n                        f\"Transform for tile {t}, round {use_rounds[r]}, channel {use_channels[c]} is zero:\"\n                        f\"\\n{transform_rc}\")\n                yxz_transform, in_range = apply_transform(yxz_base, transform_rc, tile_centre, z_scale, tile_sz)\n                yxz_transform = np.asarray(yxz_transform)\n                in_range = np.asarray(in_range)\n                yxz_transform = yxz_transform[in_range]\n                if yxz_transform.shape[0] &gt; 0:\n                    # Read in the shifted uint16 colors here, and remove shift later.\n                    if nbp_basic.is_3d:\n                        spot_colors[in_range, r, c] = utils.npy.load_tile(nbp_file, nbp_basic, t, use_rounds[r],\n                                                                          use_channels[c], yxz_transform,\n                                                                          apply_shift=False)\n                    else:\n                        spot_colors[in_range, r, c] = image_all_channels[use_channels[c]][\n                            tuple(np.asarray(yxz_transform[:, i]) for i in range(2))]\n                pbar.update(1)\n    # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value.\n    # It is impossible for any actual spot color to be this due to clipping at the extract stage.\n    spot_colors = spot_colors - nbp_basic.tile_pixel_value_shift\n    invalid_value = - nbp_basic.tile_pixel_value_shift\n    if return_in_bounds:\n        good = ~np.any(spot_colors == invalid_value, axis=(1, 2))\n        return spot_colors[good], yxz_base[good]\n    else:\n        return spot_colors\n</code></pre>"},{"location":"code/stitch/check_shifts/","title":"Check Shifts","text":""},{"location":"code/stitch/check_shifts/#coppafish.stitch.check_shifts.check_shifts_register","title":"<code>check_shifts_register(nb)</code>","text":"<p>This checks that a decent number of shifts computed in the <code>register_initial</code> stage of the pipeline are acceptable (<code>score &gt; score_thresh</code>).</p> <p>An error will be raised if the fraction of shifts with <code>score &lt; score_thresh</code> exceeds <code>config['register_initial']['n_shifts_error_fraction']</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing <code>stitch</code> page.</p> required Source code in <code>coppafish/stitch/check_shifts.py</code> <pre><code>def check_shifts_register(nb: Notebook):\n\"\"\"\n    This checks that a decent number of shifts computed in the `register_initial` stage of the pipeline\n    are acceptable (`score &gt; score_thresh`).\n\n    An error will be raised if the fraction of shifts with `score &lt; score_thresh`\n    exceeds `config['register_initial']['n_shifts_error_fraction']`.\n\n    Args:\n        nb: *Notebook* containing `stitch` page.\n    \"\"\"\n    r_ref = nb.basic_info.ref_round\n    c_ref = nb.basic_info.ref_channel\n    c_shift = nb.register_initial.shift_channel\n    use_rounds = np.asarray(nb.basic_info.use_rounds)\n    use_tiles = np.asarray(nb.basic_info.use_tiles)\n    n_shifts = len(use_rounds) * len(use_tiles)\n    n_fail = 0\n    config = nb.get_config()['register_initial']\n    shift = nb.register_initial.shift\n    score = nb.register_initial.shift_score\n    score_thresh = nb.register_initial.shift_score_thresh\n    fail_info = np.zeros((0, 7), dtype=int)\n    for r in nb.basic_info.use_rounds:\n        fail_tiles = use_tiles[np.where((score[use_tiles, r] &lt; score_thresh[use_tiles, r]).flatten())[0]]\n        n_fail += len(fail_tiles)\n        if len(fail_tiles) &gt; 0:\n            fail_info_r = np.zeros((len(fail_tiles), 7), dtype=int)\n            fail_info_r[:, 0] = r\n            fail_info_r[:, 1] = fail_tiles\n            fail_info_r[:, 2:5] = shift[fail_tiles, r]\n            fail_info_r[:, 5] = score[fail_tiles, r].flatten()\n            fail_info_r[:, 6] = score_thresh[fail_tiles, r].flatten()\n            fail_info = np.append(fail_info, fail_info_r, axis=0)\n    if n_fail &gt;= 1:\n        message = f\"\\nInfo for the {n_fail} shifts from round {r_ref}/channel {c_ref} to channel {c_shift}\" \\\n                  f\" with score &lt; score_thresh:\\n\" \\\n                  f\"Round, Tile, Y shift, X shift, Z shift, score, score_thresh\\n\" \\\n                  f\"{fail_info}\"\n        n_error_thresh = int(np.floor(config['n_shifts_error_fraction'] * n_shifts))\n        if n_fail &gt; n_error_thresh:\n            message = message + f\"\\n{n_fail}/{n_shifts} shifts have score &lt; score_thresh.\\n\" \\\n                                f\"This exceeds error threshold of {n_error_thresh}.\\nLook at the following \" \\\n                                f\"diagnostics to decide if shifts are acceptable to continue:\\n\" \\\n                                f\"coppafish.plot.view_register_shift_info\\ncoppafish.plot.view_register_search\\n\" \\\n                                f\"coppafish.plot.view_icp\\n\" \\\n                                f\"If shifts looks wrong, maybe try re-running with \" \\\n                                f\"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\"\n\n            # Recommend channel with the most spots on the tile/round for which it has the least.\n            spot_no = nb.find_spots.spot_no[np.ix_(nb.basic_info.use_tiles, nb.basic_info.use_rounds)]\n            # For each channel this is number of spots on tile/round with the least spots.\n            spot_no = np.min(spot_no, axis=(0, 1))\n            c_most_spots = np.argmax(spot_no)\n            if c_most_spots != nb.register_initial.shift_channel:\n                message = message + f\"\\nAlso consider changing config['register_initial']['shift_channel']. \" \\\n                                    f\"Current channel {c_ref} has at least {spot_no[c_ref]} on all tiles and rounds \" \\\n                                    f\"but channel {c_most_spots} has at least {spot_no[c_most_spots]}.\"\n            raise ValueError(f\"{message}\")\n        else:\n            warnings.warn(message)\n</code></pre>"},{"location":"code/stitch/check_shifts/#coppafish.stitch.check_shifts.check_shifts_stitch","title":"<code>check_shifts_stitch(nb)</code>","text":"<p>This checks that a decent number of shifts computed in the stitch stage of the pipeline are acceptable (<code>score &gt; score_thresh</code>).</p> <p>An error will be raised if the fraction of shifts with <code>score &lt; score_thresh</code> exceeds <code>config['stitch']['n_shifts_error_fraction']</code>.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing <code>stitch</code> page.</p> required Source code in <code>coppafish/stitch/check_shifts.py</code> <pre><code>def check_shifts_stitch(nb: Notebook):\n\"\"\"\n    This checks that a decent number of shifts computed in the stitch stage of the pipeline\n    are acceptable (`score &gt; score_thresh`).\n\n    An error will be raised if the fraction of shifts with `score &lt; score_thresh`\n    exceeds `config['stitch']['n_shifts_error_fraction']`.\n\n    Args:\n        nb: *Notebook* containing `stitch` page.\n    \"\"\"\n    n_shifts = 0\n    n_fail = 0\n    message = \"\"\n    config = nb.get_config()['stitch']\n    directions = ['south', 'west']\n    dir_opp = {'south': 'north', 'west': 'east'}\n    for j in directions:\n        n_shifts += len(nb.stitch.__getattribute__(f\"{j}_score\"))\n        fail_ind = np.where((nb.stitch.__getattribute__(f\"{j}_score\") &lt;\n                                nb.stitch.__getattribute__(f\"{j}_score_thresh\")).flatten())[0]\n        n_fail += len(fail_ind)\n        if len(fail_ind) &gt; 0:\n            fail_info = np.zeros((len(fail_ind), 7), dtype=int)\n            fail_info[:, :2] = nb.stitch.__getattribute__(f\"{j}_pairs\")[fail_ind]\n            fail_info[:, 2:5] = nb.stitch.__getattribute__(f\"{j}_shifts\")[fail_ind]\n            fail_info[:, 5] = nb.stitch.__getattribute__(f\"{j}_score\")[fail_ind].flatten()\n            fail_info[:, 6] = nb.stitch.__getattribute__(f\"{j}_score_thresh\")[fail_ind].flatten()\n            message = message + f\"\\nInfo for the {len(fail_ind)} shifts with score &lt; score_thresh in {j} direction:\\n\" \\\n                                f\"Tile, Tile to {dir_opp[j]}, Y shift, X shift, Z shift, score, score_thresh\\n\" \\\n                                f\"{fail_info}\"\n    n_error_thresh = int(np.floor(config['n_shifts_error_fraction'] * n_shifts))\n    if n_fail &gt; n_error_thresh:\n        message = message + f\"\\n{n_fail}/{n_shifts} shifts have score &lt; score_thresh.\\n\" \\\n                            f\"This exceeds error threshold of {n_error_thresh}.\\nLook at the following diagnostics \" \\\n                            f\"to decide if stitching is acceptable to continue:\\n\" \\\n                            f\"coppafish.plot.view_stitch_shift_info\\ncoppafish.plot.view_stitch\\n\" \\\n                            f\"coppafish.plot.view_stitch_overlap\\n\" \\\n                            f\"coppafish.plot.view_stitch_search\\nIf stitching looks wrong, maybe try re-running with \" \\\n                            f\"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\"\n        raise ValueError(f\"{message}\")\n    elif n_fail &gt;= 1:\n        warnings.warn(message)\n</code></pre>"},{"location":"code/stitch/shift/","title":"Shift","text":""},{"location":"code/stitch/shift/#coppafish.stitch.shift.compute_shift","title":"<code>compute_shift(yxz_base, yxz_transform, min_score_2d, min_score_multiplier, min_score_min_dist, min_score_max_dist, neighb_dist_thresh, y_shifts, x_shifts, z_shifts=None, widen=None, max_range=None, z_scale=1, nz_collapse=None, z_step=3)</code>","text":"<p>This finds the shift from those given that is best applied to <code>yxz_base</code> to match <code>yxz_transform</code>.</p> <p>If the <code>score</code> of this is below <code>min_score_2d</code>, a widened search is performed. If the <code>score</code> is above <code>min_score_2d</code>, a refined search is done about the best shift so as to find the absolute best shift, not the best shift among those given.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>int [n_spots_base x 3]</code>. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units).</p> required <code>yxz_transform</code> <code>np.ndarray</code> <p><code>int [n_spots_transform x 3]</code>. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units).</p> required <code>min_score_2d</code> <code>Optional[float]</code> <p>If score of best shift is below this, will search among the widened shifts. If <code>None</code>, <code>min_score_2d</code> will be computed using <code>get_score_thresh</code>.</p> required <code>min_score_multiplier</code> <code>Optional[float]</code> <p>Parameter used to find <code>min_score_2d</code> and <code>min_score_3d</code> if not given. Typical = <code>1.5</code> (definitely more than <code>1</code>).</p> required <code>min_score_min_dist</code> <code>Optional[float]</code> <p><code>min_score_2d</code> is set to max score of those scores for shifts a distance between <code>min_dist</code> and <code>max_dist</code> from the best_shift.</p> required <code>min_score_max_dist</code> <code>Optional[float]</code> <p><code>min_score_2d</code> is set to max score of those scores for shifts a distance between <code>min_dist</code> and <code>max_dist</code> from the best_shift.</p> required <code>neighb_dist_thresh</code> <code>float</code> <p>Basically the distance below which neighbours are a good match. Typical = <code>2</code>.</p> required <code>y_shifts</code> <code>np.ndarray</code> <p><code>float [n_y_shifts]</code>. All possible shifts to test in y direction, probably made with <code>np.arange</code>.</p> required <code>x_shifts</code> <code>np.ndarray</code> <p><code>float [n_x_shifts]</code>. All possible shifts to test in x direction, probably made with <code>np.arange</code>.</p> required <code>z_shifts</code> <code>Optional[np.ndarray]</code> <p><code>float [n_z_shifts]</code>. All possible shifts to test in z direction, probably made with <code>np.arange</code>. If not given, will compute automatically from initial guess when making slices and <code>z_step</code>.</p> <code>None</code> <code>widen</code> <code>Optional[List[int]]</code> <p><code>int [3]</code>. By how many shifts to extend search in <code>[y, x, z]</code> direction if score below <code>min_score</code>. This many are added above and below current range. If all widen parameters are <code>0</code>, widened search is never performed. If <code>None</code>, set to <code>[0, 0, 0]</code>.</p> <code>None</code> <code>max_range</code> <code>Optional[List[int]]</code> <p><code>int [3]</code>. The range of shifts searched over will continue to be increased according to <code>widen</code> until the <code>max_range</code> is reached in each dimension. If a good shift is still not found, the best shift will still be returned without error. If None and widen supplied, range will only be widened once.</p> <code>None</code> <code>z_scale</code> <code>Union[float, List]</code> <p>By what scale factor to multiply z coordinates to make them same units as xy. I.e. <code>z_pixel_size / xy_pixel_size</code>. If one value, given same scale used for yxz_base and yxz_transform. Otherwise, first value used for yxz_base and second for yxz_transform.</p> <code>1</code> <code>nz_collapse</code> <code>Optional[int]</code> <p>Maximum number of z-planes allowed to be flattened into a 2D slice. If <code>None</code>, <code>n_slices</code>=1. Should be <code>None</code> for 2D data.</p> <code>None</code> <code>z_step</code> <code>int</code> <p><code>int</code>. Step of shift search in z direction in uints of <code>z_pixels</code>. <code>z_shifts</code> are computed automatically as 1 shift either side of an initial guess.</p> <code>3</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>best_shift</code> - <code>float [shift_y, shift_x, shift_z]</code>. Best shift found.</li> </ul> <code>float</code> <ul> <li><code>best_score</code> - <code>float</code>. Score of best shift.</li> </ul> <code>float</code> <ul> <li><code>min_score_3d</code> - <code>float</code>. Same as <code>min_score_2d</code>, unless input was <code>None</code> in which case this is the calculated value.</li> </ul> <code>dict</code> <ul> <li><code>debug_info</code> - dict containing debugging information:</li> <li><code>shifts_2d</code>: <code>int [n_shifts_2d x 2]</code>     All yx shifts searched to get best <code>yx_shift</code>.</li> <li><code>scores_2d</code>: <code>float [n_shifts_2d]</code>     Score corresponding to each 2d shift.</li> <li><code>shifts_3d</code>: <code>int [n_shifts_3d x 3]</code>     All yxz shifts searched to get best <code>yxz_shift</code>. <code>None</code> if <code>nz_collapse is None</code> i.e. 2D point cloud.</li> <li><code>scores_3d</code>: <code>float [n_shifts_3d]</code>     Score corresponding to each 3d shift. <code>None</code> if <code>nz_collapse is None</code> i.e. 2D point cloud.</li> <li><code>shift_2d_initial</code>: <code>float [2]</code>     Best shift found after first 2D search. I.e. annulus around this shift was used     to compute <code>min_score_2d</code> and <code>shift_thresh</code>.</li> <li><code>shift_thresh</code>: <code>int [3]</code>     yxz shift corresponding to <code>min_score_3d</code>. Will be <code>None</code> if <code>min_score_2d</code> provided in advance.     <code>shift_thresh[:2]</code> is the yx shift corresponding to <code>min_score_2d</code></li> <li><code>min_score_2d</code>: Same as input <code>min_score_2d</code>, unless was <code>None</code> in     which case this is the calculated value.</li> </ul> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def compute_shift(yxz_base: np.ndarray, yxz_transform: np.ndarray, min_score_2d: Optional[float],\n                  min_score_multiplier: Optional[float], min_score_min_dist: Optional[float],\n                  min_score_max_dist: Optional[float],\n                  neighb_dist_thresh: float, y_shifts: np.ndarray, x_shifts: np.ndarray,\n                  z_shifts: Optional[np.ndarray] = None, widen: Optional[List[int]] = None,\n                  max_range: Optional[List[int]] = None, z_scale: Union[float, List] = 1,\n                  nz_collapse: Optional[int] = None, z_step: int = 3) -&gt; Tuple[np.ndarray, float, float, dict]:\n\"\"\"\n    This finds the shift from those given that is best applied to `yxz_base` to match `yxz_transform`.\n\n    If the `score` of this is below `min_score_2d`, a widened search is performed.\n    If the `score` is above `min_score_2d`, a refined search is done about the best shift so as to find the absolute\n    best shift, not the best shift among those given.\n\n    Args:\n        yxz_base: `int [n_spots_base x 3]`.\n            Coordinates of spots on base image (yx in yx pixel units, z in z pixel units).\n        yxz_transform: `int [n_spots_transform x 3]`.\n            Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units).\n        min_score_2d: If score of best shift is below this, will search among the widened shifts.\n            If `None`, `min_score_2d` will be computed using `get_score_thresh`.\n        min_score_multiplier: Parameter used to find `min_score_2d` and `min_score_3d` if not given.\n            Typical = `1.5` (definitely more than `1`).\n        min_score_min_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist`\n            and `max_dist` from the best_shift.\n        min_score_max_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist`\n            and `max_dist` from the best_shift.\n        neighb_dist_thresh: Basically the distance below which neighbours are a good match.\n            Typical = `2`.\n        y_shifts: `float [n_y_shifts]`.\n            All possible shifts to test in y direction, probably made with `np.arange`.\n        x_shifts: `float [n_x_shifts]`.\n            All possible shifts to test in x direction, probably made with `np.arange`.\n        z_shifts: `float [n_z_shifts]`.\n            All possible shifts to test in z direction, probably made with `np.arange`.\n            If not given, will compute automatically from initial guess when making slices and `z_step`.\n        widen: `int [3]`.\n            By how many shifts to extend search in `[y, x, z]` direction if score below `min_score`.\n            This many are added above and below current range.\n            If all widen parameters are `0`, widened search is never performed.\n            If `None`, set to `[0, 0, 0]`.\n        max_range: `int [3]`.\n            The range of shifts searched over will continue to be increased according to `widen` until\n            the `max_range` is reached in each dimension.\n            If a good shift is still not found, the best shift will still be returned without error.\n            If None and widen supplied, range will only be widened once.\n        z_scale: By what scale factor to multiply z coordinates to make them same units as xy.\n            I.e. `z_pixel_size / xy_pixel_size`.\n            If one value, given same scale used for yxz_base and yxz_transform.\n            Otherwise, first value used for yxz_base and second for yxz_transform.\n        nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice.\n            If `None`, `n_slices`=1. Should be `None` for 2D data.\n        z_step: `int`.\n            Step of shift search in z direction in uints of `z_pixels`.\n            `z_shifts` are computed automatically as 1 shift either side of an initial guess.\n\n    Returns:\n        - `best_shift` - `float [shift_y, shift_x, shift_z]`.\n            Best shift found.\n        - `best_score` - `float`.\n            Score of best shift.\n        - `min_score_3d` - `float`.\n            Same as `min_score_2d`, unless input was `None` in which case this is the calculated value.\n        - `debug_info` - dict containing debugging information:\n            - `shifts_2d`: `int [n_shifts_2d x 2]`\n                All yx shifts searched to get best `yx_shift`.\n            - `scores_2d`: `float [n_shifts_2d]`\n                Score corresponding to each 2d shift.\n            - `shifts_3d`: `int [n_shifts_3d x 3]`\n                All yxz shifts searched to get best `yxz_shift`. `None` if `nz_collapse is None` i.e. 2D point cloud.\n            - `scores_3d`: `float [n_shifts_3d]`\n                Score corresponding to each 3d shift. `None` if `nz_collapse is None` i.e. 2D point cloud.\n            - `shift_2d_initial`: `float [2]`\n                Best shift found after first 2D search. I.e. annulus around this shift was used\n                to compute `min_score_2d` and `shift_thresh`.\n            - `shift_thresh`: `int [3]`\n                yxz shift corresponding to `min_score_3d`. Will be `None` if `min_score_2d` provided in advance.\n                `shift_thresh[:2]` is the yx shift corresponding to `min_score_2d`\n            - `min_score_2d`: Same as input `min_score_2d`, unless was `None` in\n                which case this is the calculated value.\n    \"\"\"\n    if widen is None:\n        widen = [0, 0, 0]\n    if np.asarray(z_scale).size == 1:\n        z_scale = [z_scale, z_scale]\n    if len(z_scale) &gt; 2:\n        raise ValueError(f'Only 2 z_scale values should be provided but z_scale given was {z_scale}.')\n    yx_base_slices, yx_transform_trees, z_shift_guess = get_2d_slices(yxz_base, yxz_transform, nz_collapse)\n    if nz_collapse is not None:\n        # Only do z-scaling in 3D case\n        yxz_base = yxz_base * [1, 1, z_scale[0]]\n        yxz_transform = yxz_transform * [1, 1, z_scale[1]]\n    yxz_transform_tree = KDTree(yxz_transform)\n    shift_2d, score_2d, all_shifts_2d, all_scores_2d = get_best_shift_2d(yx_base_slices, yx_transform_trees,\n                                                                         neighb_dist_thresh, y_shifts, x_shifts)\n\n    # Only look at 3 shifts in z to start with about guess from getting the 2d slices.\n    if z_shifts is None:\n        z_shifts = np.arange(z_shift_guess - z_step, z_shift_guess + z_step + 1, z_step)\n\n    # save initial_shifts so don't look over same shifts twice\n    # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2)\n    shift_2d_initial = shift_2d.copy()\n    if min_score_2d is None:\n        min_score_2d, shift_thresh = get_score_thresh(all_shifts_2d, all_scores_2d, shift_2d, min_score_min_dist,\n                                                      min_score_max_dist, min_score_multiplier)\n        shift_thresh = np.pad(shift_thresh, (0, 1))  # add z shift = 0\n    else:\n        shift_thresh = None\n    if score_2d &lt;= min_score_2d and np.max(widen[:2]) &gt; 0:\n        shift_ranges = np.array([np.ptp(i) for i in [y_shifts, x_shifts]])\n        if max_range is None:\n            # If don't specify max_range, only widen once.\n            max_range = np.array([np.ptp(i) for i in [y_shifts, x_shifts, z_shifts]]) * (np.array(widen[:2]) &gt; 0)\n            max_range[max_range &gt; 0] += 1\n            max_range_2d = max_range[:2]\n        else:\n            max_range_2d = np.asarray(max_range[:2])\n        # keep extending range of shifts in yx until good score reached or hit max shift_range.\n        while score_2d &lt;= min_score_2d:\n            if np.all(shift_ranges &gt;= max_range_2d):\n                warnings.warn(f\"Shift search range exceeds max_range = {max_range_2d} in yxz directions but \\n\"\n                              f\"best score is only {round(score_2d, 2)} which is below \"\n                              f\"min_score = {round(min_score_2d, 2)}.\"\n                              f\"\\nBest shift found was {shift_2d}.\")\n                break\n            else:\n                warnings.warn(f\"Best shift found ({shift_2d}) has score of {round(score_2d, 2)} which is below \"\n                              f\"min_score = {round(min_score_2d, 2)}.\"\n                              f\"\\nRunning again with extended shift search range in yx.\")\n            if shift_ranges[0] &lt; max_range_2d[0]:\n                y_shifts = extend_array(y_shifts, widen[0])\n            if shift_ranges[1] &lt; max_range_2d[1]:\n                x_shifts = extend_array(x_shifts, widen[1])\n            shift_2d_new, score_2d_new, all_shifts_new, all_scores_new = \\\n                get_best_shift_2d(yx_base_slices, yx_transform_trees, neighb_dist_thresh, y_shifts, x_shifts,\n                                  all_shifts_2d)\n            if score_2d_new &gt; score_2d:\n                score_2d = score_2d_new\n                shift_2d = shift_2d_new\n            # update initial_shifts so don't look over same shifts twice\n            all_shifts_2d = np.append(all_shifts_2d, all_shifts_new, axis=0)\n            all_scores_2d = np.append(all_scores_2d, all_scores_new, axis=0)\n            # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2)\n            shift_ranges = np.array([np.ptp(i) for i in [y_shifts, x_shifts]])\n\n    if nz_collapse is None:\n        # nz_collapse not provided for 2D data.\n        shift = np.append(shift_2d, 0)\n        score = score_2d\n        all_shifts_3d = None\n        all_scores_3d = None\n        min_score_3d = min_score_2d\n    else:\n        ignore_shifts = None\n        if shift_thresh is None:\n            y_shift_2d = np.array(shift_2d[0])\n            x_shift_2d = np.array(shift_2d[1])\n        else:\n            y_shift_2d = np.array([shift_2d[0], shift_thresh[0]])\n            x_shift_2d = np.array([shift_2d[1], shift_thresh[1]])\n            if len(np.unique(y_shift_2d)) == 2 and len(np.unique(x_shift_2d)) == 2:\n                # Only find shifts for the shift_2d and shift_thresh, get rid of cross terms.\n                ignore_shifts = np.array([[shift_2d[0], shift_thresh[1]], [shift_thresh[0], shift_2d[1]]])\n                ignore_shifts = np.tile(np.pad(ignore_shifts, [(0, 0), (0, 1)]), [len(z_shifts), 1])\n                ignore_shifts[:, 2] = np.repeat(z_shifts * z_scale[0], len(y_shift_2d))\n        # z_scale for yxz_base used from now on as we are finding the shift from yxz_base to yxz_transform.\n        shift, score, all_shifts_3d, all_scores_3d = get_best_shift_3d(yxz_base, yxz_transform_tree, neighb_dist_thresh,\n                                                                       y_shift_2d, x_shift_2d, z_shifts * z_scale[0],\n                                                                       ignore_shifts=ignore_shifts)\n        if shift_thresh is not None:\n            # Set min_score_3d to max score at shift used to find min_score_2d across all z planes\n            # multiplied by min_score_multiplier\n            shift_thresh_ind = np.where(numpy_indexed.indices(shift_thresh[np.newaxis, :2].astype(int),\n                                                              all_shifts_3d[:, :2].astype(int), missing=-10) == 0)[0]\n            shift_thresh_best_ind = shift_thresh_ind[np.argmax(all_scores_3d[shift_thresh_ind])]\n            min_score_3d = all_scores_3d[shift_thresh_best_ind] * min_score_multiplier\n            shift_thresh = (all_shifts_3d[shift_thresh_best_ind] / [1, 1, z_scale[0]]).astype(int)\n        else:\n            min_score_3d = min_score_2d\n\n        if score &lt; min_score_2d and widen[2] &gt; 0:\n            # keep extending range of shifts in z until good score reached or hit max shift_range.\n            # yx shift is kept as 2d shift found when using slices.\n            max_range_z = np.asarray(max_range[2])\n            z_shift_range = np.ptp(z_shifts)\n            while score &lt; min_score_3d:\n                if z_shift_range &gt; max_range_z:\n                    warnings.warn(f\"Shift search range exceeds max_range = {max_range_z} in z directions but \\n\"\n                                  f\"best score is only {np.around(score, 2)} which is below \"\n                                  f\"min_score = {np.around(min_score_3d, 2)}.\"\n                                  f\"\\nBest shift found was {shift}.\")\n                    break\n                else:\n                    warnings.warn(f\"Best shift found ({shift}) has score of {round(score, 2)} which is below \"\n                                  f\"min_score = {np.around(min_score_3d, 2)}.\"\n                                  f\"\\nRunning again with extended shift search range in z.\")\n                z_shifts = extend_array(z_shifts, widen[2])\n                shift_new, score_new, all_shifts_new, all_scores_new = \\\n                    get_best_shift_3d(yxz_base, yxz_transform_tree, neighb_dist_thresh, y_shift_2d,\n                                      x_shift_2d, z_shifts * z_scale[0], all_shifts_3d)\n                if score_new &gt; score:\n                    score = score_new\n                    shift = shift_new\n                # update initial_shifts so don't look over same shifts twice\n                all_shifts_3d = np.append(all_shifts_3d, all_shifts_new, axis=0)\n                all_scores_3d = np.append(all_scores_3d, all_scores_new, axis=0)\n                z_shift_range = np.ptp(z_shifts)\n\n    # refined search near maxima with half the step\n    y_shifts = refined_shifts(y_shifts, shift[0])\n    x_shifts = refined_shifts(x_shifts, shift[1])\n    z_shifts = refined_shifts(z_shifts, shift[2] / z_scale[0])\n    shift2, score2, all_shifts_new, all_scores_new = \\\n        get_best_shift_3d(yxz_base, yxz_transform_tree, neighb_dist_thresh, y_shifts, x_shifts,\n                          z_shifts * z_scale[0], all_shifts_3d)\n    if score2 &gt; score:\n        shift = shift2\n    if nz_collapse is None:\n        all_shifts_2d = np.append(all_shifts_2d, all_shifts_new[:, :2], axis=0)\n        all_scores_2d = np.append(all_scores_2d, all_scores_new, axis=0)\n    else:\n        all_shifts_3d = np.append(all_shifts_3d, all_shifts_new, axis=0)\n        all_scores_3d = np.append(all_scores_3d, all_scores_new, axis=0)\n    # final search with a step of 1\n    y_shifts = refined_shifts(y_shifts, shift[0], refined_scale=1e-50, extend_scale=1)\n    x_shifts = refined_shifts(x_shifts, shift[1], refined_scale=1e-50, extend_scale=1)\n    z_shifts = refined_shifts(z_shifts, shift[2] / z_scale[0], refined_scale=1e-50, extend_scale=1)\n    shift, score, all_shifts_new, all_scores_new = \\\n        get_best_shift_3d(yxz_base, yxz_transform_tree, neighb_dist_thresh, y_shifts, x_shifts,\n                          z_shifts * z_scale[0], all_shifts_3d)\n    if nz_collapse is None:\n        all_shifts_2d = np.append(all_shifts_2d, all_shifts_new[:, :2], axis=0)\n        all_scores_2d = np.append(all_scores_2d, all_scores_new, axis=0)\n    else:\n        all_shifts_3d = np.append(all_shifts_3d, all_shifts_new, axis=0)\n        all_scores_3d = np.append(all_scores_3d, all_scores_new, axis=0)\n        all_shifts_3d[:, 2] = all_shifts_3d[:, 2] / z_scale[0]\n        all_shifts_3d = all_shifts_3d.astype(np.int16)\n    shift[2] = shift[2] / z_scale[0]\n    return shift.astype(int), score, min_score_3d, {'shifts_2d': all_shifts_2d, 'scores_2d': all_scores_2d,\n                                                    'shifts_3d': all_shifts_3d, 'scores_3d': all_scores_3d,\n                                                    'shift_2d_initial': shift_2d_initial, 'shift_thresh': shift_thresh,\n                                                    'min_score_2d': min_score_2d}\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.extend_array","title":"<code>extend_array(array, extend_scale, direction='both')</code>","text":"<p>Extrapolates array using its mean spacing in the direction specified by <code>extend_sz</code> values.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>np.ndarray</code> <p><code>float [n_values]</code>. Array probably produced using <code>np.arange</code>. It is expected to be in ascending order with constant step.</p> required <code>extend_scale</code> <code>int</code> <p>By how many values to extend the array.</p> required <code>direction</code> <code>str</code> <p>One of the following, specifying how to extend the <code>array</code> -</p> <ul> <li><code>'below'</code> - <code>array</code> extended below the min value.</li> <li><code>'above'</code> - <code>array</code> extended above the max value</li> <li><code>'both'</code> - <code>array</code> extended in both directions (by <code>extend_sz</code> in each direction).</li> </ul> <code>'both'</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_values + extend_scale * (direction == 'both' + 1)]</code>. <code>array</code> extrapolated in <code>direction</code> specified.</p> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def extend_array(array: np.ndarray, extend_scale: int, direction: str = 'both') -&gt; np.ndarray:\n\"\"\"\n    Extrapolates array using its mean spacing in the direction specified by `extend_sz` values.\n\n    Args:\n        array: `float [n_values]`.\n            Array probably produced using `np.arange`. It is expected to be in ascending order with constant step.\n        extend_scale: By how many values to extend the array.\n        direction: One of the following, specifying how to extend the `array` -\n\n            - `'below'` - `array` extended below the min value.\n            - `'above'` - `array` extended above the max value\n            - `'both'` - `array` extended in both directions (by `extend_sz` in each direction).\n\n    Returns:\n        `float [n_values + extend_scale * (direction == 'both' + 1)]`.\n            `array` extrapolated in `direction` specified.\n    \"\"\"\n    if extend_scale == 0:\n        ext_array = array\n    else:\n        step = np.mean(np.ediff1d(array))\n        ext_below = np.arange(array.min() - extend_scale * step, array.min(), step)\n        ext_above = np.arange(array.max() + step, array.max() + extend_scale * step + step / 2, step)\n        if direction == 'below':\n            ext_array = np.concatenate((ext_below, array))\n        elif direction == 'above':\n            ext_array = np.concatenate((array, ext_above))\n        elif direction == 'both':\n            ext_array = np.concatenate((ext_below, array, ext_above))\n        else:\n            raise ValueError(f\"direction specified was {direction}, whereas it should be 'below', 'above' or 'both'\")\n    return ext_array\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_2d_slices","title":"<code>get_2d_slices(yxz_base, yxz_transform, nz_collapse)</code>","text":"<p>This splits <code>yxz_base</code> and <code>yxz_transform</code> into <code>n_slices = nz / nz_collapse</code> 2D slices. Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>float [n_spots_base x 3]</code>. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units).</p> required <code>yxz_transform</code> <code>np.ndarray</code> <p><code>float [n_spots_transform x 3]</code>. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units).</p> required <code>nz_collapse</code> <code>Optional[int]</code> <p>Maximum number of z-planes allowed to be flattened into a 2D slice. If <code>None</code>, <code>n_slices</code>=1.</p> required <p>Returns:</p> Type Description <code>List[np.ndarray]</code> <ul> <li><code>yx_base_slices</code> - List of n_slices arrays indicating yx_base coordinates of spots in that slice.</li> </ul> <code>List[KDTree]</code> <ul> <li><code>yx_transform_trees</code> - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice.</li> </ul> <code>int</code> <ul> <li>transform_min_z - Guess of z shift from <code>yxz_base</code> to <code>yxz_transform</code> in units of <code>z_pixels</code>.</li> </ul> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def get_2d_slices(yxz_base: np.ndarray, yxz_transform: np.ndarray,\n                  nz_collapse: Optional[int]) -&gt; Tuple[List[np.ndarray], List[KDTree], int]:\n\"\"\"\n    This splits `yxz_base` and `yxz_transform` into `n_slices = nz / nz_collapse` 2D slices.\n    Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search.\n\n    Args:\n        yxz_base: `float [n_spots_base x 3]`.\n            Coordinates of spots on base image (yx in yx pixel units, z in z pixel units).\n        yxz_transform: `float [n_spots_transform x 3]`.\n            Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units).\n        nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice.\n            If `None`, `n_slices`=1.\n\n    Returns:\n        - `yx_base_slices` - List of n_slices arrays indicating yx_base coordinates of spots in that slice.\n        - `yx_transform_trees` - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in\n            that slice.\n        - transform_min_z - Guess of z shift from `yxz_base` to `yxz_transform` in units of `z_pixels`.\n    \"\"\"\n    if nz_collapse is not None:\n        nz = int(np.ceil(yxz_base[:, 2].max() + 1))\n        n_slices = int(np.ceil(nz / nz_collapse))\n        base_z_slices = np.array_split(np.arange(nz), n_slices)\n        slice_max_z_base = 0  # min z for slice 0\n        transform_max_z = int(np.ceil(yxz_transform[:, 2].max() + 1))\n        # transform_min_z provides an approx guess to the z shift.\n        transform_min_z = np.min([int(np.floor(yxz_transform[:, 2].min())), transform_max_z - nz])\n        slice_max_z_transform = transform_min_z  # min z for slice 0\n        yx_base_slices = []\n        yx_transform_trees = []\n        for i in range(n_slices):\n            slice_min_z_base = slice_max_z_base  # set min z to the max z of the last slice\n            slice_max_z_base = base_z_slices[i][-1] + 1\n            in_slice_base = np.array([yxz_base[:, 2] &gt;= slice_min_z_base,\n                                      yxz_base[:, 2] &lt; slice_max_z_base]).all(axis=0)\n            yx_base_slices.append(yxz_base[in_slice_base, :2])\n            # transform z coords may have systematic z shift so start from min_z not 0.\n            slice_min_z_transform = slice_max_z_transform  # set min z to the max z of the last slice\n            if i == n_slices-1:\n                # For final slice, ensure all z planes in yxz_transform included.\n                slice_max_z_transform = transform_max_z + 1\n            else:\n                slice_max_z_transform = base_z_slices[i][-1] + 1 + transform_min_z\n            in_slice_transform = np.array([yxz_transform[:, 2] &gt;= slice_min_z_transform,\n                                           yxz_transform[:, 2] &lt; slice_max_z_transform]).all(axis=0)\n            yx_transform_trees.append(KDTree(yxz_transform[in_slice_transform, :2]))\n    else:\n        transform_min_z = 0\n        yx_base_slices = [yxz_base[:, :2]]\n        yx_transform_trees = [KDTree(yxz_transform[:, :2])]\n    return yx_base_slices, yx_transform_trees, transform_min_z\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_best_shift_2d","title":"<code>get_best_shift_2d(yx_base_slices, yx_transform_trees, neighb_dist_thresh, y_shifts, x_shifts, ignore_shifts=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>yx_base_slices</code> <code>List[np.ndarray]</code> <p>List of n_slices arrays indicating yx_base coordinates of spots in that slice.</p> required <code>yx_transform_trees</code> <code>List[KDTree]</code> <p>List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice.</p> required <code>neighb_dist_thresh</code> <code>float</code> <p>Basically the distance below which neighbours are a good match. Typical = <code>2</code>.</p> required <code>y_shifts</code> <code>np.ndarray</code> <p><code>float [n_y_shifts]</code>. All possible shifts to test in y direction, probably made with <code>np.arange</code>.</p> required <code>x_shifts</code> <code>np.ndarray</code> <p><code>float [n_x_shifts]</code>. All possible shifts to test in x direction, probably made with <code>np.arange</code>.</p> required <code>ignore_shifts</code> <code>Optional[np.ndarray]</code> <p><code>float [n_ignore x 2]</code>. Contains yx shifts to not search over. If <code>None</code>, all permutations of <code>y_shifts</code>, <code>x_shifts</code> used.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>best_shift</code> - <code>float [shift_y, shift_x]</code>. Best shift found.</li> </ul> <code>float</code> <ul> <li><code>best_score</code> - <code>float</code>. Score of best shift.</li> </ul> <code>np.ndarray</code> <ul> <li><code>all_shifts</code> - <code>float [n_shifts x 2]</code>. yx shifts searched over.</li> </ul> <code>np.ndarray</code> <ul> <li><code>score</code> - <code>float [n_shifts]</code>. Score of all shifts.</li> </ul> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def get_best_shift_2d(yx_base_slices: List[np.ndarray], yx_transform_trees: List[KDTree], neighb_dist_thresh: float,\n                      y_shifts: np.ndarray, x_shifts: np.ndarray,\n                      ignore_shifts: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n\"\"\"\n\n    Args:\n        yx_base_slices: List of n_slices arrays indicating yx_base coordinates of spots in that slice.\n        yx_transform_trees: List of n_slices KDTrees, each built from the yx_transform coordinates of spots in\n            that slice.\n        neighb_dist_thresh: Basically the distance below which neighbours are a good match.\n            Typical = `2`.\n        y_shifts: `float [n_y_shifts]`.\n            All possible shifts to test in y direction, probably made with `np.arange`.\n        x_shifts: `float [n_x_shifts]`.\n            All possible shifts to test in x direction, probably made with `np.arange`.\n        ignore_shifts: `float [n_ignore x 2]`.\n            Contains yx shifts to not search over.\n            If `None`, all permutations of `y_shifts`, `x_shifts` used.\n\n    Returns:\n        - `best_shift` - `float [shift_y, shift_x]`.\n            Best shift found.\n        - `best_score` - `float`.\n            Score of best shift.\n        - `all_shifts` - `float [n_shifts x 2]`.\n            yx shifts searched over.\n        - `score` - `float [n_shifts]`.\n            Score of all shifts.\n    \"\"\"\n    all_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2)\n    if ignore_shifts is not None:\n        all_shifts = setdiff2d(all_shifts, ignore_shifts)\n    score = np.zeros(all_shifts.shape[0])\n    n_trees = len(yx_transform_trees)\n    dist_upper_bound = 3 * neighb_dist_thresh  # beyond this, score &lt; exp(-4.5) and quicker to use this.\n    for i in range(all_shifts.shape[0]):\n        for j in range(n_trees):\n            yx_shifted = yx_base_slices[j] + all_shifts[i]\n            distances = yx_transform_trees[j].query(yx_shifted, distance_upper_bound=dist_upper_bound)[0]\n            score[i] += shift_score(distances, neighb_dist_thresh)\n    best_shift_ind = score.argmax()\n    return all_shifts[best_shift_ind], score[best_shift_ind], all_shifts, score\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_best_shift_3d","title":"<code>get_best_shift_3d(yxz_base, yxz_transform_tree, neighb_dist_thresh, y_shifts, x_shifts, z_shifts, ignore_shifts=None)</code>","text":"<p>Finds the shift from those given that is best applied to <code>yx_base</code> to match <code>yx_transform</code>.</p> <p>Parameters:</p> Name Type Description Default <code>yxz_base</code> <code>np.ndarray</code> <p><code>float [n_spots_base x 3]</code>. Coordinates of spots on base image (yxz units must be same).</p> required <code>yxz_transform_tree</code> <code>KDTree</code> <p>KDTree built from coordinates of spots on transformed image (<code>float [n_spots_transform x 3]</code>, yxz units must be same).</p> required <code>neighb_dist_thresh</code> <code>float</code> <p>Basically the distance below which neighbours are a good match. Typical = <code>2</code>.</p> required <code>y_shifts</code> <code>np.ndarray</code> <p><code>float [n_y_shifts]</code>. All possible shifts to test in y direction, probably made with <code>np.arange</code>.</p> required <code>x_shifts</code> <code>np.ndarray</code> <p><code>float [n_x_shifts]</code>. All possible shifts to test in x direction, probably made with <code>np.arange</code>.</p> required <code>z_shifts</code> <code>np.ndarray</code> <p><code>float [n_z_shifts]</code>. All possible shifts to test in z direction, probably made with <code>np.arange</code>.</p> required <code>ignore_shifts</code> <code>Optional[np.ndarray]</code> <p><code>float [n_ignore x 3]</code>. Contains yxz shifts to not search over. If <code>None</code>, all permutations of <code>y_shifts</code>, <code>x_shifts</code>, <code>z_shifts</code> used.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <ul> <li><code>best_shift</code> - <code>float [shift_y, shift_x, shift_z]</code>. Best shift found.</li> </ul> <code>float</code> <ul> <li><code>best_score</code> - <code>float</code>. Score of best shift.</li> </ul> <code>np.ndarray</code> <ul> <li><code>all_shifts</code> - <code>float [n_shifts x 3]</code>. yxz shifts searched over.</li> </ul> <code>np.ndarray</code> <ul> <li><code>score</code> - <code>float [n_shifts]</code>. Score of all shifts.</li> </ul> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def get_best_shift_3d(yxz_base: np.ndarray, yxz_transform_tree: KDTree, neighb_dist_thresh: float, y_shifts: np.ndarray,\n                      x_shifts: np.ndarray, z_shifts: np.ndarray,\n                      ignore_shifts: Optional[np.ndarray] = None) -&gt; Tuple[np.ndarray, float, np.ndarray, np.ndarray]:\n\"\"\"\n    Finds the shift from those given that is best applied to `yx_base` to match `yx_transform`.\n\n    Args:\n        yxz_base: `float [n_spots_base x 3]`.\n            Coordinates of spots on base image (yxz units must be same).\n        yxz_transform_tree: KDTree built from coordinates of spots on transformed image\n            (`float [n_spots_transform x 3]`, yxz units must be same).\n        neighb_dist_thresh: Basically the distance below which neighbours are a good match.\n            Typical = `2`.\n        y_shifts: `float [n_y_shifts]`.\n            All possible shifts to test in y direction, probably made with `np.arange`.\n        x_shifts: `float [n_x_shifts]`.\n            All possible shifts to test in x direction, probably made with `np.arange`.\n        z_shifts: `float [n_z_shifts]`.\n            All possible shifts to test in z direction, probably made with `np.arange`.\n        ignore_shifts: `float [n_ignore x 3]`.\n            Contains yxz shifts to not search over.\n            If `None`, all permutations of `y_shifts`, `x_shifts`, `z_shifts` used.\n\n    Returns:\n        - `best_shift` - `float [shift_y, shift_x, shift_z]`.\n            Best shift found.\n        - `best_score` - `float`.\n            Score of best shift.\n        - `all_shifts` - `float [n_shifts x 3]`.\n            yxz shifts searched over.\n        - `score` - `float [n_shifts]`.\n            Score of all shifts.\n    \"\"\"\n    all_shifts = np.array(np.meshgrid(y_shifts, x_shifts, z_shifts)).T.reshape(-1, 3)\n    if ignore_shifts is not None:\n        all_shifts = setdiff2d(all_shifts, ignore_shifts)\n    score = np.zeros(all_shifts.shape[0])\n    dist_upper_bound = 3 * neighb_dist_thresh  # beyond this, score &lt; exp(-4.5) and quicker to use this.\n    for i in range(all_shifts.shape[0]):\n        yxz_shifted = yxz_base + all_shifts[i]\n        distances = yxz_transform_tree.query(yxz_shifted, distance_upper_bound=dist_upper_bound)[0]\n        score[i] = shift_score(distances, neighb_dist_thresh)\n    best_shift_ind = score.argmax()\n    return all_shifts[best_shift_ind], score[best_shift_ind], all_shifts, score\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_score_thresh","title":"<code>get_score_thresh(all_shifts, all_scores, best_shift, min_dist, max_dist, thresh_multiplier)</code>","text":"<p>Score thresh is the max of all scores from transforms between a <code>distance=min_dist</code> and <code>distance=max_dist</code> from the <code>best_shift</code>. I.e. we expect just for actual shift, there will be sharp gradient in score near it, so threshold is multiple of nearby score. If not the actual shift, then expect scores in this annulus will also be quite large.</p> <p>Parameters:</p> Name Type Description Default <code>all_shifts</code> <code>np.ndarray</code> <p><code>float [n_shifts x 2]</code>. yx shifts searched over.</p> required <code>all_scores</code> <code>np.ndarray</code> <p><code>float [n_shifts]</code>. <code>all_scores[s]</code> is the score corresponding to <code>all_shifts[s]</code>.</p> required <code>best_shift</code> <code>Union[np.ndarray, List]</code> <p><code>float [2]</code>. yx shift with the best score.</p> required <code>min_dist</code> <code>float</code> <p><code>score_thresh</code> computed from <code>all_shifts</code> a distance between <code>min_shift</code> and <code>max_shift</code> from <code>best_shifts</code>.</p> required <code>max_dist</code> <code>float</code> <p><code>score_thresh</code> computed from <code>all_shifts</code> a distance between <code>min_shift</code> and <code>max_shift</code> from <code>best_shifts</code>.</p> required <code>thresh_multiplier</code> <code>float</code> <p><code>score_thresh</code> is <code>thresh_multiplier</code> * mean of scores of shifts the correct distance from <code>best_shift</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>score_thresh - Threshold used to determine if <code>best_shift</code> found is legitimate.</p> <code>Optional[np.ndarray]</code> <p>shift_thresh - <code>float [2]</code> shift corresponding to <code>score_thresh</code>. Will be None if there were no shifts in the range set by <code>min_dist</code> and <code>max_dist</code>.</p> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def get_score_thresh(all_shifts: np.ndarray, all_scores: np.ndarray, best_shift: Union[np.ndarray, List], min_dist: float,\n                     max_dist: float, thresh_multiplier: float) -&gt; Tuple[float, Optional[np.ndarray]]:\n\"\"\"\n    Score thresh is the max of all scores from transforms between a `distance=min_dist` and `distance=max_dist`\n    from the `best_shift`.\n    I.e. we expect just for actual shift, there will be sharp gradient in score near it,\n    so threshold is multiple of nearby score.\n    If not the actual shift, then expect scores in this annulus will also be quite large.\n\n    Args:\n        all_shifts: `float [n_shifts x 2]`.\n            yx shifts searched over.\n        all_scores: `float [n_shifts]`.\n            `all_scores[s]` is the score corresponding to `all_shifts[s]`.\n        best_shift: `float [2]`.\n            yx shift with the best score.\n        min_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift`\n            from `best_shifts`.\n        max_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift`\n            from `best_shifts`.\n        thresh_multiplier: `score_thresh` is `thresh_multiplier` * mean of scores of shifts the correct distance\n            from `best_shift`.\n\n    Returns:\n        score_thresh - Threshold used to determine if `best_shift` found is legitimate.\n        shift_thresh - `float [2]`\n            shift corresponding to `score_thresh`. Will be None if there were no shifts\n            in the range set by `min_dist` and `max_dist`.\n    \"\"\"\n    dist_to_best = pairwise_distances(np.array(all_shifts), np.array(best_shift)[np.newaxis]).squeeze()\n    use = np.where(np.logical_and(dist_to_best &lt;= max_dist, dist_to_best &gt;= min_dist))[0]\n    if len(use) &gt; 0:\n        thresh_ind = use[np.argmax(all_scores[use])]\n        score_thresh = thresh_multiplier * all_scores[thresh_ind]\n        shift_thresh = all_shifts[thresh_ind]\n    else:\n        score_thresh = thresh_multiplier * np.median(all_scores)\n        shift_thresh = None\n    return score_thresh, shift_thresh\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.refined_shifts","title":"<code>refined_shifts(shifts, best_shift, refined_scale=0.5, extend_scale=2)</code>","text":"<p>If <code>shifts</code> is an array with mean spacing <code>step</code> then this builds array that covers from</p> <p><code>best_shift - extend_scale * step</code> to <code>best_shift + extend_scale * step</code> with a spacing of <code>step*refined_scale</code>.</p> <p>The new step, <code>step*refined_scale</code>, is forced to be an integer.</p> <p>If only one <code>shift</code> provided, doesn't do anything.</p> <p>Parameters:</p> Name Type Description Default <code>shifts</code> <code>np.ndarray</code> <p><code>float [n_shifts]</code>. Array probably produced using <code>np.arange</code>. It is expected to be in ascending order with constant step.</p> required <code>best_shift</code> <code>float</code> <p>Value in <code>shifts</code> to build new shifts around.</p> required <code>refined_scale</code> <code>float</code> <p>Scaling to apply to find new shift spacing.</p> <code>0.5</code> <code>extend_scale</code> <code>float</code> <p>By how many steps to build new shifts.</p> <code>2</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_new_shifts]</code>. Array covering from</p> <code>np.ndarray</code> <p><code>best_shift - extend_scale * step</code> to <code>best_shift + extend_scale * step</code> with a spacing of <code>step*refined_scale</code>.</p> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def refined_shifts(shifts: np.ndarray, best_shift: float, refined_scale: float = 0.5,\n                   extend_scale: float = 2) -&gt; np.ndarray:\n\"\"\"\n    If `shifts` is an array with mean spacing `step` then this builds array\n    that covers from\n\n    `best_shift - extend_scale * step` to `best_shift + extend_scale * step`\n    with a spacing of `step*refined_scale`.\n\n    The new step, `step*refined_scale`, is forced to be an integer.\n\n    If only one `shift` provided, doesn't do anything.\n\n    Args:\n        shifts: `float [n_shifts]`.\n            Array probably produced using `np.arange`. It is expected to be in ascending order with constant step.\n        best_shift: Value in `shifts` to build new shifts around.\n        refined_scale: Scaling to apply to find new shift spacing.\n        extend_scale: By how many steps to build new shifts.\n\n    Returns:\n        `float [n_new_shifts]`. Array covering from\n\n        `best_shift - extend_scale * step` to `best_shift + extend_scale * step` with a spacing of `step*refined_scale`.\n    \"\"\"\n    if np.size(shifts) == 1:\n        refined_shifts = shifts\n    else:\n        step = np.mean(np.ediff1d(shifts))\n        refined_step = np.ceil(refined_scale * step).astype(int)\n        refined_shifts = np.arange(best_shift - extend_scale * step,\n                                   best_shift + extend_scale * step + refined_step / 2, refined_step)\n    return refined_shifts\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.shift_score","title":"<code>shift_score(distances, thresh)</code>","text":"<p>Computes a score to quantify how good a shift is based on the distances between the neighbours found. the value of this score is approximately the number of close neighbours found.</p> <p>Parameters:</p> Name Type Description Default <code>distances</code> <code>np.ndarray</code> <p><code>float [n_neighbours]</code>. Distances between each pair of neighbours.</p> required <code>thresh</code> <code>float</code> <p>Basically the distance in pixels below which neighbours are a good match. Typical = <code>2</code>.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Score to quantify how good a shift is based on the distances between the neighbours found.</p> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def shift_score(distances: np.ndarray, thresh: float) -&gt; float:\n\"\"\"\n    Computes a score to quantify how good a shift is based on the distances between the neighbours found.\n    the value of this score is approximately the number of close neighbours found.\n\n    Args:\n        distances: `float [n_neighbours]`.\n            Distances between each pair of neighbours.\n        thresh: Basically the distance in pixels below which neighbours are a good match.\n            Typical = `2`.\n\n    Returns:\n        Score to quantify how good a shift is based on the distances between the neighbours found.\n    \"\"\"\n    return np.sum(np.exp(-distances ** 2 / (2 * thresh ** 2)))\n</code></pre>"},{"location":"code/stitch/shift/#coppafish.stitch.shift.update_shifts","title":"<code>update_shifts(search_shifts, prev_found_shifts)</code>","text":"<p>Returns a new array of <code>search_shifts</code> around the mean of <code>prev_found_shifts</code> if new array has fewer entries or if mean of <code>prev_found_shifts</code> is outside initial range of <code>search_shifts</code>. If more than one <code>prev_found_shifts</code> is outside the <code>search_shifts</code> in the same way i.e. too high or too low, <code>search_shifts</code> will be updated too.</p> <p>Parameters:</p> Name Type Description Default <code>search_shifts</code> <code>np.ndarray</code> <p><code>int [n_shifts]</code>. Indicates all shifts currently searched over.</p> required <code>prev_found_shifts</code> <code>np.ndarray</code> <p><code>int [n_shifts_found]</code>. Indicate shifts found on all previous runs of <code>compute_shift</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [n_new_shifts]</code>.</p> <code>np.ndarray</code> <p>New set of shifts around mean of previously found shifts.</p> <code>np.ndarray</code> <p>Will only return updated shifts if new array has fewer entries than before or mean of <code>prev_found_shifts</code></p> <code>np.ndarray</code> <p>is outside range of <code>search_shifts</code>.</p> Source code in <code>coppafish/stitch/shift.py</code> <pre><code>def update_shifts(search_shifts: np.ndarray, prev_found_shifts: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Returns a new array of `search_shifts` around the mean of `prev_found_shifts` if new array has fewer entries or if\n    mean of `prev_found_shifts` is outside initial range of `search_shifts`.\n    If more than one `prev_found_shifts` is outside the `search_shifts` in the same way i.e. too high or too low,\n    `search_shifts` will be updated too.\n\n    Args:\n        search_shifts: `int [n_shifts]`.\n            Indicates all shifts currently searched over.\n        prev_found_shifts: `int [n_shifts_found]`.\n            Indicate shifts found on all previous runs of `compute_shift`.\n\n    Returns:\n        `int [n_new_shifts]`.\n\n        New set of shifts around mean of previously found shifts.\n        Will only return updated shifts if new array has fewer entries than before or mean of `prev_found_shifts`\n        is outside range of `search_shifts`.\n    \"\"\"\n    n_shifts = len(search_shifts)\n    n_prev_shifts = len(prev_found_shifts)\n    if n_shifts &gt; 1 and n_prev_shifts &gt; 0:\n        step = np.mean(np.ediff1d(search_shifts))\n        mean_shift = np.mean(prev_found_shifts, dtype=int)\n        n_shifts_new = 2 * np.ceil((np.max(prev_found_shifts) - mean_shift) / step + 1).astype(int) + 1\n        if n_shifts_new &lt; n_shifts or mean_shift &lt;= search_shifts.min() or mean_shift &gt;= search_shifts.max():\n            # only update shifts if results in less to search over.\n            search_shifts = refined_shifts(search_shifts, mean_shift, 1, ((n_shifts_new - 1) / 2).astype(int))\n        if np.sum(prev_found_shifts &gt; search_shifts.max()) &gt; 1:\n            search_shifts = np.arange(search_shifts.min(), prev_found_shifts.max() + step, step)\n        if np.sum(prev_found_shifts &lt; search_shifts.min()) &gt; 1:\n            search_shifts = np.arange(prev_found_shifts.min(), search_shifts.max() + step, step)\n    return search_shifts\n</code></pre>"},{"location":"code/stitch/starting_shifts/","title":"Starting Shifts","text":""},{"location":"code/stitch/starting_shifts/#coppafish.stitch.starting_shifts.get_shifts_to_search","title":"<code>get_shifts_to_search(config, nbp_basic, nbp_debug=None)</code>","text":"<p>Using information in config dictionary to get range of shifts to search over when finding overlap between overlapping tiles in south and west directions.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>'stitch' section of .ini document.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>nbp_debug</code> <code>Optional[NotebookPage]</code> <p><code>stitch</code> notebook page where debugging information for stitching is kept. If provided, south_start_shift_search and west_start_shift_search variables added to page.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p><code>shifts[j][i]</code> contains the shifts to search over for overlap in the <code>j</code> direction for coordinate <code>i</code> where: <code>j = 'south', 'west'</code>. <code>i = 'y', 'x', 'z'</code>.</p> Source code in <code>coppafish/stitch/starting_shifts.py</code> <pre><code>def get_shifts_to_search(config: dict, nbp_basic: NotebookPage, nbp_debug: Optional[NotebookPage] = None) -&gt; dict:\n\"\"\"\n    Using information in config dictionary to get range of shifts to search over when finding overlap between\n    overlapping tiles in south and west directions.\n\n    Args:\n        config: 'stitch' section of .ini document.\n        nbp_basic: `basic_info` notebook page\n        nbp_debug: `stitch` notebook page where debugging information for stitching is kept.\n            If provided, south_start_shift_search and west_start_shift_search variables added to page.\n\n    Returns:\n        `shifts[j][i]` contains the shifts to search over for overlap in the `j` direction for coordinate `i` where:\n            `j = 'south', 'west'`.\n            `i = 'y', 'x', 'z'`.\n    \"\"\"\n    expected_shift_south = np.array([-(1 - config['expected_overlap']) * nbp_basic.tile_sz, 0, 0]).astype(int)\n    auto_shift_south_extent = np.array(config['auto_n_shifts']) * np.array(config['shift_step'])\n    expected_shift_west = expected_shift_south[[1, 0, 2]]\n    auto_shift_west_extent = auto_shift_south_extent[[1, 0, 2]]\n    if config['shift_south_min'] is None:\n        config['shift_south_min'] = list(expected_shift_south - auto_shift_south_extent)\n    if config['shift_south_max'] is None:\n        config['shift_south_max'] = list(expected_shift_south + auto_shift_south_extent)\n    if config['shift_west_min'] is None:\n        config['shift_west_min'] = list(expected_shift_west - auto_shift_west_extent)\n    if config['shift_west_max'] is None:\n        config['shift_west_max'] = list(expected_shift_west + auto_shift_west_extent)\n    directions = ['south', 'west']\n    coords = ['y', 'x', 'z']\n    shifts = {'south': {}, 'west': {}}\n    for j in directions:\n        if nbp_debug is not None:\n            nbp_debug.__setattr__(j + '_' + 'start_shift_search', np.zeros((3, 3), dtype=int))\n        for i in range(len(coords)):\n            shifts[j][coords[i]] = np.arange(config['shift_' + j + '_min'][i],\n                                             config['shift_' + j + '_max'][i] +\n                                             config['shift_step'][i] / 2, config['shift_step'][i]).astype(int)\n            if nbp_debug is not None:\n                nbp_debug.__getattribute__(j + '_' + 'start_shift_search')[i, :] = [config['shift_' + j + '_min'][i],\n                                                                                    config['shift_' + j + '_max'][i],\n                                                                                    config['shift_step'][i]]\n    if not nbp_basic.is_3d:\n        shifts['south']['z'] = np.array([0], dtype=int)\n        shifts['west']['z'] = np.array([0], dtype=int)\n        if nbp_debug is not None:\n            for j in directions:\n                nbp_debug.__getattribute__(j + '_' + 'start_shift_search')[2, :2] = 0\n    return shifts\n</code></pre>"},{"location":"code/stitch/tile_origin/","title":"Tile Origin","text":""},{"location":"code/stitch/tile_origin/#coppafish.stitch.tile_origin.get_tile_origin","title":"<code>get_tile_origin(v_pairs, v_shifts, h_pairs, h_shifts, n_tiles, home_tile)</code>","text":"<p>This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles.</p> <p>Parameters:</p> Name Type Description Default <code>v_pairs</code> <code>np.ndarray</code> <p><code>int [n_v_pairs x 2]</code>. <code>v_pairs[i,1]</code> is the tile index of the tile to the south of <code>v_pairs[i,0]</code>.</p> required <code>v_shifts</code> <code>np.ndarray</code> <p><code>int [n_v_pairs x 3]</code>. <code>v_shifts[i, :]</code> is the yxz shift from <code>v_pairs[i,0]</code> to <code>v_pairs[i,1]</code>. <code>v_shifts[:, 0]</code> should all be negative.</p> required <code>h_pairs</code> <code>np.ndarray</code> <p><code>int [n_h_pairs x 2]</code>. <code>h_pairs[i,1]</code> is the tile index of the tile to the west of <code>h_pairs[i,0]</code>.</p> required <code>h_shifts</code> <code>np.ndarray</code> <p><code>int [n_h_pairs x 3]</code>. <code>h_shifts[i, :]</code> is the yxz shift from <code>h_pairs[i,0]</code> to <code>h_pairs[i,1]</code>. <code>h_shifts[:, 1]</code> should all be negative.</p> required <code>n_tiles</code> <code>int</code> <p>Number of tiles (including those not used) in data set.</p> required <code>home_tile</code> <code>int</code> <p>Index of tile that is anchored to a fixed coordinate when finding tile origins. It should be the tile nearest to the centre.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_tiles x 3]</code>. yxz origin of each tile.</p> Source code in <code>coppafish/stitch/tile_origin.py</code> <pre><code>def get_tile_origin(v_pairs: np.ndarray, v_shifts: np.ndarray, h_pairs: np.ndarray, h_shifts: np.ndarray,\n                    n_tiles: int, home_tile: int) -&gt; np.ndarray:\n\"\"\"\n    This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles.\n\n    Args:\n        v_pairs: `int [n_v_pairs x 2]`.\n            `v_pairs[i,1]` is the tile index of the tile to the south of `v_pairs[i,0]`.\n        v_shifts: `int [n_v_pairs x 3]`.\n            `v_shifts[i, :]` is the yxz shift from `v_pairs[i,0]` to `v_pairs[i,1]`.\n            `v_shifts[:, 0]` should all be negative.\n        h_pairs: `int [n_h_pairs x 2]`.\n            `h_pairs[i,1]` is the tile index of the tile to the west of `h_pairs[i,0]`.\n        h_shifts: `int [n_h_pairs x 3]`.\n            `h_shifts[i, :]` is the yxz shift from `h_pairs[i,0]` to `h_pairs[i,1]`.\n            `h_shifts[:, 1]` should all be negative.\n        n_tiles: Number of tiles (including those not used) in data set.\n        home_tile: Index of tile that is anchored to a fixed coordinate when finding tile origins.\n            It should be the tile nearest to the centre.\n\n    Returns:\n        `float [n_tiles x 3]`. yxz origin of each tile.\n    \"\"\"\n\n    # solve a set of linear equations for each shift,\n    # This will be of the form M*x = c, where x and c are both of length n_tiles.\n    # The t'th row is the equation for tile t. c has columns for y, x and z coordinates\n    pairs = {'v': v_pairs, 'h': h_pairs}\n    shifts = {'v': v_shifts, 'h': h_shifts}\n    M = np.zeros((n_tiles+1, n_tiles))\n    c = np.zeros((n_tiles+1, 3))\n    for j in ['v', 'h']:\n        for i in range(pairs[j].shape[0]):\n            t1 = pairs[j][i, 0]\n            t2 = pairs[j][i, 1]\n            M[t1, t1] = M[t1, t1] + 1\n            M[t1, t2] = M[t1, t2] - 1\n            c[t1, :] = c[t1, :] + shifts[j][i, :]   # this is -shifts in MATLAB, but t1, t2 flipped in python\n            M[t2, t2] = M[t2, t2] + 1\n            M[t2, t1] = M[t2, t1] - 1\n            c[t2, :] = c[t2, :] - shifts[j][i, :]   # this is +shifts in MATLAB, but t1, t2 flipped in python\n\n    # now we want to anchor one of the tiles to a fixed coordinate. We do this\n    # for a home tile in the middle, because it is going to be connected; and we set\n    # its coordinate to a large value, so any non-connected ones can be detected.\n    # (BTW this is why spectral clustering works!!)\n    huge = 1e6\n    M[n_tiles, home_tile] = 1\n    c[n_tiles, :] = huge\n\n    tiny = 1e-4  # for regularization\n    tile_offset0 = np.linalg.lstsq(M + tiny * np.eye(n_tiles + 1, n_tiles), c, rcond=None)[0]\n    # find tiles that are connected to the home tile\n    aligned_ok = tile_offset0[:, 0] &gt; huge/2\n    tile_offset1 = np.ones((n_tiles, 3)) * np.nan\n    tile_offset1[aligned_ok] = tile_offset0[aligned_ok] - huge\n    tile_origin = tile_offset1 - np.nanmin(tile_offset1, axis=0)\n\n    return tile_origin\n</code></pre>"},{"location":"code/utils/base/","title":"Base","text":""},{"location":"code/utils/base/#coppafish.utils.base.round_any","title":"<code>round_any(x, base, round_type='round')</code>","text":"<p>Rounds <code>x</code> to the nearest multiple of <code>base</code> with the rounding done according to <code>round_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[float, np.ndarray]</code> <p>Number or array to round.</p> required <code>base</code> <code>float</code> <p>Rounds <code>x</code> to nearest integer multiple of value of <code>base</code>.</p> required <code>round_type</code> <code>str</code> <p>One of the following, indicating how to round <code>x</code> -</p> <ul> <li><code>'round'</code></li> <li><code>'ceil'</code></li> <li><code>'float'</code></li> </ul> <code>'round'</code> <p>Returns:</p> Type Description <code>Union[float, np.ndarray]</code> <p>Rounded version of <code>x</code>.</p> Example <pre><code>round_any(3, 5) = 5\nround_any(3, 5, 'floor') = 0\n</code></pre> Source code in <code>coppafish/utils/base.py</code> <pre><code>def round_any(x: Union[float, np.ndarray], base: float, round_type: str = 'round') -&gt; Union[float, np.ndarray]:\n\"\"\"\n    Rounds `x` to the nearest multiple of `base` with the rounding done according to `round_type`.\n\n    Args:\n        x: Number or array to round.\n        base: Rounds `x` to nearest integer multiple of value of `base`.\n        round_type: One of the following, indicating how to round `x` -\n\n            - `'round'`\n            - `'ceil'`\n            - `'float'`\n\n    Returns:\n        Rounded version of `x`.\n\n    Example:\n        ```\n        round_any(3, 5) = 5\n        round_any(3, 5, 'floor') = 0\n        ```\n    \"\"\"\n    if round_type == 'round':\n        return base * np.round(x / base)\n    elif round_type == 'ceil':\n        return base * np.ceil(x / base)\n    elif round_type == 'floor':\n        return base * np.floor(x / base)\n    else:\n        raise ValueError(f\"round_type specified was {round_type} but it should be one of the following:\\n\"\n                         f\"round, ceil, floor\")\n</code></pre>"},{"location":"code/utils/base/#coppafish.utils.base.setdiff2d","title":"<code>setdiff2d(array1, array2)</code>","text":"<p>Finds all elements in <code>array1</code> that are not in <code>array2</code>. Returned array will only contain unique elements E.g.</p> <p>If <code>array1</code> has <code>[4,0]</code> twice, <code>array2</code> has <code>[4,0]</code> once, returned array will not have <code>[4,0]</code>.</p> <p>If <code>array1</code> has <code>[4,0]</code> twice, <code>array2</code> does not have <code>[4,0]</code>, returned array will have <code>[4,0]</code> once.</p> <p>Parameters:</p> Name Type Description Default <code>array1</code> <code>np.ndarray</code> <p><code>float [n_elements1 x element_dim]</code>.</p> required <code>array2</code> <code>np.ndarray</code> <p><code>float [n_elements2 x element_dim]</code>.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_elements_diff x element_dim]</code>.</p> Source code in <code>coppafish/utils/base.py</code> <pre><code>def setdiff2d(array1: np.ndarray, array2: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Finds all elements in `array1` that are not in `array2`.\n    Returned array will only contain unique elements E.g.\n\n    If `array1` has `[4,0]` twice, `array2` has `[4,0]` once, returned array will not have `[4,0]`.\n\n    If `array1` has `[4,0]` twice, `array2` does not have `[4,0]`, returned array will have `[4,0]` once.\n\n    Args:\n        array1: `float [n_elements1 x element_dim]`.\n        array2: `float [n_elements2 x element_dim]`.\n\n    Returns:\n        `float [n_elements_diff x element_dim]`.\n    \"\"\"\n    set1 = set([tuple(x) for x in array1])\n    set2 = set([tuple(x) for x in array2])\n    return np.array(list(set1-set2))\n</code></pre>"},{"location":"code/utils/errors/","title":"Errors","text":""},{"location":"code/utils/errors/#coppafish.utils.errors.ColorInvalidWarning","title":"<code>ColorInvalidWarning</code>","text":"Source code in <code>coppafish/utils/errors.py</code> <pre><code>class ColorInvalidWarning:\n    def __init__(self, colors: np.ndarray, nbp_basic: NotebookPage, invalid_value: float,\n                 round_no: Optional[int] = None, channel_no: Optional[int] = None, code_no: Optional[int] = None):\n\"\"\"\n        Warning raised because `spot_colors` contains a `invalid_value` where it should not.\n\n        Args:\n            colors: `int or float [n_codes x n_rounds x n_channels]`\n                `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`.\n                This is likely to be `spot_colors` if `int` or `bled_codes` if `float`.\n            nbp_basic: basic_info NotebookPage\n            invalid_value: This is the value that colors should only be in rounds/channels not used.\n                Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer.\n            round_no: round to flag error for.\n            channel_no: channel to flag error for.\n            code_no: Spot or gene index to flag error for.\n        \"\"\"\n        n_spots, n_rounds, n_channels = colors.shape\n        if round_no is not None and code_no is None:\n            self.message = f\"colors contains a value other than invalid_value={invalid_value} in round {round_no}\\n\" \\\n                           f\"which is not in use_rounds = {nbp_basic.use_rounds}.\"\n        elif channel_no is not None and code_no is None:\n            self.message = f\"colors contains a value other than invalid_value={invalid_value} in channel {channel_no}\\n\" \\\n                           f\"which is not in use_channels = {nbp_basic.use_channels}.\"\n        elif round_no is not None and channel_no is not None and code_no is not None:\n            self.message = f\"colors contains a invalid_value={invalid_value} for code {code_no}, round {round_no}, \" \\\n                           f\"channel {channel_no}.\\n\" \\\n                           f\"There should be no invalid_values in this round and channel.\"\n        else:\n            self.message = f\"colors has n_rounds = {n_rounds} and n_channels = {n_channels}.\\n\" \\\n                           f\"This is neither matches the total_rounds = {nbp_basic.n_rounds} and \" \\\n                           f\"total_channels = {nbp_basic.n_channels}\\n\" \\\n                           f\"nor the number of use_rounds = {len(nbp_basic.use_rounds)} and use_channels = \" \\\n                           f\"{len(nbp_basic.use_channels)}\"\n        warnings.warn(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.ColorInvalidWarning.__init__","title":"<code>__init__(colors, nbp_basic, invalid_value, round_no=None, channel_no=None, code_no=None)</code>","text":"<p>Warning raised because <code>spot_colors</code> contains a <code>invalid_value</code> where it should not.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>np.ndarray</code> <p><code>int or float [n_codes x n_rounds x n_channels]</code> <code>colors[s, r, c]</code> is the color for code <code>s</code> in round <code>r</code>, channel <code>c</code>. This is likely to be <code>spot_colors</code> if <code>int</code> or <code>bled_codes</code> if <code>float</code>.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p>basic_info NotebookPage</p> required <code>invalid_value</code> <code>float</code> <p>This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer.</p> required <code>round_no</code> <code>Optional[int]</code> <p>round to flag error for.</p> <code>None</code> <code>channel_no</code> <code>Optional[int]</code> <p>channel to flag error for.</p> <code>None</code> <code>code_no</code> <code>Optional[int]</code> <p>Spot or gene index to flag error for.</p> <code>None</code> Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, colors: np.ndarray, nbp_basic: NotebookPage, invalid_value: float,\n             round_no: Optional[int] = None, channel_no: Optional[int] = None, code_no: Optional[int] = None):\n\"\"\"\n    Warning raised because `spot_colors` contains a `invalid_value` where it should not.\n\n    Args:\n        colors: `int or float [n_codes x n_rounds x n_channels]`\n            `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`.\n            This is likely to be `spot_colors` if `int` or `bled_codes` if `float`.\n        nbp_basic: basic_info NotebookPage\n        invalid_value: This is the value that colors should only be in rounds/channels not used.\n            Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer.\n        round_no: round to flag error for.\n        channel_no: channel to flag error for.\n        code_no: Spot or gene index to flag error for.\n    \"\"\"\n    n_spots, n_rounds, n_channels = colors.shape\n    if round_no is not None and code_no is None:\n        self.message = f\"colors contains a value other than invalid_value={invalid_value} in round {round_no}\\n\" \\\n                       f\"which is not in use_rounds = {nbp_basic.use_rounds}.\"\n    elif channel_no is not None and code_no is None:\n        self.message = f\"colors contains a value other than invalid_value={invalid_value} in channel {channel_no}\\n\" \\\n                       f\"which is not in use_channels = {nbp_basic.use_channels}.\"\n    elif round_no is not None and channel_no is not None and code_no is not None:\n        self.message = f\"colors contains a invalid_value={invalid_value} for code {code_no}, round {round_no}, \" \\\n                       f\"channel {channel_no}.\\n\" \\\n                       f\"There should be no invalid_values in this round and channel.\"\n    else:\n        self.message = f\"colors has n_rounds = {n_rounds} and n_channels = {n_channels}.\\n\" \\\n                       f\"This is neither matches the total_rounds = {nbp_basic.n_rounds} and \" \\\n                       f\"total_channels = {nbp_basic.n_channels}\\n\" \\\n                       f\"nor the number of use_rounds = {len(nbp_basic.use_rounds)} and use_channels = \" \\\n                       f\"{len(nbp_basic.use_channels)}\"\n    warnings.warn(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.EmptyListError","title":"<code>EmptyListError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>class EmptyListError(Exception):\n    def __init__(self, var_name: str):\n\"\"\"\n        Error raised because the variable indicated by `var_name` contains no data.\n\n        Args:\n            var_name: Name of list or numpy array\n        \"\"\"\n        self.message = f\"\\n{var_name} contains no data\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.EmptyListError.__init__","title":"<code>__init__(var_name)</code>","text":"<p>Error raised because the variable indicated by <code>var_name</code> contains no data.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of list or numpy array</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, var_name: str):\n\"\"\"\n    Error raised because the variable indicated by `var_name` contains no data.\n\n    Args:\n        var_name: Name of list or numpy array\n    \"\"\"\n    self.message = f\"\\n{var_name} contains no data\"\n    super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.NoFileError","title":"<code>NoFileError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>class NoFileError(Exception):\n    def __init__(self, file_path: str):\n\"\"\"\n        Error raised because `file_path` does not exist.\n\n        Args:\n            file_path: Path to file of interest.\n        \"\"\"\n        self.message = f\"\\nNo file with the following path:\\n{file_path}\\nexists\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.NoFileError.__init__","title":"<code>__init__(file_path)</code>","text":"<p>Error raised because <code>file_path</code> does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to file of interest.</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, file_path: str):\n\"\"\"\n    Error raised because `file_path` does not exist.\n\n    Args:\n        file_path: Path to file of interest.\n    \"\"\"\n    self.message = f\"\\nNo file with the following path:\\n{file_path}\\nexists\"\n    super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.OutOfBoundsError","title":"<code>OutOfBoundsError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>class OutOfBoundsError(Exception):\n    def __init__(self, var_name: str, oob_val: float, min_allowed: float, max_allowed: float):\n\"\"\"\n        Error raised because `oob_val` is outside expected range between\n        `min_allowed` and `max_allowed` inclusive.\n\n        Args:\n            var_name: Name of variable testing.\n            oob_val: Value in array that is not in expected range.\n            min_allowed: Smallest allowed value i.e. `&gt;= min_allowed`.\n            max_allowed: Largest allowed value i.e. `&lt;= max_allowed`.\n        \"\"\"\n        self.message = f\"\\n{var_name} contains the value {oob_val}.\" \\\n                       f\"\\nThis is outside the expected inclusive range between {min_allowed} and {max_allowed}\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.OutOfBoundsError.__init__","title":"<code>__init__(var_name, oob_val, min_allowed, max_allowed)</code>","text":"<p>Error raised because <code>oob_val</code> is outside expected range between <code>min_allowed</code> and <code>max_allowed</code> inclusive.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of variable testing.</p> required <code>oob_val</code> <code>float</code> <p>Value in array that is not in expected range.</p> required <code>min_allowed</code> <code>float</code> <p>Smallest allowed value i.e. <code>&gt;= min_allowed</code>.</p> required <code>max_allowed</code> <code>float</code> <p>Largest allowed value i.e. <code>&lt;= max_allowed</code>.</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, var_name: str, oob_val: float, min_allowed: float, max_allowed: float):\n\"\"\"\n    Error raised because `oob_val` is outside expected range between\n    `min_allowed` and `max_allowed` inclusive.\n\n    Args:\n        var_name: Name of variable testing.\n        oob_val: Value in array that is not in expected range.\n        min_allowed: Smallest allowed value i.e. `&gt;= min_allowed`.\n        max_allowed: Largest allowed value i.e. `&lt;= max_allowed`.\n    \"\"\"\n    self.message = f\"\\n{var_name} contains the value {oob_val}.\" \\\n                   f\"\\nThis is outside the expected inclusive range between {min_allowed} and {max_allowed}\"\n    super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.ShapeError","title":"<code>ShapeError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>class ShapeError(Exception):\n    def __init__(self, var_name: str, var_shape: tuple, expected_shape: tuple):\n\"\"\"\n        Error raised because variable indicated by `var_name` has wrong shape.\n\n        Args:\n            var_name: Name of numpy array.\n            var_shape: Shape of numpy array.\n            expected_shape: Expected shape of numpy array.\n        \"\"\"\n        self.message = f\"\\nShape of {var_name} is {var_shape} but should be {expected_shape}\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.ShapeError.__init__","title":"<code>__init__(var_name, var_shape, expected_shape)</code>","text":"<p>Error raised because variable indicated by <code>var_name</code> has wrong shape.</p> <p>Parameters:</p> Name Type Description Default <code>var_name</code> <code>str</code> <p>Name of numpy array.</p> required <code>var_shape</code> <code>tuple</code> <p>Shape of numpy array.</p> required <code>expected_shape</code> <code>tuple</code> <p>Expected shape of numpy array.</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, var_name: str, var_shape: tuple, expected_shape: tuple):\n\"\"\"\n    Error raised because variable indicated by `var_name` has wrong shape.\n\n    Args:\n        var_name: Name of numpy array.\n        var_shape: Shape of numpy array.\n        expected_shape: Expected shape of numpy array.\n    \"\"\"\n    self.message = f\"\\nShape of {var_name} is {var_shape} but should be {expected_shape}\"\n    super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.TiffError","title":"<code>TiffError</code>","text":"<p>             Bases: <code>Exception</code></p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>class TiffError(Exception):\n    def __init__(self, scale_tiff: float, scale_nbp: float, shift_tiff: int, shift_nbp: int):\n\"\"\"\n        Error raised because parameters used to produce tiff files are different to those in the current notebook.\n\n        Args:\n            scale_tiff: Scale factor applied to tiff. Found from tiff description.\n            scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`.\n            shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description.\n            shift_nbp: Shift applied to tiff to ensure pixel values positive.\n                Found from `nb.basic_info.tile_pixel_value_shift`.\n        \"\"\"\n        self.message = f\"\\nThere are differences between the parameters used to make the tiffs and the parameters \" \\\n                       f\"in the Notebook:\"\n        if scale_tiff != scale_nbp:\n            self.message = self.message + f\"\\nScale used to make tiff was {scale_tiff}.\" \\\n                                          f\"\\nCurrent scale in extract_params notebook page is {scale_nbp}.\"\n        if shift_tiff != shift_nbp:\n            self.message = self.message + f\"\\nShift used to make tiff was {shift_tiff}.\" \\\n                                          f\"\\nCurrent tile_pixel_value_shift in basic_info notebook page is \" \\\n                                          f\"{shift_nbp}.\"\n        super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.TiffError.__init__","title":"<code>__init__(scale_tiff, scale_nbp, shift_tiff, shift_nbp)</code>","text":"<p>Error raised because parameters used to produce tiff files are different to those in the current notebook.</p> <p>Parameters:</p> Name Type Description Default <code>scale_tiff</code> <code>float</code> <p>Scale factor applied to tiff. Found from tiff description.</p> required <code>scale_nbp</code> <code>float</code> <p>Scale factor applied to tiff. Found from <code>nb.extract_debug.scale</code>.</p> required <code>shift_tiff</code> <code>int</code> <p>Shift applied to tiff to ensure pixel values positive. Found from tiff description.</p> required <code>shift_nbp</code> <code>int</code> <p>Shift applied to tiff to ensure pixel values positive. Found from <code>nb.basic_info.tile_pixel_value_shift</code>.</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def __init__(self, scale_tiff: float, scale_nbp: float, shift_tiff: int, shift_nbp: int):\n\"\"\"\n    Error raised because parameters used to produce tiff files are different to those in the current notebook.\n\n    Args:\n        scale_tiff: Scale factor applied to tiff. Found from tiff description.\n        scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`.\n        shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description.\n        shift_nbp: Shift applied to tiff to ensure pixel values positive.\n            Found from `nb.basic_info.tile_pixel_value_shift`.\n    \"\"\"\n    self.message = f\"\\nThere are differences between the parameters used to make the tiffs and the parameters \" \\\n                   f\"in the Notebook:\"\n    if scale_tiff != scale_nbp:\n        self.message = self.message + f\"\\nScale used to make tiff was {scale_tiff}.\" \\\n                                      f\"\\nCurrent scale in extract_params notebook page is {scale_nbp}.\"\n    if shift_tiff != shift_nbp:\n        self.message = self.message + f\"\\nShift used to make tiff was {shift_tiff}.\" \\\n                                      f\"\\nCurrent tile_pixel_value_shift in basic_info notebook page is \" \\\n                                      f\"{shift_nbp}.\"\n    super().__init__(self.message)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.check_color_nan","title":"<code>check_color_nan(colors, nbp_basic)</code>","text":"<p><code>colors</code> should only contain the <code>invalid_value</code> in rounds/channels not in use_rounds/channels. This raises an error if this is not the case or if a round/channel not in use_rounds/channels contains a value other than <code>invalid_value</code>. <code>invalid_value = -nbp_basic.tile_pixel_value_shift - 1</code> if colors is integer i.e. the non-normalised colors, usually spot_colors. <code>invalid_value = -np.nan</code> if colors is float i.e. the normalised colors or most likely the bled_codes.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>np.ndarray</code> <p><code>int or float [n_codes x n_rounds x n_channels]</code> <code>colors[s, r, c]</code> is the color for code <code>s</code> in round <code>r</code>, channel <code>c</code>. This is likely to be <code>spot_colors</code> if <code>int</code> or <code>bled_codes</code> if <code>float</code>.</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p>basic_info NotebookPage</p> required Source code in <code>coppafish/utils/errors.py</code> <pre><code>def check_color_nan(colors: np.ndarray, nbp_basic: NotebookPage):\n\"\"\"\n    `colors` should only contain the `invalid_value` in rounds/channels not in use_rounds/channels.\n    This raises an error if this is not the case or if a round/channel not in use_rounds/channels\n    contains a value other than `invalid_value`.\n    `invalid_value = -nbp_basic.tile_pixel_value_shift - 1` if colors is integer i.e. the non-normalised colors,\n    usually spot_colors.\n    `invalid_value = -np.nan` if colors is float i.e. the normalised colors or most likely the bled_codes.\n\n\n    Args:\n        colors: `int or float [n_codes x n_rounds x n_channels]`\n            `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`.\n            This is likely to be `spot_colors` if `int` or `bled_codes` if `float`.\n        nbp_basic: basic_info NotebookPage\n    \"\"\"\n    diff_to_int = np.round(colors).astype(int) - colors\n    if np.abs(diff_to_int).max() == 0:\n        # if not normalised, then invalid_value is an integer value that is impossible for a spot_color to be\n        invalid_value = -nbp_basic.tile_pixel_value_shift\n    else:\n        # if is normalised then expect nan value to be normal np.nan.\n        invalid_value = np.nan\n\n    # decide which rounds/channels should be ignored i.e. only contain invalid_value.\n    n_spots, n_rounds, n_channels = colors.shape\n    if n_rounds == nbp_basic.n_rounds and n_channels == nbp_basic.n_channels:\n        use_rounds = nbp_basic.use_rounds\n        use_channels = nbp_basic.use_channels\n    elif n_rounds == len(nbp_basic.use_rounds) and n_channels == len(nbp_basic.use_channels):\n        use_rounds = np.arange(n_rounds)\n        use_channels = np.arange(n_channels)\n    else:\n        ColorInvalidWarning(colors, nbp_basic, invalid_value)\n\n    ignore_rounds = np.setdiff1d(np.arange(n_rounds), use_rounds)\n    for r in ignore_rounds:\n        unique_vals = np.unique(colors[:, r, :])\n        for val in unique_vals:\n            if not invalid_value in unique_vals:\n                ColorInvalidWarning(colors, nbp_basic, invalid_value, round_no=r)\n            if not np.array_equal(val, invalid_value, equal_nan=True):\n                ColorInvalidWarning(colors, nbp_basic, invalid_value, round_no=r)\n\n    ignore_channels = np.setdiff1d(np.arange(n_channels), use_channels)\n    for c in ignore_channels:\n        unique_vals = np.unique(colors[:, :, c])\n        for val in unique_vals:\n            if not invalid_value in unique_vals:\n                ColorInvalidWarning(colors, nbp_basic, invalid_value, channel_no=c)\n            if not np.array_equal(val, invalid_value, equal_nan=True):\n                ColorInvalidWarning(colors, nbp_basic, invalid_value, channel_no=c)\n\n    # see if any spots contain invalid_values.\n    use_colors = colors[np.ix_(np.arange(n_spots), use_rounds, use_channels)]\n    if np.array_equal(invalid_value, np.nan, equal_nan=True):\n        nan_codes = np.where(np.isnan(use_colors))\n    else:\n        nan_codes = np.where(use_colors == invalid_value)\n    n_nan_spots = nan_codes[0].size\n    if n_nan_spots &gt; 0:\n        s = nan_codes[0][0]\n        # round, channel number in spot_colors different from in use_spot_colors.\n        r = np.arange(n_rounds)[nan_codes[1][0]]\n        c = np.arange(n_channels)[nan_codes[2][0]]\n        ColorInvalidWarning(colors, nbp_basic, invalid_value, round_no=r, channel_no=c, code_no=s)\n</code></pre>"},{"location":"code/utils/errors/#coppafish.utils.errors.check_shape","title":"<code>check_shape(array, expected_shape)</code>","text":"<p>Checks to see if <code>array</code> has the shape indicated by <code>expected_shape</code>.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>np.ndarray</code> <p>Array to check the shape of.</p> required <code>expected_shape</code> <code>Union[list, tuple, np.ndarray]</code> <p><code>int [n_array_dims]</code>. Expected shape of array.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if shape of array is correct.</p> Source code in <code>coppafish/utils/errors.py</code> <pre><code>def check_shape(array: np.ndarray, expected_shape: Union[list, tuple, np.ndarray]) -&gt; bool:\n\"\"\"\n    Checks to see if `array` has the shape indicated by `expected_shape`.\n\n    Args:\n        array: Array to check the shape of.\n        expected_shape: `int [n_array_dims]`.\n            Expected shape of array.\n\n    Returns:\n        `True` if shape of array is correct.\n    \"\"\"\n    correct_shape = array.ndim == len(expected_shape)  # first check if number of dimensions are correct\n    if correct_shape:\n        correct_shape = np.abs(np.array(array.shape) - np.array(expected_shape)).max() == 0\n    return correct_shape\n</code></pre>"},{"location":"code/utils/matlab/","title":"Matlab","text":""},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_array","title":"<code>load_array(file_name, var_names)</code>","text":"<p>This is used to load info from v7.3 or later matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays.</p> <p>If <code>var_names</code> is <code>str</code>, one value is returned, otherwise <code>tuple</code> of all values requested is returned.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path of MATLAB file.</p> required <code>var_names</code> <code>Union[str, List[str]]</code> <p><code>str [n_vars]</code>. Names of variables desired.</p> required <p>Returns:</p> Type Description <code>Union[tuple, np.ndarray]</code> <p><code>Tuple</code> of <code>n_vars</code> numpy arrays.</p> Source code in <code>coppafish/utils/matlab.py</code> <pre><code>def load_array(file_name: str, var_names: Union[str, List[str]]) -&gt; Union[tuple, np.ndarray]:\n\"\"\"\n    This is used to load info from v7.3 or later matlab files.\n    It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays.\n\n    If `var_names` is `str`, one value is returned, otherwise `tuple` of all values requested is returned.\n\n    Args:\n        file_name: Path of MATLAB file.\n        var_names: `str [n_vars]`.\n            Names of variables desired.\n\n    Returns:\n        `Tuple` of `n_vars` numpy arrays.\n    \"\"\"\n    f = h5py.File(file_name)\n    if not isinstance(var_names, list):\n        output = np.array(f[var_names]).transpose()\n    else:\n        output = []\n        for var_name in var_names:\n            output.append(np.array(f[var_name]).transpose())\n        output = tuple(output)\n    return output\n</code></pre>"},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_cell","title":"<code>load_cell(file_name, var_name)</code>","text":"<p>If cell is <code>M x N</code>, will return list of length <code>M</code> where each entry is another list of length <code>N</code> and each element of this list is a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path of MATLAB file.</p> required <code>var_name</code> <code>str</code> <p>Names of variable in MATLAB file.</p> required <p>Returns:</p> Type Description <code>list</code> <p>MATLAB cell <code>var_name</code> as a list of numpy arrays.</p> Source code in <code>coppafish/utils/matlab.py</code> <pre><code>def load_cell(file_name: str, var_name: str) -&gt; list:\n\"\"\"\n    If cell is `M x N`, will return list of length `M` where each entry is another list of length `N`\n    and each element of this list is a numpy array.\n\n    Args:\n        file_name: Path of MATLAB file.\n        var_name: Names of variable in MATLAB file.\n\n    Returns:\n        MATLAB cell `var_name` as a list of numpy arrays.\n    \"\"\"\n    # MAYBE CHANGE THIS TO OBJECT NUMPY ARRAY\n    f = h5py.File(file_name)\n    data = []\n    for column in np.transpose(f[var_name]):\n        row_data = []\n        for row_number in range(len(column)):\n            row_data.append(np.array(f[column[row_number]]).transpose())\n        data.append(row_data)\n    return data\n</code></pre>"},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_v_less_7_3","title":"<code>load_v_less_7_3(file_name, var_names)</code>","text":"<p>This is used to load info from earlier than v7.3  matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays.</p> <p>If <code>var_names</code> is <code>str</code>, one value is returned, otherwise tuple of all values requested is returned.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>Path of MATLAB file.</p> required <code>var_names</code> <code>Union[str, List[str]]</code> <p><code>str [n_vars]</code>. Names of variables desired.</p> required <p>Returns:</p> Type Description <code>Union[tuple, np.ndarray]</code> <p><code>Tuple</code> of <code>n_vars</code> numpy arrays.</p> Source code in <code>coppafish/utils/matlab.py</code> <pre><code>def load_v_less_7_3(file_name: str, var_names: Union[str, List[str]]) -&gt; Union[tuple, np.ndarray]:\n\"\"\"\n    This is used to load info from earlier than v7.3  matlab files.\n    It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays.\n\n    If `var_names` is `str`, one value is returned, otherwise tuple of all values requested is returned.\n\n    Args:\n        file_name: Path of MATLAB file.\n        var_names: `str [n_vars]`.\n            Names of variables desired.\n\n    Returns:\n        `Tuple` of `n_vars` numpy arrays.\n    \"\"\"\n    f = io.loadmat(file_name)\n    if not isinstance(var_names, list):\n        output = f[var_names]\n    else:\n        output = []\n        for var_name in var_names:\n            output.append(f[var_name])\n        output = tuple(output)\n    return output\n</code></pre>"},{"location":"code/utils/matlab/#coppafish.utils.matlab.save_nb_results","title":"<code>save_nb_results(nb, file_name, score_thresh_ref_spots=0.15, score_thresh_omp=0.15)</code>","text":"<p>Saves important information in notebook as a .mat file so can load in MATLAB and plot using python_testing/iss_object_from_python.m scripts</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing at least <code>call_spots</code> page.</p> required <code>file_name</code> <code>str</code> <p>Path to save matlab version of Notebook.</p> required <code>score_thresh_ref_spots</code> <code>float</code> <p>Only <code>ref_round</code> spots with score exceeding this will be saved.</p> <code>0.15</code> <code>score_thresh_omp</code> <code>float</code> <p>Only <code>omp</code> spots with score exceeding this will be saved.</p> <code>0.15</code> Source code in <code>coppafish/utils/matlab.py</code> <pre><code>def save_nb_results(nb: Notebook, file_name: str, score_thresh_ref_spots: float = 0.15, score_thresh_omp: float = 0.15):\n\"\"\"\n    Saves important information in notebook as a .mat file so can load in MATLAB and plot\n    using python_testing/iss_object_from_python.m scripts\n\n    Args:\n        nb: Notebook containing at least `call_spots` page.\n        file_name: Path to save matlab version of Notebook.\n        score_thresh_ref_spots: Only `ref_round` spots with score exceeding this will be saved.\n        score_thresh_omp: Only `omp` spots with score exceeding this will be saved.\n\n    \"\"\"\n    mdic = {}\n    if nb.has_page('file_names'):\n        mdic['tile_file_names'] = nb.file_names.tile\n    if nb.has_page('stitch'):\n        mdic['tile_origin'] = nb.stitch.tile_origin\n    if nb.has_page('register'):\n        mdic['transform'] = nb.register.transform\n\n    if nb.has_page('call_spots'):\n        call_spots_dict = nb.call_spots.to_serial_dict()\n        call_spots_dict = {k: v for k, v in call_spots_dict.items() if not '___' in k}\n        del call_spots_dict['PAGEINFO']\n        mdic.update(call_spots_dict)\n\n    if nb.has_page('ref_spots'):\n        ref_spots_dict = nb.ref_spots.to_serial_dict()\n        # Give 'ref_spots' prefix to key names as same variables in omp page.\n        ref_spots_dict = {ref_spots_dict['PAGEINFO'] + '_' + k: v for k, v in ref_spots_dict.items() if not '___' in k}\n        del ref_spots_dict['ref_spots_PAGEINFO']\n        ref_spots_dict = update_dict(nb.ref_spots, nb, ref_spots_dict, score_thresh_ref_spots)\n        mdic.update(ref_spots_dict)\n\n    if nb.has_page('omp'):\n        omp_dict = nb.omp.to_serial_dict()\n        omp_dict = {omp_dict['PAGEINFO'] + '_' + k:v for k,v in omp_dict.items() if not '___' in k}\n        del omp_dict['omp_PAGEINFO']\n        omp_dict = update_dict(nb.omp, nb, omp_dict, score_thresh_omp)\n        mdic.update(omp_dict)\n    for k, v in mdic.items():\n        if v is None:\n            mdic[k] = []  # Can't save None values\n    io.savemat(file_name, mdic)\n</code></pre>"},{"location":"code/utils/matlab/#coppafish.utils.matlab.update_dict","title":"<code>update_dict(nbp, nb, spots_info_dict, score_thresh)</code>","text":"<p>Used in <code>save_nb_results</code> to reduced amount of spots saved. Only spots with <code>score &gt; score_thresh</code> kept.</p> <p>Parameters:</p> Name Type Description Default <code>nbp</code> <code>NotebookPage</code> <p>Either <code>omp</code> or <code>ref_spots</code> NotebookPage.</p> required <code>nb</code> <code>Notebook</code> <p>Full notebook</p> required <code>spots_info_dict</code> <code>dict</code> <p>Dictionary produced in <code>save_nb_results</code> containing information to save about each spot.</p> required <code>score_thresh</code> <code>float</code> <p>All spots with a score above this threshold will be returned.</p> required <p>Returns:</p> Type Description <code>dict</code> <p><code>spots_info_dict</code> is returned, just including spots for which <code>score &gt; score_thresh</code>.</p> Source code in <code>coppafish/utils/matlab.py</code> <pre><code>def update_dict(nbp: NotebookPage, nb: Notebook, spots_info_dict: dict, score_thresh: float) -&gt; dict:\n\"\"\"\n    Used in `save_nb_results` to reduced amount of spots saved. Only spots with `score &gt; score_thresh` kept.\n\n    Args:\n        nbp: Either `omp` or `ref_spots` NotebookPage.\n        nb: Full notebook\n        spots_info_dict: Dictionary produced in `save_nb_results` containing information to save about each spot.\n        score_thresh: All spots with a score above this threshold will be returned.\n\n    Returns:\n        `spots_info_dict` is returned, just including spots for which `score &gt; score_thresh`.\n\n    \"\"\"\n    pf = nbp.name + '_'\n    if pf != 'omp_' and pf != 'ref_spots_':\n        raise ValueError(\"Wrong page given, should be 'omp' or 'ref_spots'\")\n    nbp.finalized = False\n    score_thresh_old = nbp.score_thresh\n    del nbp.score_thresh\n    nbp.score_thresh = score_thresh\n    if pf == 'omp_':\n        keep = quality_threshold(nb, 'omp')\n    else:\n        keep = quality_threshold(nb, 'ref')\n    del nbp.score_thresh\n    nbp.score_thresh = score_thresh_old\n    nbp.finalized = True\n    for var in ['local_yxz', 'tile', 'colors', 'intensity', 'background_coef', 'gene_no']:\n        spots_info_dict[pf + var] = spots_info_dict[pf + var][keep]\n    if pf == 'ref_spots_':\n        del spots_info_dict[pf + 'background_coef']\n        for var in ['isolated', 'score', 'score_diff']:\n            spots_info_dict[pf + var] = spots_info_dict[pf + var][keep]\n    if pf == 'omp_':\n        for var in ['shape_spot_local_yxz', 'shape_spot_gene_no', 'spot_shape_float']:\n            del spots_info_dict[pf + var]\n        for var in ['coef', 'n_neighbours_pos', 'n_neighbours_neg']:\n            spots_info_dict[pf + var] = spots_info_dict[pf + var][keep]\n    return spots_info_dict\n</code></pre>"},{"location":"code/utils/morphology/","title":"Morphology","text":""},{"location":"code/utils/morphology/#base","title":"Base","text":""},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.convolve_2d","title":"<code>convolve_2d(image, kernel)</code>","text":"<p>Convolves <code>image</code> with <code>kernel</code>, padding by replicating border pixels.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2]</code>. Image to convolve.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>float [kernel_sz1 x kernel_sz2]</code>. 2D kernel</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2]</code>. <code>image</code> after being convolved with <code>kernel</code>.</p> <p>Note</p> <p><code>np.flip</code> is used to give same result as <code>convn</code> with replicate padding in MATLAB.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def convolve_2d(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Convolves `image` with `kernel`, padding by replicating border pixels.\n\n    Args:\n        image: `float [image_sz1 x image_sz2]`.\n            Image to convolve.\n        kernel: `float [kernel_sz1 x kernel_sz2]`.\n            2D kernel\n\n    Returns:\n        `float [image_sz1 x image_sz2]`.\n            `image` after being convolved with `kernel`.\n\n    !!! note\n        `np.flip` is used to give same result as `convn` with replicate padding in MATLAB.\n    \"\"\"\n    return cv2.filter2D(image.astype(float), -1, np.flip(kernel), borderType=cv2.BORDER_REPLICATE)\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.dilate","title":"<code>dilate(image, kernel)</code>","text":"<p>Dilates <code>image</code> with <code>kernel</code>, using zero padding.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [image_sz1 x ... x image_szN]</code>. Image to be dilated.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>int [kernel_sz1 x ... x kernel_szN]</code>. Dilation kernel containing only zeros or ones.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2]</code>. <code>image</code> after being dilated with <code>kernel</code>.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def dilate(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Dilates `image` with `kernel`, using zero padding.\n\n    Args:\n        image: `float [image_sz1 x ... x image_szN]`.\n            Image to be dilated.\n        kernel: `int [kernel_sz1 x ... x kernel_szN]`.\n            Dilation kernel containing only zeros or ones.\n\n    Returns:\n        `float [image_sz1 x image_sz2]`.\n            `image` after being dilated with `kernel`.\n    \"\"\"\n    kernel = ensure_odd_kernel(kernel)\n    # mode refers to the padding. We pad with zeros to keep results the same as MATLAB\n    return grey_dilation(image, footprint=kernel, mode='constant')\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.ensure_odd_kernel","title":"<code>ensure_odd_kernel(kernel, pad_location='start')</code>","text":"<p>This ensures all dimensions of <code>kernel</code> are odd by padding even dimensions with zeros. Replicates MATLAB way of dealing with even kernels.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>np.ndarray</code> <p><code>float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]</code>.</p> required <code>pad_location</code> <code>str</code> <p>One of the following, indicating where to pad with zeros -</p> <ul> <li><code>'start'</code> - Zeros at start of kernel.</li> <li><code>'end'</code> - Zeros at end of kernel.</li> </ul> <code>'start'</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN]</code>. <code>kernel</code> padded with zeros so each dimension is odd.</p> Example <p>If <code>pad_location</code> is <code>'start'</code> then <code>[[5,4];[3,1]]</code> becomes <code>[[0,0,0],[0,5,4],[0,3,1]]</code>.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def ensure_odd_kernel(kernel: np.ndarray, pad_location: str = 'start') -&gt; np.ndarray:\n\"\"\"\n    This ensures all dimensions of `kernel` are odd by padding even dimensions with zeros.\n    Replicates MATLAB way of dealing with even kernels.\n\n    Args:\n        kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`.\n        pad_location: One of the following, indicating where to pad with zeros -\n\n            - `'start'` - Zeros at start of kernel.\n            - `'end'` - Zeros at end of kernel.\n\n    Returns:\n        `float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN]`.\n            `kernel` padded with zeros so each dimension is odd.\n\n    Example:\n        If `pad_location` is `'start'` then `[[5,4];[3,1]]` becomes `[[0,0,0],[0,5,4],[0,3,1]]`.\n    \"\"\"\n    even_dims = (np.mod(kernel.shape, 2) == 0).astype(int)\n    if max(even_dims) == 1:\n        if pad_location == 'start':\n            pad_dims = [tuple(np.array([1, 0]) * val) for val in even_dims]\n        elif pad_location == 'end':\n            pad_dims = [tuple(np.array([0, 1]) * val) for val in even_dims]\n        else:\n            raise ValueError(f\"pad_location has to be either 'start' or 'end' but value given was {pad_location}.\")\n        return np.pad(kernel, pad_dims, mode='constant')\n    else:\n        return kernel\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.ftrans2","title":"<code>ftrans2(b, t=None)</code>","text":"<p>Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, <code>b</code>, using the transform, <code>t</code>. Copied from MATLAB <code>ftrans2</code>.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>np.ndarray</code> <p><code>float [Q]</code>. 1D convolve kernel.</p> required <code>t</code> <code>Optional[np.ndarray]</code> <p><code>float [M x N]</code>. Transform to make <code>b</code> a 2D convolve kernel. If <code>None</code>, McClellan transform used.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1]</code>. 2D convolve kernel.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def ftrans2(b: np.ndarray, t: Optional[np.ndarray] = None) -&gt; np.ndarray:\n\"\"\"\n    Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, `b`, using the transform, `t`.\n    Copied from [MATLAB `ftrans2`](https://www.mathworks.com/help/images/ref/ftrans2.html).\n\n    Args:\n        b: `float [Q]`.\n            1D convolve kernel.\n        t: `float [M x N]`.\n            Transform to make `b` a 2D convolve kernel.\n            If `None`, McClellan transform used.\n\n    Returns:\n        `float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1]`.\n            2D convolve kernel.\n    \"\"\"\n    if t is None:\n        # McClellan transformation\n        t = np.array([[1, 2, 1], [2, -4, 2], [1, 2, 1]]) / 8\n\n    # Convert the 1-D convolve_2d b to SUM_n a(n) cos(wn) form\n    n = int(round((len(b) - 1) / 2))\n    b = b.reshape(-1, 1)\n    b = np.rot90(np.fft.fftshift(np.rot90(b)))\n    a = np.concatenate((b[:1], 2 * b[1:n + 1]))\n\n    inset = np.floor((np.array(t.shape) - 1) / 2).astype(int)\n\n    # Use Chebyshev polynomials to compute h\n    p0 = 1\n    p1 = t\n    h = a[1] * p1\n    rows = inset[0]\n    cols = inset[1]\n    h[rows, cols] += a[0] * p0\n    for i in range(2, n + 1):\n        p2 = 2 * scipy.signal.convolve2d(t, p1)\n        rows = rows + inset[0]\n        cols = cols + inset[1]\n        p2[rows, cols] -= p0\n        rows = inset[0] + np.arange(p1.shape[0])\n        cols = (inset[1] + np.arange(p1.shape[1])).reshape(-1, 1)\n        hh = h.copy()\n        h = a[i] * p2\n        h[rows, cols] += hh\n        p0 = p1.copy()\n        p1 = p2.copy()\n    h = np.rot90(h)\n    return h\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.hanning_diff","title":"<code>hanning_diff(r1, r2)</code>","text":"<p>Gets difference of two hanning window 2D convolve kernel. Central positive, outer negative with sum of <code>0</code>.</p> <p>Parameters:</p> Name Type Description Default <code>r1</code> <code>int</code> <p>radius in pixels of central positive hanning convolve kernel.</p> required <code>r2</code> <code>int</code> <p>radius in pixels of outer negative hanning convolve kernel.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [2*r2+1 x 2*r2+1]</code>. Difference of two hanning window 2D convolve kernel.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def hanning_diff(r1: int, r2: int) -&gt; np.ndarray:\n\"\"\"\n    Gets difference of two hanning window 2D convolve kernel.\n    Central positive, outer negative with sum of `0`.\n\n    Args:\n        r1: radius in pixels of central positive hanning convolve kernel.\n        r2: radius in pixels of outer negative hanning convolve kernel.\n\n    Returns:\n        `float [2*r2+1 x 2*r2+1]`.\n            Difference of two hanning window 2D convolve kernel.\n    \"\"\"\n    if not 0 &lt;= r1 &lt;= r2-1:\n        raise errors.OutOfBoundsError(\"r1\", r1, 0, r2-1)\n    if not r1+1 &lt;= r2 &lt;= np.inf:\n        raise errors.OutOfBoundsError(\"r2\", r1+1, np.inf)\n    h_outer = np.hanning(2 * r2 + 3)[1:-1]  # ignore zero values at first and last index\n    h_outer = -h_outer / h_outer.sum()\n    h_inner = np.hanning(2 * r1 + 3)[1:-1]\n    h_inner = h_inner / h_inner.sum()\n    h = h_outer.copy()\n    h[r2 - r1:r2 + r1 + 1] += h_inner\n    h = ftrans2(h)\n    return h\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.top_hat","title":"<code>top_hat(image, kernel)</code>","text":"<p>Does tophat filtering of <code>image</code> with <code>kernel</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2]</code>. Image to filter.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>np.uint8 [kernel_sz1 x kernel_sz2]</code>. Top hat <code>kernel</code> containing only zeros or ones.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2]</code>. <code>image</code> after being top hat filtered with <code>kernel</code>.</p> Source code in <code>coppafish/utils/morphology/base.py</code> <pre><code>def top_hat(image: np.ndarray, kernel: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Does tophat filtering of `image` with `kernel`.\n\n    Args:\n        image: `float [image_sz1 x image_sz2]`.\n            Image to filter.\n        kernel: `np.uint8 [kernel_sz1 x kernel_sz2]`.\n            Top hat `kernel` containing only zeros or ones.\n\n    Returns:\n        `float [image_sz1 x image_sz2]`.\n            `image` after being top hat filtered with `kernel`.\n    \"\"\"\n    if kernel.dtype != np.uint8:\n        if sum(np.unique(kernel) == [0, 1]) == len(np.unique(kernel)):\n            kernel = kernel.astype(np.uint8)  # kernel must be uint8\n        else:\n            raise ValueError(f'kernel is of type {kernel.dtype} but must be of data type np.uint8.')\n    image_dtype = image.dtype   # so returned image is of same dtype as input\n    if image.dtype == int:\n        if image.min() &gt;= 0 and image.max() &lt;= np.iinfo(np.uint16).max:\n            image = image.astype(np.uint16)\n    if not (image.dtype == float or image.dtype == np.uint16):\n        raise ValueError(f'image is of type {image.dtype} but must be of data type np.uint16 or float.')\n    if np.max(np.mod(kernel.shape, 2) == 0):\n        # With even kernel, gives different results to MATLAB\n        raise ValueError(f'kernel dimensions are {kernel.shape}. Require all dimensions to be odd.')\n    # kernel = ensure_odd_kernel(kernel)  # doesn't work for tophat at start or end.\n    return cv2.morphologyEx(image, cv2.MORPH_TOPHAT, kernel).astype(image_dtype)\n</code></pre>"},{"location":"code/utils/morphology/#filter","title":"Filter","text":""},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter.imfilter","title":"<code>imfilter(image, kernel, padding=0, corr_or_conv='corr', oa=True)</code>","text":"<p>Copy of MATLAB <code>imfilter</code> function with <code>'output_size'</code> equal to <code>'same'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2 x ... x image_szN]</code>. Image to be filtered.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]</code>. Multidimensional filter.</p> required <code>padding</code> <code>Union[float, str]</code> <p>One of the following, indicated which padding to be used.</p> <ul> <li>numeric scalar - Input array values outside the bounds of the array are assigned the value <code>X</code>.     When no padding option is specified, the default is <code>0</code>.</li> <li><code>\u2018symmetric\u2019</code> - Input array values outside the bounds of the array are computed by     mirror-reflecting the array across the array border.</li> <li><code>\u2018edge\u2019</code>- Input array values outside the bounds of the array are assumed to equal     the nearest array border value.</li> <li><code>'wrap'</code> - Input array values outside the bounds of the array are computed by implicitly     assuming the input array is periodic.</li> </ul> <code>0</code> <code>corr_or_conv</code> <code>str</code> <ul> <li><code>'corr'</code> - Performs multidimensional filtering using correlation.</li> <li><code>'conv'</code> - Performs multidimensional filtering using convolution.</li> </ul> <code>'corr'</code> <code>oa</code> <code>bool</code> <p>Whether to use oaconvolve or scipy.ndimage.convolve. scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes).</p> <code>True</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [image_sz1 x image_sz2 x ... x image_szN]</code>. <code>image</code> after being filtered.</p> Source code in <code>coppafish/utils/morphology/filter.py</code> <pre><code>def imfilter(image: np.ndarray, kernel: np.ndarray, padding: Union[float, str] = 0,\n             corr_or_conv: str = 'corr', oa: bool = True) -&gt; np.ndarray:\n\"\"\"\n    Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`.\n\n    Args:\n        image: `float [image_sz1 x image_sz2 x ... x image_szN]`.\n            Image to be filtered.\n        kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`.\n            Multidimensional filter.\n        padding: One of the following, indicated which padding to be used.\n\n            - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`.\n                When no padding option is specified, the default is `0`.\n            - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by\n                mirror-reflecting the array across the array border.\n            - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal\n                the nearest array border value.\n            - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly\n                assuming the input array is periodic.\n        corr_or_conv:\n            - `'corr'` - Performs multidimensional filtering using correlation.\n            - `'conv'` - Performs multidimensional filtering using convolution.\n        oa: Whether to use oaconvolve or scipy.ndimage.convolve.\n            scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes).\n\n    Returns:\n        `float [image_sz1 x image_sz2 x ... x image_szN]`.\n            `image` after being filtered.\n    \"\"\"\n    if oa:\n        if corr_or_conv == 'corr':\n            kernel = np.flip(kernel)\n        elif corr_or_conv != 'conv':\n            raise ValueError(f\"corr_or_conv should be either 'corr' or 'conv' but given value is {corr_or_conv}\")\n        kernel = ensure_odd_kernel(kernel, 'end')\n        if kernel.ndim &lt; image.ndim:\n            kernel = np.expand_dims(kernel, axis=tuple(np.arange(kernel.ndim, image.ndim)))\n        pad_size = [(int((ax_size-1)/2),)*2 for ax_size in kernel.shape]\n        if isinstance(padding, numbers.Number):\n            return oaconvolve(np.pad(image, pad_size, 'constant', constant_values=padding), kernel, 'valid')\n        else:\n            return oaconvolve(np.pad(image, pad_size, padding), kernel, 'valid')\n    else:\n        if padding == 'symmetric':\n            padding = 'reflect'\n        elif padding == 'edge':\n            padding = 'nearest'\n        # Old method, about 3x slower for filtering large 3d image with small 3d kernel\n        if isinstance(padding, numbers.Number):\n            pad_value = padding\n            padding = 'constant'\n        else:\n            pad_value = 0.0  # doesn't do anything for non-constant padding\n        if corr_or_conv == 'corr':\n            kernel = ensure_odd_kernel(kernel, 'start')\n            return correlate(image, kernel, mode=padding, cval=pad_value)\n        elif corr_or_conv == 'conv':\n            kernel = ensure_odd_kernel(kernel, 'end')\n            return convolve(image, kernel, mode=padding, cval=pad_value)\n        else:\n            raise ValueError(f\"corr_or_conv should be either 'corr' or 'conv' but given value is {corr_or_conv}\")\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter.imfilter_coords","title":"<code>imfilter_coords(image, kernel, coords, padding=0, corr_or_conv='corr')</code>","text":"<p>Copy of MATLAB <code>imfilter</code> function with <code>'output_size'</code> equal to <code>'same'</code>. Only finds result of filtering at specific locations but still filters entire image.</p> <p>Note</p> <p>image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>int [image_szY x image_szX (x image_szZ)]</code>. Image to be filtered. Must be 2D or 3D.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>int [kernel_szY x kernel_szX (x kernel_szZ)]</code>. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1.</p> required <code>coords</code> <code>np.ndarray</code> <p><code>int [n_points x image.ndims]</code>. Coordinates where result of filtering is desired.</p> required <code>padding</code> <code>Union[float, str]</code> <p>One of the following, indicated which padding to be used.</p> <ul> <li>numeric scalar - Input array values outside the bounds of the array are assigned the value <code>X</code>.     When no padding option is specified, the default is <code>0</code>.</li> <li><code>\u2018symmetric\u2019</code> - Input array values outside the bounds of the array are computed by     mirror-reflecting the array across the array border.</li> <li><code>\u2018edge\u2019</code>- Input array values outside the bounds of the array are assumed to equal     the nearest array border value.</li> <li><code>'wrap'</code> - Input array values outside the bounds of the array are computed by implicitly     assuming the input array is periodic.</li> </ul> <code>0</code> <code>corr_or_conv</code> <code>str</code> <ul> <li><code>'corr'</code> - Performs multidimensional filtering using correlation.</li> <li><code>'conv'</code> - Performs multidimensional filtering using convolution.</li> </ul> <code>'corr'</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <p><code>int [n_points]</code>. Result of filtering of <code>image</code> at each point in <code>coords</code>.</p> Source code in <code>coppafish/utils/morphology/filter.py</code> <pre><code>def imfilter_coords(image: np.ndarray, kernel: np.ndarray, coords: np.ndarray, padding: Union[float, str] = 0,\n                    corr_or_conv: str = 'corr') -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n\"\"\"\n    Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`.\n    Only finds result of filtering at specific locations but still filters entire image.\n\n    !!! note\n        image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error.\n\n    Args:\n        image: `int [image_szY x image_szX (x image_szZ)]`.\n            Image to be filtered. Must be 2D or 3D.\n        kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`.\n            Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1.\n        coords: `int [n_points x image.ndims]`.\n            Coordinates where result of filtering is desired.\n        padding: One of the following, indicated which padding to be used.\n\n            - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`.\n                When no padding option is specified, the default is `0`.\n            - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by\n                mirror-reflecting the array across the array border.\n            - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal\n                the nearest array border value.\n            - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly\n                assuming the input array is periodic.\n        corr_or_conv:\n            - `'corr'` - Performs multidimensional filtering using correlation.\n            - `'conv'` - Performs multidimensional filtering using convolution.\n\n    Returns:\n        `int [n_points]`.\n            Result of filtering of `image` at each point in `coords`.\n    \"\"\"\n    im_filt = imfilter(image.astype(int), kernel, padding, corr_or_conv, oa=False)\n    return im_filt[tuple([coords[:, j] for j in range(im_filt.ndim)])]\n</code></pre>"},{"location":"code/utils/morphology/#filter-optimised","title":"Filter Optimised","text":""},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.get_shifts_from_kernel","title":"<code>get_shifts_from_kernel(kernel)</code>","text":"<p>Returns where kernel is positive as shifts in y, x and z. I.e. <code>kernel=jnp.ones((3,3,3))</code> would return <code>y_shifts = x_shifts = z_shifts = -1, 0, 1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>jnp.ndarray</code> <p>int [kernel_szY x kernel_szX x kernel_szY]</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <ul> <li><code>int [n_shifts]</code>. y_shifts.</li> </ul> <code>jnp.ndarray</code> <ul> <li><code>int [n_shifts]</code>. x_shifts.</li> </ul> <code>jnp.ndarray</code> <ul> <li><code>int [n_shifts]</code>. z_shifts.</li> </ul> Source code in <code>coppafish/utils/morphology/filter_optimised.py</code> <pre><code>def get_shifts_from_kernel(kernel: jnp.ndarray) -&gt; Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n\"\"\"\n    Returns where kernel is positive as shifts in y, x and z.\n    I.e. `kernel=jnp.ones((3,3,3))` would return `y_shifts = x_shifts = z_shifts = -1, 0, 1`.\n    Args:\n        kernel: int [kernel_szY x kernel_szX x kernel_szY]\n\n    Returns:\n        - `int [n_shifts]`.\n            y_shifts.\n        - `int [n_shifts]`.\n            x_shifts.\n        - `int [n_shifts]`.\n            z_shifts.\n    \"\"\"\n    shifts = list(jnp.where(kernel &gt; 0))\n    for i in range(kernel.ndim):\n        shifts[i] = (shifts[i] - (kernel.shape[i] - 1) / 2).astype(int)\n    return tuple(shifts)\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.imfilter_coords","title":"<code>imfilter_coords(image, kernel, coords, padding=0, corr_or_conv='corr')</code>","text":"<p>Copy of MATLAB <code>imfilter</code> function with <code>'output_size'</code> equal to <code>'same'</code>. Only finds result of filtering at specific locations.</p> <p>Note</p> <p>image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>int [image_szY x image_szX (x image_szZ)]</code>. Image to be filtered. Must be 2D or 3D.</p> required <code>kernel</code> <code>np.ndarray</code> <p><code>int [kernel_szY x kernel_szX (x kernel_szZ)]</code>. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1.</p> required <code>coords</code> <code>np.ndarray</code> <p><code>int [n_points x image.ndims]</code>. Coordinates where result of filtering is desired.</p> required <code>padding</code> <code>Union[float, str]</code> <p>One of the following, indicated which padding to be used.</p> <ul> <li>numeric scalar - Input array values outside the bounds of the array are assigned the value <code>X</code>.     When no padding option is specified, the default is <code>0</code>.</li> <li><code>\u2018symmetric\u2019</code> - Input array values outside the bounds of the array are computed by     mirror-reflecting the array across the array border.</li> <li><code>\u2018edge\u2019</code>- Input array values outside the bounds of the array are assumed to equal     the nearest array border value.</li> <li><code>'wrap'</code> - Input array values outside the bounds of the array are computed by implicitly     assuming the input array is periodic.</li> </ul> <code>0</code> <code>corr_or_conv</code> <code>str</code> <ul> <li><code>'corr'</code> - Performs multidimensional filtering using correlation.     This is the default when no option specified.</li> <li><code>'conv'</code> - Performs multidimensional filtering using convolution.</li> </ul> <code>'corr'</code> <p>Returns:</p> Type Description <code>Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]</code> <p><code>int [n_points]</code>. Result of filtering of <code>image</code> at each point in <code>coords</code>.</p> Source code in <code>coppafish/utils/morphology/filter_optimised.py</code> <pre><code>def imfilter_coords(image: np.ndarray, kernel: np.ndarray, coords: np.ndarray, padding: Union[float, str] = 0,\n                    corr_or_conv: str = 'corr') -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n\"\"\"\n    Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`.\n    Only finds result of filtering at specific locations.\n\n    !!! note\n        image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error.\n\n    Args:\n        image: `int [image_szY x image_szX (x image_szZ)]`.\n            Image to be filtered. Must be 2D or 3D.\n        kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`.\n            Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1.\n        coords: `int [n_points x image.ndims]`.\n            Coordinates where result of filtering is desired.\n        padding: One of the following, indicated which padding to be used.\n\n            - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`.\n                When no padding option is specified, the default is `0`.\n            - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by\n                mirror-reflecting the array across the array border.\n            - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal\n                the nearest array border value.\n            - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly\n                assuming the input array is periodic.\n        corr_or_conv:\n            - `'corr'` - Performs multidimensional filtering using correlation.\n                This is the default when no option specified.\n            - `'conv'` - Performs multidimensional filtering using convolution.\n\n    Returns:\n        `int [n_points]`.\n            Result of filtering of `image` at each point in `coords`.\n    \"\"\"\n    if corr_or_conv == 'corr':\n        kernel = np.flip(kernel)\n    elif corr_or_conv != 'conv':\n        raise ValueError(f\"corr_or_conv should be either 'corr' or 'conv' but given value is {corr_or_conv}\")\n    kernel = ensure_odd_kernel(kernel, 'end')\n\n    # Ensure shape of image and kernel correct\n    if image.ndim != coords.shape[1]:\n        raise ValueError(f\"Image has {image.ndim} dimensions but coords only have {coords.shape[1]} dimensions.\")\n    if image.ndim == 2:\n        image = np.expand_dims(image, 2)\n    elif image.ndim != 3:\n        raise ValueError(f\"image must have 2 or 3 dimensions but given image has {image.ndim}.\")\n    if kernel.ndim == 2:\n        kernel = np.expand_dims(kernel, 2)\n    elif kernel.ndim != 3:\n        raise ValueError(f\"kernel must have 2 or 3 dimensions but given image has {image.ndim}.\")\n    if kernel.max() &gt; 1:\n        raise ValueError(f'kernel is expected to be binary, only containing 0 or 1 but kernel.max = {kernel.max()}')\n\n    if coords.shape[1] == 2:\n        # set all z coordinates to 0 if 2D.\n        coords = np.append(coords, np.zeros((coords.shape[0], 1), dtype=int), axis=1)\n    if (coords.max(axis=0) &gt;= np.array(image.shape)).any():\n        raise ValueError(f\"Max yxz coordinates provided are {coords.max(axis=0)} but image has shape {image.shape}.\")\n\n    pad_size = [(int((ax_size-1)/2),)*2 for ax_size in kernel.shape]\n    pad_coords = jnp.asarray(coords) + jnp.array([val[0] for val in pad_size])\n    if isinstance(padding, numbers.Number):\n        image_pad = jnp.pad(jnp.asarray(image), pad_size, 'constant', constant_values=padding).astype(int)\n    else:\n        image_pad = jnp.pad(jnp.asarray(image), pad_size, padding).astype(int)\n    y_shifts, x_shifts, z_shifts = get_shifts_from_kernel(jnp.asarray(np.flip(kernel)))\n    return np.asarray(manual_convolve(image_pad, y_shifts, x_shifts, z_shifts, pad_coords))\n</code></pre>"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.manual_convolve","title":"<code>manual_convolve(image, y_kernel_shifts, x_kernel_shifts, z_kernel_shifts, coords)</code>","text":"<p>Finds result of convolution at specific locations indicated by <code>coords</code> with binary kernel. I.e. instead of convolving whole <code>image</code>, just find result at these <code>points</code>.</p> <p>Note</p> <p>image needs to be padded before this function is called otherwise get an error when go out of bounds.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>jnp.ndarray</code> <p><code>int [image_szY x image_szX x image_szZ]</code>. Image to be filtered. Must be 3D.</p> required <code>y_kernel_shifts</code> <code>jnp.ndarray</code> <p><code>int [n_nonzero_kernel]</code> Shifts indicating where kernel equals 1. I.e. if <code>kernel = np.ones((3,3))</code> then <code>y_shift = x_shift = z_shift = [-1, 0, 1]</code>.</p> required <code>x_kernel_shifts</code> <code>jnp.asarray</code> <p><code>int [n_nonzero_kernel]</code> Shifts indicating where kernel equals 1. I.e. if <code>kernel = np.ones((3,3))</code> then <code>y_shift = x_shift = z_shift = [-1, 0, 1]</code>.</p> required <code>z_kernel_shifts</code> <code>jnp.ndarray</code> <p><code>int [n_nonzero_kernel]</code> Shifts indicating where kernel equals 1. I.e. if <code>kernel = np.ones((3,3))</code> then <code>y_shift = x_shift = z_shift = [-1, 0, 1]</code>.</p> required <code>coords</code> <code>jnp.ndarray</code> <p><code>int [n_points x 3]</code>. yxz coordinates where result of filtering is desired.</p> required <p>Returns:</p> Type Description <code>jnp.ndarray</code> <p><code>int [n_points]</code>. Result of filtering of <code>image</code> at each point in <code>coords</code>.</p> Source code in <code>coppafish/utils/morphology/filter_optimised.py</code> <pre><code>@jax.jit\ndef manual_convolve(image: jnp.ndarray, y_kernel_shifts: jnp.ndarray, x_kernel_shifts: jnp.asarray,\n                    z_kernel_shifts: jnp.ndarray, coords: jnp.ndarray) -&gt; jnp.ndarray:\n\"\"\"\n    Finds result of convolution at specific locations indicated by `coords` with binary kernel.\n    I.e. instead of convolving whole `image`, just find result at these `points`.\n\n    !!! note\n        image needs to be padded before this function is called otherwise get an error when go out of bounds.\n\n    Args:\n        image: `int [image_szY x image_szX x image_szZ]`.\n            Image to be filtered. Must be 3D.\n        y_kernel_shifts: `int [n_nonzero_kernel]`\n            Shifts indicating where kernel equals 1.\n            I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`.\n        x_kernel_shifts: `int [n_nonzero_kernel]`\n            Shifts indicating where kernel equals 1.\n            I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`.\n        z_kernel_shifts: `int [n_nonzero_kernel]`\n            Shifts indicating where kernel equals 1.\n            I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`.\n        coords: `int [n_points x 3]`.\n            yxz coordinates where result of filtering is desired.\n\n    Returns:\n        `int [n_points]`.\n            Result of filtering of `image` at each point in `coords`.\n    \"\"\"\n    return jax.vmap(manual_convolve_single, in_axes=(None, None, None, None, 0),\n                    out_axes=0)(image, y_kernel_shifts, x_kernel_shifts,z_kernel_shifts, coords)\n</code></pre>"},{"location":"code/utils/nd2/","title":"nd2","text":""},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_image","title":"<code>get_image(images, fov, channel, use_z=None)</code>","text":"<p>Using dask array from nd2 file, this loads the image of the desired fov and channel.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>np.ndarray</code> <p>Dask array with <code>fov</code>, <code>channel</code>, y, x, z as index order.</p> required <code>fov</code> <code>int</code> <p><code>fov</code> index of desired image</p> required <code>channel</code> <code>int</code> <p><code>channel</code> of desired image</p> required <code>use_z</code> <code>Optional[List[int]]</code> <p><code>int [n_use_z]</code>. Which z-planes of image to load. If <code>None</code>, will load all z-planes.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>uint16 [im_sz_y x im_sz_x x n_use_z]</code>. Image of the desired <code>fov</code> and <code>channel</code>.</p> Source code in <code>coppafish/utils/nd2.py</code> <pre><code>def get_image(images: np.ndarray, fov: int, channel: int, use_z: Optional[List[int]] = None) -&gt; np.ndarray:\n\"\"\"\n    Using dask array from nd2 file, this loads the image of the desired fov and channel.\n\n    Args:\n        images: Dask array with `fov`, `channel`, y, x, z as index order.\n        fov: `fov` index of desired image\n        channel: `channel` of desired image\n        use_z: `int [n_use_z]`.\n            Which z-planes of image to load.\n            If `None`, will load all z-planes.\n\n    Returns:\n        `uint16 [im_sz_y x im_sz_x x n_use_z]`.\n            Image of the desired `fov` and `channel`.\n    \"\"\"\n    if use_z is None:\n        use_z = np.arange(images.shape[-1])\n    return np.asarray(images[fov, channel, :, :, use_z])\n</code></pre>"},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_metadata","title":"<code>get_metadata(file_path)</code>","text":"<p>Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of tiles/channels/z-planes.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to desired nd2 file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing -</p> <code>dict</code> <ul> <li><code>xy_pos</code> - <code>List [n_tiles x 2]</code>. xy position of tiles in pixels.</li> </ul> <code>dict</code> <ul> <li><code>pixel_microns</code> - <code>float</code>. xy pixel size in microns.</li> </ul> <code>dict</code> <ul> <li><code>pixel_microns_z</code> - <code>float</code>. z pixel size in microns.</li> </ul> <code>dict</code> <ul> <li><code>sizes</code> - dict with fov (<code>t</code>), channels (<code>c</code>), y, x, z-planes (<code>z</code>) dimensions.</li> </ul> Source code in <code>coppafish/utils/nd2.py</code> <pre><code>def get_metadata(file_path: str) -&gt; dict:\n\"\"\"\n    Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of\n    tiles/channels/z-planes.\n\n    Args:\n        file_path: Path to desired nd2 file.\n\n    Returns:\n        Dictionary containing -\n\n        - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels.\n        - `pixel_microns` - `float`. xy pixel size in microns.\n        - `pixel_microns_z` - `float`. z pixel size in microns.\n        - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise errors.NoFileError(file_path)\n    images = nd2.ND2File(file_path)\n    metadata = {'sizes': {'t': images.sizes['P'], 'c': images.sizes['C'], 'y': images.sizes['Y'],\n                          'x': images.sizes['X'], 'z': images.sizes['Z']},\n                'pixel_microns': images.metadata.channels[0].volume.axesCalibration[0],\n                'pixel_microns_z': images.metadata.channels[0].volume.axesCalibration[2]}\n    xy_pos = np.array([images.experiment[0].parameters.points[i].stagePositionUm[:2]\n                       for i in range(images.sizes['P'])])\n    metadata['xy_pos'] = (xy_pos - np.min(xy_pos, 0)) / metadata['pixel_microns']\n    metadata['xy_pos'] = metadata['xy_pos'].tolist()\n    return metadata\n</code></pre>"},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_nd2_tile_ind","title":"<code>get_nd2_tile_ind(tile_ind_npy, tile_pos_yx_nd2, tile_pos_yx_npy)</code>","text":"<p>Gets index of tiles in nd2 file from tile index of npy file.</p> <p>Parameters:</p> Name Type Description Default <code>tile_ind_npy</code> <code>Union[int, List[int]]</code> <p>Indices of tiles in npy file</p> required <code>tile_pos_yx_nd2</code> <code>np.ndarray</code> <p><code>int [n_tiles x 2]</code>. <code>[i,:]</code> contains YX position of tile with nd2 index <code>i</code>. Index 0 refers to <code>YX = [0, 0]</code>. Index 1 refers to <code>YX = [0, 1] if MaxX &gt; 0</code>.</p> required <code>tile_pos_yx_npy</code> <code>np.ndarray</code> <p><code>int [n_tiles x 2]</code>. <code>[i,:]</code> contains YX position of tile with npy index <code>i</code>. Index 0 refers to <code>YX = [MaxY, MaxX]</code>. Index 1 refers to <code>YX = [MaxY, MaxX - 1] if MaxX &gt; 0</code>.</p> required <p>Returns:</p> Type Description <code>Union[int, List[int]]</code> <p>Corresponding indices in nd2 file</p> Source code in <code>coppafish/utils/nd2.py</code> <pre><code>def get_nd2_tile_ind(tile_ind_npy: Union[int, List[int]], tile_pos_yx_nd2: np.ndarray,\n                     tile_pos_yx_npy: np.ndarray) -&gt; Union[int, List[int]]:\n\"\"\"\n    Gets index of tiles in nd2 file from tile index of npy file.\n\n    Args:\n        tile_ind_npy: Indices of tiles in npy file\n        tile_pos_yx_nd2: ```int [n_tiles x 2]```.\n            ```[i,:]``` contains YX position of tile with nd2 index ```i```.\n            Index 0 refers to ```YX = [0, 0]```.\n            Index 1 refers to ```YX = [0, 1] if MaxX &gt; 0```.\n        tile_pos_yx_npy: ```int [n_tiles x 2]```.\n            ```[i,:]``` contains YX position of tile with npy index ```i```.\n            Index 0 refers to ```YX = [MaxY, MaxX]```.\n            Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX &gt; 0```.\n\n    Returns:\n        Corresponding indices in nd2 file\n    \"\"\"\n    if isinstance(tile_ind_npy, numbers.Number):\n        tile_ind_npy = [tile_ind_npy]\n    nd2_index = numpy_indexed.indices(tile_pos_yx_nd2, tile_pos_yx_npy[tile_ind_npy]).tolist()\n    if len(nd2_index) == 1:\n        return nd2_index[0]\n    else:\n        return nd2_index\n</code></pre>"},{"location":"code/utils/nd2/#coppafish.utils.nd2.load","title":"<code>load(file_path)</code>","text":"<p>Returns dask array with indices in order <code>fov</code>, <code>channel</code>, <code>y</code>, <code>x</code>, <code>z</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to desired nd2 file.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Dask array indices in order <code>fov</code>, <code>channel</code>, <code>y</code>, <code>x</code>, <code>z</code>.</p> Source code in <code>coppafish/utils/nd2.py</code> <pre><code>def load(file_path: str) -&gt; np.ndarray:\n\"\"\"\n    Returns dask array with indices in order `fov`, `channel`, `y`, `x`, `z`.\n\n    Args:\n        file_path: Path to desired nd2 file.\n\n    Returns:\n        Dask array indices in order `fov`, `channel`, `y`, `x`, `z`.\n    \"\"\"\n    if not os.path.isfile(file_path):\n        raise errors.NoFileError(file_path)\n    images = nd2.ND2File(file_path)\n    images = images.to_dask()\n    # images = nd2.imread(file_name, dask=True)  # get python crashing with this in get_image for some reason\n    images = np.moveaxis(images, 1, -1)  # put z index to end\n    return images\n</code></pre>"},{"location":"code/utils/nd2/#coppafish.utils.nd2.save_metadata","title":"<code>save_metadata(json_file, nd2_file, use_channels=None)</code>","text":"<p>Saves the required metadata as a json file which will contain</p> <ul> <li><code>xy_pos</code> - <code>List [n_tiles x 2]</code>. xy position of tiles in pixels.</li> <li><code>pixel_microns</code> - <code>float</code>. xy pixel size in microns.</li> <li><code>pixel_microns_z</code> - <code>float</code>. z pixel size in microns.</li> <li><code>sizes</code> - dict with fov (<code>t</code>), channels (<code>c</code>), y, x, z-planes (<code>z</code>) dimensions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>json_file</code> <code>str</code> <p>Where to save json file</p> required <code>nd2_file</code> <code>str</code> <p>Path to nd2 file</p> required <code>use_channels</code> <code>Optional[List]</code> <p>The channels which have been extracted from the nd2 file. If <code>None</code>, assume all channels in nd2 file used</p> <code>None</code> Source code in <code>coppafish/utils/nd2.py</code> <pre><code>def save_metadata(json_file: str, nd2_file: str, use_channels: Optional[List] = None):\n\"\"\"\n    Saves the required metadata as a json file which will contain\n\n    - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels.\n    - `pixel_microns` - `float`. xy pixel size in microns.\n    - `pixel_microns_z` - `float`. z pixel size in microns.\n    - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions.\n\n    Args:\n        json_file: Where to save json file\n        nd2_file: Path to nd2 file\n        use_channels: The channels which have been extracted from the nd2 file.\n            If `None`, assume all channels in nd2 file used\n\n    \"\"\"\n    metadata = get_metadata(nd2_file)\n    if use_channels is not None:\n        if len(use_channels) &gt; metadata['sizes']['c']:\n            raise ValueError(f\"use_channels contains {len(use_channels)} channels but there \"\n                             f\"are only {metadata['sizes']['c']} channels in the nd2 metadata.\")\n        metadata['sizes']['c'] = len(use_channels)\n        metadata['use_channels'] = use_channels   # channels extracted from nd2 file\n    json.dump(metadata, open(json_file, 'w'))\n</code></pre>"},{"location":"code/utils/npy/","title":"npy","text":""},{"location":"code/utils/npy/#coppafish.utils.npy.get_npy_tile_ind","title":"<code>get_npy_tile_ind(tile_ind_nd2, tile_pos_yx_nd2, tile_pos_yx_npy)</code>","text":"<p>Gets index of tile in npy file from tile index of nd2 file.</p> <p>Parameters:</p> Name Type Description Default <code>tile_ind_nd2</code> <code>Union[int, List[int]]</code> <p>Index of tile in nd2 file</p> required <code>tile_pos_yx_nd2</code> <code>np.ndarray</code> <p><code>int [n_tiles x 2]</code>. <code>[i,:]</code> contains YX position of tile with nd2 index <code>i</code>. Index 0 refers to <code>YX = [0, 0]</code>. Index 1 refers to <code>YX = [0, 1] if MaxX &gt; 0</code>.</p> required <code>tile_pos_yx_npy</code> <code>np.ndarray</code> <p><code>int [n_tiles x 2]</code>. <code>[i,:]</code> contains YX position of tile with npy index <code>i</code>. Index 0 refers to <code>YX = [MaxY, MaxX]</code>. Index 1 refers to <code>YX = [MaxY, MaxX - 1] if MaxX &gt; 0</code>.</p> required <p>Returns:</p> Type Description <code>Union[int, List[int]]</code> <p>Corresponding indices in npy file</p> Source code in <code>coppafish/utils/npy.py</code> <pre><code>def get_npy_tile_ind(tile_ind_nd2: Union[int, List[int]], tile_pos_yx_nd2: np.ndarray,\n                     tile_pos_yx_npy: np.ndarray) -&gt; Union[int, List[int]]:\n\"\"\"\n    Gets index of tile in npy file from tile index of nd2 file.\n\n    Args:\n        tile_ind_nd2: Index of tile in nd2 file\n        tile_pos_yx_nd2: ```int [n_tiles x 2]```.\n            ```[i,:]``` contains YX position of tile with nd2 index ```i```.\n            Index 0 refers to ```YX = [0, 0]```.\n            Index 1 refers to ```YX = [0, 1] if MaxX &gt; 0```.\n        tile_pos_yx_npy: ```int [n_tiles x 2]```.\n            ```[i,:]``` contains YX position of tile with npy index ```i```.\n            Index 0 refers to ```YX = [MaxY, MaxX]```.\n            Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX &gt; 0```.\n\n    Returns:\n        Corresponding indices in npy file\n    \"\"\"\n    if isinstance(tile_ind_nd2, numbers.Number):\n        tile_ind_nd2 = [tile_ind_nd2]\n    npy_index = numpy_indexed.indices(tile_pos_yx_npy, tile_pos_yx_nd2[tile_ind_nd2]).tolist()\n    if len(npy_index) == 1:\n        return npy_index[0]\n    else:\n        return npy_index\n</code></pre>"},{"location":"code/utils/npy/#coppafish.utils.npy.load_tile","title":"<code>load_tile(nbp_file, nbp_basic, t, r, c, yxz=None, apply_shift=True)</code>","text":"<p>Loads in image corresponding to desired tile, round and channel from the relavent npy file.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>t</code> <code>int</code> <p>npy tile index considering</p> required <code>r</code> <code>int</code> <p>Round considering</p> required <code>c</code> <code>int</code> <p>Channel considering</p> required <code>yxz</code> <code>Optional[Union[List, Tuple, np.ndarray, jnp.ndarray]]</code> <p>If <code>None</code>, whole image is loaded otherwise there are two choices:</p> <ul> <li><code>int [2 or 3]</code>. List containing y,x,z coordinates of sub image to load in.     E.g. if <code>yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])]</code>     returned <code>image</code> will have shape <code>[1 x 3 x 2]</code>.     if <code>yxz = [None, None, z_planes]</code>, all pixels on given z_planes will be returned     i.e. shape of image will be <code>[tile_sz x tile_sz x n_z_planes]</code>.</li> <li><code>int [n_pixels x (2 or 3)]</code>. Array containing yxz coordinates for which the pixel value is desired.     E.g. if <code>yxz = np.ones((10,3))</code>,     returned <code>image</code> will have shape <code>[10,]</code> with all values indicating the pixel value at <code>[1,1,1]</code>.</li> </ul> <code>None</code> <code>apply_shift</code> <code>bool</code> <p>If <code>True</code>, dtype will be <code>int32</code> otherwise dtype will be <code>uint16</code> with the pixels values shifted by <code>+nbp_basic.tile_pixel_value_shift</code>. May want to disable <code>apply_shift</code> to save memory and/or make loading quicker as there will be no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift.</p> <code>True</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int32 [ny x nx (x nz)]</code> or <code>int32 [n_pixels x (2 or 3)]</code> Loaded image.</p> Source code in <code>coppafish/utils/npy.py</code> <pre><code>def load_tile(nbp_file: NotebookPage, nbp_basic: NotebookPage, t: int, r: int, c: int,\n              yxz: Optional[Union[List, Tuple, np.ndarray, jnp.ndarray]] = None,\n              apply_shift: bool = True) -&gt; np.ndarray:\n\"\"\"\n    Loads in image corresponding to desired tile, round and channel from the relavent npy file.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        t: npy tile index considering\n        r: Round considering\n        c: Channel considering\n        yxz: If `None`, whole image is loaded otherwise there are two choices:\n\n            - `int [2 or 3]`. List containing y,x,z coordinates of sub image to load in.\n                E.g. if `yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])]`\n                returned `image` will have shape `[1 x 3 x 2]`.\n                if `yxz = [None, None, z_planes]`, all pixels on given z_planes will be returned\n                i.e. shape of image will be `[tile_sz x tile_sz x n_z_planes]`.\n            - `int [n_pixels x (2 or 3)]`. Array containing yxz coordinates for which the pixel value is desired.\n                E.g. if `yxz = np.ones((10,3))`,\n                returned `image` will have shape `[10,]` with all values indicating the pixel value at `[1,1,1]`.\n        apply_shift: If `True`, dtype will be `int32` otherwise dtype will be `uint16`\n            with the pixels values shifted by `+nbp_basic.tile_pixel_value_shift`.\n            May want to disable `apply_shift` to save memory and/or make loading quicker as there will be\n            no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift.\n\n\n    Returns:\n        `int32 [ny x nx (x nz)]` or `int32 [n_pixels x (2 or 3)]`\n            Loaded image.\n    \"\"\"\n    if yxz is not None:\n        # Use mmap when only loading in part of image\n        if isinstance(yxz, (list, tuple)):\n            if nbp_basic.is_3d:\n                if len(yxz) != 3:\n                    raise ValueError(f'Loading in a 3D tile but dimension of coordinates given is {len(yxz)}.')\n                if yxz[0] is None and yxz[1] is None:\n                    image = np.load(nbp_file.tile[t][r][c], mmap_mode='r')[yxz[2]]\n                    if image.ndim == 3:\n                        image = np.moveaxis(image, 0, 2)\n                else:\n                    coord_index = np.ix_(yxz[0], yxz[1], yxz[2])\n                    image = np.moveaxis(np.load(nbp_file.tile[t][r][c], mmap_mode='r'), 0, 2)[coord_index]\n            else:\n                if len(yxz) != 2:\n                    raise ValueError(f'Loading in a 2D tile but dimension of coordinates given is {len(yxz)}.')\n                coord_index = np.ix_(np.array([c]), yxz[0], yxz[1])  # add channel as first coordinate in 2D.\n                # [0] below is to remove channel index of length 1.\n                image = np.load(nbp_file.tile[t][r], mmap_mode='r')[coord_index][0]\n        elif isinstance(yxz, (np.ndarray, jnp.ndarray)):\n            if nbp_basic.is_3d:\n                if yxz.shape[1] != 3:\n                    raise ValueError(f'Loading in a 3D tile but dimension of coordinates given is {yxz.shape[1]}.')\n                coord_index = tuple(np.asarray(yxz[:, i]) for i in range(3))\n                image = np.moveaxis(np.load(nbp_file.tile[t][r][c], mmap_mode='r'), 0, 2)[coord_index]\n            else:\n                if yxz.shape[1] != 2:\n                    raise ValueError(f'Loading in a 2D tile but dimension of coordinates given is {yxz.shape[1]}.')\n                coord_index = tuple(np.asarray(yxz[:, i]) for i in range(2))\n                coord_index = (np.full(yxz.shape[0], c, int),) + coord_index  # add channel as first coordinate in 2D.\n                image = np.load(nbp_file.tile[t][r], mmap_mode='r')[coord_index]\n        else:\n            raise ValueError(f'yxz should either be an [n_spots x n_dim] array to return an n_spots array indicating '\n                             f'the value of the image at these coordinates or \\n'\n                             f'a list containing {2 + int(nbp_basic.is_3d)} arrays indicating the sub image to load.')\n    else:\n        if nbp_basic.is_3d:\n            # Don't use mmap when loading in whole image\n            image = np.moveaxis(np.load(nbp_file.tile[t][r][c]), 0, 2)\n        else:\n            # Use mmap when only loading in part of image\n            image = np.load(nbp_file.tile[t][r], mmap_mode='r')[c]\n    if apply_shift and not (r == nbp_basic.anchor_round and c == nbp_basic.dapi_channel):\n        image = image.astype(np.int32) - nbp_basic.tile_pixel_value_shift\n    return image\n</code></pre>"},{"location":"code/utils/npy/#coppafish.utils.npy.save_stitched","title":"<code>save_stitched(im_file, nbp_file, nbp_basic, tile_origin, r, c, from_raw=False, zero_thresh=0)</code>","text":"<p>Stitches together all tiles from round <code>r</code>, channel <code>c</code> and saves the resultant compressed npz at <code>im_file</code>. Saved image will be uint16 if from nd2 or from DAPI filtered npy files. Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range.</p> <p>Parameters:</p> Name Type Description Default <code>im_file</code> <code>Optional[str]</code> <p>Path to save file. If <code>None</code>, stitched <code>image</code> is returned (with z axis last) instead of saved.</p> required <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>tile_origin</code> <code>np.ndarray</code> <p><code>float [n_tiles x 3]</code>. yxz origin of each tile on round <code>r</code>.</p> required <code>r</code> <code>int</code> <p>save_stitched will save stitched image of all tiles of round <code>r</code>, channel <code>c</code>.</p> required <code>c</code> <code>int</code> <p>save_stitched will save stitched image of all tiles of round <code>r</code>, channel <code>c</code>.</p> required <code>from_raw</code> <code>bool</code> <p>If <code>False</code>, will stitch together tiles from saved npy files, otherwise will load in raw un-filtered images from nd2/npy file.</p> <code>False</code> <code>zero_thresh</code> <code>int</code> <p>All pixels with absolute value less than or equal to <code>zero_thresh</code> will be set to 0. The larger it is, the smaller the compressed file will be.        save: If True, saves image as im_file, otherwise returns image</p> <code>0</code> Source code in <code>coppafish/utils/npy.py</code> <pre><code>def save_stitched(im_file: Optional[str], nbp_file: NotebookPage, nbp_basic: NotebookPage, tile_origin: np.ndarray,\n                  r: int, c: int, from_raw: bool = False, zero_thresh: int = 0):\n\"\"\"\n    Stitches together all tiles from round `r`, channel `c` and saves the resultant compressed npz at `im_file`.\n    Saved image will be uint16 if from nd2 or from DAPI filtered npy files.\n    Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range.\n\n    Args:\n        im_file: Path to save file.\n            If `None`, stitched `image` is returned (with z axis last) instead of saved.\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        tile_origin: `float [n_tiles x 3]`.\n            yxz origin of each tile on round `r`.\n        r: save_stitched will save stitched image of all tiles of round `r`, channel `c`.\n        c: save_stitched will save stitched image of all tiles of round `r`, channel `c`.\n        from_raw: If `False`, will stitch together tiles from saved npy files,\n            otherwise will load in raw un-filtered images from nd2/npy file.\n        zero_thresh: All pixels with absolute value less than or equal to `zero_thresh` will be set to 0.\n            The larger it is, the smaller the compressed file will be.\\\n        save: If True, saves image as im_file, otherwise returns image\n    \"\"\"\n    yx_origin = np.round(tile_origin[:, :2]).astype(int)\n    z_origin = np.round(tile_origin[:, 2]).astype(int).flatten()\n    yx_size = np.max(yx_origin, axis=0) + nbp_basic.tile_sz\n    if nbp_basic.is_3d:\n        z_size = z_origin.max() + nbp_basic.nz\n        stitched_image = np.zeros(np.append(z_size, yx_size), dtype=np.uint16)\n    else:\n        z_size = 1\n        stitched_image = np.zeros(yx_size, dtype=np.uint16)\n    if from_raw:\n        round_dask_array = utils.raw.load(nbp_file, nbp_basic, r=r)\n        shift = 0  # if from nd2 file, data type is already un-shifted uint16\n    else:\n        if r == nbp_basic.anchor_round and c == nbp_basic.dapi_channel:\n            shift = 0  # if filtered dapi, data type is already un-shifted uint16\n        else:\n            # if from filtered npy files, data type is shifted uint16, want to save stitched as un-shifted int16.\n            shift = nbp_basic.tile_pixel_value_shift\n    if shift != 0:\n        # change dtype to accommodate negative values and set base value to be zero in the shifted image.\n        stitched_image = stitched_image.astype(np.int32) + shift\n    with tqdm(total=z_size * len(nbp_basic.use_tiles)) as pbar:\n        for t in nbp_basic.use_tiles:\n            if from_raw:\n                image_t = utils.raw.load(nbp_file, nbp_basic, round_dask_array, r, t, c, nbp_basic.use_z)\n                # replicate non-filtering procedure in extract_and_filter\n                if not nbp_basic.is_3d:\n                    image_t = extract.focus_stack(image_t)\n                image_t, bad_columns = extract.strip_hack(image_t)  # find faulty columns\n                image_t[:, bad_columns] = 0\n                if nbp_basic.is_3d:\n                    image_t = np.moveaxis(image_t, 2, 0)  # put z-axis back to the start\n            else:\n                if nbp_basic.is_3d:\n                    image_t = np.load(nbp_file.tile[t][r][c], mmap_mode='r')\n                else:\n                    image_t = load_tile(nbp_file, nbp_basic, t, r, c, apply_shift=False)\n            for z in range(z_size):\n                # any tiles not used will be kept as 0.\n                pbar.set_postfix({'tile': t, 'z': z})\n                if nbp_basic.is_3d:\n                    file_z = z - z_origin[t]\n                    if file_z &lt; 0 or file_z &gt;= nbp_basic.nz:\n                        # Set tile to 0 if currently outside its area\n                        local_image = np.zeros((nbp_basic.tile_sz, nbp_basic.tile_sz))\n                    else:\n                        local_image = image_t[file_z]\n                    stitched_image[z, yx_origin[t, 0]:yx_origin[t, 0]+nbp_basic.tile_sz,\n                                   yx_origin[t, 1]:yx_origin[t, 1]+nbp_basic.tile_sz] = local_image\n                else:\n                    stitched_image[yx_origin[t, 0]:yx_origin[t, 0]+nbp_basic.tile_sz,\n                                   yx_origin[t, 1]:yx_origin[t, 1]+nbp_basic.tile_sz] = image_t\n                pbar.update(1)\n    pbar.close()\n    if shift != 0:\n        # remove shift and re-scale so fits the whole int16 range\n        stitched_image = stitched_image - shift\n        stitched_image = stitched_image * np.iinfo(np.int16).max / np.abs(stitched_image).max()\n        stitched_image = np.rint(stitched_image, np.zeros_like(stitched_image, dtype=np.int16), casting='unsafe')\n    if zero_thresh &gt; 0:\n        stitched_image[np.abs(stitched_image) &lt;= zero_thresh] = 0\n\n    if im_file is None:\n        if z_size &gt; 1:\n            stitched_image = np.moveaxis(stitched_image, 0, -1)\n        return stitched_image\n    else:\n        np.savez_compressed(im_file, stitched_image)\n</code></pre>"},{"location":"code/utils/npy/#coppafish.utils.npy.save_tile","title":"<code>save_tile(nbp_file, nbp_basic, image, t, r, c=None)</code>","text":"<p>Wrapper function to save tiles as npy files with correct shift. Moves z-axis to start before saving as it is quicker to load in this order.</p> <p>Parameters:</p> Name Type Description Default <code>nbp_file</code> <code>NotebookPage</code> <p><code>file_names</code> notebook page</p> required <code>nbp_basic</code> <code>NotebookPage</code> <p><code>basic_info</code> notebook page</p> required <code>image</code> <code>np.ndarray</code> <p><code>int32 [ny x nx x nz]</code> or <code>int32 [n_channels x ny x nx]</code>. Image to save.</p> required <code>t</code> <code>int</code> <p>npy tile index considering</p> required <code>r</code> <code>int</code> <p>Round considering</p> required <code>c</code> <code>Optional[int]</code> <p>Channel considering</p> <code>None</code> Source code in <code>coppafish/utils/npy.py</code> <pre><code>def save_tile(nbp_file: NotebookPage, nbp_basic: NotebookPage, image: np.ndarray,\n              t: int, r: int, c: Optional[int] = None):\n\"\"\"\n    Wrapper function to save tiles as npy files with correct shift.\n    Moves z-axis to start before saving as it is quicker to load in this order.\n\n    Args:\n        nbp_file: `file_names` notebook page\n        nbp_basic: `basic_info` notebook page\n        image: `int32 [ny x nx x nz]` or `int32 [n_channels x ny x nx]`.\n            Image to save.\n        t: npy tile index considering\n        r: Round considering\n        c: Channel considering\n    \"\"\"\n    if nbp_basic.is_3d:\n        if c is None:\n            raise ValueError('3d image but channel not given.')\n        if r == nbp_basic.anchor_round and c == nbp_basic.dapi_channel:\n            # If dapi is given then image should already by uint16 so no clipping\n            image = image.astype(np.uint16)\n        else:\n            # need to shift and clip image so fits into uint16 dtype.\n            # clip at 1 not 0 because 0 (or -tile_pixel_value_shift)\n            # will be used as an invalid value when reading in spot_colors.\n            image = np.clip(image + nbp_basic.tile_pixel_value_shift, 1, np.iinfo(np.uint16).max,\n                            np.zeros_like(image, dtype=np.uint16), casting=\"unsafe\")\n        # In 3D, cannot possibly save any un-used channel hence no exception for this case.\n        expected_shape = (nbp_basic.tile_sz, nbp_basic.tile_sz, nbp_basic.nz)\n        if not utils.errors.check_shape(image, expected_shape):\n            raise utils.errors.ShapeError(\"tile to be saved\", image.shape, expected_shape)\n        np.save(nbp_file.tile[t][r][c], np.moveaxis(image, 2, 0))\n    else:\n        if r == nbp_basic.anchor_round:\n            if nbp_basic.anchor_channel is not None:\n                # If anchor round, only shift and clip anchor channel, leave DAPI and un-used channels alone.\n                image[nbp_basic.anchor_channel] = \\\n                    np.clip(image[nbp_basic.anchor_channel] + nbp_basic.tile_pixel_value_shift, 1,\n                            np.iinfo(np.uint16).max, image[nbp_basic.anchor_channel])\n            image = image.astype(np.uint16)\n            use_channels = [val for val in [nbp_basic.dapi_channel, nbp_basic.anchor_channel] if val is not None]\n        else:\n            image = np.clip(image + nbp_basic.tile_pixel_value_shift, 1, np.iinfo(np.uint16).max,\n                            np.zeros_like(image, dtype=np.uint16), casting=\"unsafe\")\n            use_channels = nbp_basic.use_channels\n        # set un-used channels to be 0, not clipped to 1.\n        image[np.setdiff1d(np.arange(nbp_basic.n_channels), use_channels)] = 0\n\n        expected_shape = (nbp_basic.n_channels, nbp_basic.tile_sz, nbp_basic.tile_sz)\n        if not utils.errors.check_shape(image, expected_shape):\n            raise utils.errors.ShapeError(\"tile to be saved\", image.shape, expected_shape)\n        np.save(nbp_file.tile[t][r], image)\n</code></pre>"},{"location":"code/utils/pciseq/","title":"pciSeq","text":""},{"location":"code/utils/pciseq/#coppafish.utils.pciseq.export_to_pciseq","title":"<code>export_to_pciseq(nb)</code>","text":"<p>This saves .csv files containing plotting information for pciseq-</p> <ul> <li>y - y coordinate of each spot in stitched coordinate system.</li> <li>x - x coordinate of each spot in stitched coordinate system.</li> <li>z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels).</li> <li>Gene - Name of gene each spot was assigned to.</li> </ul> <p>Only spots which pass <code>quality_threshold</code> are saved. This depends on parameters given in <code>config['thresholds']</code>.</p> <p>One .csv file is saved for each method: omp and ref_spots if the notebook contains both pages. Also adds the thresholds page to the notebook and re-saves. This is so the thresholds section in the config file cannot be further changed.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook for the experiment containing at least the ref_spots page.</p> required Source code in <code>coppafish/utils/pciseq.py</code> <pre><code>def export_to_pciseq(nb: Notebook):\n\"\"\"\n    This saves .csv files containing plotting information for pciseq-\n\n    - y - y coordinate of each spot in stitched coordinate system.\n    - x - x coordinate of each spot in stitched coordinate system.\n    - z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels).\n    - Gene - Name of gene each spot was assigned to.\n\n    Only spots which pass `quality_threshold` are saved.\n    This depends on parameters given in `config['thresholds']`.\n\n    One .csv file is saved for each method: *omp* and *ref_spots* if the notebook contains\n    both pages.\n    Also adds the *thresholds* page to the notebook and re-saves.\n    This is so the *thresholds* section in the config file cannot be further changed.\n\n    Args:\n        nb: Notebook for the experiment containing at least the *ref_spots* page.\n\n    \"\"\"\n    page_names = ['omp', 'ref_spots']\n    method = ['omp', 'anchor']  # for calling qual_ok\n    files_saved = 0\n    for i in range(2):\n        if not nb.has_page(page_names[i]):\n            warnings.warn(f'No file saved for method {method[i]} as notebook does not have a {page_names[i]} page.')\n            continue\n        if os.path.isfile(nb.file_names.pciseq[i]):\n            warnings.warn(f\"File {nb.file_names.pciseq[i]} already exists\")\n            continue\n        qual_ok = quality_threshold(nb, method[i])  # only keep spots which pass quality thresholding\n        # get coordinates in stitched image\n        global_spot_yxz = nb.__getattribute__(page_names[i]).local_yxz + \\\n                          nb.stitch.tile_origin[nb.__getattribute__(page_names[i]).tile]\n        spot_gene = nb.call_spots.gene_names[nb.__getattribute__(page_names[i]).gene_no[qual_ok]]\n        global_spot_yxz = global_spot_yxz[qual_ok]\n        df_to_export = pd.DataFrame(data=global_spot_yxz, index=spot_gene, columns=['y', 'x', 'z_stack'])\n        df_to_export['Gene'] = df_to_export.index\n        df_to_export.to_csv(nb.file_names.pciseq[i], index=False)\n        print(f'pciSeq file saved for method = {method[i]}: ' + nb.file_names.pciseq[i])\n        files_saved += 1\n\n    if files_saved &gt; 0:\n        # If saved any files, add thresholds page to notebook so cannot make any further changes to\n        # config - will trigger save\n        if not nb.has_page('thresholds'):\n            nbp_thresholds = get_thresholds_page(nb)\n            nb += nbp_thresholds\n        else:\n            warnings.warn('thresholds', utils.warnings.NotebookPageWarning)\n    else:\n        warnings.warn(f\"No files saved\")\n</code></pre>"},{"location":"code/utils/pciseq/#coppafish.utils.pciseq.get_thresholds_page","title":"<code>get_thresholds_page(nb)</code>","text":"<p>Makes notebook page from thresholds section of config file.</p> <p>Parameters:</p> Name Type Description Default <code>nb</code> <code>Notebook</code> <p>Notebook containing all experiment information.</p> required <p>Returns:</p> Type Description <code>NotebookPage</code> <p>thresholds NotebookPage.</p> Source code in <code>coppafish/utils/pciseq.py</code> <pre><code>def get_thresholds_page(nb: Notebook) -&gt; NotebookPage:\n\"\"\"\n    Makes notebook page from thresholds section of config file.\n\n    Args:\n        nb: Notebook containing all experiment information.\n\n    Returns:\n        thresholds NotebookPage.\n    \"\"\"\n    config = nb.get_config()['thresholds']\n    if config['intensity'] is None:\n        config['intensity'] = nb.call_spots.gene_efficiency_intensity_thresh\n    nbp = NotebookPage('thresholds')\n    nbp.intensity = config['intensity']\n    nbp.score_ref = config['score_ref']\n    nbp.score_omp = config['score_omp']\n    nbp.score_omp_multiplier = config['score_omp_multiplier']\n    return nbp\n</code></pre>"},{"location":"code/utils/spot_images/","title":"Spot images","text":""},{"location":"code/utils/spot_images/#coppafish.utils.spot_images.get_average_spot_image","title":"<code>get_average_spot_image(spot_images, av_type='mean', symmetry=None, annulus_width=1.0)</code>","text":"<p>Given an array of spot images, this returns the average spot image.</p> <p>Parameters:</p> Name Type Description Default <code>spot_images</code> <code>np.ndarray</code> <p><code>float [n_peaks x y_shape x x_shape (x z_shape)]</code>. <code>spot_images[s]</code> is the small image surrounding spot <code>s</code>. Any nan values will be ignored when computing the average spot image.</p> required <code>av_type</code> <code>str</code> <p>Optional, one of the following indicating which average to use:</p> <ul> <li><code>'mean'</code></li> <li><code>'median'</code></li> </ul> <code>'mean'</code> <code>symmetry</code> <code>Optional[str]</code> <p>Optional, one of the following:</p> <ul> <li><code>None</code> - Just finds mean at every pixel.</li> <li><code>'quadrant_2d'</code> - Assumes each quadrant of each z-plane expected to look the same so concatenates     these.</li> <li><code>'annulus_2d'</code> - assumes each z-plane is circularly symmetric about central pixel.     I.e. only finds only pixel value from all pixels a certain distance from centre.</li> <li><code>'annulus_3d'</code> - Same as <code>'annulus_2d'</code>, except now z-planes are symmetric about the mid-plane.     I.e. <code>av_image[:,:,mid-i] = av_image[:,:,mid+i]</code> for all <code>i</code>.</li> </ul> <code>None</code> <code>annulus_width</code> <code>float</code> <p>If <code>symmetry = 'annulus'</code>, this specifies how big an annulus to use, within which we expect all pixel values to be the same.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [y_shape x x_shape (x z_shape)]</code>. Average small image about a spot.</p> Source code in <code>coppafish/utils/spot_images.py</code> <pre><code>def get_average_spot_image(spot_images: np.ndarray, av_type: str = 'mean', symmetry: Optional[str] = None,\n                           annulus_width: float = 1.0) -&gt; np.ndarray:\n\"\"\"\n    Given an array of spot images, this returns the average spot image.\n\n    Args:\n        spot_images: ```float [n_peaks x y_shape x x_shape (x z_shape)]```.\n            ```spot_images[s]``` is the small image surrounding spot ```s```.\n            Any nan values will be ignored when computing the average spot image.\n        av_type: Optional, one of the following indicating which average to use:\n\n            - ```'mean'```\n            - ```'median'```\n        symmetry: Optional, one of the following:\n\n            - ```None``` - Just finds mean at every pixel.\n            - ```'quadrant_2d'``` - Assumes each quadrant of each z-plane expected to look the same so concatenates\n                these.\n            - ```'annulus_2d'``` - assumes each z-plane is circularly symmetric about central pixel.\n                I.e. only finds only pixel value from all pixels a certain distance from centre.\n            - ```'annulus_3d'``` - Same as ```'annulus_2d'```, except now z-planes are symmetric about the mid-plane.\n                I.e. `av_image[:,:,mid-i] = av_image[:,:,mid+i]` for all `i`.\n        annulus_width: If ```symmetry = 'annulus'```, this specifies how big an annulus to use,\n            within which we expect all pixel values to be the same.\n\n    Returns:\n        ```float [y_shape x x_shape (x z_shape)]```. Average small image about a spot.\n    \"\"\"\n    # avoid nan in average because some spot_images may have nans because the image ran out of bounds of the tile.\n    if av_type == 'mean':\n        av_func = lambda x, axis: np.nanmean(x, axis)\n    elif av_type == 'median':\n        av_func = lambda x, axis: np.nanmedian(x, axis)\n    else:\n        raise ValueError(f\"av_type must be 'mean' or 'median' but value given was {av_type}\")\n\n    mid_index = np.ceil(np.array(spot_images.shape[1:]) / 2).astype(int) - 1\n\n    if symmetry is None:\n        av_image = av_func(spot_images, 0)\n    elif symmetry == \"quadrant_2d\":\n        # rotate all quadrants so spot is at bottom right corner\n        quad1 = spot_images[:, 0:mid_index[0] + 1, 0:mid_index[1] + 1]\n        quad2 = np.rot90(spot_images[:, 0:mid_index[0] + 1, mid_index[1]:], 1, axes=(1, 2))\n        quad3 = np.rot90(spot_images[:, mid_index[0]:, mid_index[1]:], 2, axes=(1, 2))\n        quad4 = np.rot90(spot_images[:, mid_index[0]:, 0:mid_index[1] + 1], 3, axes=(1, 2))\n        all_quads = np.concatenate((quad1, quad2, quad3, quad4))\n        av_quad = av_func(all_quads, 0)\n        if spot_images.ndim == 4:\n            av_image = np.pad(av_quad, [[0, mid_index[0] + 1], [0, mid_index[1] + 1], [0, 0]], 'symmetric')\n        else:\n            av_image = np.pad(av_quad, [[0, mid_index[0] + 1], [0, mid_index[1] + 1]], 'symmetric')\n        # remove repeated central column and row\n        av_image = np.delete(av_image, mid_index[0] + 1, axis=0)\n        av_image = np.delete(av_image, mid_index[1] + 1, axis=1)\n    elif symmetry == \"annulus_2d\" or symmetry == \"annulus_3d\":\n        X, Y = np.meshgrid(np.arange(spot_images.shape[1]) - mid_index[0],\n                           np.arange(spot_images.shape[2]) - mid_index[1])\n        d = np.sqrt(X ** 2 + Y ** 2)\n        annulus_bins = np.arange(0, d.max(), annulus_width)\n        # find which bin each pixel should contribute to.\n        bin_index = np.abs(np.expand_dims(d, 2) - annulus_bins).argmin(axis=2)\n        av_image = np.zeros_like(spot_images[0])\n        if symmetry == \"annulus_3d\":\n            if spot_images.ndim != 4:\n                raise ValueError(\"Must give 3D images with symmetry = 'annulus_3d'\")\n            n_z = spot_images.shape[3]\n            if n_z % 2 == 0:\n                raise ValueError(\"Must have odd number of z-planes with symmetry = 'annulus_3d'\")\n            # ensure each z-plane has unique set of indices so can average each separately.\n            bin_index = np.tile(np.expand_dims(bin_index, 2), [1, 1, n_z])\n            for i in range(mid_index[2]):\n                current_max_index = bin_index[:, :, mid_index[2] - i].max()\n                bin_index[:, :, mid_index[2] - i - 1] = bin_index[:, :, mid_index[2]] + current_max_index + 1\n                bin_index[:, :, mid_index[2] + i + 1] = bin_index[:, :, mid_index[2] - i - 1]\n        for i in np.unique(bin_index):\n            current_bin = bin_index == i\n            av_image[current_bin] = av_func(spot_images[:, current_bin], (0, 1))\n    else:\n        raise ValueError(f\"symmetry must be None, 'quadrant_2d', 'annulus_2d' or 'annulus_3d' but value given was \"\n                         f\"{symmetry}\")\n\n    if symmetry is not None:\n        is_odd = (np.array(spot_images.shape[1:3]) % 2).astype(bool)\n        if not is_odd.all():\n            warnings.warn(f\"spot_images shape is {av_image.shape} which is even in some dimensions.\"\n                          f\"\\nThis means centre of symmetry will be off-centre.\")\n    return av_image\n</code></pre>"},{"location":"code/utils/spot_images/#coppafish.utils.spot_images.get_spot_images","title":"<code>get_spot_images(image, spot_yxz, shape)</code>","text":"<p>Builds an image around each spot of size given by shape and returns array containing all of these.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p><code>float [nY x nX (x nZ)]</code>. Image that spots were found on.</p> required <code>spot_yxz</code> <code>np.ndarray</code> <p><code>int [n_peaks x image.ndim]</code>. yx or yxz location of spots found.</p> required <code>shape</code> <code>Union[np.ndarray, List[int]]</code> <p><code>int [image.ndim]</code> <code>[y_shape, x_shape, (z_shape)]</code>: Desired size of image for each spot in each direction.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [n_peaks x y_shape x x_shape (x z_shape)]</code>. <code>[s]</code> is the small image surrounding spot <code>s</code>.</p> Source code in <code>coppafish/utils/spot_images.py</code> <pre><code>def get_spot_images(image: np.ndarray, spot_yxz: np.ndarray, shape: Union[np.ndarray, List[int]]) -&gt; np.ndarray:\n\"\"\"\n    Builds an image around each spot of size given by shape and returns array containing all of these.\n\n    Args:\n        image: ```float [nY x nX (x nZ)]```.\n            Image that spots were found on.\n        spot_yxz: ```int [n_peaks x image.ndim]```.\n            yx or yxz location of spots found.\n        shape: ```int [image.ndim]```\n            ```[y_shape, x_shape, (z_shape)]```: Desired size of image for each spot in each direction.\n\n    Returns:\n        ```float [n_peaks x y_shape x x_shape (x z_shape)]```. ```[s]``` is the small image surrounding spot ```s```.\n    \"\"\"\n    if min(np.array(shape) % 2) == 0:\n        raise ValueError(f\"Require shape to be odd in each dimension but given shape was {shape}.\")\n    mid_index = np.ceil(np.array(shape) / 2).astype(\n        int) - 1  # index in spot_images where max intensity is for each spot.\n    spot_images = np.empty((spot_yxz.shape[0], *shape))\n    spot_images[:] = np.nan  # set to nan if spot image goes out of bounds of image.\n    max_image_index = np.array(image.shape)\n    n_spots = spot_yxz.shape[0]\n    no_verbose = n_spots &lt; 6000 / len(shape)  # show progress bar with lots of pixels.\n    with tqdm(total=n_spots, disable=no_verbose) as pbar:\n        pbar.set_description(\"Loading in spot images from tiff files\")\n        for s in range(n_spots):\n            min_pos = np.clip((spot_yxz[s] - mid_index), 0, max_image_index)\n            max_pos = np.clip((spot_yxz[s] + mid_index + 1), 0, max_image_index)\n            spot_images_min_index = mid_index - (spot_yxz[s] - min_pos)\n            spot_images_max_index = mid_index + (max_pos - spot_yxz[s])\n            if len(shape) == 2:\n                small_im = image[min_pos[0]:max_pos[0], min_pos[1]:max_pos[1]]\n                spot_images[s, spot_images_min_index[0]:spot_images_max_index[0],\n                spot_images_min_index[1]:spot_images_max_index[1]] = small_im\n            elif len(shape) == 3:\n                small_im = image[min_pos[0]:max_pos[0], min_pos[1]:max_pos[1], min_pos[2]:max_pos[2]]\n                spot_images[s, spot_images_min_index[0]:spot_images_max_index[0],\n                spot_images_min_index[1]:spot_images_max_index[1],\n                spot_images_min_index[2]:spot_images_max_index[2]] = small_im\n            pbar.update(1)\n    pbar.close()\n    return spot_images\n</code></pre>"},{"location":"code/utils/strel/","title":"Strel","text":""},{"location":"code/utils/strel/#coppafish.utils.strel.annulus","title":"<code>annulus(r0, r_xy, r_z=None)</code>","text":"<p>Gets structuring element used to assess if spot isolated.</p> <p>Parameters:</p> Name Type Description Default <code>r0</code> <code>float</code> <p>Inner radius within which values are all zero.</p> required <code>r_xy</code> <code>float</code> <p>Outer radius in xy direction. Can be float not integer because all values with <code>radius &lt; r_xy1</code> and <code>&gt; r0</code> will be set to <code>1</code>.</p> required <code>r_z</code> <code>Optional[float]</code> <p>Outer radius in z direction. Size in z-pixels. None means 2D annulus returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1]</code>. Structuring element with each element either <code>0</code> or <code>1</code>.</p> Source code in <code>coppafish/utils/strel.py</code> <pre><code>def annulus(r0: float, r_xy: float, r_z: Optional[float] = None) -&gt; np.ndarray:\n\"\"\"\n    Gets structuring element used to assess if spot isolated.\n\n    Args:\n        r0: Inner radius within which values are all zero.\n        r_xy: Outer radius in xy direction.\n            Can be float not integer because all values with `radius &lt; r_xy1` and `&gt; r0` will be set to `1`.\n        r_z: Outer radius in z direction. Size in z-pixels.\n            None means 2D annulus returned.\n\n    Returns:\n        `int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1]`.\n            Structuring element with each element either `0` or `1`.\n    \"\"\"\n    r_xy1_int = floor(r_xy)\n    if r_z is None:\n        y, x = np.meshgrid(np.arange(-r_xy1_int, r_xy1_int + 1), np.arange(-r_xy1_int, r_xy1_int + 1))\n        m = x ** 2 + y ** 2\n    else:\n        r_z1_int = floor(r_z)\n        y, x, z = np.meshgrid(np.arange(-r_xy1_int, r_xy1_int + 1), np.arange(-r_xy1_int, r_xy1_int + 1),\n                              np.arange(-r_z1_int, r_z1_int + 1))\n        m = x ** 2 + y ** 2 + z ** 2\n    # only use upper radius in xy direction as z direction has different pixel size.\n    annulus = r_xy ** 2 &gt;= m\n    annulus = np.logical_and(annulus, m &gt; r0 ** 2)\n    return annulus.astype(int)\n</code></pre>"},{"location":"code/utils/strel/#coppafish.utils.strel.disk","title":"<code>disk(r, n=4)</code>","text":"<p>Creates a flat disk-shaped structuring element with the specified radius, <code>r</code>.</p> <p><code>r</code> must be a nonnegative integer.</p> <p><code>n</code> must be <code>0, 4, 6, or 8</code>. When <code>n</code> is greater than <code>0</code>, the disk-shaped structuring element is approximated by a sequence of <code>n</code> (or sometimes <code>n+2</code>) periodic-line structuring elements. When <code>n</code> is <code>0</code>, no approximation is used, and the structuring element members comprise all pixels whose centers are no greater than <code>r</code> away from the origin.  <code>n</code> can be omitted, in which case its default value is <code>4</code>.</p> <p>Note</p> <p>Morphological operations using disk approximations (<code>n&gt;0</code>) run much faster than when <code>n=0</code>. Also, the structuring elements resulting from choosing <code>n&gt;0</code> are suitable for computing granulometries, which is not the case for <code>vn=0</code>.  Sometimes it is necessary for STREL to use two extra line structuring elements in the approximation, in which case the number of decomposed structuring elements used is <code>n+2</code>.</p> <p>Copy of MATLAB <code>strel('disk')</code>.</p> Source code in <code>coppafish/utils/strel.py</code> <pre><code>def disk(r: int, n: int = 4) -&gt; np.ndarray:\n\"\"\"\n    Creates a flat disk-shaped structuring element with the specified radius, `r`.\n\n    `r` must be a nonnegative integer.\n\n    `n` must be `0, 4, 6, or 8`.\n    When `n` is greater than `0`, the disk-shaped structuring\n    element is approximated by a sequence of `n` (or sometimes `n+2`)\n    periodic-line structuring elements.\n    When `n` is `0`, no approximation is used, and the structuring element members comprise all pixels whose\n    centers are no greater than `r` away from the origin.  `n` can be omitted, in which case its default value is `4`.\n\n    !!! note\n        Morphological operations using disk approximations (`n&gt;0`) run much faster than when `n=0`.\n        Also, the structuring elements resulting from choosing `n&gt;0` are suitable for\n        computing granulometries, which is not the case for `vn=0`.  Sometimes it\n        is necessary for STREL to use two extra line structuring elements in the\n        approximation, in which case the number of decomposed structuring\n        elements used is `n+2`.\n\n    Copy of MATLAB `strel('disk')`.\n    \"\"\"\n    if r &lt; 3:\n        # Radius is too small to use decomposition, so force n=0.\n        n = 0\n    if n == 0:\n        xx, yy = np.meshgrid(np.arange(-r, r + 1), np.arange(-r, r + 1))\n        nhood = xx ** 2 + yy ** 2 &lt;= r ** 2\n    else:\n        # Reference for radial decomposition of disks:  Rolf Adams, \"Radial\n        # Decomposition of Discs and Spheres,\" CVGIP:  Graphical Models and\n        # Image Processing, vol. 55, no. 5, September 1993, pp. 325-332.\n        #\n        # The specific decomposition technique used here is radial\n        # decomposition using periodic lines.  The reference is:  Ronald\n        # Jones and Pierre Soille, \"Periodic lines: Definition, cascades, and\n        # application to granulometries,\" Pattern Recognition Letters,\n        # vol. 17, 1996, pp. 1057-1063.\n        #\n        # Determine the set of \"basis\" vectors to be used for the\n        # decomposition.  The rows of v will be used as offset vectors for\n        # periodic line strels.\n        if n == 4:\n            v = np.array([[1, 0], [1, 1], [0, 1], [-1, 1]])\n        elif n == 6:\n            v = np.array([[1, 0], [1, 2], [2, 1], [0, 1], [-1, 2], [-2, 1]])\n        elif n == 8:\n            v = np.array([[1, 0], [2, 1], [1, 1], [1, 2], [0, 1], [-1, 2], [-1, 1], [-2, 1]])\n        else:\n            raise ValueError(f'Value of n provided ({n}) is not 0, 4, 6 or 8.')\n        # Determine k, which is the desired radial extent of the periodic\n        # line strels.  For the origin of this formula, see the second\n        # paragraph on page 328 of the Rolf Adams paper.\n        theta = np.pi / (2 * n)\n        k = 2 * r / (1 / np.tan(theta) + 1 / np.sin(theta))\n\n        # For each periodic line strel, determine the repetition parameter,\n        # rp.  The use of floor() in the computation means that the resulting\n        # strel will be a little small, but we will compensate for this\n        # below.\n        nhood = np.ones((2 * r - 1, 2 * r - 1), np.uint8) * -np.inf\n        nhood[int((nhood.shape[0] - 1) / 2), int((nhood.shape[0] - 1) / 2)] = 1\n        for q in range(n):\n            rp = int(np.floor(k / np.linalg.norm(v[q, :])))\n            decomposition = periodic_line(rp, v[q, :])\n            nhood = dilate(nhood, decomposition)\n        nhood = nhood &gt; 0\n\n        # Now we are going to add additional vertical and horizontal line\n        # strels to compensate for the fact that the strel resulting from the\n        # above decomposition tends to be smaller than the desired size.\n        extra_strel_size = int(sum(np.sum(nhood, axis=1) == 0) + 1)\n        if extra_strel_size &gt; 0:\n            # Update the computed neighborhood to reflect the additional strels in\n            # the decomposition.\n            nhood = cv2.dilate(nhood.astype(np.uint8), np.ones((1, extra_strel_size), dtype=np.uint8))\n            nhood = cv2.dilate(nhood, np.ones((extra_strel_size, 1), dtype=np.uint8))\n            nhood = nhood &gt; 0\n    return nhood.astype(int)\n</code></pre>"},{"location":"code/utils/strel/#coppafish.utils.strel.disk_3d","title":"<code>disk_3d(r_xy, r_z)</code>","text":"<p>Gets structuring element used to find spots when dilated with 3d image.</p> <p>Parameters:</p> Name Type Description Default <code>r_xy</code> <code>float</code> <p>Radius in xy direction.</p> required <code>r_z</code> <code>float</code> <p>Radius in z direction.</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>int [2*r_xy+1, 2*r_xy+1, 2*r_z+1]</code>. Structuring element with each element either <code>0</code> or <code>1</code>.</p> Source code in <code>coppafish/utils/strel.py</code> <pre><code>def disk_3d(r_xy: float, r_z: float) -&gt; np.ndarray:\n\"\"\"\n    Gets structuring element used to find spots when dilated with 3d image.\n\n    Args:\n        r_xy: Radius in xy direction.\n        r_z: Radius in z direction.\n\n    Returns:\n        `int [2*r_xy+1, 2*r_xy+1, 2*r_z+1]`.\n            Structuring element with each element either `0` or `1`.\n    \"\"\"\n    y, x, z = np.meshgrid(np.arange(-np.ceil(r_xy), np.ceil(r_xy) + 1), np.arange(-np.ceil(r_xy), np.ceil(r_xy) + 1),\n                          np.arange(-np.ceil(r_z), np.ceil(r_z) + 1))\n    se = x ** 2 + y ** 2 + z ** 2 &lt;= r_xy ** 2\n    # Crop se to remove zeros at extremities\n    se = se[:, :, ~np.all(se == 0, axis=(0, 1))]\n    se = se[:, ~np.all(se == 0, axis=(0, 2)), :]\n    se = se[~np.all(se == 0, axis=(1, 2)), :, :]\n    return se.astype(int)\n</code></pre>"},{"location":"code/utils/strel/#coppafish.utils.strel.fspecial","title":"<code>fspecial(r_y, r_x=None, r_z=None)</code>","text":"<p>Creates an ellipsoidal 3D filter kernel if <code>r_y</code>, <code>r_x</code> and <code>r_z</code> given. Copy of MATlAB <code>fspecial3('ellipsoid')</code>.</p> <p>Creates a disk 2D filter kernel if just <code>r_y</code> given. Copy of MATlAB <code>fspecial('disk')</code>.</p> <p>Parameters:</p> Name Type Description Default <code>r_y</code> <code>float</code> <p>Radius in y direction or radius of disk if only parameter provided.</p> required <code>r_x</code> <code>Optional[float]</code> <p>Radius in x direction.</p> <code>None</code> <code>r_z</code> <code>Optional[float]</code> <p>Radius in z direction.</p> <code>None</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p><code>float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1]</code>. Filtering kernel.</p> Source code in <code>coppafish/utils/strel.py</code> <pre><code>def fspecial(r_y: float, r_x: Optional[float] = None, r_z: Optional[float] = None) -&gt; np.ndarray:\n\"\"\"\n    Creates an ellipsoidal 3D filter kernel if `r_y`, `r_x` and `r_z` given.\n    Copy of MATlAB `fspecial3('ellipsoid')`.\n\n    Creates a disk 2D filter kernel if just `r_y` given. Copy of MATlAB `fspecial('disk')`.\n\n    Args:\n        r_y: Radius in y direction or radius of disk if only parameter provided.\n        r_x: Radius in x direction.\n        r_z: Radius in z direction.\n\n    Returns:\n        `float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1]`.\n            Filtering kernel.\n    \"\"\"\n    if r_x is None and r_z is None:\n        r = r_y\n        crad = ceil(r - 0.5)\n        x, y = np.meshgrid(np.arange(-crad, crad + 1), np.arange(-crad, crad + 1))\n        max_xy = np.maximum(np.abs(x), np.abs(y))\n        min_xy = np.minimum(np.abs(x), np.abs(y))\n        m1 = (r ** 2 &lt; (max_xy + 0.5) ** 2 + (min_xy - 0.5) ** 2) * (min_xy - 0.5) + \\\n             (r ** 2 &gt;= (max_xy + 0.5) ** 2 + (min_xy - 0.5) ** 2) * np.sqrt(r ** 2 - (max_xy + 0.5) ** 2,\n                                                                             dtype=np.complex_)\n        m1 = np.real(m1)\n        m2 = (r ** 2 &gt; (max_xy - 0.5) ** 2 + (min_xy + 0.5) ** 2) * (min_xy + 0.5) + \\\n             (r ** 2 &lt;= (max_xy - 0.5) ** 2 + (min_xy + 0.5) ** 2) * np.sqrt(r ** 2 - (max_xy - 0.5) ** 2,\n                                                                             dtype=np.complex_)\n        m2 = np.real(m2)\n        sgrid = (r ** 2 * (0.5 * (np.arcsin(m2 / r) - np.arcsin(m1 / r)) +\n                           0.25 * (np.sin(2 * np.arcsin(m2 / r)) - np.sin(2 * np.arcsin(m1 / r)))) - (max_xy - 0.5) * (\n                             m2 - m1) +\n                 (m1 - min_xy + 0.5)) * ((((r ** 2 &lt; (max_xy + 0.5) ** 2 + (min_xy + 0.5) ** 2) &amp;\n                                           (r ** 2 &gt; (max_xy - 0.5) ** 2 + (min_xy - 0.5) ** 2)) |\n                                          ((min_xy == 0) &amp; (max_xy - 0.5 &lt; r) &amp; (max_xy + 0.5 &gt;= r))))\n        sgrid = sgrid + ((max_xy + 0.5) ** 2 + (min_xy + 0.5) ** 2 &lt; r ** 2)\n        sgrid[crad, crad] = min(np.pi * r ** 2, np.pi / 2)\n        if crad &gt; 0.0 and r &gt; crad - 0.5 and r ** 2 &lt; (crad - 0.5) ** 2 + 0.25:\n            m1 = np.sqrt(r ** 2 - (crad - 0.5) ** 2)\n            m1n = m1 / r\n            sg0 = 2 * (r ** 2 * (0.5 * np.arcsin(m1n) + 0.25 * np.sin(2 * np.arcsin(m1n))) - m1 * (crad - 0.5))\n            sgrid[2 * crad, crad] = sg0\n            sgrid[crad, 2 * crad] = sg0\n            sgrid[crad, 0] = sg0\n            sgrid[0, crad] = sg0\n            sgrid[2 * crad - 1, crad] = sgrid[2 * crad - 1, crad] - sg0\n            sgrid[crad, 2 * crad - 1] = sgrid[crad, 2 * crad - 1] - sg0\n            sgrid[crad, 1] = sgrid[crad, 1] - sg0\n            sgrid[1, crad] = sgrid[1, crad + 1] - sg0\n        sgrid[crad, crad] = min(sgrid[crad, crad], 1)\n        h = sgrid / np.sum(sgrid)\n    else:\n        x, y, z = np.meshgrid(np.arange(-ceil(r_x), ceil(r_x) + 1), np.arange(-ceil(r_y), ceil(r_y) + 1),\n                              np.arange(-ceil(r_z), ceil(r_z) + 1))\n        h = (1 - x ** 2 / r_x ** 2 - y ** 2 / r_y ** 2 - z ** 2 / r_z ** 2) &gt;= 0\n        h = h / np.sum(h)\n    return h\n</code></pre>"},{"location":"code/utils/strel/#coppafish.utils.strel.periodic_line","title":"<code>periodic_line(p, v)</code>","text":"<p>Creates a flat structuring element containing <code>2*p+1</code> members.</p> <p><code>v</code> is a two-element vector containing integer-valued row and column offsets.</p> <p>One structuring element member is located at the origin. The other members are located at <code>1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v</code>.</p> <p>Copy of MATLAB <code>strel('periodicline')</code>.</p> Source code in <code>coppafish/utils/strel.py</code> <pre><code>def periodic_line(p: int, v: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    Creates a flat structuring element containing `2*p+1` members.\n\n    `v` is a two-element vector containing integer-valued row and column offsets.\n\n    One structuring element member is located at the origin.\n    The other members are located at `1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v`.\n\n    Copy of MATLAB `strel('periodicline')`.\n    \"\"\"\n    pp = np.repeat(np.arange(-p, p + 1).reshape(-1, 1), 2, axis=1)\n    rc = pp * v\n    r = rc[:, 0]\n    c = rc[:, 1]\n    M = 2 * np.abs(r).max() + 1\n    N = 2 * np.abs(c).max() + 1\n    nhood = np.zeros((M, N), dtype=bool)\n    # idx = np.ravel_multi_index([r + np.abs(r).max(), c + np.abs(c).max()], (M, N))\n    nhood[r + np.abs(r).max(), c + np.abs(c).max()] = True\n    return nhood.astype(np.uint8)\n</code></pre>"},{"location":"pipeline/call_reference_spots/","title":"Call Reference Spots","text":"<p>The call reference spots step of the pipeline uses the  \\(n_{rounds} \\times n_{channels}\\) <code>color</code> obtained for each reference spot  (detected on the reference round/reference channel, \\(r_{ref}\\)/\\(c_{ref}\\)) in the  get reference spots step of the pipeline to compute the <code>bleed_matrix</code>, accounting for crosstalk between color channels. We then compute the <code>gene_efficiency</code> which allows for varying round strengths for each gene, before assigning each reference spot to a gene.</p> <p>These gene assignments are saved in the <code>ref_spots</code> NotebookPage,  while the <code>bleed_matrix</code> and expected <code>bled_code</code> for each gene are saved in the  <code>call_spots</code> NotebookPage. The distribution of the genes can be  seen using <code>coppafish.Viewer</code> once these pages have been added.</p> <p>Note: <code>config</code> in this section, with no section specified, means <code>config['call_spots']</code></p>"},{"location":"pipeline/call_reference_spots/#re-run-call_reference_spots","title":"Re-run <code>call_reference_spots</code>","text":"<p>To re-run the call reference spots step of the pipeline  of the pipeline with different parameters in the configuration file, the  <code>call_spots</code> page must be deleted and then the <code>run_reference_spots</code>  function must be called with <code>overwrite_ref_spots = True</code>.</p> <p>This is so the variables <code>gene_no</code>, <code>score</code>, <code>score_diff</code>, <code>intensity</code> in the  <code>ref_spots</code> page will be updated.</p> Example <p>The code below illustrates how you can re-run the  call reference spots step of the pipeline step of the pipeline without weighting or <code>gene_efficiency</code>.</p> CodeOutputnb._config (saved to Notebook)/Users/user/coppafish/experiment/settings_new.ini <pre><code>import numpy as np\nfrom coppafish import Notebook\nfrom coppafish.pipeline.run import run_reference_spots\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\n\n# Save new notebook with different name so it does not overwrite old notebook\n# Make sure notebook_name is specified in [file_names] section \n# of settings_new.ini file to be same as name given here.\nnb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz'\nini_file_new = '/Users/user/coppafish/experiment/settings_new.ini'\n\n# config_file not given so will use last one saved to Notebook\nnb = Notebook(nb_file)\nconfig = nb.get_config()['call_spots']\nprint('Using config file saved to notebook:')\nprint(f\"alpha: {config['alpha']}\")\nprint(f\"gene_efficiency_n_iter: {config['gene_efficiency_n_iter']}\")\nprint(f\"First 5 gene assignments: {nb.ref_spots.gene_no[:5]}\")\nprint(f\"First 5 spot scores: {np.around(nb.ref_spots.score[:5], 3)}\")\nprint(f\"Bled Code of Gene 0, Round 0: {np.around(nb.call_spots.bled_codes_ge[0,0], 2)}\")\n\n# Change call_spots\ndel nb.call_spots           # delete old call_spots\nnb.save(nb_file_new)        # save Notebook with no call_spots page to new file \n                            # so does not overwrite old Notebook\n# Load in new notebook with new config file\nnb_new = Notebook(nb_file_new, ini_file_new)\nconfig_new = nb_new.get_config()['call_spots']\n# Show that config params have changed\nprint(f'\\nUsing new config file but before re-running:')\nprint(f\"alpha: {config_new['alpha']}\")\nprint(f\"gene_efficiency_n_iter: {config_new['gene_efficiency_n_iter']}\")\n# Show that ref_spots page variables are still the same\nprint(f\"First 5 gene assignments: {nb_new.ref_spots.gene_no[:5]}\")\nprint(f\"First 5 spot scores: {np.around(nb_new.ref_spots.score[:5], 3)}\")\n\n# Get new call_spots page\nrun_reference_spots(nb_new, overwrite_ref_spots=True)\nprint(f'\\nUsing new config file after re-running:')\nprint(f\"alpha: {config_new['alpha']}\")\nprint(f\"gene_efficiency_n_iter: {config_new['gene_efficiency_n_iter']}\")\n# Show that ref_spots and call_spots page variables have been updated\nprint(f\"First 5 gene assignments: {nb_new.ref_spots.gene_no[:5]}\")\nprint(f\"First 5 spot scores: {np.around(nb_new.ref_spots.score[:5], 3)}\")\nprint(f\"Bled Code of Gene 0, Round 0: {np.around(nb_new.call_spots.bled_codes_ge[0,0], 2)}\")\n</code></pre> <pre><code>Using config file saved to notebook:\nalpha: 120.0\ngene_efficiency_n_iter: 10\nFirst 5 gene assignments: [17 17 17 13 17]\nFirst 5 spot scores: [0.645 0.806 0.702 0.692 0.796]\nBled Code of Gene 0, Round 0: [ 0.04  0.    0.   -0.    0.   -0.   -0.  ]\n\nUsing new config file but before re-running:\nalpha: 0.0\ngene_efficiency_n_iter: 0\nFirst 5 gene assignments: [17 17 17 13 17]\nFirst 5 spot scores: [0.645 0.806 0.702 0.692 0.796]\n\nUsing new config file after re-running:\nalpha: 0.0\ngene_efficiency_n_iter: 0\nFirst 5 gene assignments: [17 17 17 13 17]\nFirst 5 spot scores: [0.65  0.776 0.744 0.65  0.797]\nBled Code of Gene 0, Round 0: [ 0.25  0.    0.03 -0.    0.01 -0.   -0.  ]\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n</code></pre> <pre><code>[file_names]\ninput_dir = /Users/user/coppafish/experiment1/raw\noutput_dir = /Users/user/coppafish/experiment1/output\ntile_dir = /Users/user/coppafish/experiment1/tiles\nround = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6\nanchor = Exp1_anchor\ncode_book = /Users/user/coppafish/experiment1/codebook.txt\nnotebook_name = notebook_new\n\n[basic_info]\nis_3d = True\nanchor_channel = 4\ndapi_channel = 0\n\n[call_spots]\ngene_efficiency_n_iter = 0\nalpha = 0\n</code></pre>"},{"location":"pipeline/call_reference_spots/#color-normalisation","title":"Color Normalisation","text":"<p>We assign a spot \\(s\\) to a gene \\(g\\), based on its \\(n_{rounds} \\times n_{channels}\\) <code>color</code>, \\(\\pmb{\\acute{\\zeta}_s}\\),  indicating the intensity in each round and channel.</p> <p>However, we first need to equalize color channels, so that no one color channel dominates the others when  it comes to gene assignment. The normalised <code>spot_colors</code>, \\(\\pmb{\\zeta}\\), are obtained by dividing the saved  <code>spot_colors</code>, \\(\\pmb{\\acute{\\zeta}}\\), by a \\(n_{rounds} \\times n_{channels}\\)  normalisation factor, <code>nb.call_spots.color_norm_factor</code>.</p> Why do we need to normalise color channels? <p>The gene assignment of the spot below indicates why we need to normalise color channels. With no normalisation, the gene assignment is overly influenced by channel 4, which  is one of the most intense channels (see <code>color_norm_factor</code> in example box below). Thus it matches  to Reln which appears in channel 4 in rounds 0, 1, 5 and 6. It also doesn't care at all about  channel 2 because it is the weakest channel and Reln does not appear in channel 2 in any rounds.</p> <p>With normalisation, the most obvious effect is channel 2 gets boosted and now has an influence. Given that Id2 appears in channel 2 in rounds 1, 3, 5 and 6, if channel 2 was never considered, it would always make the score to Id2 low. When we include channel 2 though, we get a high score even though we are not matching the round 0, channel 4 intensity which contributed to the Reln assignment without normalisation.</p> No NormalisationWith Normalisation <p></p> <p></p> <p>This is obtained  using the parameters <code>config['color_norm_intensities']</code> and  <code>config['color_norm_probs']</code> such that for each round, \\(r\\), and channel \\(c\\),  the probability of \\(\\zeta_{rc}\\) being larger than  <code>config['color_norm_intensities'][i]</code> is less than <code>config['color_norm_probs'][i]</code> for each \\(i\\).</p> <p>The probabilities come from the histograms produced in the extract and filter step i.e. if <code>config['color_norm_intensities'] = 0.5</code> and <code>config['color_norm_probs'] = 0.01</code>, then  in each round and channel, only 1% of pixels on the mid z-plane across all tiles would have \\(\\zeta_{rc} &gt; 0.5\\).</p> <p>If <code>config[bleed_matrix_method] = 'single'</code>, then we combine all rounds for each channel so that  <code>nb.call_spots.color_norm_factor[r, c]</code> is the same for all \\(r\\) of a particular \\(c\\). </p> Example <p>With <code>config['color_norm_intensities'] = 0.5, 1, 5</code> and <code>config['color_norm_probs'] = 0.01, 5e-4, 1e-5</code> and the histograms shown in the extract and filter step,  the two methods of <code>config[bleed_matrix_method]</code> produce the following <code>color_norm_factor</code>:</p> SingleSeparate <p></p> <p></p> <p>The normalised histogram shown was normalised using the Single <code>color_norm_factor</code> and you can see that for each round and channel, there is a similar area under the curve (probability) beyond \\(\\zeta_{rc}=0.5\\), as expected from <code>config['color_norm_intensities']</code>.</p>"},{"location":"pipeline/call_reference_spots/#background","title":"Background","text":"<p>After we have the normalised spot colors, \\(\\pmb{\\zeta}\\), we  remove some background genes from them.  There is one background gene for each channel, \\(\\pmb{B}_C\\). The background gene for channel \\(C\\) is defined by: \\(\\pmb{B}_{C_{rc}}=1\\) if \\(c = C\\) and 0 otherwise i.e. it is just a strip in channel \\(C\\):</p> <p></p> <p>It is also normalised to have an L2 norm of 1. These are saved as <code>nb.call_spots.background_codes</code>.</p> Why do we need to fit background genes? <p>We fit the background genes because it is fairly common for \\(\\pmb{\\zeta}_s\\) to have high intensity  across all rounds of a channel as shown for the example <code>view_codes</code>  plot below:</p> No Background RemovalWith Background Removal <p></p> <p></p> <p>No gene in the codebook looks that much like a background gene but if the background genes have not been fit, as with the first image above, spots like this will match to the gene which has the most  rounds in the relavent channel/s. Here, Thsd7a has intensity in channel 2 in all rounds apart from round 2. This problem will be exacerbated in the omp step, because at each iteration of the OMP  algorithm, it will just try and fit more and more genes to explain the intense channel.</p> <p>If we do remove background though, as with the second image, the gene assignment will be based  on the other channels where not all rounds were intense. In this case, we get a match to Sst due to round 2 and 3 in channel 0.</p> <p>If we look at <code>histogram_score</code> for Thsd7a, we see that without background removal (purple), the peak in score is significantly larger:</p> <p></p> <p>This indicates that without background removal, we would end up with a lot more spots assigned to genes like Thsd7a which have high intensity in most rounds of a single channel.</p> <p>The coefficient, \\(\\mu_{sC}\\), for the channel \\(C\\) background gene, \\(\\pmb{B}_C\\), fit to the spot \\(s\\) color,  \\(\\pmb{\\zeta}_s\\) is:</p> \\[ \\mu_{sC} = \\frac{\\sum_rw^2_{rC}\\zeta_{s_{rC}}B_{C_{rC}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}};  w_{rC} = \\frac{1}{|\\zeta_{s_{rC}}| + \\lambda_b} \\] <p>Where, \\(\\lambda_b\\) is <code>config['background_weight_shift']</code> and the sum is over all rounds used.</p> Value of \\(\\lambda_b\\) <p>If <code>config['background_weight_shift']</code> is left blank, it is set to the median of the <code>intensity</code>  computed from the absolute colors, \\(\\tilde{\\chi}\\),  of all pixels in the middle z-plane (<code>nb.call_spots.norm_shift_tile</code>)  of the central tile (<code>nb.call_spots.norm_shift_z</code>).</p> <p>This is because it gives an estimate of what \\(|\\zeta_{s_{rC}}|\\) would be for an average non-spot pixel. If we set \\(\\lambda_b\\) to be equal to this, it is saying that if a spot had a round and channel with very low intensity, then that round and channel would have as much influence on the final coefficient as an average pixel. </p> <p>If \\(\\lambda_b = 0\\), then \\(w_{rc}\\) would go to \\(\\infty\\) for low intensity rounds and channels, so the value of \\(\\lambda_b\\) chosen also provides an upper bound on the contribution of low intensity  rounds and channels to the final coefficient.</p> <p>If the weighting, \\(\\pmb{w}\\), was a constant across rounds and channels (\\(\\lambda_b = \\infty\\)), this would just be  the least squares solution. After we have found the coefficients, we remove the background contribution from the spot colors to give \\(\\pmb{\\zeta}_{{s0}}\\) which we use from now on.</p> \\[ \\zeta_{{s0}_{rC}} = \\zeta_{{s}_{rC}} - \\mu_{sC}B_{C_{rC}} \\] Why do we need a weighting? <p>We find \\(\\mu_{sC}\\) via weighted least squares, because it limits the influence of outliers.  The example below shows that with \\(\\lambda_b = \\infty\\) (normal least squares), it really tries to remove the outlier in round 4, channel 0. The result of this is that all the other rounds of channel 0 become  very negative.</p> \\(\\pmb{\\zeta}_s\\)\\(\\pmb{\\zeta}_{s0} (\\lambda_b = 0.08)\\)\\(\\pmb{\\zeta}_{s0} (\\lambda_b = \\infty)\\) <p></p> <p></p> <p></p> <p>This spot should be Slc6a1 as shown from the \\(\\lambda_b = 0.08\\) image, but Slc6a1 is expected to be  in 4 of the 6 rounds that were set to negative by the background fitting with \\(\\lambda_b = \\infty\\). Thus, this spot can no longer be assigned to the correct gene after \\(\\lambda_b = \\infty\\) background fitting.</p> <p>So basically, the job of the background fitting is to remove intensity in a particular channel if  that channel is intense across all rounds. This is because there are no actual genes which can explain this. We do not want it to tone down outlier rounds and channels because outliers are usually due to actual genes.</p>"},{"location":"pipeline/call_reference_spots/#view_background","title":"<code>view_background</code>","text":"<p>The background coefficient calculation can be visualised by using the  <code>view_background</code> function:</p> <p></p> <p>For each plot, each row corresponds to a different background gene coefficient calculation i.e. a different  channel. There is no overlap between the background codes hence we can view all the calculations at the same time.</p> <p>Round \\(r\\), channel \\(c\\) in the Weighted Dot Product plot refers to the value of \\(\\frac{w^2_{rc}\\zeta_{s_{rc}}B_{C_{rc}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}}\\).  The Dot Product plot is the same as the Weighted Dot Product plot except \\(\\lambda_b = \\infty\\).</p> <p>The Weighted Coef plot thus shows the coefficient computed for the current value of \\(\\lambda_b\\)  (0.08 here, but can be specified in the textbox). The Coef plot shows the coefficient that would be computed with \\(\\lambda_b = \\infty\\).</p> <p>The main difference between the two in this case is that the channel 0 coefficient is much larger for the \\(\\lambda_b = \\infty\\) case. This is because the Weight Squared, \\(\\Omega^2_{s_{rc}}\\) term acts to increase the contribution of the weak round 1, channel 0 and decrease the contribution of the strong rounds: 0, 2, 3 and 6.</p>"},{"location":"pipeline/call_reference_spots/#bleed-matrix","title":"Bleed Matrix","text":"<p>Crosstalk can occur between color channels. Some crosstalk may occur due to optical bleedthrough;  additional crosstalk can occur due to chemical cross-reactivity of probes. The precise degree of crosstalk does not seem to vary much between sequencing rounds. It is therefore possible to largely compensate for this crosstalk by learning the precise amount of crosstalk  between each pair of color channels.</p> <p>To estimate the crosstalk,  we use the spot colors, \\(\\pmb{\\zeta}_{s0}\\), of all well isolated spots. We  reshape these, so we have a set of \\(n_{isolated} \\times n_{rounds}\\) vectors, each of dimension  \\(n_{channels}\\), \\(\\pmb{v}_i\\) (\\(v_{0_c} = \\zeta_{{s=0,0}_{r=0,c}}\\)). Only well-isolated spots are used to ensure that crosstalk estimation is  not affected by spatial overlap of spots corresponding to different genes.</p> <p>Crosstalk is then estimated by running a  scaled k-means  algorithm on these vectors, which finds a set of \\(n_{dyes}\\) vectors, \\(\\pmb{c}_d\\), such that the error function:</p> \\[ \\sum_i\\min_{\\lambda_i, d(i)}|\\pmb{v}_i - \\lambda_i\\pmb{c}_{d(i)}|^2 \\] <p>is minimized. In other words, it finds the \\(n_{dyes}\\) intensity vectors, \\(\\pmb{c}_d\\), each of dimension \\(n_{channels}\\), such that the spot color of each well isolated spot on every round is close to a scaled version of  one of them. The \\(n_{dyes} \\times n_{channels}\\) array of dye vectors is termed the <code>bleed matrix</code> and is saved as <code>nb.call_spots.bleed_matrix</code> (a <code>bleed_matrix</code>  is saved for each round, but if <code>config['bleed_matrix_method'] = 'single'</code>, it will be the same for each round). The <code>view_bleed_matrix</code> function can be used to show it:</p> Single Bleed MatrixSeparate Bleed Matrix For Each Round <p></p> <p></p> <p>As shown in the second plot, if <code>config['bleed_matrix_method'] = 'separate'</code>, we compute a different  <code>bleed_matrix</code> for each round - i.e. we loosen the assumption that crosstalk does not vary between sequencing rounds.</p>"},{"location":"pipeline/call_reference_spots/#initial-bleed-matrix","title":"Initial Bleed Matrix","text":"<p>To estimate the dye intensity vectors, \\(\\pmb{c}_d\\), the  <code>scaled_k_means</code> algorithm needs to know the number of dyes and a starting guess for what each dye vector looks like.</p> <p>This is specified in the <code>basic_info</code> section of the configuration file as explained  here.</p>"},{"location":"pipeline/call_reference_spots/#scaled-k-means","title":"Scaled K Means","text":"<p>The pseudocode for the <code>scaled_k_means</code> algorithm to obtain the dye intensity vectors, \\(\\pmb{c}_d\\), is given below:</p> <p><pre><code>v: Single round intensity vectors of well isolated spots \n   [n_vectors x n_channels]\nc: Initial Guess of Bleed Matrix\n   [n_dyes x n_channels]\ndye_ind_old: [n_vectors] array of zeros.\n\nv_norm = v normalised so each vector has an L2 norm of 1.\nNormalise c so that each dye vector has an L2 norm of 1.\n\ni = 0\nwhile i &lt; n_iter:\n    score = dot product between each vector in v_norm and each\n        dye in c. [n_vectors x n_dyes]\n    top_score = highest score across dyes for each vector\n        in v_norm. [n_vectors].\n    dye_ind = dye corresponding to top_score for each vector\n        in v_norm. [n_vectors].\n\n    if dye_ind == dye_ind_old:\n        Stop iteration because we have reached convergence\n        i.e. i = n_iter\n    dye_ind_old = dye_ind\n\n    for d in range(n_dyes):\n        v_use = all vectors in v with dye_ind = d and \n            top_score &gt; score_thresh.\n            Use un-normalised as to avoid overweighting weak points.\n            [n_use x n_channels]\n        if n_use &lt; min_size:\n            c[d] = 0\n        else:\n            Update c[d] to be top svd component of v_use i.e.\n                v_matrix = v_use.transpose() @ v_use / n_use\n                    [n_channels x n_channels]\n                c[d] = np.linalg.svd(v_matrix)[0][:, 0]\n                    [n_channels]\n\n    i = i + 1    \nreturn c\n</code></pre> There are a few parameters in the config file which are used:</p> <ul> <li><code>bleed_matrix_n_iter</code>: This is <code>n_iter</code> in the above code.</li> <li><code>bleed_matrix_min_cluster_size</code>: This is <code>min_size</code> in the above code.</li> <li><code>bleed_matrix_score_thresh</code>: This is <code>score_thresh</code> in the above code.</li> <li><code>bleed_matrix_anneal</code>: If this is <code>True</code>, the <code>scaled_k_means</code> algorithm will be run twice.     The second time will use \\(\\pmb{c}_d\\) returned from the first run as the starting point,     and it will have a different <code>score_thresh</code> for each dye. <code>score_thresh</code> for dye \\(d\\)      will be equal to the median of <code>top_score[dye_ind == d]</code> in the last iteration of the first run.     The idea is that for the second run, we only use vectors which have a large score,     to get a more accurate estimate.</li> </ul>"},{"location":"pipeline/call_reference_spots/#view_scaled_k_means","title":"<code>view_scaled_k_means</code>","text":"<p>The <code>scaled_k_means</code> algorithm  can be visualised using the <code>view_scaled_k_means</code> function:</p> <p></p> <p>In each column, in the top row, boxplot \\(d\\) is for <code>top_score[dye_ind == d]</code> (only showing scores above <code>score_thresh</code> - 0 for the first two columns). The dye vectors, \\(\\pmb{c}_d\\), are indicated in the second row. The number of vectors assigned to each dye is indicated by the number within each boxplot.</p> <p>The first column is for the first iteration i.e. with the initial guess of the bleed matrix. The second column is after the first <code>scaled_k_means</code> algorithm has finished. The third column is after the second <code>scaled_k_means</code> algorithm has finished (only shown if <code>bleed_matrix_anneal=True</code>). The bottom whisker of the boxplots in the third column indicate the <code>score_thresh</code> used for each dye.</p> <p>This is useful for debugging the <code>bleed_matrix</code> computation, as you want the boxplots to show high scores and for those scores to increase from left to right as the algorithm is run.</p>"},{"location":"pipeline/call_reference_spots/#gene-bled-codes","title":"Gene Bled Codes","text":"<p>Once the <code>bleed_matrix</code> has been computed, the expected code for each gene can be  obtained.</p> <p>Each gene appears with a single dye in each imaging round as indicated by the codebook and saved as <code>nb.call_spots.gene_codes</code>. <code>bled_code[g, r, c]</code> for gene \\(g\\) in round \\(r\\), channel \\(c\\) is then given by <code>bleed_matrix[r, c, gene_codes[g, r]]</code>. If <code>gene_codes[g, r]</code> is outside <code>nb.basic_info.use_dyes</code>, then <code>bled_code[g, r]</code> will be set to 0.</p> <p>Each <code>bled_code</code> is also normalised to have an L2 norm of 1. They are saved as <code>nb.call_spots.bled_codes</code>.</p> Example <p>Using the Single Bleed Matrix shown as an example earlier, the bled_code for Kcnk2 with <code>gene_code = 6304152</code> is:</p> <p></p>"},{"location":"pipeline/call_reference_spots/#dot-product-score","title":"Dot Product Score","text":"<p>To assign spot \\(s\\), with spot color, \\(\\pmb{\\zeta}_{{si}}\\), to a gene, we compute a dot product score, \\(\\Delta_{sig}\\) to each gene, \\(g\\), with <code>bled_code</code> \\(\\pmb{b}_g\\). This is  defined to be:</p> \\[ \\Delta_{sig} = \\sum_{r=0}^{n_r-1}\\sum_{c=0}^{n_c-1}\\omega^2_{{si}_{rc}}\\tilde{\\zeta}_{{si}_{rc}}b_{g_{rc}} \\] <p>Where:</p> \\[ \\tilde{\\zeta}_{{si}_{rc}} = \\frac{\\zeta_{{si}_{rc}}} {\\sqrt{\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\zeta^2_{{si}_{\\mathscr{rc}}}} + \\lambda_d} \\] \\[ \\sigma^2_{{si}_{rc}} = \\beta^2 + \\alpha\\sum_g\\mu^2_{sig}b^2_{g_{rc}} \\] \\[ \\omega^2_{{si}_{rc}} = n_rn_c\\frac{\\sigma^{-2}_{{si}_{rc}}} {\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\sigma^{-2}_{{si}_{\\mathscr{rc}}}} \\] <ul> <li>\\(n_{r}\\) is the number of rounds.</li> <li>\\(n_{c}\\) is the number of channels.</li> <li>\\(\\lambda_d\\) is <code>config['dp_norm_shift'] * sqrt(n_rounds)</code> (typical <code>config['dp_norm_shift']</code> is 0.1).</li> <li>\\(\\alpha\\) is <code>config['alpha']</code> (default is 120).</li> <li>\\(\\beta\\) is <code>config['beta']</code> (default is 1).</li> <li>The sum over genes, \\(\\sum_g\\), is over all real and background genes i.e. \\(\\sum_{g=0}^{n_g+n_c-1}\\). \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i\\) and \\(\\pmb{b}_{g=n_g+C} = \\pmb{B}_C\\) where \\(\\mu_{sC}\\) and \\(\\pmb{B}_C\\) were introduced in the background section.</li> <li>\\(i\\) refers to the number of actual genes fit prior to this iteration of OMP. Here, because we are only fitting one gene, \\(i=0\\), meaning only background has been fit (\\(\\sum_{g=0}^{n_g-1}\\mu^2_{sig}b^2_{g_{rc}}=0\\)).</li> </ul> <p>So if \\(\\lambda_d = 0\\) and \\(\\pmb{\\omega}^2_{si}=1\\) for each round and channel (achieved through \\(\\alpha=0\\)), then  this would just be the normal dot product between two vectors with L2 norm of one.  The min value is 0 and max value is 1. </p> <p>The purpose of the weighting, \\(\\pmb{\\omega}^2_{{si}}\\), is to decrease the contribution of rounds/channels  which already have a gene in. It is really more relevant to the  OMP algorithm.  It can be turned off in this part of the pipeline by setting <code>config['alpha'] = 0</code>. The \\(n_rn_c\\) term at the start of the \\(\\omega^2_{{si}_{rc}}\\) equation is a normalisation term such that the max possible value of \\(\\Delta_{sig}\\) is approximately 1 (it can be more though).</p> Value of \\(\\lambda_d\\) <p>If in the \\(\\Delta_{sig}\\) equation, we used \\(\\pmb{\\zeta}_{{si}}\\) instead of \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\), then the max value would no longer have an upper limit and a high score could be achieved by having  large values of \\(\\pmb{\\zeta}_{{si}}\\) in some rounds and channels as well as by having \\(\\pmb{\\zeta}_{{si}}\\) being similar to \\(\\pmb{b}_g\\). </p> <p>We use the intensity value to indicate the strength of the spot, so for \\(\\Delta_{sig}\\), we  really just want a variable which indicates how alike the spot color is to the gene, indepenent of strength. Hence, we use \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\).</p> <p>If \\(\\lambda_d = 0\\), it would mean that even background pixels with very small intensity could get a high score. So, we use a non-zero value of \\(\\lambda_d\\) to prevent very weak spots getting a large \\(\\Delta_{sig}\\).</p> <p>If <code>config['dp_norm_shift']</code> is not specified, it is set to the median of the L2 norm in a single round computed from the colors of all pixels in the middle z-plane  (<code>nb.call_spots.norm_shift_tile</code>) of the central tile (<code>nb.call_spots.norm_shift_z</code>).</p> <p>The idea behind this, is that the L2 norm of an average background pixel would be  <code>config['dp_norm_shift'] * sqrt(n_rounds)</code>. So if \\(\\lambda_d\\) was set to this, it is giving  a penalty to any spot which is less intense than the average background pixel. This is  desirable since any pixels of such low intensity are unlikely to be spots.</p> <p>The spot \\(s\\), is assigned to the gene \\(g\\), for which \\(\\Delta_{sig}\\) is the largest.</p>"},{"location":"pipeline/call_reference_spots/#view_score","title":"<code>view_score</code>","text":"<p>How the various parameters in the dot product score calculation affect the final value can be investigated, for a single spot, through the function <code>view_score</code>:</p> <code>view_score</code><code>view_weight</code> <p></p> <p></p> <ul> <li>The top left plot shows the spot color prior to any removal of genes or background.</li> <li>The bottom left plot shows the spot color after all genes fit prior to the current iteration have been removed (just background for iteration 0).</li> <li>The top right plot shows the dot product score obtained without weighting (\\(\\alpha=0\\)).</li> <li>The bottom right plot shows the actual score obtained with the current weighting parameters.</li> <li>To get to the gene with the highest dot product score for the current iteration, you can enter an impossible value in the Gene textbox. As well as typing the index of the gene, you can also type in the gene name to  look at the calculation for a specific gene.</li> <li>Clicking on the Weight Squared plot shows the <code>view_weight</code>  plot indicating how it is calculated (second image above).  This is more useful for iterations other than 0.</li> </ul> <p>Looking at the <code>view_weight</code> image and the far right plots of the <code>view_score</code> image,  we see that the effect of the weighting is to down-weight color channel 0 because this is where the background coefficient is the largest. The channels with a smaller background coefficient (1, 5 and 6) then have  a weighting greater than 1. Thus, the weighted score is greater than the non-weighted one because channel 6, where this spot is particularly strong has a greater contribution. I.e. because the intensity in channel 6 cannot be explained by background genes, but it can be explained by Plp1,  we boost the score. For most spots, the background coefficients are very small and so the weighting has little effect.</p> <p>Using the <code>histogram_score</code> function, we see that the effect of weighting (blue) is to  add a tail of scores greater than 1 for spots where an increased contribution is given to the rounds/channels where they are most intense. The mode score does not change though:</p> <p></p>"},{"location":"pipeline/call_reference_spots/#gene-efficiency","title":"Gene Efficiency","text":"<p>Once we have a score and gene assigned to each spot, we can update the <code>bled_codes</code> for each gene, \\(\\pmb{b}_g\\) based on all the spot colors assigned to them, \\(\\pmb{\\zeta}_{{s0}}\\).  We do this by determining <code>nb.call_spots.gene_efficiency</code>. <code>gene_efficiency[g, r]</code> gives the expected intensity of gene \\(g\\) in round \\(r\\), as determined by the spots assigned to it, compared to that expected by the <code>bleed_matrix</code>.</p> <p>The pseudocode below explains how it is computed.</p> <p><pre><code>spot_colors: Intensity of each spot in each channel\n    [n_spots x n_rounds x n_channels]\nspot_gene_no: Gene each spot was assigned to.\n    [n_spots]\nbm: Bleed Matrix, indicates expected intensity of each\n    dye in each round and channel.\n    [n_rounds x n_channels x n_dyes]\ngene_codes: Indicates dye each gene should appear with in each round.\n    [n_genes x n_rounds]\n\nfor g in range(n_genes):\n    Only use spots assigned to current gene.\n        use = spot_gene_no == g\n\n    for r in use_rounds:\n        Get bleed matrix prediction for strength of gene g in round r.\n            bm_pred = bm[r, :, gene_codes[g, r]]\n            [n_channels]\n        Get spot colors for this round.\n            spot_colors_r = spot_colors[use, r]\n            [n_use x n_channels]\n        For each spot, s, find the least squares coefficient, coef, such that\n            spot_colors_r[s] = coef * bm_pred\n    Store coef for each spot and round as spot_round_strength\n        [n_use x n_rounds]\n\n    for r in use_rounds:\n        av_round_strength[r] = median(spot_round_strength[:, r])\n    Find av_round which is the round such that av_round_strength[av_round]\n        is the closest to median(av_round_strength).\n\n    Update spot_round_strength to only use spots with positive strength in\n        av_round.\n        keep = spot_round_strength[:, av_round] &gt; 0\n        spot_round_strength = spot_round_strength[keep]\n        [n_use2 x n_rounds]\n\n    For each spot, determine the strength of each round relative to av_round.\n    for s in range(n_use2):\n        for r in use_rounds:\n            relative_round_strength[s, r] = spot_round_strength[s, r] /\n                                            spot_round_strength[s, av_round]\n\n    Update relative_round_strength based on maximum value.\n        max_round_strength is max of relative_round_strength for \n            each spot across rounds [n_use2].\n        keep = max_round_strength &lt; max_thresh\n        relative_round_strength = relative_round_strength[keep]\n        [n_use3 x n_rounds]\n\n    Update relative_round_strength based on low values.\n        Count number of rounds for each spot below min_thresh.\n        for s in range(n_use3):\n            n_min[s] = sum(relative_round_strength[s] &lt; min_thresh)\n        keep = n_min &lt;= n_min_thresh\n        relative_round_strength = relative_round_strength[keep]\n        [n_use4 x n_rounds]\n\n    for r in use_rounds:\n        if n_use4 &gt; min_spots:\n            gene_efficiency[g, r] = median(relative_round_strength[:, r])\n        else:\n            Not enought spots to compute gene efficiency so just set to 1 in \n                every round.\n                gene_efficiency[g, r] = 1    \n\nClip negative gene efficiency at 0.\n    gene_efficiency[gene_efficiency &lt; 0] = 0  \nreturn gene_efficiency         \n</code></pre> There are a few parameters in the configuration file which are used:</p> <ul> <li><code>gene_efficiency_max</code>: This is <code>max_thresh</code> in the above code.</li> <li><code>gene_efficiency_min</code>: This is <code>min_thresh</code> in the above code.</li> <li><code>gene_efficiency_min_factor</code>: <code>n_min_thresh</code> in the above code is set to  <code>ceil(gene_efficiency_min_factor * n_rounds)</code>.</li> <li><code>gene_efficiency_min_spots</code>: This is <code>min_spots</code> in the above code.</li> </ul> <p>In the gene_efficiency calculation, we computed the strength of each spot relative to <code>av_round</code> because, as with the <code>bleed_matrix</code> calculation, we expect each spot color to be a scaled version of one of the <code>bled_codes</code>. So we are trying to find out, once a spot color has been normalised such that its strength in <code>av_round</code> is 1, what is the corresponding strength in the other rounds. We do this normalisation relative to the average round so that half the <code>gene_efficiency</code> values will be more than 1 and half less than 1 for each gene. For gene \\(g\\), one value of <code>gene_efficiency[g]</code> will be 1, corresponding to <code>av_round</code> but this round will be different for each gene.</p> Why do we need <code>gene_efficiency</code>? <p>We need <code>gene_efficiency</code> because there is a high variance in the strength with which each gene appears in each round. For example, in the the <code>bled_code</code> plot below, we see that the effect of incorporating gene_efficiency is to reduce the strength of rounds 0, 5 and 6 while boosting rounds 2 and 3.</p> Note <p>In the example below, it seems that rounds corresponding to the same dye (0 and 5; 1 and 4; 2 and 3)  have similar strengths, so it may be that different dyes (instead of rounds)  have different strengths for different genes.</p> <code>bled_code</code>histogram <p></p> <p></p> <p>The histogram plot above then shows that when gene efficiency is included (blue line),  the score distribution is shifted considerably.  This indicates that gene efficiency is required to truly capture what spot colors corresponding to <code>Serpini1</code> look like.</p> <p>The <code>histogram_score</code> plot combining all genes, also shows a shift in the peak  of the distribution towards higher scores when gene efficiency is included:</p> <p></p>"},{"location":"pipeline/call_reference_spots/#spots-used","title":"Spots used","text":"<p>Because we use the <code>gene_efficiency</code> to update the <code>bled_codes</code>, we only want to use spots, which we are fairly certain have been assigned to the correct gene. Thus, only spots which satisfy all the following are used in the <code>gene_efficiency</code> calculation:</p> <ul> <li>Like with the <code>scaled_k_means</code> calculation, only spots identified as isolated  in the find spots step of the pipeline are used.</li> <li>The dot product score to the best gene, \\(g_0\\), \\(\\Delta_{s0g_0}\\) must exceed <code>config['gene_efficiency_score_thresh']</code>.</li> <li>The difference between the dot product score to the best gene, \\(g_0\\), and the second best gene, \\(g_1\\): \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) must exceed <code>config['gene_efficiency_score_diff_thresh']</code>.</li> <li>The intensity, \\(\\chi_s\\), must exceed <code>config['gene_efficiency_intensity_thresh']</code>.</li> </ul> Value of <code>config['gene_efficiency_intensity_thresh']</code> <p>If <code>config['gene_efficiency_intensity_thresh']</code> is not specified,  it is set to the percentile indicated by <code>config['gene_efficiency_intensity_thresh_percentile']</code>  of the <code>intensity</code>  computed from the colors of all pixels in the middle z-plane  (<code>nb.call_spots.norm_shift_tile</code>) of the central tile (<code>nb.call_spots.norm_shift_z</code>).</p> <p>It is then clipped to be between <code>config[gene_efficiency_intensity_thresh_min]</code> and  <code>config[gene_efficiency_intensity_thresh_max]</code>.</p> <p>The idea is that this is quite a low threshold (default percentile is 37),  just ensuring that the intensity is not amongst the weakest background pixels. If the intensity threshold was too high, we would end up losing spots which look a lot like genes just because they are weak. But if it was too low, we would identify some background pixels as genes.</p>"},{"location":"pipeline/call_reference_spots/#updating-bled_codes","title":"Updating <code>bled_codes</code>","text":"<p>Once the <code>gene_efficiency</code> has been computed, the <code>bled_codes</code> can be updated:</p> <pre><code>bled_codes: Those computed from the bleed_matrix\n    [n_genes x n_rounds x n_channels].\ngene_efficiency: [n_genes x n_rounds]\n\nfor g in range (n_genes):\n    for r in use_rounds:\n        for c in use_channels:\n            bled_codes[g, r, c] = bled_codes[g, r, c] * gene_efficiency[g, r]\n    Normalise bled_codes[g] so it has an L2 norm of 1.\n</code></pre> <p>We then re-compute the dot product score and gene assignment  for each spot with the new <code>bled_codes</code>. We continue this process of computing the gene efficiency,  updating the dot product score until the same spots have been used to compute the gene efficiency in two subsequent iterations or until <code>config[gene_efficiency_n_iter]</code> iterations have been run.</p> <p>The <code>bled_codes</code> computed from the final iteration will be saved as <code>nb.call_spots.bled_codes_ge</code>. This will be the same as <code>nb.call_spots.bled_codes</code> if <code>config[gene_efficiency_n_iter] = 0</code>. These are the ones used to compute dot product score to the best gene, \\(g_0\\), \\(\\Delta_{s0g_0}\\). These are saved as <code>nb.ref_spots.gene_no</code> and <code>nb.ref_spots.score</code> respectively. The difference between the dot product score to the best gene, \\(g_0\\), and the second best gene, \\(g_1\\): \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) is saved as <code>nb.ref_spots.score_diff</code>.</p>"},{"location":"pipeline/call_reference_spots/#intensity","title":"Intensity","text":"<p>As well as a variable indicating how closely a spot matches a gene (<code>nb.ref_spots.score</code>), we also save  a variable indicating the overall fluorescence of a spot, independent of which gene it belongs to. This intensity, \\(\\chi\\), is saved as <code>nb.ref_spots.intensity</code> and for a spot \\(s\\), it is  defined by:</p> \\[ \\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}}) \\] <p>I.e. for each round, we take the max color across channels to give a set of \\(n_{rounds}\\) values. We then take the median of these. </p> <p>The logic behind this is that if the spot is actually a gene, then there should be at least one channel in every round which is intense, because the relevant dye shows up in it. If the spot was not actually a gene though, you would expect all channels in any given round  to be similarly weakly intense and thus the max over channels would give a low value.</p>"},{"location":"pipeline/call_reference_spots/#view_intensity","title":"<code>view_intensity</code>","text":"<p>The intensity calculation can be visualised with the <code>view_intensity</code> function:</p> <p></p> <p>\\(\\chi_s = 0.542\\) for this example spot, which is the median of all the values shown with a green border.</p>"},{"location":"pipeline/call_reference_spots/#diagnostics","title":"Diagnostics","text":"<p>As well as <code>view_background</code>, <code>view_scaled_k_means</code>,  <code>view_score</code> and <code>view_intensity</code>,  there are a few other functions using matplotlib which may help to debug this section of the pipeline.</p>"},{"location":"pipeline/call_reference_spots/#histogram_score","title":"<code>histogram_score</code>","text":"<p>This shows the histogram of the dot product score, \\(\\Delta_s\\),  assigned to every reference spot:</p> Dot Product ScoreAll Plots <p></p> <p></p> <p>This is useful for checking how well the gene assignment worked. The higher the score where the distribution  peaks, the better. Certainly, if the peak is around 0.8, as with this example, then it probably worked well.</p> <p>The Dot Product Score image above is showing the histogram of <code>nb.ref_spots.score</code>, but there are 4 other plots  which can be selected, as shown in the All Plots image above:</p> <ul> <li>No Weighting: This is the score that would be computed if \\(\\alpha=0\\) in the  dot product score calculation. The max possible score in this case is 1.</li> <li>No Background: This is the score that would be computed if the background genes were not removed before determining the score. This also has no weighting because in the dot product calculation,  \\(\\omega^2_{si_{rc}} = 1\\) if no background has been fitted. Hence, the max score is 1 as with No Weighting.</li> <li>No Gene Efficiency: This is the score that would be computed if the <code>nb.call_spots.bled_codes</code> were used instead of <code>nb.call_spots.bled_codes_ge</code>. \\(\\omega^2_{si_{rc}} \\neq 1\\) here so the max score is over 1.</li> <li>No Background / No Gene Efficiency: This is the score that would be computed if the background genes were not removed before determining the score and if <code>nb.call_spots.bled_codes</code> were used instead of <code>nb.call_spots.bled_codes_ge</code>. The max score is 1 in this case.</li> </ul> <p>The Gene textbox can also be used to view the histogram of a single gene. Either the index of the gene or the gene name can be entered. To go back to viewing all genes, type in all into the textbox.</p> <p>The Histogram Spacing textbox can be used to change the bin size of the histogram.</p>"},{"location":"pipeline/call_reference_spots/#gene_counts","title":"<code>gene_counts</code>","text":"<p>This plot indicates the number of spots assigned to each gene which also have <code>nb.ref_spots.score &gt; score_thresh</code> and <code>nb.ref_spots.intensity &gt; intensity_thresh</code>. The default <code>score_thresh</code> and <code>intensity_thresh</code>  are <code>config['thresholds']['score_ref']</code> and <code>config['thresholds']['intensity']</code> respectively. They can be changed with the textboxes though. This  thresholding  is the same that is done in the results <code>Viewer</code>  and when exporting to pciSeq. </p> Gene CountsGene Counts with Fake Genes <p></p> <p></p> <p>There is also a second Ref Spots - Fake Genes plot which can be shown in yellow. This shows the results  of the gene assignment if we added some fake <code>bled_codes</code> as well as the ones corresponding to genes. The idea is to choose fake <code>bled_codes</code> which are well separated from the actual <code>bled_codes</code>. If spots then match to these fake genes, then it probably means the initial gene assignment is not reliable.</p> <p>The fake <code>bled_codes</code> can be specified, but by default there is one fake <code>bled_code</code> added for each round, \\(r\\), and channel \\(c\\), which is 1 in round \\(r\\), channel \\(c\\) and 0 everywhere else. In the second image above, we see that  there is not much change in the gene counts when we add the fake genes, indicating the initial assignment is probably reliable.</p> Example Dataset with lots of Fake Genes <p>The example below indicates a case where the fake genes functionality may be useful.</p> <p>When we open <code>coppafish.Viewer</code>, we see that there seems to be too many spots assigned to Penk and  Vip.</p> <code>coppafish.Viewer</code><code>gene_counts</code>PenkVip <p></p> <p></p> <p></p> <p></p> <p>If we then look at the <code>gene_counts</code>, we see that when we include fake genes, the number of spots assigned to Penk and Vip decreases drastically because they have been assigned to the \\(r0c18\\) fake gene.</p> <p>When we look at the Penk and Vip <code>bled_codes</code>, we see that they are very intense in round 0, channel 18. So most spots seem to only have been assigned to these genes on the basis of this one round and channel.</p>"},{"location":"pipeline/call_reference_spots/#view_bleed_matrix","title":"<code>view_bleed_matrix</code>","text":"<p>This function is useful for seeing if the dye vectors in the  <code>bleed_matrix</code> are easily distinguished.</p>"},{"location":"pipeline/call_reference_spots/#view_bled_codes","title":"<code>view_bled_codes</code>","text":"<p>This function is useful for seeing how the <code>gene_efficiency</code>  affected the <code>bled_codes</code>.</p>"},{"location":"pipeline/call_reference_spots/#view_codes","title":"<code>view_codes</code>","text":"<p>This function is useful for seeing how a particular spot matches the gene it was assigned to.</p>"},{"location":"pipeline/call_reference_spots/#view_spot","title":"<code>view_spot</code>","text":"<p>This function is useful for seeing if the neighbourhood of a  particular spot  has high intensity in all rounds/channels where the gene it was assigned, expects it to.</p>"},{"location":"pipeline/call_reference_spots/#psuedocode","title":"Psuedocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline. There is more detailed pseudocode about how the <code>bleed_matrix</code> and  <code>gene_efficiency</code> are found.</p> <pre><code>Determine color_norm_factor from nb.extract.hist_counts\n    [n_rounds x n_channels]\n\nLoad in pixel colors of all pixel of middle z-plane of\n    central tile. Use these to determine the following\n    if not provided in the config file:\n    - nb.call_spots.background_weight_shift    \n    - nb.call_spots.dp_norm_shift\n    - nb.call_spots.gene_efficiency_intensity_thresh\n    - nb.call_spots.abs_intensity_percentile\n\nNormalise reference spot colors\n    spot_colors = nb.ref_spots.colors / color_norm_factor\n    [n_spots x n_rounds x n_channels]\n\nCompute Spot Intensity  (nb.ref_spots.intensity)\nRemove Background from spot_colors\nCompute Bleed Matrix    (nb.call_spots.bleed_matrix)\nCompute Bled Codes      (nb.call_spots.bled_codes)\n\nuse_ge_last = array of length n_spots where all values are False.\ni = 0\nwhile i &lt; gene_efficiency_n_iter:\n    Determine all_scores, the dot product score of each spot to \n        each bled_code.\n        [n_spots x n_genes]\n    Determine gene_no, the gene for which all_scores is the greatest\n        for each spot.\n        [n_spots]\n    Determine score, the score in all_scores, corresponding to gene_no\n        for each spot.\n        [n_spots]\n    Determine score_diff, the difference between score and the second \n        largest value in all_scores for each spot.\n        [n_spots]\n    Determine whether each spot was used for gene efficiency calculation.\n        use_ge = score &gt; ge_score_thresh            and \n                 score_diff &gt; ge_score_diff_thresh  and \n                 intensity &gt; ge_intensity_thresh    and \n                 nb.ref_spots.isolated.\n        [n_spots]\n    Compute gene_efficiency with spots indicated by use_ge\n    Update bled_codes based on gene_efficiency   \n    If use_ge == use_ge_last:\n        End iteration i.e. i = gene_efficiency_n_iter\n    use_ge_last = use_ge\n    i += 1\n\nSave final bled_codes as        nb.call_spots.bled_codes_ge\nSave final gene_efficiency as   nb.call_spots.gene_efficiency\nSave final gene_no as           nb.ref_spots.gene_no\nSave final score as             nb.ref_spots.score\nSave final score_diff as        nb.ref_spots.score_diff              \n</code></pre>"},{"location":"pipeline/extract/","title":"Extract and Filter","text":"<p>The extract and filter step of the pipeline loads in the raw images,  filters them and saves the resultant filtered images for each tile/round/channel combination  as npy files.</p> <p>It also adds the <code>extract</code> and  <code>extract_debug</code> NotebookPages to the Notebook.</p> <p>If the extract and filter step of the pipeline bugs out halfway through for some reason, it can be re-run without needing to remake all the tiles already saved to the tile directory. It will just start with the first tile yet to be saved.  The scale values must not be changed when re-running though.</p>"},{"location":"pipeline/extract/#variables-in-extract-page","title":"Variables in <code>extract</code> page","text":""},{"location":"pipeline/extract/#auto_thresh","title":"<code>auto_thresh</code>","text":"<p>The <code>extract</code> NotebookPage contains the variable <code>auto_thresh</code>. <code>auto_thresh[t, r, c]</code> is the threshold spot intensity for tile \\(t\\), round \\(r\\), channel \\(c\\) used for spot detection  in the find spots step of the pipeline.</p> <p><pre><code>auto_thresh[t, r, c] = config['extract']['auto_thresh_multiplier'] * median(abs(image))\n</code></pre> where <code>image</code> is the mid z-plane (<code>nb.extract_debug.z_info</code>) of the image saved to <code>tile_dir</code> for tile \\(t\\), round \\(r\\),  channel \\(c\\) during the extract step of the pipeline. This is just saying that we expect <code>median(abs(image))</code> to be the characteristic intensity of background pixels and spot pixels should be much more intense that this.</p> <p>These values can be viewed with the function <code>thresh_box_plots</code>:</p> <p></p> <p>For each round and channel, this shows a box plot combining all tiles (i.e. a box plot of <code>auto_thresh[:, r, c]</code>). For the <code>anchor_round</code>, only one channel (<code>anchor_channel</code>) is shown as it is the only one used on this round. In this plot, we expect for a given channel, <code>auto_thresh</code> should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any  outlier tiles (white crosses, +) not far from the boxplot). </p>"},{"location":"pipeline/extract/#hist_counts","title":"<code>hist_counts</code>","text":"<p>The <code>extract</code> NotebookPage also contains the variable <code>hist_counts</code>. <code>hist_counts[i, r, c]</code> is the number of pixels across the mid z-plane (<code>nb.extract_debug.z_info</code>)  of all tiles in round \\(r\\), channel \\(c\\) which had the value <code>nb.extract.hist_values[i]</code>.  It is used for normalisation  (see Norm Button box here)  between channels in the call reference spots step.</p> <p>The histograms can be viewed using <code>histogram_plots</code>. Initially, this will show the <code>hist_counts[:, r, c]</code> vs <code>hist_values</code> for each round and channel. There is also a Norm Button which  equalises the channels according to  <code>config['call_spots']['color_norm_intensities']</code> and <code>config['call_spots']['color_norm_probs']</code>. In the normalised histograms, most values will be between \u00b11.</p> Un-normalised HistogramsNormalised Histograms <p></p> <p></p> <p>In the normalised histograms, we want to see a sharp peak at 0 accounting for the background pixels with a  long tail to larger values accounting for the spot pixels and a tail to  negative values accounting for the pixels in annuli surrounding spots.</p> <p>So in this example, channel 2 will likely prove the most problematic because the peak centered on 0 is much wider than for any other channel. This indicates that there is quite a lot of variance in the background pixels,  making it harder to distinguish the spots from the background.</p> <p>Also, from the un-normalised histograms we can see that the peak centered on 0 is widest for channel 0.  Thus, the median of absolute values will be largest for this channel. This explains why <code>auto_thresh</code>  is significantly larger for channel 0 than any other channel.</p>"},{"location":"pipeline/extract/#raw-data","title":"Raw data","text":"<p>The raw data can be viewed using <code>view_raw</code>. It can either be called for an experiment which already has a Notebook, or for one for which no code has been run yet, but the <code>config_file</code>  has been made:</p> With NotebookWithout Notebook <pre><code>from coppafish import Notebook\nfrom coppafish.plot import view_raw\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\ntiles = [0, 1]      # tiles to view\nrounds = [3, 5]     # rounds to view\nchannels = [1, 6]   # channels to view\nview_raw(nb, tiles, rounds, channels)\n</code></pre> <pre><code>from coppafish.plot import view_raw\nini_file = '/Users/user/coppafish/experiment/settings.ini'\ntiles = [0, 1]      # tiles to view\nrounds = [3, 5]     # rounds to view\nchannels = [1, 6]   # channels to view\nview_raw(None, tiles, rounds, channels, config_file=ini_file)\n</code></pre> <p>This will open a napari viewer with up to 4 scrollbars to change tile, round, channel and z-plane. When any of these scrollbars are used, the status in the bottom left corner will indicate the current tile, round,  channel and z-plane being shown (e.g. below, the round at index 0 is 3 and the channel at index 1 is 6).</p> <p></p> <p>Using this tool before running the pipeline may be useful for deciding which z-planes to use. For example, if say the first 10 z-planes don't show any clear spots, then removing them with <code>use_z</code> in the basic_info  section of the configuration file would make the pipeline run much quicker,  especially in the omp and extract and filter sections.</p>"},{"location":"pipeline/extract/#filtering","title":"Filtering","text":"<p>Once the raw images are loaded in, they are  convolved with a 2D difference of hanning kernel.</p> Difference with 2D pipeline <p>If <code>config['basic_info']['is_3d'] == False</code>, before the convolution with the difference of hanning kernel, the 3D raw data will be focus stacked so that  it becomes 2D.</p>"},{"location":"pipeline/extract/#difference-of-hanning-kernel","title":"Difference of hanning kernel","text":"<p>The difference of hanning kernel is made up by adding together a positive hanning window (yellow below) of radius \\(r_1\\) and an outer negative hanning window  (cyan below) of radius \\(r_2\\) (typically twice \\(r_1\\)). It is normalised such that the sum of the difference of hanning kernel is 0. An example for a 1D version of the  kernel with \\(r_1 = 3\\) and \\(r_2 = 6\\) is shown below:</p> <p></p> Conversion to 2D <p></p> <p>The 1D kernel shown in purple above is converted to the 2D kernel shown on the right via the  <code>ftrans2</code> function.</p> <p>In the pipeline, the value of \\(r_1\\) is set to <code>config['extract']['r1']</code>  and \\(r_2\\) is set to <code>config['extract']['r2']</code>. If <code>config['extract']['r1']</code> is not specified, it is converted to units of pixels from the micron value <code>config['extract']['r1_auto_microns']</code> (0.5\\(\\mu\\)m typically gives \\(r_1=3\\)). If <code>config['extract']['r2']</code> is not  specified, \\(r_2\\) is set to twice \\(r_1\\). </p> <p>In general, \\(r_1\\) should be the typical radius of a spot in the raw image and \\(r_2\\) should be twice this.</p>"},{"location":"pipeline/extract/#smoothing","title":"Smoothing","text":"<p>After the convolution with the difference of hanning kernel, there is an option to smooth the image by applying  a correlation with an averaging kernel. This can be included by setting the <code>config['extract']['r_smooth']</code> parameter. </p>"},{"location":"pipeline/extract/#dapi","title":"DAPI","text":"<p>For the <code>dapi_channel</code> of the <code>anchor_round</code>, convolution with the difference of hanning kernel is not appropriate  as the features that need extracting do not look like spots. Instead, tophat filtering can be performed by  setting <code>config['extract']['r_dapi']</code>. No smoothing is permitted.</p>"},{"location":"pipeline/extract/#viewer","title":"Viewer","text":"<p>The purpose of filtering the raw images is to make the spots appear much more prominently compared to the background  i.e. extract the spots. We can see this effect and how the various parameters affect things with  <code>view_filter</code>.  This can be called in a similar way to <code>view_raw</code>:</p> With NotebookWithout Notebook <pre><code>from coppafish import Notebook\nfrom coppafish.plot import view_filter\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\nt = 1       # tile to view\nr = 3       # round to view\nc = 6       # channel to view\nview_filter(nb, t, r, c)\n</code></pre> <pre><code>from coppafish.plot import view_filter\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nt = 1       # tile to view\nr = 3       # round to view\nc = 6       # channel to view\nview_filter(None, t, r, c, config_file=ini_file)\n</code></pre> <p>This will open a napari viewer with up to 2 scrollbars. One to change z-plane and another to change the filter method. The filter method scrollbar can change between the raw image, the result of convolution with difference of hanning  kernel and the result with smoothing in addition to this.</p> <p>There are also up to 3 of the following sliders in the bottom left:</p> <ul> <li>Difference of Hanning Radius: This is the value of <code>config['extract']['r1']</code>.  Whenever this is changed, <code>config['extract']['r2']</code> will be set to twice the new value.</li> <li>Tophat kernel radius: If <code>r == anchor_round</code> and <code>c == dapi_channel</code>, this slider will appear and refers to the  value of <code>config['extract']['r_dapi']</code>.</li> <li>Smooth Radius YX: This is the value of <code>config['extract']['r_smooth'][0]</code> and <code>config['extract']['r_smooth'][1]</code>. Both will be set to the same value.</li> <li>Smooth Radius Z: This is the value of <code>config['extract']['r_smooth'][2]</code>. When both this slider and  the Smooth Radius YX slider are set to 1, no smoothing will be performed and the last two images in the filter method scrollbar will be identical.</li> </ul> <p>Whenever any of these are changed, the filtering will be redone using the new values of the parameters and thus the last two images of the filter method scrollbar will be updated. The time taken will be printed to the console.</p> <p>The 1D version of the current difference of hanning kernel can be seen at any time by pressing the h key.</p>"},{"location":"pipeline/extract/#effect-of-filtering","title":"Effect of Filtering","text":"<p>The images below show the effect of filtering with <code>config['extract']['r1'] = 3</code>, <code>config['extract']['r2'] = 6</code> and <code>config['extract']['r_smooth'] = 1, 1, 2</code>:</p> RawDifference of Hanning ConvolutionSmoothing <p></p> <p></p> <p></p> <p>From this, we see that away from the spots, the raw image has a non-zero intensity value (around 300). After convolution with the difference of hanning kernel though, these regions become a lot darker (approximately 0). This is because the sum of the difference of hanning kernel is 0 so its effect on a background region with a uniform  non-zero value is to set it to 0.</p> <p>Looking at the spots, we see that the convolution helps isolate them from the background and separate spots which are close together. There is also a very dark (negative) region surrounding the spots.  It is a feature of convolution with the difference of hanning kernel that it produces a negative annulus about spots.</p> Why negative annulus is expected <p>The convolution in the annulus of a spot is like the sum of the multiplication of the spot line (yellow) with the kernel line (cyan). This multiplication produces the purple line, the sum of which is negative.</p> <p></p> <p>The smoothing in this example is only in the z direction (averaging over 3 z-planes: the one shown, 1 above and 1 below) and seems to emphasise the spots and the negative annulus even more.  This is because on one of the neighbouring z-planes, the spot has a larger intensity than on the z-plane shown so  averaging increases the absolute intensity.</p>"},{"location":"pipeline/extract/#varying-difference-of-hanning-kernel-radius","title":"Varying difference of hanning kernel radius","text":"<p>The plots below show the results of the convolution with the difference of hanning kernel for four different values of <code>config['extract']['r1']</code>. In each case, <code>config['extract']['r2']</code> is twice this value.</p> 2346 <p></p> <p></p> <p></p> <p></p> <p>From this, we see that with \\(r_1 = 2\\), the background regions away from the spots appear less uniform than with \\(r_1 = 3\\), with quite a few patches of negative values. Also, the shape of the second spot from the left appears distorted. These both indicate that the kernel is wanting to extract features smaller than those of interest.</p> <p>As \\(r_1\\) increases, we see that the negative annulus around the spots becomes larger and eventually at \\(r_1=6\\), the  spots start merging together, indicating the kernel is wanting to extract features larger than those of interest.</p>"},{"location":"pipeline/extract/#varying-smoothing-radius","title":"Varying smoothing radius","text":"<p>The plots below show the results of the convolution with the difference of hanning kernel followed by smoothing for four different values of <code>config['extract']['r_smooth']</code>. In each case, <code>config['extract']['r1'] = 3</code> and  <code>config['extract']['r2'] = 6</code>.</p> 1, 1, 21, 1, 31, 1, 52, 2, 24, 4, 1 <p></p> <p></p> <p></p> <p></p> <p></p> <p>From this, we see that smoothing in the z direction makes spots which appear most prominantly on other z-planes appear much more intense in the z-plane shown. For example, the feature towards the bottom just to right of centre is barely visible with <code>r_smooth = 1, 1, 2</code> but is clear with <code>r_smooth = 1, 1, 5</code>.</p> <p>We also see that the difference between the <code>r_smooth = 1, 1, 2</code> and <code>r_smooth = 2, 2, 2</code> plots is barely perceivable. This suggests that the z averaging is more important, which makes sense, seen as the convolution with the  difference of hanning kernel is done in 2D, so treats each z-plane independently. In the <code>r_smooth = 4, 4, 1</code> image with no z-averaging, we see that the spots have more of a gradual increase in intensity instead of a sharp peak.</p>"},{"location":"pipeline/extract/#scale","title":"Scale","text":"<p>The filtered images produced are of float data type with negatives,  but they are saved to <code>config['file_names']['tile_dir']</code> in uint16 format.</p> <p>To do this conversion, the images are first multiplied by a scale factor so that they fill most of the uint16  range (between 0 and 65535) to keep the maximum amount of information. There are two different scale factors, <code>scale</code> which is applied to all tiles and channels of the imaging rounds  (<code>config['file_names']['round']</code>) and <code>scale_anchor</code> which is applied to all tiles of  the <code>anchor_channel</code> of the <code>anchor_round</code>.</p> Potential error if <code>scale</code> changed <p>It is important that the value of <code>scale</code> used does not vary between tiles/rounds/channels as it would affect the assignment of spots to genes. For example, if the value of <code>config['extract']['scale']</code>  was larger for round 2, channel 3, then spots will be more likely to be assigned to  genes which appear here according to their barcode in the <code>code_book</code> (<code>scale_anchor</code> is allowed to differ from <code>scale</code> because the <code>anchor_round</code> is not used in gene assignment).</p> <p>To stop this possibility, the values of <code>scale</code> and <code>scale_anchor</code> used are saved to the <code>config['file_names']['tile_dir']</code>  in a text file (<code>config['file_names']['scale']</code>). Then if these differ  from <code>config['extract']['scale']</code> and <code>config['extract']['scale_anchor']</code>, an error will occur.</p> <p><code>scale</code> can be specified through <code>config['extract']['scale']</code> but if this is empty, it will be  set to: </p> <pre><code>scale = config['extract']['scale_norm']/max(scale_image)\n</code></pre> <p><code>scale_image</code> is the  <code>nb.basic_info.tile_sz x nb.basic_info.tile_sz</code> raw image belonging to the channel and z-plane containing  the pixel with maximum intensity of the central tile (saved as <code>scale_channel</code>, <code>scale_z</code>, <code>scale_tile</code> in  <code>nb.extract_debug</code>) in round 0. It is then filtered/smoothed according to the parameters in <code>config['extract']</code> before being used in the <code>scale</code> calculation.</p> <p><code>scale_anchor</code> can be specified through <code>config['extract']['scale_anchor']</code>. If it is left empty, it is computed in the same way as <code>scale</code>  (the channel used is <code>anchor_channel</code> and the tile and z-plane used are saved as <code>scale_ancor_tile</code> and  <code>scale_anchor_z</code> in <code>nb.extract_debug</code>).</p> <p>After the tiles are multiplied by the scale factor, they still contain negative values, so when they are saved, a shift  (<code>config['basic_info']['tile_pixel_value_shift']</code>) in intensity is added to each pixel. This shift is then subtracted when the tiles are loaded.</p>"},{"location":"pipeline/extract/#error-clipped-pixels","title":"Error - clipped pixels","text":"<p>Because <code>scale</code> is computed from one tile and round, there is a possibility during the course of the extract step of the pipeline that a much more intense tile/round will be encountered such that  the pixel values will have to be clipped after scaling, to be kept within the uint16 range.</p> <p>The number of pixels for which this happens on tile \\(t\\), round \\(r\\), channel \\(c\\) is saved as  <code>nb.extract_debug.n_clip_pixels[t, r, c]</code>.</p> <p>Clipped pixels can cause more spots to be detected in the find spots section of the pipeline, as shown below, so are best avoided:</p> Spot detection with no clipped pixelsSpot detection with clipped pixels <p></p> <p></p> <p>If more than <code>config['extract']['n_clip_error']</code> (will be set to 1% of pixels on single z-plane if not specified)  pixels have been clipped for <code>config['extract']['n_clip_error_images_thresh']</code> images, an error will be raised stopping the extract section of the pipeline. </p> <p>When this error occurs, a Notebook called notebook_extract_error.npz will be saved to the output directory with the pages extract_fail and extract_debug_fail. <code>nb.extract_fail.fail_trc</code> records the tile, round, channel where it terminated.</p>"},{"location":"pipeline/extract/#solution","title":"Solution","text":"<p>If the failed round, <code>nb.extract_fail.fail_trc[1]</code> is not the <code>anchor_round</code>, then delete everything in the tile directory including the <code>scale.txt</code> file. Then set <code>config['extract']['scale']</code> to <code>new_scale</code> and re-run:</p> <pre><code>scale_clip = nb.extract_debug_fail.clip_extract_scale\nnew_scale = scale_clip[scale_clip &gt; 0].min()\n</code></pre> <p>This is the scale such that all tiles saved so far will not have any clipped pixels.</p> <p>If the failed round, <code>nb.extract_fail.fail_trc[1]</code> is the <code>anchor_round</code>, then delete all .npy files belonging to the anchor round in the tile directory as well as the <code>scale.txt</code> file.  Then set <code>config['extract']['scale_anchor']</code> to <code>new_anchor_scale</code> and re-run:</p> <pre><code>anchor_scale_clip = \\\n    nb.extract_debug_fail.clip_extract_scale[:, anchor_round, anchor_channel]\nnew_anchor_scale = anchor_scale_clip[anchor_scale_clip &gt; 0].min()\n</code></pre> <p>This is the scale such that all anchor tiles saved so far will not have any clipped pixels.</p>"},{"location":"pipeline/extract/#psuedocode","title":"Psuedocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline.</p> <pre><code>for r in use_rounds:\n    for t in use_tiles:\n        for c in use_channels:\n            im = load image from raw data in input_dir\n            if 2D:\n                im = focus_stack(im)\n            if r is anchor_round and c is dapi_channel:\n                im = tophat_filter(im, dapi_kernel)\n            else:\n                im = convolve(im, diff_hanning_kernel)\n                im = im * scale\n                if smooth:\n                    im = correlate(im, smooth_kernel)\n                Compute auto_thresh[t, r, c] and hist_counts[t, r, c] from\n                 mid z-plane of im.\n                Save im to tile directory.\nAdd information needed for later stages of pipeline to extract NotebookPage\nAdd useful debugging info to extract_debug NotebookPage.\nReturn both.            \n</code></pre>"},{"location":"pipeline/find_spots/","title":"Find Spots","text":"<p>The find spots step of the pipeline loads in the filtered images for each  tile, round, channel saved during the extract step and detects spots on them.  We obtain a point cloud from the images because in the stitch and register sections of the pipeline, it is quicker to use point clouds than the full images.</p> <p>The <code>find_spots</code> NotebookPage is added to the Notebook after this stage is finished.</p>"},{"location":"pipeline/find_spots/#spot-detection","title":"Spot detection","text":"<p>The spots on tile \\(t\\), round \\(r\\), channel \\(c\\) are  the local maxima in the filtered image (loaded in through  <code>load_tile(nb.file_names, nb.basic_info, t, r, c)</code>)  with an intensity greater than <code>auto_thresh[t, r, c]</code>.</p> <p>Local maxima means pixel with the largest intensity in a neighbourhood defined by <code>config['find_spots']['radius_xy']</code> and <code>config['find_spots']['radius_z']</code>:</p> <pre><code>kernel = np.ones((2*radius_xy-1, 2*radius_xy-1, 2*radius_z-1))\n</code></pre> <p>The position of the local maxima is found to be where the  dilation of the image with the <code>kernel</code>  is equal to the image. </p> <p>The spot detection process can be visualised with <code>view_find_spots</code>.</p> Optimised spot detection <p>The dilation method is quite slow, so if jax is installed, a  different spot detection method  is used.</p> <p>In this method, we look at all pixels with intensity greater than <code>auto_thresh[t, r, c]</code>. For each of these, we say that the pixel is a spot if it has a greater intensity than all of its neighbouring pixels, where the  neighbourhood is determined by the <code>kernel</code>. </p> <p>The larger <code>auto_thresh[t, r, c]</code> and the smaller the <code>kernel</code>, the faster this method is, whereas the value of <code>auto_thresh[t, r, c]</code> makes no difference to the speed of the dilation method. In our case,  <code>auto_thresh[t, r, c]</code> is pretty large as the whole point is that all background pixels (the vast majority)  have intensity less than it.</p> <p>For images of size \\(2048 \\times 2048 \\times 50\\), the optimised method is around 9 times faster.</p> Dealing with duplicates <p>If there are two neighbouring pixels which have the same intensity, which is the local maxima intensity, by default both pixels will be declared to be local maxima. However, if <code>remove_duplicates == True</code> in  <code>detect_spots</code>, only one will be deemed a  local maxima.</p> <p>This is achieved by adding a random shift to the intensity of each pixel. The max possible shift is 0.2 so it  will not change the integer version of the image but it will ensure each pixel has a different intensity to  its neighbour.</p>"},{"location":"pipeline/find_spots/#imaging-spots","title":"Imaging spots","text":"<p>For non reference spots (all round/channel combinations apart from <code>ref_round</code>/<code>ref_channel</code>), we only use the spots for registration to the reference spots, so the quantity of spots is not important. In fact, registration tends to work better if there are fewer but more reliable spots, as this means there is a lesser chance of matching up spots by chance. </p> <p>To exploit this, for each imaging tile, round, channel, the point cloud  is made up of the <code>max_spots</code> most intense spots on each z-plane. In 2D, <code>max_spots</code> is  <code>config['find_spots']['max_spots_2d']</code> and in 3D, it is <code>config['find_spots']['max_spots_3d']</code>. If there are fewer than <code>max_spots</code> spots detected on a particular z-plane, all the spots will be kept.</p>"},{"location":"pipeline/find_spots/#reference-spots","title":"Reference spots","text":"<p>We want to assign a gene to each reference spot (<code>ref_round</code>/<code>ref_channel</code>), as well as use it for registration, so it is beneficial to maximise the number of reference spots. As such, we do not do the <code>max_spots</code> thresholding for reference spots.</p> <p>However, we want to know which reference spots are isolated because when it comes to the  <code>bleed_matrix</code> calculation, we do not want to use overlapping spots.</p>"},{"location":"pipeline/find_spots/#isolated-spots","title":"Isolated spots","text":"<p>We deem a spot to be isolated  if it has a prominent negative annulus, because if there was an overlapping spot, you would expect positive intensity in the annulus around the spot. We find the intensity of the annulus by computing the correlation of the image with an  annulus kernel obtained from <code>annulus(r0, r_xy, r_z)</code> where:</p> <ul> <li><code>r0 = config['find_spots']['isolation_radius_inner']</code></li> <li><code>r_xy = config['find_spots']['isolation_radius_xy']</code> </li> <li><code>r_z = config['find_spots']['isolation_radius_z']</code>.</li> </ul> <p>If the value of this correlation at the location of a spot is less than <code>config['find_spots']['isolation_thresh']</code>, then we deem the spot to be isolated. If <code>config['find_spots']['isolation_thresh']</code> is not given, it is set to:</p> <p><pre><code>config['find_spots']['auto_isolation_thresh_multiplier'] * auto_thresh[t, r, c]\n</code></pre> The final isolation thresholds used for each tile are saved as  <code>nb.find_spots.isolation_thresh</code>.  The process of obtaining isolated spots can be visualised with <code>view_find_spots</code>.</p> Annulus kernel <p>The annulus kernel should be equal to 1 over the pixels in the neighbourhood of an isolated spot  which are usually negative. The example images below show a typical spot (left) and the annulus kernel used for this data (right) with <code>r0 = 4</code>, <code>r_xy = 14</code> and <code>r_z = 1</code>.  The dimensions of each image is 29 x 29 pixels, red is positive and blue is negative.</p> z = -1z = 0z = 1 <p></p> <p></p> <p></p> z = -1z = 0z = 1 <p></p> <p></p> <p></p>"},{"location":"pipeline/find_spots/#error-too-few-spots","title":"Error - too few spots","text":"<p>After the <code>find_spots</code> NotebookPage has been added to the Notebook, <code>check_n_spots</code> will be run.</p> <p>This will produce a warning for any tile, round, channel for which fewer than</p> <pre><code>n_spots_warn = config['find_spots']['n_spots_warn_fraction'] * max_spots * nb.basic_info.nz\n</code></pre> <p>spots were detected, where <code>max_spots</code> is <code>config['find_spots']['max_spots_2d']</code> if 2D and  <code>config['find_spots']['max_spots_3d']</code> if 3D. </p> <p>An error will be raised if any of the following is satisfied:</p> <ul> <li> <p>For any given channel, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of tiles/rounds.</p> <p>The faulty channels should then be removed from <code>use_channels</code>.</p> </li> <li> <p>For any given tile, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of rounds/channels. </p> <p>The faulty tiles should then be removed from <code>use_tiles</code>.</p> </li> <li> <p>For any given round, the number of spots found was less than <code>n_spots_warn</code> for at least the fraction <code>n_spots_error_fraction</code> of tiles/channels.</p> <p>The faulty rounds should then be removed from <code>use_rounds</code>.</p> </li> </ul> Example <p>The following is the \\(n_{tiles}\\) (3) x \\(n_{rounds}\\) (5) array of number of spots found for a given channel:</p> <p><pre><code>spot_no = array([[1295, 1016,  869,  719,  829],\n                 [1055,  888,  687,  556,  824],\n                 [5901, 4208, 5160, 4069, 4006]])\n</code></pre> The value of <code>n_spots_warn</code> for this experiment is 3500 so a warning will be raised for the 10 tiles/rounds for which <code>spot_no[t, r] &lt; n_spots_warn</code>: <pre><code>array([[0, 0],\n       [0, 1],\n       [0, 2],\n       [0, 3],\n       [0, 4],\n       [1, 0],\n       [1, 1],\n       [1, 2],\n       [1, 3],\n       [1, 4]])\n</code></pre> The value of <code>n_spots_error_fraction</code> for this experiment is 0.5 so the threshold number of  failed tiles/rounds to give an error is \\(0.5 \\times n_{tiles} \\times n_{rounds} = 7.5\\). We have 10 failed tiles/rounds so an error would be raised in this case.</p> <p>The <code>use_tiles</code>, <code>use_rounds</code> and <code>use_channels</code> parameters can be changed without having to re-run the <code>find_spots</code> section of the pipeline as explained here. If tiles/rounds/channels are added instead of removed though, it will need re-running, as will the <code>extract</code> step.</p>"},{"location":"pipeline/find_spots/#n_spots_grid","title":"<code>n_spots_grid</code>","text":"<p>The <code>n_spots_grid</code> function is  useful to visualise the number of spots detected on each tile, round and channel:</p> GoodBad <p></p> <p></p> <p>In the good example, you can see from the minimum on the colorbar that lots of spots have been detected on  every image. You can also see that tile 13 seems to have significantly fewer spots than the other tiles and  that round 0 often has fewer spots than the other rounds.</p> <p>In the bad example, all tiles, rounds and channels where <code>n_spots &lt; n_spots_warn</code> are highlighted by a red border. Clearly, channels 0 and 3 did not work for this experiment and channel 1 probably didn't. Also, tile 1 appears  to have fewer spots than the other tiles.</p>"},{"location":"pipeline/find_spots/#viewer","title":"Viewer","text":"<p>We can see how the various parameters affect which spots are detected using  <code>view_find_spots</code>.  This can be called as follows (in the Without Notebook case, the raw images will be loaded and  then filtered according to parameters in <code>config['extract']</code>).</p> With NotebookWithout NotebookReference round/channel showing isolated spots <pre><code>from coppafish import Notebook\nfrom coppafish.plot import view_find_spots\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\nt = 1       # tile to view\nr = 3       # round to view\nc = 6       # channels to view\nview_find_spots(nb, t, r, c)\n</code></pre> <pre><code>from coppafish.plot import view_filter\nini_file = '/Users/user/coppafish/experiment/settings.ini'\nt = 1       # tile to view\nr = 3       # round to view\nc = 6       # channel to view\nview_find_spots(None, t, r, c, config_file=ini_file)\n</code></pre> <pre><code>from coppafish import Notebook\nfrom coppafish.plot import view_find_spots\nnb_file = '/Users/user/coppafish/experiment/notebook.npz'\nnb = Notebook(nb_file)\nt = 1                               # tile to view\nr = nb.basic_info.ref_round         # round to view\nc = nb.basic_info.ref_channel       # channel to view\nview_find_spots(nb, t, r, c, show_isolated=True)\n</code></pre> <p>This will open a napari viewer with up to 5 sliders in the bottom left:</p> <ul> <li>Detection Radius YX: This is the value of <code>config['find_spots']['r_xy']</code>.</li> <li>Detection Radius Z: This is the value of <code>config['find_spots']['r_z']</code>.</li> <li>Intensity Threshold: This is the value of <code>nb.extract.auto_thresh[t, r, c]</code>.</li> <li>Isolation Threshold: This is the value of <code>nb.find_spots.isolation_thresh[t]</code>. It will only appear if <code>show_isolated == True</code>, <code>r = nb.basic_info.ref_round</code> and <code>c = nb.basic_info.ref_channel</code>.</li> <li>Z Thickness: Spots detected on the current z-plane and this many z-planes either side of it  will be shown. Initially, this will be set to 1 so spots from the current z-plane and 1 either side of it will be shown. </li> </ul> <p>Whenever the first two are changed, the dilation will be redone using the new values of the radii and the  time taken will be printed to the console. The correlation calculation required to determine which spots are  isolated is slow hence, by default <code>show_isolated == False</code>.</p>"},{"location":"pipeline/find_spots/#z-thickness","title":"Z thickness","text":"<p>The images below show the effect of changing the z-thickness. The size of the spots is related to the  z-plane they were detected on. The closer to the current z-plane, the larger they appear:</p> 012 <p></p> <p></p> <p></p> <p>The blue spot has a  neighbouring pixel with negative intensity  and is not kept in the final point cloud.</p>"},{"location":"pipeline/find_spots/#detection-radius","title":"Detection radius","text":""},{"location":"pipeline/find_spots/#yx","title":"YX","text":"<p>The images below show the effect of using the slider to change <code>config['find_spots']['r_xy']</code> with  <code>config['find_spots']['r_z']</code> fixed at 2 and z thickness = 2:</p> 2310 <p></p> <p></p> <p></p> <p>Clearly, as the radius increases the spots have to be separated by a larger distance, resulting in less spots found.</p>"},{"location":"pipeline/find_spots/#z","title":"Z","text":"<p>The images below show the effect of using the slider to change <code>config['find_spots']['r_z']</code>  with <code>config['find_spots']['r_yx']</code> fixed at 2 and z thickness = 2:</p> 2612 <p></p> <p></p> <p></p> <p>Again we see the number of spots reducing as this increases. However, it is less clear as to why, because the minimum separation in z is what is changing but, we are only seeing spots from 5 z-planes imposed on a single z-plane.</p>"},{"location":"pipeline/find_spots/#intensity-threshold","title":"Intensity threshold","text":"<p>The images below show the effect of using the slider to change <code>nb.extract.auto_thresh[t, r, c]</code>. This is for a  2D experiment with <code>config['find_spots']['r_yx'] = 2</code>.</p> 44722344757 <p></p> <p></p> <p></p> <p>This is useful to see what a suitable intensity threshold should be. For example, the 447 plot identifies spots which do not look real while the 4757 plot misses some obvious spots.</p> <p>The value of <code>nb.extract.auto_thresh[t, r, c]</code> obtained for this data is approximately 2234.</p> <p>The green spots are those which are identified as isolated. </p>"},{"location":"pipeline/find_spots/#changing-auto_thresh","title":"Changing <code>auto_thresh</code>","text":"<p>If after playing with this slider, it is decided that the <code>find_spots</code> part of the pipeline should be run (or re-run) with the updated intensity threshold, <code>new_thresh</code>, for tile \\(t\\), round \\(r\\), channel \\(c\\),  this can be achieved by setting <code>nb.extract.auto_thresh[t, r, c] = new_thresh</code>.</p> <p>Note, this is an abuse of the rules of the Notebook as it is  changing a variable after it has been added  and thus should not be allowed, but it does work. To keep the <code>new_thresh</code> value, the Notebook will need saving after <code>nb.extract.auto_thresh</code> has been updated. It should be saved to a different location, so it does  not overwrite the initial Notebook i.e. <code>nb.save('/Users/user/experiment1/output/notebook_new_auto_thresh.npz')</code>.</p>"},{"location":"pipeline/find_spots/#isolation-threshold","title":"Isolation threshold","text":"<p>The images below show the effect of using the slider to change <code>nb.find_spots.isolation_thresh[t]</code>.  This is for a 2D experiment with:</p> <ul> <li><code>config['find_spots']['r_yx'] = 2</code>, </li> <li><code>config['find_spots']['isolation_radius_inner'] = 2</code></li> <li><code>config['find_spots']['isolation_radius_xy'] = 2</code>.</li> </ul> <p>There is no slider to change the isolation radii because the dilation calculation is quite slow and has to be  re-done everytime the radii change.</p> -200-438-494 <p></p> <p></p> <p></p> <p>Here, we see that as the absolute threshold increases, the spots need a darker (more negative) annulus to be considered isolated (green).</p> <p>The value of -438 is approximately the value of <code>nb.find_spots.isolation_thresh[t]</code> used for this data (i.e. <code>config['find_spots']['auto_isolation_thresh_multiplier'] = -0.2</code> and  <code>auto_thresh[t, r, c] = 2234</code> gives -447 which is almost the same).</p>"},{"location":"pipeline/find_spots/#pseudocode","title":"Pseudocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline.</p> <pre><code>for r in use_rounds:\n    for t in use_tiles:\n        for c in use_channels:\n            if r is anchor_round and c is not anchor_channel:\n                Skip to next channel as no spots need detecting on this channel.\n            im = load image from npy file in tile directory\n            spots_trc = detect spots(im)\n            Remove spots with negative neighbouring pixel from spots_trc\n            if r is ref_round and c is ref_channel:\n                Determine which spots are isolated.\n            else:\n                Keep only most intense spots on each z-plane.\n            Set spot_no[t, r, c] to be the number of spots found.\n            For spot s, record in spot_details[s]:\n              - tile: tile found on\n              - round: round found on\n              - channel: channel found on\n              - isolated: whether spot isolated (if not ref_round/ref_channel, \n                this will be False)\n              - y: y coordinate of spot in tile\n              - x: x coordinate of spot in tile\n              - z: z coordinate of spot in tile\nAdd isolation_threshold, spot_no and spot_details to find_spots NotebookPage\nReturn find_spots NotebookPage          \n</code></pre>"},{"location":"pipeline/get_reference_spots/","title":"Get Reference Spots","text":"<p>The get reference spots step of the pipeline uses the affine transforms found in the register step of the pipeline to compute the corresponding coordinate of each reference spot (detected on the reference round/reference channel (\\(r_{ref}\\)/\\(c_{ref}\\)) in  find spots step) in each imaging round and channel.  By reading off the intensity values at these coordinates, an \\(n_{rounds} \\times n_{channels}\\)  intensity vector or <code>spot_color</code> can be found for each reference spot. </p> <p>These intensity vectors are saved as <code>colors</code> in the <code>ref_spots</code> NotebookPage  which is added to the Notebook after this stage is finished and are used for assigning each spot to a gene in the call reference spots step. </p> Variables in <code>ref_spots</code> NotebookPage <p>All variables in the <code>ref_spots</code> NotebookPage are arrays  where the size of the first axis is \\(n_{spots}\\) i.e. each variable has info for each reference spot.</p> <p>The variables <code>local_yxz</code>, <code>isolated</code> and <code>tile</code> are just copied from <code>nb.find_spots.spot_details</code>.</p> <p>The variables <code>gene_no</code>, <code>score</code>, <code>score_diff</code>, <code>intensity</code> are not computed until the  call reference spots step but because each is an array of size \\(n_{spots}\\),  they are saved in the <code>ref_spots</code> page instead of the  <code>call_spots</code> page.</p> <p>Before the call reference spots step though, their values will be set to <code>None</code>. This is so if there is an error in <code>call_reference_spots</code>,  <code>get_reference_spots</code> won't have to be re-run.</p>"},{"location":"pipeline/get_reference_spots/#spot-colors","title":"Spot Colors","text":""},{"location":"pipeline/get_reference_spots/#duplicates","title":"Duplicates","text":"<p>We don't find the <code>spot_color</code> for every reference spot because there will be duplicates - the same spot detected on more than 1 tile. This is because there is an overlap between the tiles. </p> <p>To remove these duplicates, we only keep spots  which were detected on a tile (saved in <code>nb.find_spots.spot_details</code>  during the find spots step) which is also the tile whose centre they are closest to in the  global coordinate system (<code>nb.stitch.tile_origin + nb.basic_info.tile_centre</code>).</p> <p>The <code>view_stitch</code> function shows the duplicate spots in blue.</p>"},{"location":"pipeline/get_reference_spots/#applying-transform","title":"Applying transform","text":"<p>To determine the aligned coordinate of each reference spot detected on tile \\(t\\) in round \\(r\\), channel \\(c\\), we must apply the affine transform found for  tile \\(t\\), round \\(r\\), channel \\(c\\): <code>nb.register.transform[t, r, c]</code> to the \\(yxz\\) coordinates of the spots.</p> <p>First the \\(yxz\\) coordinates must be centered (subtract <code>nb.basic_info.tile_centre</code>) and the z-coordinate must be converted into units of yx-pixels (multiply by <code>z_scale = nb.basic_info.pixel_size_z /  nb.basic_info.pixel_size_xy</code>). The \\(n_{spots} \\times 3\\) array must then be padded with ones to form an \\(n_{spots} \\times 4\\) array, so it can be  multiplied by the \\(4 \\times 3\\) transform. The coordinates are prepared in this way because they must be in the same  form as was used to compute the transform (see Preparing point clouds and Padding <code>ref_spot_yxz</code> notes  here). </p> <p>Once the \\(n_{spots} \\times 4\\) spot coordinate array is multiplied by the \\(4 \\times 3\\) transform, a  \\(n_{spots} \\times 3\\) array is obtained, and after the z-scaling and centering are removed, this gives the corresponding \\(yxz\\) coordinates in round \\(r\\), channel \\(c\\). </p>"},{"location":"pipeline/get_reference_spots/#reading-off-intensity","title":"Reading off intensity","text":"<p>After the \\(yxz\\) coordinates in tile \\(t\\), round \\(r\\), channel \\(c\\), are found, the intensity values at these coordinates are obtained by supplying the \\(n_{spots} \\times 3\\) array as the parameter <code>yxz</code> in the function  <code>load_tile</code>.</p> <p>After doing this for all tiles,  rounds and channels, we obtain the <code>spot_color</code> where <code>spot_color[s, r, c]</code> is the intensity value found for  spot <code>s</code> in round <code>r</code>, channel <code>c</code>.</p>"},{"location":"pipeline/get_reference_spots/#invalid-values","title":"Invalid Values","text":"<p>For some spots, the corresponding coordinates in round \\(r\\), channel \\(c\\), will be outside the bounds of the tile and  thus the intensity cannot be read off. We therefore only save to the Notebook spots which remain in the tile bounds across all rounds and channels, allowing the full <code>spot_color</code> array to be computed.</p> <p>Once the Notebook has the <code>ref_spots</code> NotebookPage, when  <code>view_stitch</code> is run, there will a button called No Spot Color which shows in blue  all spots removed for this reason:</p> <p></p> <code>spot_color</code> outside rounds/channels used <p><code>nb.ref_spots.colors</code> is an <code>n_spots x nb.basic_info.n_rounds x nb.basic_info.n_channels</code> array. <code>nb.ref_spots.colors[s, r, c]</code> will be set to <code>-nb.basic_info.tile_pixel_value_shift</code> for all spots, <code>s</code>,  if either <code>r</code> is not in <code>nb.basic_info.use_rounds</code> or <code>c</code> is not in <code>nb.basic_info.use_channels</code>.</p> <p>This is because it is impossible for an actual pixel to have this intensity, due to clipping  done in the extract step when  saving the tiles.  So basically, this is an integer version of <code>nan</code>.</p>"},{"location":"pipeline/get_reference_spots/#pseudocode","title":"Pseudocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline.</p> <pre><code>r_ref = reference round\nc_ref = reference round\nspot_yxz[t, r, c] = yxz coordinates for spots detected on tile t,\n                    round r, channel c.\ntransform[t, r, c] = affine transform between tile t, round r_ref, channel c_ref\n                     and round r, channel c. \n                     [n_tiles x n_rounds x n_channels x 4 x 3]                   \n\nRemove duplicate spots from spot_yxz[:, r_ref, c_ref].\nCenter reference point cloud:\n    spot_yxz[:, r_ref, c_ref] = spot_yxz[:, r_ref, c_ref] - tile_centre\nConvert z coordinate into yx-pixels:\n    spot_yxz[:, r_ref, c_ref][:, 2] = spot_yxz[:, r_ref, c_ref][:, 2] * z_scale\nPad reference point cloud with ones:\n    spot_yxz[:, r_ref, c_ref][:, 3] = 1 \n\nfor t in use_tiles:\n    spot_colors_t: [n_spots_t x n_rounds x n_channels]              \n    for r in use_rounds:\n        for c in use_channels:\n            Apply transform to get new coordinates in round r, channel c:\n                spot_yxz_rc = spot_yxz[t, r_ref, c_ref] @ transform[t, r, c]\n            Convert z coordinate back to z-pixels:\n                spot_yxz_rc[:, 2] = spot_yxz_rc[:, 2] / z_scale\n            Remove centering:\n                spot_yxz_rc = spot_yxz_rc + tile_centre\n\n            For spots where spot_yxz_rc is outside the tile bounds, \n            cannot read off intensity: \n                spot_colors_t[oob_spots, r, c] = nan\n            For all other spots, we read off the intensity at the \n            coordinates we found\n                image_trc = load in image for tile t, round r, channel c\n                from npy file in tile directory\n                spot_colors_t[good_spots, r, c] = image_trc[spot_yxz_rc[good_spots]]\n\nConcatenate all spot_colors_t together so have one large \n[n_spots x n_rounds x n_channels] array\ngiving colors for all spots across all tiles.\n\nGet rid of any spot for which at least one round and channel has the nan value \ni.e. they were out of bounds on at least one round/channel.\n\nAdd colors to ref_spots NotebookPage      \nAdd local_yxz, isolated and tile variables by reshaping information\n    in nb.find_spots.spot_details.\nAdd gene_no, score, score_diff, intensity to ref_spots NotebookPage \n    all with the value None.\n</code></pre>"},{"location":"pipeline/omp/","title":"OMP","text":"<p>The OMP step of the pipeline runs an Orthogonal Matching Pursuit (OMP) algorithm on every  pixel, adding multiple gene <code>bled_codes</code>  to explain the \\(n_{rounds} \\times n_{channels}\\) intensity vector  at each pixel. This gives us a coefficient, \\(\\mu_{sg}\\) for every pixel, \\(s\\), and gene, \\(g\\), and thus we can  create a coefficient image for every gene.</p> <p>By doing a local maxima search on these images, we can obtain another estimate of the distribution of genes, which  we can compare to that obtained from the reference spots. The number and location of spots found by the OMP method will be different though.</p> <p>There are some variables obtained for each spot (<code>local_yxz</code>, <code>tile</code>,  <code>colors</code>, <code>gene_no</code> and  <code>intensity</code>) saved in the  <code>omp</code> NotebookPage which are equivalent to the same variables saved  in the <code>ref_spots</code> NotebookPage.  There are also some other variables added to the <code>omp</code> NotebookPage which  relate to the typical shape of a spot in the gene coefficient images.</p> <p>The OMP section takes quite a long time, so <code>config['omp']['use_z']</code> can be used to only run  OMP on a subset of z-planes.</p> <p>Note: <code>config</code> in this section, with no section specified, means <code>config['omp']</code></p>"},{"location":"pipeline/omp/#re-run-call_spots_omp","title":"Re-run <code>call_spots_omp</code>","text":"<p>To re-run the OMP section, the files generated during the OMP step which were saved at the paths indicated by:</p> <ul> <li><code>nb.file_names.omp_spot_shape</code></li> <li><code>nb.file_names.omp_spot_info</code></li> <li><code>nb.file_names.omp_spot_coef</code></li> </ul> <p>need to be deleted or re-named (alternatively the <code>omp_spot_shape</code>, <code>omp_spot_info</code> and <code>omp_spot_coef</code>  parameters in the file names section of the configuration file can be changed). This is so the old data is not loaded in when the OMP part of the pipeline is run. Other than that, the usual instructions can be followed.</p>"},{"location":"pipeline/omp/#why-bother-with-omp","title":"Why bother with OMP?","text":"<p>There are two main reasons to use the OMP method for finding the distribution of genes instead of reference spots. The first is that OMP fits multiple coefficients to every  pixel and thus allows for overlapping spots:</p> <code>view_omp_fit</code><code>view_omp</code> <p></p> <p></p> <p>In the <code>view_omp_fit</code> plot, it shows an example of a spot color,  \\(\\pmb{\\zeta}_s\\), which requires the <code>bled_code</code> of both Plp1 and Aldoc to explain it. The  <code>view_omp</code> plot then shows that spots for both these genes are detected with  the OMP method. The reference spots method though, can only fit one gene  to each pixel, so here it only detects the Plp1 spot.</p> <p>The second reason is that the reference spots method can only assign genes to spots detected on the reference round / reference channel images. It is thus restricted, because there may  be genes present at pixels other than the finite amount considered. Also, the  <code>config['find_spots']['radius_xy']</code> and <code>config['find_spots']['radius_z']</code> parameters in the spot detection  necessitates a minimum distance between any two genes.</p> <p>The OMP method says that spots are local maxima in the gene coefficient images, and thus it can find spots at locations other than the location of the reference spots. Also, because it does a separate local maxima search for each gene, the <code>config['radius_xy']</code> and <code>config['radius_z']</code> parameters in spot detection  only necessitates a minimum distance between two spots of the same gene.</p> <p>The consequence of this, is that the spots detected by the OMP method tend to be in more dense clusters, as shown  by the <code>coppafish.Viewer</code> images below. This  is then more useful for cell typing</p> Reference SpotsOMP <p></p> <p></p>"},{"location":"pipeline/omp/#initial-intensity-threshold","title":"Initial Intensity Threshold","text":"<p>To produce the gene coefficient images, as shown by the <code>view_omp_fit</code>  function, we need to run OMP on every pixel in the image. However, for a single \\(2048\\times2048\\times50\\) tile,  there are \\(2.1\\times10^8\\) pixels. Thus, we do an initial thresholding so as not to consider all of them.</p> <p>We only consider pixel \\(s\\), with pixel color \\(\\pmb{\\zeta}_s\\), if it has an  intensity computed from its absolute pixel color,  \\(\\tilde{\\chi}_s\\), greater than <code>config['initial_intensity_thresh']</code>. I.e.</p> \\[ \\tilde{\\chi}_s = \\underset{r}{\\mathrm{median}}(\\max_c|\\zeta_{s_{rc}}|) \\] Why is absolute pixel color is used? <p>We use \\(|\\pmb{\\zeta}_s|\\) because we are also interested in whether a pixel has a negative gene coefficient. We expect a negative coefficient in an annulus around a spot, as shown in the  <code>view_omp</code> plots, as a result of the  difference of hanning kernel used in the initial filtering. If a gene has a negative coefficient annulus around the local maxima, then it boosts our confidence that it is legitimate.</p> <p>If <code>config['initial_intensity_thresh']</code> is not specified, it is  set to the percentile indicated by  <code>config['initial_intensity_thresh_percentile']</code> of \\(\\tilde{\\chi}\\) computed for all  pixels in the middle z-plane (<code>nb.call_spots.norm_shift_tile</code>) of the central tile (<code>nb.call_spots.norm_shift_z</code>). I.e., it is set to: <pre><code>nb.call_spots.abs_intensity_percentile[config['initial_intensity_thresh_percentile']]\n</code></pre> So this is saying that for a tile and z-plane which we expect to be among the most fluorescent, we are getting  rid of a quarter of the pixels (with the default value of <code>config['initial_intensity_thresh'] = 25</code>) which  are the least intense. On other z-planes, we will get rid of more.</p>"},{"location":"pipeline/omp/#omp-algorithm","title":"OMP Algorithm","text":"<p>For every pixel that passes the initial intensity threshold,  \\(s\\), we run an Orthogonal Matching Pursuit (OMP) algorithm to find a coefficient,  \\(\\mu_{sg}\\), for every gene \\(g\\). The pseudocode for how this is done  for each pixel is given below:</p> <p><pre><code>color: Intensity read from .npy files in tile directory for \n    each round and channel.\n    [n_rounds x n_channels]\ncolor_norm_factor: nb.call_spots.color_norm_factor\n    [n_rounds x n_channels]\nbled_codes: nb.call_spots.bled_codes_ge\n    [n_genes x n_rounds x n_channels]\nbackground_codes: nb.call_spots.background_codes\n    [n_channels x n_rounds x n_channels]\n\n1. Normalise color\n    color = color / color_norm_factor\n2. Compute background_coefs.\n    Remove background from color - \n        for c in range(n_channels):\n            color = color - background_coefs[c] * \n                            background_codes[c]\n3. Initialize variables for iterating.\n    residual = color  \n    added_genes = [] \n    Append background_codes to bled_codes so has shape\n        [(n_genes+n_channels) x n_rounds x n_channels]\n    Initialize coefs with background_coefs which will not change - \n        coefs = zeros(n_genes+n_channels)\n            coefs[n_genes:] = background_coefs\n\ni = 0\nwhile i &lt; n_iter:\n    4. Find best_gene to add based on dot product score between residual and \n        bled_codes.\n        If score &lt; score_thresh or best_gene is background or best_gene \n        already added:\n            Stop - go to step 7.\n        Append best_gene to added_genes.\n    5. Obtain added_coefs [i+1] \n        for how the bled_codes of all genes in\n        added_genes can be combined to best explain color.\n        Update coefs -\n            for g_ind in range(i+1):\n                coefs[added_genes[g_ind]] = added_coefs[g_ind]\n    6. Update residual -\n        residual = color\n        for g in added_genes:\n            residual = residual - coefs[g] * bled_codes[g]\n    i += 1\n7. return coefs            \n</code></pre> There are a few parameters in the configuration file which are used:</p> <ul> <li><code>max_genes</code>: This is <code>n_iter</code> in the above code.</li> <li><code>dp_thresh</code>: This is <code>score_thresh</code> in the above code.</li> </ul>"},{"location":"pipeline/omp/#pre-iteration-procedure","title":"Pre-Iteration Procedure","text":"<p>Prior to Step 1, <code>color</code> is \\(\\pmb{\\acute{\\zeta}}_s\\) found through  <code>get_spot_colors</code>  by obtaining the aligned coordinate of pixel \\(s\\) in each round and  channel and then reading off the corresponding intensity.</p> <p>Step 1 is then just converting <code>color</code> from \\(\\pmb{\\acute{\\zeta}}_s\\) to \\(\\pmb{\\zeta}_s\\) so the color channels  are equalised, as is done in <code>call_reference_spots</code>.</p> <p>Step 2 is just finding a coefficient \\(\\mu_{sC}\\) for each background gene, \\(\\pmb{B}_C\\) as is done in <code>call_reference_spots</code>. The value of \\(\\lambda_b\\) in the \\(w_{rC}\\) equation is set to <code>nb.call_spots.background_weight_shift</code> so the same value is used for the  reference spots and OMP methods. After this step, <code>color</code> is \\(\\pmb{\\zeta}_{s0}\\) and will always remain so. I.e. once background is fit once, it is never updated.</p> <p>Step 3 is adding the artificial background genes to our actual genes. After this step, we have a set of <code>bled_codes</code>, \\(\\pmb{b}\\) such that \\(\\pmb{b}_{g=n_{g}+C} = \\pmb{B}_C\\). We also have a set of coefficients, \\(\\pmb{\\mu}_{s0}\\) such that \\(\\mu_{s0g=n_g+C} = \\mu_{sC}\\). The set of coefficients after \\(i\\) actual genes have been fit is \\(\\pmb{\\mu}_{si}\\) (\\(i=0\\) means just background has been fit) but \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i,C\\)  as the background coefficients are never updated.</p>"},{"location":"pipeline/omp/#finding-best-gene","title":"Finding Best Gene","text":"<p>Step 4 is finding the gene, \\(g_i\\), with the <code>bled_code</code>, \\(b_{g_i}\\) which best represents the residual after \\(i\\)  actual genes have been added, \\(\\pmb{\\zeta}_{si}\\). </p> <p>We determine \\(g_i\\) to be the gene for which \\(|\\Delta_{sig}|\\) is the largest, where \\(\\Delta_{sig}\\) is exactly  the same dot product score used in <code>call_reference_spots</code>. We use  the absolute score here because negative gene coefficients also provide useful information as  explained earlier.</p> Dot Product Score Parameters <p>In the configuration file, <code>alpha</code> and <code>beta</code> are specified in both the <code>call_spots</code> and <code>omp</code> sections, so different values of these parameters can be used for each method.</p> <p>The value of \\(\\lambda_d\\) in the \\(\\tilde{\\zeta}_{{si}_{rc}}\\) equation is set  to <code>nb.call_spots.dp_norm_shift * sqrt(n_rounds)</code> so the same value is used for the  reference spots and OMP methods.</p> <p>After we have found \\(g_i\\), we stop the algorithm if any of the following are satisfied:</p> <ul> <li>\\(|\\Delta_{si}| = |\\Delta_{sig_i}|\\) &lt; <code>config['dp_thresh']</code>.</li> <li>\\(g_i \\geq n_g\\) i.e. \\(g_i\\) is a background gene.</li> <li>\\(g_i\\) was found to be the best gene on a previous iteration.</li> </ul> <p>This second condition occurred for the third gene added in the <code>view_omp_fit</code> example shown  earlier. This case is quite rare though, since the background genes have already been fit. We include it because if there is an artificial gene being identified as the best one, then we are also likely to erroneously assign actual genes to the pixel which are not actually there. But stopping the algorithm prevents  this from happening. It may be more appropriate to recompute the background coefficient, for the background gene identified if this happens though.</p> <p>The third condition is just to stop getting in a loop where we are always fitting the same gene, but it is even rarer  than the second.</p>"},{"location":"pipeline/omp/#finding-gene-coefficients","title":"Finding Gene Coefficients","text":"<p>Once we have decided that a gene is acceptable, in Step 5 we find the coefficient of that gene as well as updating the coefficients of all genes previously fit. On iteration \\(i\\), there will be \\(i+1\\) genes to find the coefficient of. The coefficients are found  through normal least squares:</p> \\[ \\tilde{\\pmb{\\mu}}_{si} = (\\pmb{G}_i^T\\pmb{G}_i)^{-1}\\pmb{G}_i^T\\pmb{\\zeta}_{s0} \\] <p>Where:</p> <ul> <li>\\(\\tilde{\\pmb{\\mu}}_{si}\\) is a vector of \\(i+1\\) values such that \\(\\mu_{sig_j} = \\tilde{\\mu}_{si_j}\\) where \\(\\mu_{sig_j}\\) is the coefficient found for gene \\(g_j\\) on iteration \\(i\\) for pixel \\(s\\).</li> <li>\\(\\pmb{G}_i\\) is a matrix of shape \\([n_{rounds}n_{channels} \\times (i+1)]\\) such that column \\(j\\) is the flattened <code>bled_code</code>, \\(\\pmb{b}_{g_j}\\), for gene \\(g_j\\), added on iteration \\(j\\).</li> <li>\\(\\pmb{\\zeta}_{s0}\\) is the color for pixel \\(s\\) after background removal flattened, so it has shape  \\([n_{rounds}n_{channels} \\times 1]\\).</li> </ul> Weighted Least Squares <p>If <code>config['weight_coef_fit'] = True</code>, then the coefficients are found through  weighted least squares.</p> <p>In this case, both \\(\\pmb{\\zeta}_{s0}\\) and every column of \\(\\pmb{G}_i\\) are multiplied by \\(\\pmb{\\omega}_{si}\\) where \\(\\pmb{\\omega}^2_{si}\\) is defined in the  dot product score calculation.</p> <p>The idea behind this is that for normal least squares, the coefficients will be overly influenced by outliers. In the Least Squares <code>view_omp_fit</code>  example below, when fitting Plp1, it is so concerned about getting rid of the very intense values in channel 4  that it makes lots of channel 2 and 6 values negative.</p> Least SquaresWeighted Least SquaresWeight - Iteration 1Weight - Iteration 2 <p></p> <p></p> <p></p> <p></p> <p>In the Weighted Least Squares, we see that after Aldoc has been fit, the  channel 4 intensity is much larger than in the Least Squares case and the channel 6 intensities are much less negative.</p> <p>If we look at the Weight - Iteration 1 <code>view_weight</code> plot, we see that this occurs because the contribution of the channel 4 rounds is very small.</p> <p>A problem with this though, is that the weight is re-computed at each iteration which can cause the coefficient of each gene to change drastically. For example, after BG2 has been fit, we see that channel 6 has become very negative again. Looking at the Weight - Iteration 2 plot, we see that this is because the weight of round 4, channel 4 has increased.</p> <p>The coefficient for Plp1 in the Weighted Least Squares case changes from 0.84 to 0.93 after BG2 has been  fit but in the Least Squares case, it changes from 1.00 to 1.01.</p> <p>After we have found the coefficients on iteration \\(i\\), we compute the residual for the next iteration in Step 6:</p> \\[ \\pmb{\\zeta}_{si+1} = \\pmb{\\zeta}_{s0} - \\sum_{g=0}^{n_g-1}\\mu_{sig}\\pmb{b}_g  = \\pmb{\\zeta}_s - \\sum_{g=0}^{n_g+n_c-1}\\mu_{sig}\\pmb{b}_g  \\] <p>Where \\(n_c\\) is the number of channels and \\(n_g\\) is the number of genes. The second equation includes the combination from the background genes. In the first equation, \\(\\mu_{sig}\\) will only be non-zero for \\(i+1\\) genes (in  the second equation, it will be for \\(i+1+n_c\\) genes, because we include the \\(n_c\\) background genes).</p> <p>We continue iterating between Step 4 and Step 6 until any of the stopping criteria are met,  or we fit <code>config['max_genes']</code> to the pixel.</p>"},{"location":"pipeline/omp/#weighting-in-dot-product-score","title":"Weighting in Dot Product Score","text":"<p>The difference between this algorithm and the standard Orthogonal Matching Pursuit algorithm is the weight factor, \\(\\pmb{\\omega}^2_{si}\\), used when computing  \\(\\Delta_{sig}\\). Normal OMP would have  \\(\\alpha = 0\\) so \\(\\pmb{\\omega}^2_{si}=1\\). </p> <p>By using this weighting, we are trying to say that if a gene has already been fit with high intensity in round \\(r\\), channel \\(c\\), then the remaining intensity in round \\(r\\), channel \\(c\\), after it has been fit  is probably because the coefficient of that gene was not fit correctly, rather than because another gene is  present. As the example below shows, without it, genes are fit to really try and get rid of any anomalously  intense rounds/channels even if a gene has already been fit there.</p> <code>view_omp_fit</code> - \\(\\alpha=120\\)<code>view_omp_fit</code> - \\(\\alpha=0\\)<code>view_weight</code> - \\(\\alpha=120\\), Iteration 4<code>view_score</code> - \\(\\alpha=120\\), Iteration 4 <p></p> <p></p> <p></p> <p></p> <p>If we compare the <code>view_omp_fit</code> plots, we see that  in the \\(\\alpha=0\\) case, Rgs4 has been fit to explain the anomalously negative round 6, channel 5 and Aldoc has been fit to explain the anomalously positive round 2, channel 1.</p> <p>The <code>view_weight</code> image shows the calculation of the weight factor for \\(i=4\\) (all 4 of the actual genes shown have been added). This shows that round 2, channel 1 has a low weighting because Lhx6 has already been fit and this gene has high intensity in round 2, channel 1.</p> <p>If we look the <code>view_score</code> image, we see that in the top right, with \\(\\alpha=0\\), we get a large score for Aldoc solely because of the contribution from round 2, channel 1. If we look at the bottom right though, we see that with \\(\\alpha=120\\), the score for Aldoc is very small, as we  would expect it should be, when comparing the 2 plots in the second column of the <code>view_score</code> image.</p> <p>Basically, we need the weighting because we know that the least squares  coefficient fitting will produce some anomalously intense rounds and channels.  When selecting the best gene, we need to be robust to this.</p>"},{"location":"pipeline/omp/#finding-spots","title":"Finding Spots","text":"<p> After we have run the OMP algorithm on every pixel of a tile, we can  produce an image for each gene  based on the \\(n_{pixels}\\times n_{genes}\\) array of coefficients, \\(\\pmb{\\mu}\\).</p> <p>A \\(200\\times 200\\) pixel section of such an image for three genes is shown on the right. Clearly, most values in each image are zero because each pixel only has a non-zero coefficient for a very small fraction of genes.</p> <p></p> <p>We then take each gene in turn and find spots,  which are the local maxima in the coefficient image. These local maxima are  found in exactly the same way as in the find spots part of the pipeline. But here, the threshold intensity is 0 and the neighbourhood (kernel for dilation) is defined by the parameters <code>config['radius_xy']</code> and <code>config['radius_z']</code>.</p>"},{"location":"pipeline/omp/#spot-shape","title":"Spot Shape","text":"<p>In the gene coefficient images above, the spots can clearly be seen as red (positive) circles surrounded by a blue  (negative) annulus. So to decide whether a particular spot is legitimate, we want to compare its shape to  the average spot shape.</p> <p>This average spot shape can be specified in advance (e.g. if you have already run an experiment, which is expected to be similar to the current one, and you want the same shape to be used) with a npy file in the output directory  with the name given by <code>config['file_names']['omp_spot_shape']</code>. This file must contain an image (axis in  the order z-y-x) indicating the expected sign of a coefficient (only values are 1, 0, -1)  in the neighbourhood of a spot.</p> <p>If the file indicated by <code>config['file_names']['omp_spot_shape']</code> does not exist, then it will be  computed from the spots found on a specific tile. The tile used is the one for which the most spots were found with the  reference spots method, and it is saved as <code>nb.omp.shape_tile</code>. </p> <p>The psuedocode for obtaining the spot shape is given below:</p> <p><pre><code>spot_yxz: yxz coordinates of all spots found on the tile\n    [n_spots x 3]\ngene_no: gene_no[s] is the gene whose coefficient image\n    spot s was found on.\n    [n_spots].\ntile_sz: nb.basic_info.tile_sz\nnz: nb.basic_info.nz\ngene_coef_im: gene_coef_im[g] is the gene coefficient image\n    for gene g, which can be made after running OMP on each pixel.\n    [n_genes x tile_sz x tile_sz x nz]\n\nfor s in range(n_spots):\n    1. use = True if number of pixels neighbouring\n        spot_yxz[s] in gene_coef_im[gene_no[s]]\n        with a positive value equals use_thresh.\n    2. If use, then obtain the [shape_ny x shape_nx x shape_nz] image \n        centered on spot_yxz[s] in gene_coef_im[gene_no[s]]. \n\n3. We now have spot_images which is a \n    [n_use x shape_ny x shape_nx x shape_nz] array.\n    We update this by only keeping images from isolated spots.\n    [n_use2 x shape_ny x shape_nx x shape_nz]\n4. Compute spot_sign_images by taking the sign of every value in spot_images \n    so the only values are 1, 0, -1.\n    Next, we compute av_spot_sign_image which is the average of \n    spot_sign_images across spots.\n    Where abs(av_spot_sign_image) &lt; sign_thresh, set it to 0.\n    Take sign of av_spot_sign_image so it only contains 1, 0, -1.\n    [shape_ny x shape_nx x shape_nz]\n</code></pre> There are a few parameters in the configuration file which are used:</p> <ul> <li><code>shape_pos_neighbour_thresh</code>: In 2D, this is <code>use_thresh</code> in the above code. In 3D, this is <code>use_thresh - 2</code>.</li> <li><code>shape_max_size</code>: This is <code>[shape_ny, shape_nx, shape_nz]</code> in the above code.</li> <li><code>shape_sign_thresh</code>: This is <code>sign_thresh</code> in the above code.</li> </ul> <p>Step 1 is a thresholding procedure, so we only use spots we are quite confident are real for computing the average shape. For a spot to be used in computing the shape, on the z-plane of the gene coefficient image that it was found on,  within the \\(n_{pos_{use}}\\times n_{pos_{use}}\\) neighbourhood with the local maxima in the centre, all  <code>n_use =</code>\\(n_{pos_{use}}^2\\) coefficients must be positive. In 3D, the coefficient at the same \\(yx\\) coordinate as the local maxima but on 1 z-plane either side must be also positive (now require <code>n_use =</code>\\(n_{pos_{use}}^2 + 2\\)  positive coefficients in the neighbourhood of the spot). The value of \\(n_{pos_{use}}^2\\) is given by  <code>config['shape_pos_neighbour_thresh']</code>, the default value is 9 meaning all pixels in a \\(3\\times 3\\) grid centered on the local maxima must be positive.</p> Example <p>The first image below shows a spot that would be used to compute the shape with  <code>config['shape_pos_neighbour_thresh'] = 3</code>because all 9 pixels in the central z-plane have  a positive coefficient, as do those on 1 z plane either side but in the middle yx pixel.</p> <p>The second image shows a spot that would not be used.</p> \u2705\u274c <p></p> <p></p> <p>In Step 2, we just get the cropped  gene coefficient image in the neighbourhood of the local maxima. 3 examples are shown as Spot 1, Spot 2 and Spot 3 below.</p> <p>In Step 3, we are saying that most spots are probably not overlapping with any other genes so to get an estimate of what an average spot looks like, let us only use spots which are quite well isolated. Our definition of isolated here, is that the distance between each spot used in Step 2 and any other spot used in Step 2 must exceed <code>config[shape_isolation_dist]</code>.</p> Spot 1Spot 2Spot 3Average SignSpot Shape <p></p> <p></p> <p></p> <p></p> <p></p> <p>In Step 4, we first take the sign of the spot images and then compute the average of these using  <code>get_average_spot_image</code>. We set <code>av_type = 'mean'</code> and <code>symmetry = 'annulus_3d'</code>. We use this symmetry because we assume that the  spot should be circular within a z-plane and symmetric in z. This procedure produces the Average Sign image shown above. This is saved to the Notebook as <code>nb.omp.spot_shape_float</code>.</p> <p>After we have this <code>av_sign_image</code>, we get our final <code>spot_shape</code> via:</p> <pre><code>import numpy as np\nav_sign_image[np.abs(av_sign_image)] &lt; config['omp']['shape_sign_thresh']] = 0\nspot_shape = np.sign(av_sign_image)\n</code></pre> <p>This is just saying that if the absolute value of the mean sign across all these spots was less than  <code>config['omp']['shape_sign_thresh']</code>, then we are not sure what sign these pixels  should be, so we won't assign them a sign in our final <code>spot_shape</code>. Otherwise,  we are fairly confident what sign they should be, so we do assign the sign to our final shape. The Spot Shape image above shows what this looks like.</p> Why does the spot shape indicate the expected sign of the coefficient? <p>The <code>spot_shape</code> indicates the expected sign of a coefficient, not the expected value. This is because, through the OMP algorithm, for a pixel to have a non-zero coefficient  for any gene, it has already gone through a thresholding procedure which has decided that  the gene <code>bled_code</code> is required to explain the pixel color.</p> <p>The idea behind counting the number of coefficients with the correct sign is  that if there are lots of pixels in the neighbourhood of the spot which required a certain gene, then that gives us confidence that the gene is actually present. </p> <p>If we instead convolved the actual coefficent image (e.g. Spot 1 image shown above)  with the expected coefficient kernel (e.g. mean of spot_images in Step 4 of the pseudocode), it  would boost the score of intense pixels which we don't want to do.</p> <p>The final <code>spot_shape</code> is saved to the Notebook as <code>nb.omp.spot_shape</code> and is also saved as a  npy file in the output directory with the name indicated by <code>config['file_names']['omp_spot_shape']</code>. The coordinates and corresponding gene of spots used to compute <code>nb.omp.spot_shape</code> are saved as <code>nb.shape_spot_local_yxz</code> and <code>nb.shape_spot_gene_no</code> respectively.</p>"},{"location":"pipeline/omp/#error-no-negative-values-in-nbompspot_shape","title":"Error - No negative values in <code>nb.omp.spot_shape</code>","text":"<p>If <code>config['shape_sign_thresh']</code> is too large, <code>nb.omp.spot_shape</code> will not have any -1 values. This will then raise an error because <code>nb.omp_n_neighbours_neg</code> cannot be computed. To get past this error, the OMP step needs to be re-run with either  <code>config['shape_sign_thresh']</code> set to a lower value or  <code>nb.omp.spot_shape</code> specified through <code>config['file_names']['omp_spot_shape']</code>.</p> Example <p>The following error was hit when running with <code>config['shape_sign_thresh'] = 0.7</code>:</p> <p></p> <p>The error saves the spot shape, <code>nb.omp.spot_shape_float</code>, which is converted to <code>nb.omp.spot_shape</code> through  thresholding with <code>config['shape_sign_thresh']</code> and then taking the sign.</p> <p>If we then load this file, we can get an idea of a suitable value of <code>config['shape_sign_thresh']</code>:</p> Codenapari Viewer <pre><code>import numpy as np\nimport napari\nim_file = \"/Users/joshduffield/Documents/UCL/ISS/Python/play/B8S5_Slice001_npy/omp_spot_shape_float_ERROR.npy\"\nim = np.load(im_file)\nnapari.view_image(im, contrast_limits=[-1, 1], colormap=\"PiYG\")\n</code></pre> <p></p> <p>Say we decide after looking at the image in napari, that we want values more negative than at the blue cross to be included in the final shape. Then we set <code>config['shape_sign_thresh']</code> to be just below the absolute value at this cross (0.336 as indicated in bottom left).</p> <p>The value of <code>config['shape_sign_thresh']</code> indicated in the error is the maximum allowed value such that <code>nb.omp.spot_shape</code> will contain at least one pixel with the value of -1.</p> <p>If the saved <code>nb.omp.spot_shape_float</code> image has no negative values, then <code>config['file_names']['omp_spot_shape']</code> must be used to specify <code>nb.omp.spot_shape</code>. The saved image can still be used as a guide  as to where <code>nb.omp.spot_shape=1</code> though.</p>"},{"location":"pipeline/omp/#n_neighbours","title":"<code>n_neighbours</code>","text":"<p>Once we have found the <code>spot_shape</code>, we want to quantify how much each spot resembles it. To do this,  if spot \\(s\\), was found on the gene \\(g\\) coefficient image, we first  obtain <code>coef_im_s</code> which is the gene  \\(g\\) coefficient image centered on the coordinates of spot \\(s\\), with the same size as <code>spot_shape</code>.</p> <p>We then count the number of pixels for which  <code>coef_im_s</code> and <code>spot_shape</code> both have positive coefficients. We also count the number of pixels for which  <code>coef_im_s</code> and <code>spot_shape</code> both have negative coefficients. Once this has been done for all spots, these  are saved as <code>nb.omp.n_neighbours_pos</code> and <code>nb.omp.n_neighbours_neg</code> respectively.</p> <p>To reduce the memory of the Notebook file, we only save spots to the Notebook if <code>n_neighbours_pos</code> exceeds <code>config['initial_pos_neighbour_thresh']</code>. If this is left blank, it will be set to the fraction <code>config['initial_pos_neighbour_thresh_param']</code> of the maximum possible value (i.e. <code>sum(spot_shape&gt;0)</code>). It will also be clipped between <code>config['initial_pos_neighbour_thresh_min']</code> and  <code>config['initial_pos_neighbour_thresh_max']</code>.</p>"},{"location":"pipeline/omp/#view_omp_score","title":"<code>view_omp_score</code>","text":"<p>The way <code>nb.omp.n_neighbours_pos</code> and <code>nb.omp.n_neighbours_neg</code> are obtained, can be visualised for a specific spot with the function <code>view_omp_score</code>:</p> \\(\\rho = 0.95\\)\\(\\rho = 0.1\\)\\(\\rho = 20\\) <p></p> <p></p> <p></p> <p>Here, the top plot shows <code>nb.omp.spot_shape</code> and the bottom plot shows the coefficient image for Snca in the  neighbourhood of spot \\(371046\\) which has \\(yxz\\) coordinates in the global coordinate system of  \\([3970, 509, 25]\\). </p> <p>The green hatching in the top plot indicates where both plots have the same sign. So <code>nb.omp.n_neighbours_pos[371046]</code> is the number of red pixels with green hatching (316) and <code>nb.omp.n_neighbours_neg[371046]</code> is the number of  blue pixels with green hatching (192).</p>"},{"location":"pipeline/omp/#omp-score","title":"OMP Score","text":"<p>The final score, \\(\\gamma\\), used for  thresholding OMP spots in <code>coppafish.Viewer</code> and when exporting to  pciSeq is: </p> \\[ \\gamma_s = \\frac{n_{neg_s} + \\rho n_{pos_s}}{n_{neg_{max}} + \\rho n_{pos_{max}}} \\] <p>Where:</p> <ul> <li>\\(n_{neg_s}\\) is <code>nb.omp.n_neighbours_neg[s]</code></li> <li>\\(n_{neg_s}\\) is <code>nb.omp.n_neighbours_pos[s]</code></li> <li>\\(n_{neg_{max}}\\) is <code>sum(nb.omp.spot_shape&lt;0)</code></li> <li>\\(n_{pos_{max}}\\) is <code>sum(nb.omp.spot_shape&gt;0)</code></li> <li>\\(\\rho\\) is <code>config['thresholds']['score_omp_multiplier']</code></li> </ul> <p>So if \\(\\rho = 1\\), this would just be the fraction of pixels in the neighbourhood of spot \\(s\\) with the correct sign. The larger \\(\\rho\\), the greater the contribution of the positive pixels to \\(\\gamma_s\\).</p> <p>With the <code>view_omp_score</code> function, the effect of varying \\(\\rho\\) on the score, can  be seen by using the textbox. The top row of plots are also normalised so \\(\\gamma_s\\) is equal to the sum  of the absolute value of the pixels with the green hatching. I.e. in the \\(\\rho=0.1\\) image, the negative pixels are much larger than the positive and in the \\(\\rho=20\\) image, the positive pixels are much larger than the negative.</p>"},{"location":"pipeline/omp/#saving-omp-results","title":"Saving OMP results","text":"<p>Because the OMP section of the pipeline takes a long time, we want to save the results as we go along, so  we don't have to restart the whole thing if it crashes for some reason e.g. a memory error.</p> <p>Thus, after we have found the spots on a tile, we save the information for all of them in a npy file in the output directory with a name indicated by <code>config['file_names']['omp_spot_info']</code>. For each spot, \\(s\\),  this file contains 7 integer values. The first 3 are the local \\(yxz\\) coordinates (z-coordinate in units of z-pixels) on  the tile it was found on. The fourth is the gene it was assigned to. The fifth is <code>n_neighbours_pos</code> and the sixth is <code>n_neighbours_neg</code>. The seventh is the tile it was found on.</p> <p>We also save the coefficients of each spot \\(s\\) for each gene, \\(\\pmb{\\mu}_s\\),  found via the OMP algorithm. This \\(n_{spots}\\times n_{genes}\\) sparse array is saved as a  npz file in the output directory with a name indicated by <code>config['file_names']['omp_spot_coef']</code>. This  is not actually used anymore but may be useful for debugging purposes.</p> <p>If these files already exist, the <code>call_spots_omp</code> function will simply skip over all tiles  for which spots have been saved and then load them all in after spots have been found on all the other tiles.</p> <p>After spots have been found on all tiles, duplicate spots are removed in the same way as in the  get reference spots step of the pipeline.</p> <p>We then save details of each spot to the OMP NotebookPage i.e. <code>local_yxz</code>, <code>tile</code>,  <code>colors</code>, <code>gene_no</code>,  <code>intensity</code>, <code>n_neighbours_neg</code> and  <code>n_neighbours_pos</code>.</p>"},{"location":"pipeline/omp/#diagnostics","title":"Diagnostics","text":"<p>As well as <code>view_omp_score</code>, there are a few other functions using matplotlib which may help to  debug this section of the pipeline.</p> <p>The <code>view_codes</code>, <code>view_spot</code>  and <code>view_intensity</code>  functions can also be used for OMP spots if the argument <code>method</code> has the value <code>'omp'</code> when calling them.</p>"},{"location":"pipeline/omp/#histogram_score","title":"<code>histogram_score</code>","text":"<p>When <code>histogram_score</code> is run with <code>method = 'omp'</code>, the histogram  of the OMP score, \\(\\gamma_s\\), will be plotted:</p> OMP ScoreAll Plots <p></p> <p></p> <p>The Score Multiplier textbox can then be used to see how the value of \\(\\rho\\) affects the distribution.</p> <p>There is also the option to show the 5 histograms available when  <code>histogram_score</code> is run with <code>method = 'anchor'</code>. These show the distribution of the dot product score, \\(\\Delta_{s0g_s}\\), of each spot, \\(s\\), with gene  \\(g_s=\\) <code>nb.omp.gene_no[s]</code>, if gene \\(g_s\\) was added on the first iteration.  I.e. this is the score the spot would have had if it was a reference spot instead of an OMP spot. Clearly, we see that \\(\\Delta_{s0g_s}\\) tends to be larger than \\(\\gamma_s\\).</p>"},{"location":"pipeline/omp/#histogram_2d_score","title":"<code>histogram_2d_score</code>","text":"<p>The <code>histogram_2d_score</code> function shows the bivariate histogram to  see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score, \\(\\Delta_{s0g_s}\\)  (between the orange \\(\\gamma_s\\) and cyan Dot Product Score line in the above plot):</p> \\(\\rho=0.95\\)\\(\\rho=10\\)Plp1 <p></p> <p></p> <p></p> <p>Like with the <code>histogram_score</code> plot, you can use the textboxes to change the bin spacing, see how \\(\\rho\\) affects the distribution (compare first and second plots to see how a larger \\(\\rho\\) creates two clusters of spots, one at \\(\\Delta=0.4, \\gamma=0.1\\) and one at \\(\\Delta=0.9, \\gamma=0.8\\))  and see the distribution of a single gene (see Plp1 image above).</p>"},{"location":"pipeline/omp/#gene_counts","title":"<code>gene_counts</code>","text":"<p>If the <code>gene_counts</code> function is run and the Notebook contains the  OMP page, then the number of OMP spots with \\(\\gamma_s &gt;\\) <code>omp_score_thresh</code> and  <code>nb.omp.intensity &gt; intensity_thresh</code> assigned to each gene will also be shown:</p> Default Score ThresholdsScore Thresholds = 0 <p></p> <p></p> <p>The default <code>omp_score_thresh</code>, <code>omp_score_multiplier</code> and <code>intensity_thresh</code>  are <code>config['thresholds']['score_omp']</code>, <code>config['thresholds']['score_omp_multiplier']</code> and  <code>config['thresholds']['intensity']</code> respectively.</p> <p>Clearly, when all the score thresholds are set to 0, there are many more OMP spots because we can detect  multiple spots at the same location if they belong to different genes.</p> <p>The Fake Genes plot has nothing to do with the OMP spots.</p>"},{"location":"pipeline/omp/#view_omp","title":"<code>view_omp</code>","text":"<p>This function is useful for seeing all gene coefficients fit in the neighbourhood of a spot.</p>"},{"location":"pipeline/omp/#view_omp_fit","title":"<code>view_omp_fit</code>","text":"<p>This function is useful for seeing how the OMP algorithm  proceeded on the single pixel at the location of the spot.</p> <p>If you right-click on a column, it will run the <code>view_score</code> function to indicate how  the dot product was calculated for that gene at that iteration. For example, right cicking  the third column in the <code>view_omp_fit</code> image below, produces the <code>view_omp_score</code> image indicating how the dot product score for Trp53i11 was calculated on iteration 1.</p> <code>view_omp_fit</code><code>view_score</code><code>view_weight</code><code>view_background</code> <p></p> <p></p> <p></p> <p></p>"},{"location":"pipeline/omp/#view_score","title":"<code>view_score</code>","text":"<p>The <code>view_score</code> is the same as described for reference spots method. However, for the OMP method, \\(\\Delta_s\\) is computed for each gene on each iteration. The calculation of all these different \\(\\Delta_s\\) values can be viewed by changing the gene/iteration in the textboxes.</p> <p>For example, the <code>view_score</code> image above shows the calculation for Trp53i11 on iteration 1, but if you wanted  to know why Pde1a was not fit on iteration 1, you can type in Pde1a or 42 in the Gene textbox:</p> <p></p> <p>To get back to the best gene on any iteration, just type in any invalid number/letter into the Gene textbox.</p> <p>After iteration 0, once actual genes have been fit, the role of the weighting becomes  much more important. If the Weight Squared plot is clicked on,  it will run the <code>view_weight</code> function for the current iteration, as shown in the <code>view_weight</code> image above.</p>"},{"location":"pipeline/omp/#view_weight","title":"<code>view_weight</code>","text":"<p>The <code>view_weight</code> function produces a plot to indicate how the weight squared value for spot \\(s\\),  \\(\\pmb{\\omega}^2_{si}\\), is calculated  from the genes fit prior to iteration \\(i\\).</p> <p>The <code>view_weight</code> image above shows how the background and Snca gene combine to produce \\(\\pmb{\\omega}^2_{si}\\) on iteration 1. Clearly, the rounds/channels where Snca is strong have an  extremely low weight, as does channel 0 where the strongest background has been fit.</p> <p>The \\(\\alpha\\) and \\(\\beta\\) textboxes can be used to see how these parameters affect the final weighting. The iteration textbox can be used to see how the weighting and gene coefficients (\\(\\mu\\) values in titles of bottom row plots) change once more or less genes have been added.</p> <p>If the Background plot is clicked on, it will run the <code>view_background</code> function, as shown  in the <code>view_background</code> image above.</p>"},{"location":"pipeline/omp/#view_background","title":"<code>view_background</code>","text":"<p>The background calculation and <code>view_background</code> function are exactly the same as  described for reference spots method.</p>"},{"location":"pipeline/omp/#psuedocode","title":"Psuedocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline. There is more detailed pseudocode about how OMP was run on each pixel and how the  <code>spot_shape</code> was found.</p> <pre><code>color_norm_factor: nb.call_spots.color_norm_factor\n    [n_rounds x n_channels]\nt_most_spots: tile with most spots found on it \n    in the call_reference_spots function.\ntile_sz: nb.basic_info.tile_sz\nnz: nb.basic_info.nz\n\nAlter use_tiles so t_most_spots will be the first tile.\n\nfor t in use_tiles:\n    for z in use_z:\n        Load in pixel_colors of all pixels on tile t, z-plane z.\n            [n_pixels_z x n_rounds x n_channels]\n        Normalise pixel_colors\n            pixel_colors = pixel_colors / color_norm_factor\n        Compute pixel_intensity from abs(pixel_colors)\n            [n_pixels_z]\n        Only keep pixels with intensity above threshold\n            pixel_colors = pixel_colors[pixel_intensity &gt; \n                           initial_intensity_thresh\n            [n_keep x n_rounds x n_channels]\n        for s in range(n_keep):\n            Remove background from pixel_colors[s]\n\n            Run OMP algorithm to obtain coefficient of each gene.\n            Save coefficient for spot s, gene g as pixel_coefs_z[s, g]\n            [n_keep x n_genes]\n    Concatenate all pixel_coefs_z arrays into one big pixel_coefs array\n        containing all pixels across all z-planes.\n        [n_pixels x n_genes]\n\n    If t == use_tiles[0]:\n        Compute spot_shape from pixel_coefs on first tile if not provided.\n        Save spot_shape to output directory.\n\n    for g in range(n_genes):\n        Convert pixel_coefs[:, g] into coef_image\n            [tile_sz x tile_sz x nz]\n        Find yxz coordinates of local maxima in coef_image\n            [n_spots_g x 3]\n        Using spot_shape, find n_neighbours_pos\n            [n_spots_g]\n        Using spot_shape, find n_neighbours_neg\n            [n_spots_g]\n    Combine spots on all genes to create\n        spot_info_t [n_spots_t x 7]\n        Where:\n            spot_info_t[s, :3] are the yxz coordinates of spot s.\n            spot_info_t[s, 3] is the gene assigned to spot s.\n            spot_info_t[s, 4] is n_neighbours_pos for spot s.\n            spot_info_t[s, 5] is n_neighbours_neg for spot s.\n            spot_info_t[:, 6] = t\n\n    Threshold based on n_neighbours_pos so only keep spots for which\n        n_neighbours_pos &gt; n_pos_thresh:\n            spot_info_t = spot_info_t[spot_info_t[s, 4]&gt;n_pos_thresh, :]\n            [n_save_t x 7]\n\n    Save spot_info_t to output directory\n\nLoad in spot_info_t from all tiles and combine into large spot_info array\n    [n_spots x 7]\n\nRemove duplicate spots from spot_info\n    [n_non_duplicate x 7]\n\nLoad in spot_colors corresponding to the spots with yxz coordinates given \n    by spot_info[:, :3]\n    [n_non_duplicate x n_rounds x n_channels]\n\nSave spot_info to Notebook:\n    nb.omp.local_yxz =          spot_info[:, :3]\n    nb.omp.gene_no =            spot_info[:, 3] \n    nb.omp.n_neighbours_pos =   spot_info[:, 4] \n    nb.omp.n_neighbours_neg =   spot_info[:, 5] \n    nb.omp.tile =               spot_info[:, 6]  \n    nb.omp.colors =             spot_colors          \n</code></pre>"},{"location":"pipeline/overview/","title":"Pipeline","text":"<p>Here we outline in detail, how each part of the pipeline works including what each parameter in the configuration file is used for. Some useful plots  that can be run for debugging purposes or to help understand what is going on are indicated. Potential errors that can be hit during each stage of the pipeline are also mentioned.</p> <p>Instructions on how to re-run a section of the pipeline are given here.</p> <p>A brief description on what each step of the pipeline does is given below:</p> Times <p>The times taken for a 3D experiment with 4 tiles, 7 rounds,  7 channels and tiles of dimension \\(2048\\times 2048\\times 50\\) are given for each section.  There are three sets of profiling results:</p> <ul> <li>One for the whole pipeline, but with code that is a bit outdated.</li> <li>One with up to date code, excluding the extract and filter step.</li> <li>One with up to date code,  excluding the extract and filter step and not using the optimised code.</li> </ul> <p>These times were obtained on an M1 2021 Macbook Pro. Because the nd2  library does not work on this computer, the nd2reader  library was used instead, hence the time taken for the extract and filter stage may not be accurate.</p> <p>Note that nd2reader does not work for QuadCam data, hence we use the nd2 library.</p> <ul> <li> <p>Extract and Filter: This loads in an image from the  input directory for each tile,  round and channel. It then filters each one and saves them as npy files in the tile directory.</p> <p>Time: 57 minutes</p> </li> <li> <p>Find Spots: For each tile, round and channel, this loads in the filtered image from the tile directory. It then detects spots  on each one and saves the \\(yxz\\) coordinates of each spot to the Notebook. This now gives us a point cloud  for each tile, round and channel.</p> <p>Time: 7 minutes, 10 seconds Time (not optimised): 61 minutes, 50 seconds</p> </li> <li> <p>Stitching: This takes the point clouds on neighbouring tiles of the  reference round / reference channel and uses them to find the overlap between the tiles. After doing this for all sets of neighbouring tiles, we obtain a \\(yxz\\) origin coordinate (bottom left corner) for each tile such that a global coordinate system is created (i.e. given a coordinate on a tile, we can obtain the global coordinate by adding the origin coordinate of that tile).</p> <p>Time: 43 seconds (14 seconds per shift)</p> </li> <li> <p>Register Initial: For each tile, this finds the shift between the reference round / reference channel and  each sequencing round through an exhaustive search using the point clouds.  In total, \\(n_{tiles} \\times n_{rounds}\\) shifts are found.</p> <p>Time: 5 minutes, 48 seconds (12.4 seconds per shift)</p> </li> <li> <p>Register: This takes the shifts found in the Register Initial step and uses them as a starting point to determine the affine transform between the  reference round / reference channel and each sequencing round and channel for each tile. This is done through an iterative closest point  algorithm (ICP) and in total, \\(n_{tiles} \\times n_{rounds} \\times n_{channels}\\) transforms are found.</p> <p>Time: 20 seconds</p> </li> <li> <p>Get Reference Spots: For each spot detected on the  reference round / reference channel,  this uses the transforms obtained in the Register step to determine the corresponding  coordinate in each sequencing round and channel. For each sequencing round and chanel, it then loads in the filtered image from the tile directory and reads off the  intensity at the computed coordinate. This gives a \\(n_{rounds}\\times n_{channels}\\) spot color for each reference spot.</p> <p>Time: 3 minutes, 18 seconds Time (not optimised): 3 minutes, 18 seconds </p> </li> <li> <p>Call Reference Spots: For each gene, a \\(n_{rounds}\\times n_{channels}\\) bled code is obtained which indicates what the spot color of spots assigned to that gene should look like. For each spot, we determine which gene it corresponds to by computing a dot product  between its spot color and each gene bled code. The spot is assigned to the gene for which this dot product is the largest.</p> <p>Time: 16 seconds Time (not optimised): 30 seconds </p> </li> <li> <p>OMP: For each pixel in the global coordinates, the spot color is obtained. Then multiple  gene bled codes are fit until the residual spot color cannot be explained by any  further genes. For each  gene that is fit, we obtain a coefficient indicating how much of that gene's bled code is required to explain the  spot color. Once this is done for all pixels, we have a coefficient for each gene at each pixel. This allows us to build a coefficient image for each gene.  Spots are then detected on these images. This gives us a second distribution of genes which allows for  overlapping spots and spots at locations not detected in the  reference round / reference channel.</p> <p>Time: 1 hour, 16 minutes Time (not optimised): 20 hours, 53 minutes </p> </li> </ul> <p>Total Time: 2 hours, 33 minutes Total Time (not optimised): 23 hours, 6 minutes</p> Order of Steps <p>The results of the Stitching part of the pipeline are first used in the  Get Reference Spots step. Thus, the Stitching part can actually be run anywhere between the Find Spots and Get Reference Spots steps.</p> <p>All other steps must be run in the order indicated.</p>"},{"location":"pipeline/register/","title":"Register","text":"<p>The register step of the pipeline finds an affine transform between the reference round/reference channel (\\(r_{ref}\\)/\\(c_{ref}\\)) and each imaging round and channel for every tile. The affine transform to tile \\(t\\), round \\(r\\), channel \\(c\\) is found through iterative closest point using the shift, <code>nb.register_initial.shift[t, r]</code>, found during the register_initial step of  the pipeline as the starting point. It is saved to the Notebook as <code>nb.register.transform[t, r, c]</code>. These transforms are used later in the pipeline to  determine the intensity of a pixel in each round and channel.</p> <p>The <code>register</code> and <code>register_debug</code> NotebookPages are added to the Notebook after this stage is finished.</p>"},{"location":"pipeline/register/#affine-transform","title":"Affine Transform","text":"<p>We want to find the transform from tile \\(t\\), round \\(r_{ref}\\), channel \\(c_{ref}\\) to round \\(r\\), channel \\(c\\) for all tiles, rounds and channels such that there is pixel-level alignment between the images. The pixel-level  alignment is important because most spots are only a few pixels in size, so even a one-pixel registration error  can compromise the <code>spot_colors</code> found in the next stage of the pipeline  and thus the gene assignment.</p> <p>The shifts found in the register_initial step of  the pipeline are not sufficient for this, because of chromatic aberration which will cause a scaling between  color channels. There may also be small rotational or non-rigid shifts; thus we find affine transformations  which can include shifts, scalings, rotations and shears.</p>"},{"location":"pipeline/register/#starting-transform","title":"Starting Transform","text":"<p>The affine transforms are found using the iterative-closest point  (ICP) algorithm. This is highly sensitive to local maxima, so it is initialized with the shifts found in the  register_initial step, <code>nb.register_initial.shift</code>.</p> <p>The starting transform to a particular round and tile is the same for all channels. The shifts are put into the form of an affine transform (\\(4 \\times 3\\) array) using the  <code>transform_from_scale_shift</code> function.</p> Starting transform from shifts <p>The code below indicates how the initial shifts (\\(n_{tiles} \\times n_{rounds} \\times 3\\) array)  are converted into initial transforms (\\(n_{tiles} \\times n_{rounds} \\times n_{channels} \\times 4 \\times 3\\) array).</p> CodeOutput <pre><code>import numpy as np\nfrom coppafish.register.base import transform_from_scale_shift\nt_print = 1\nr_print = 1\ninitial_shifts = nb.register_initial.shift   # z shift is in z-pixel units\nprint(f\"Initial shift for tile {t_print}, round {r_print}:\\n{initial_shifts[t_print, r_print]}\")\n\n# Convert z shift into same units as yx pixels\ninitial_shifts = initial_shifts.astype(float)\nz_scale = [1, 1, nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy]\nfor t in nb.basic_info.use_tiles:\n    for r in nb.basic_info.use_rounds:\n        initial_shifts[t, r] = initial_shifts[t, r] * z_scale\nprint(f\"Initial shift for tile {t_print}, round {r_print} (z shift in YX pixels):\\n\"\n      f\"{np.around(initial_shifts[t_print, r_print], 2)}\")\n\n# Convert shifts to affine transform 4 x 3 form\ninitial_scale = np.ones((nb.basic_info.n_channels, 3))  # set all scalings to 1 initially \n                                                        # as just want shift.\ninitial_transforms = transform_from_scale_shift(initial_scale, initial_shifts)\n\n# Show transform different for rounds but same for channel within round\nfor r in range(2):\n    for c in range(2):\n        print(f\"Initial transform for tile {t_print}, round {r}, channel {c}:\\n\"\n              f\"{np.around(initial_transforms[t_print, r, c], 2)}\")\n</code></pre> <pre><code>Initial Shifts for tile 1, round 1:\n[25 14  1]\nInitial shift for tile 1, round 1 (z shift in YX pixels):\n[25.   14.    5.99]\nInitial transform for tile 1, round 0, channel 0:\n[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [48. 30.  0.]]\nInitial transform for tile 1, round 0, channel 1:\n[[ 1.  0.  0.]\n [ 0.  1.  0.]\n [ 0.  0.  1.]\n [48. 30.  0.]]\nInitial transform for tile 1, round 1, channel 0:\n[[ 1.    0.    0.  ]\n [ 0.    1.    0.  ]\n [ 0.    0.    1.  ]\n [25.   14.    5.99]]\nInitial transform for tile 1, round 1, channel 1:\n[[ 1.    0.    0.  ]\n [ 0.    1.    0.  ]\n [ 0.    0.    1.  ]\n [25.   14.    5.99]]\n</code></pre>"},{"location":"pipeline/register/#icp","title":"ICP","text":"<p>The pseudocode for the ICP algorithm to  find the affine transform for tile \\(t\\) round \\(r\\), channel \\(c\\) is indicated below. The shape of the various arrays are indicated in the comments (#).</p> Preparing point clouds <p>Prior to the ICP algorithm, the \\(yxz\\) coordinates of spots on a tile are centered. This is because, it makes more sense to me, if any rotation is applied around  the centre of the tile.</p> <p>Also, like for the initial shifts, the z-coordinates must be converted into units of yx-pixels, so overall:</p> <pre><code>spot_yxz = spot_yxz - nb.basic_info.tile_centre  # center coordinates\n# Convert units of z-coordinates\nz_scale = [1, 1, nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy]\nspot_yxz = spot_yxz * z_scale\n</code></pre> <p>For the non-reference point clouds we only keep spots which are isolated. This  is because the ICP algorithm is less likely to fall into local maxima if  the spots are quite well separated.</p> <p>We deam a spot isolated  if the nearest spot to it is further away than <code>2 * neighb_dist</code>. Where <code>neighb_dist = config['register']['neighb_dist_thresh_2d']</code> if 2D and <code>config['register']['neighb_dist_thresh_3d']</code> if 3D. </p> <p><code>n_iter</code> is the maximum number of iterations and is set by <code>config['register']['n_iter']</code>.  <code>neighb_dist_thresh</code> is the distance in \\(yx\\) pixels below which neighbours are a good match.  It is given by <code>config['register']['neighb_dist_thresh_2d']</code> if 2D and <code>config['register']['neighb_dist_thresh_3d']</code> if 3D.  Only neighbours which are closer than this are used when computing the transform.</p> Padding <code>ref_spot_yxz</code> <p><code>ref_spot_yxz</code> has shape <code>[n_ref x 4]</code> not <code>[n_ref x 3]</code> because to be able to be multiplied by  the affine transform (<code>[4 x 3]</code>), it must be padded with ones i.e. <code>ref_spot_yxz[:, 3] = 1</code>.</p> <p><code>ref_spot_yxz @ transform</code> is then the same as <code>ref_spot_yxz[:, :3] @ transform[:3, :] + transform[3]</code> i.e. rotation/scaling matrix applied and then the shift is added.</p> <pre><code>spot_yxz = yxz coordinates for spots detected on tile t,\n           round r, channel c.  # [n_base x 3]\nref_spot_yxz = padded yxz coordinates for spots detected on tile t,\n               reference round, reference channel.  # [n_ref x 4]                    \ntransform_initial = starting transform for tile t, round r, channel c.\n\ntransform = transform_initial  # [4 x 3]\nneighb_ind = zeros(n_ref)      # [n_ref]\nneighb_ind_last = neighb_ind   # [n_ref]\ni = 0\nwhile i &lt; n_iter:\n    1. Transform ref_spot_yxz according to transform to give \n       ref_spot_yxz_transform. # [n_ref x 3]\n       This is just matrix multiplication.\n\n    2. neighb_ind[s] is index of spot in spot_yxz closest to \n       ref_spot_yxz_transform[s].\n\n    3. dist[s] is the distance between spot_yxz[neighb_ind[s]]\n       and ref_spot_yxz_transform[s]  # [n_ref]\n\n    4. Choose which spots to use to update the transform:\n        use = dist &lt; neighb_dist_thresh\n        spot_yxz_use = spot_yxz[neighb_ind[use]]  # [n_use x 3]\n        ref_spot_yxz_use = ref_spot_yxz[use]  # [n_use x 4]\n\n    5. Update transform to be the 4x3 matrix which multiplies ref_spot_yxz_use\n       such that the distances between the transformed spots \n       and spot_yxz_use are minimised. This is the least squares solution.\n\n    6. If neighb_ind and neighb_ind_last are identical, then stop iteration\n       i.e. i = n_iter.\n\n    7. neighb_ind_last = neighb_ind\n\n    8. i = i + 1\n</code></pre>"},{"location":"pipeline/register/#checking-icp-results","title":"Checking ICP results","text":"<p>Once the ICP algorithm has obtained a transform, <code>transform[t, r, c]</code> for each tile, round and channel,  we want to determine if they are acceptable. Three criteria must be satisfied for  <code>transform[t, r, c]</code> to be considered acceptable:</p> <ul> <li>Number of matches exceeds threshold</li> <li>Diagonal elements of <code>transform[t, r, c]</code> are near to what we broadly expect them to be.</li> <li>Shift part of <code>transform[t, r, c]</code> i.e. <code>transform[t, r, c][3]</code> is near to what we broadly expect it to be.</li> </ul>"},{"location":"pipeline/register/#number-of-matches","title":"Number of matches","text":"<p>The number of matches, <code>n_matches[t, r, c]</code>,  are the number of pairs of spots used to compute <code>transform[t, r, c]</code>, i.e.  <code>n_use</code> in the above pseudocode, and thus it depends on the <code>neighb_dist_thresh</code> parameter.  The number of matches are saved to the Notebook as <code>nb.register_debug.n_matches[t, r, c]</code>.</p> <p><code>transform[t, r, c]</code> is deemed to have failed if <code>n_matches[t, r, c] &lt; n_matches_thresh[t, r, c]</code>. <code>n_matches_thresh[t, r, c]</code> is computed as follows:</p> <pre><code># n_ref[t] - number of spots detected on tile t,\n#            reference round, reference channel\n# n_base[t, r, c] - number of spots detected on tile t, \n#                   round r, channel c\n\n# Set threshold to be fraction of max possible n_matches\nn_matches_thresh[t, r, c] = thresh_fract * min(n_ref[t], n_base[t, r, c])\n\n# Clip threshold between min and max\nif n_matches_thresh[t, r, c] &lt; thresh_min:\n    n_matches_thresh[t, r, c] = thresh_min\nif n_matches_thresh[t, r, c] &gt; thresh_max:\n    n_matches_thresh[t, r, c] = thresh_max\n</code></pre> <p>Where:</p> <ul> <li><code>thresh_fract = config['register']['matches_thresh_fract]</code>. Default: 0.25</li> <li><code>thresh_min = config['register']['matches_thresh_min]</code>. Default: 25</li> <li><code>thresh_max = config['register']['matches_thresh_max]</code>. Default: 300</li> </ul> <p>The default are parameters are such that in most cases, <code>n_matches_thresh[t, r, c] = thresh_max</code> so  we require more than 300 matches for a transform to be acceptable.  The thresholds are saved to the Notebook as <code>nb.register_debug.n_matches_thresh[t, r, c]</code>.</p>"},{"location":"pipeline/register/#chromatic-aberration","title":"Chromatic Aberration","text":"<p>Our expectation of the transforms found is that there should be a scaling factor to  correct for chromatic aberration between color channels. We do not expect significant scaling between rounds or tiles though.</p> <p>Thus, we can compute the expected scaling for each transform to channel \\(c\\) by averaging over all tiles  and rounds. We also avoid transforms which have failed based on their matches.</p> CodeOutput <pre><code># transforms: [n_tiles x n_rounds x n_channels x 4 x 3]\nimport numpy as np\nfailed = n_matches &lt; n_matches_thresh   # [n_tiles x n_rounds x n_channels]\nscaling = transforms[:, :, :, np.arange(3), np.arange(3)]  # [n_tiles x n_rounds x n_channels x 3]\nscaling = np.moveaxis(scaling, -1, 0)  # [3 x n_tiles x n_rounds x n_channels]\nfailed = np.expand_dims(failed, 0).repeat(3, 0)  # [3 x n_tiles x n_riounds x n_channels]\nscaling[failed] = np.nan  # don't use failed t/r/c in average\n\n# Use median to take average as to avoid outlier values\nav_scaling = np.nanmedian(scaling, axis=[1, 2])  # average over tiles and rounds\nav_scaling = np.moveaxis(av_scaling, 0, -1)      # [n_channels x 3]\nprint(av_scaling)\n</code></pre> <pre><code>[[0.99863349 0.99863439 1.        ]\n [1.001767   1.00164772 1.        ]\n [1.00026354 1.00017105 1.        ]\n [1.00129138 1.00123909 1.        ]\n [1.00002731 0.99993372 1.        ]\n [0.99826052 0.99822415 1.        ]\n [0.99554262 0.99556903 1.        ]]\n</code></pre> <p>This gives us <code>av_scaling[c, i]</code> which is the scale factor for channel \\(c\\) in dimension \\(i\\) (order is \\(yxz\\)).  This is saved to the Notebook as <code>nb.register_debug.av_scaling</code>.  From the Output above, there is a clear difference between channels, as we expect. Also, the \\(y\\) and \\(x\\) scaling for a particular channel tend to be very similar but the \\(z\\) scaling is always 1. This is because the z-pixels are larger than the yx-pixels so there are no two pixels very close in \\(z\\).</p> <p><code>transform[t, r, c]</code> is deemed to have failed if the scaling is significantly different from <code>av_scaling[c]</code>  as quantified by:</p> <p><code>np.abs(transform[t, r, c][i, i] - av_scaling[c, i]) &gt; scale_thresh[i]</code> for any dimension \\(i\\). </p> <p>Here, <code>scale_thresh</code> is <code>config['register']['scale_dev_thresh']</code>. The default value  of <code>[0.01, 0.01, 0.1]</code> is  intended to be quite hard to exceed i.e. only really awful scaling will fail in this way. The \\(z\\) threshold is larger because if there is scaling in \\(z\\), you would expect larger variance because the \\(z\\)-pixels are larger than the \\(yx\\)-pixels (this may need looking at though, I am not too sure about it, but I think I made it larger because quite a lot of transforms were failing based on the z-scaling).</p>"},{"location":"pipeline/register/#shift","title":"Shift","text":"<p>Our expectation of the transforms found is that there should be a shift between rounds  for a particular tile (in register_initial  we expected the shift to a particular round should be quite similar across tiles - we relax that here). We do not expect significant shifts between channels for the same tile and round though.</p> <p>Thus, we can compute the expected shift for each transform of tile \\(t\\) to round \\(r\\) by averaging over all channels.  We also avoid transforms which have failed based on their matches or  scaling.</p> CodeOutput <pre><code># transforms: [n_tiles x n_rounds x n_channels x 4 x 3]\nimport numpy as np\nfailed = np.logical_or(failed_matches, failed_scale)   # [n_tiles x n_rounds x n_channels]\nshifts = np.moveaxis(transforms[:, :, :, 3], -1, 0)  # [3 x n_tiles x n_rounds x n_channels]\nfailed = np.expand_dims(failed, 0).repeat(3, 0)  # [3 x n_tiles x n_rounds x n_channels]\nshifts[failed] = np.nan  # don't use failed t/r/c in average\n\n# Use median to take average as to avoid outlier values\nav_shifts = np.nanmedian(shifts, axis=3)  # average over channels\nav_shifts = np.moveaxis(av_shifts, 0, -1)      # [n_tiles x n_rounds x 3]\nprint(np.around(av_shifts[:4, :3,:], 2))\n</code></pre> <pre><code>[[[35.79 24.04  0.  ]\n  [14.49  9.23 -0.  ]\n  [-2.25 -2.43  5.99]]\n\n [[47.74 30.01  0.  ]\n  [25.26 13.78  5.99]\n  [10.35  4.43 -0.  ]]\n\n [[52.54 36.04  0.  ]\n  [32.24 21.99  5.99]\n  [16.64 11.6   5.99]]\n\n [[57.75 38.31 -5.99]\n  [38.57 27.41 -0.  ]\n  [26.15 16.23  0.  ]]]\n</code></pre> <p>This gives us <code>av_shifts[t, r, i]</code> which is the average shift for tile \\(t\\), round \\(r\\) in dimension \\(i\\) (order is \\(yxz\\)).  This is saved to the Notebook as <code>nb.register_debug.av_shifts</code>. From the first 4 tiles and 3 rounds printed in  Output above, we see that for a given round, there is significant variance across tiles  (note that the z-shift is in units of \\(yx\\)-pixels, 5.99 in these units is 1 in z-pixels). </p> Systematic shift between tiles <p>In the above Output, there seems to be a systematic shift between tiles:</p> <ul> <li>The \\(y\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 11.</li> <li>The \\(x\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 6.</li> <li>The \\(y\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 6.</li> <li>The \\(x\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 7.</li> <li>The \\(y\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5.</li> <li>The \\(x\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5.</li> <li>The \\(z\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts - 5.99.</li> </ul> <p>I am not sure if this is seen in all experiments but if it is, it may be useful to  incorporate it into the code, both here and in the register initial section.</p> <p><code>transform[t, r, c]</code> is deemed to have failed if the shift is significantly different from <code>av_shifts[t, r]</code>  as quantified by:</p> <p><code>np.abs(transform[t, r, c][3, i] - av_shifts[t, r, i]) &gt; shift_thresh[i]</code> for any dimension \\(i\\). </p> <p>Here, <code>shift_thresh</code> is <code>config['register']['shift_dev_thresh']</code>. The default value  of <code>[15, 15, 5]</code> is  intended to be quite hard to exceed i.e. only really awful shifts will fail in this way.  <code>config['register']['shift_dev_thresh'][2]</code> is in units of z-pixels and is converted to \\(yx\\)-pixels before applying the thresholding (the default value of 5 will become 29.95 for our examples).</p>"},{"location":"pipeline/register/#error-average-transform","title":"Error - average transform","text":"<p>When computing <code>av_shifts[t, r]</code>,  we require that for tile \\(t\\), round \\(r\\), at least one channel has not failed based on matches or scaling. If they have all failed,  it cannot be computed and an error will be raised indicating the problematic tile/round pairs.</p> <p>If this error occurs, it is probably worth using the diagnostics to see why registration produces few matches or bad scaling for these tile/round pairs. If it seems to be a single tile or round that is the problem, it may be worth removing it from  <code>use_tiles</code>/<code>use_rounds</code> and re-running.</p> <p>When computing <code>av_scaling[c]</code>,  we require that for channel \\(c\\), at least one round and tile has not failed based on matches. If they have all failed,  it cannot be computed and an error will be raised indicating the problematic channels.</p> <p>As \\(n_{tiles} \\times n_{rounds} &gt; n_{channels}\\), this error is less likely to occur, but if it does, it  is probably worth removing the indicated channels from <code>use_channels</code> and  re-running.</p>"},{"location":"pipeline/register/#regularized-icp","title":"Regularized ICP","text":"<p>For the transforms which failed (those for which <code>nb.register_debug.failed[t, r, c] == True</code>), we save the failed transform as <code>nb.register_debug.transform_outlier[t, r, c]</code>. We then re-run ICP to compute a new transform. This time, though we want to ensure that <code>transform[t, r, c]</code> is pretty close to the <code>av_transform[t, r, c]</code>.</p> <code>av_transform</code> <p><code>av_transform[t, r, c]</code> is the expected transform for tile \\(t\\), round \\(r\\), channel \\(c\\). It is assumed to have no rotation, a scaling consistent with acceptable transforms to channel \\(c\\) and a shift consistent with acceptable transforms for tile \\(t\\) to round \\(r\\).</p> <p>The Output below shows the average transform for tile 1, round 0, channel 0 using the  <code>av_scaling</code> and <code>av_shifts</code> produced in the earlier sections.</p> CodeOutput <pre><code>av_transform = np.zeros((n_tiles, n_rounds, n_channels, 4, 3))\nfor t in use_tiles:\n    for r in use_rounds:\n        for c in use_channels:\n            av_transform[t, r, c, 3] = av_shifts[t, r]\n            for i in range(3):\n                av_transform[t, r, c, i, i] = av_scaling[c, i]\nprint(np.around(av_transform[1, 0, 0], 5))\n</code></pre> <pre><code>[[ 0.99863  0.       0.     ]\n [ 0.       0.99863  0.     ]\n [ 0.       0.       1.     ]\n [47.73948 30.00533  0.     ]]\n</code></pre> <p>To do this, we set the initial transform for <code>transform[t, r, c]</code> to be <code>av_transform[t, r, c]</code>.  The ICP algorithm is then the same as in the pseudocode except for section 5. For normal least squares, we are finding the \\(4\\times3\\) transform \\(M\\),  such that the following loss function is minimised:</p> \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 = \\sum_{s=0}^{n_{neighb}-1} (y_s - x_sM)^2 \\] <p>Where:</p> <ul> <li>\\(n_{neighb}\\) is <code>n_use</code> introduced in step 4 of the pseudocode.</li> <li>\\(y_s\\) is <code>spot_yxz_use[s]</code> introduced in step 4 of the pseudocode (\\(n_{neighb} \\times 3\\))</li> <li>\\(x_s\\) is <code>ref_spot_yxz_use[s]</code> introduced in step 4 of the pseudocode (\\(n_{neighb} \\times 4\\))</li> </ul> <p>To keep \\(M\\) close to the average transform, \\(M_a\\), we find \\(M\\) through regularized least squares which minimises the following loss function:</p> \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2) \\] <p>Where:</p> <ul> <li>\\(D_{scale}^2 = \\sum_{i=0}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2\\) is the squared distance between <code>transform[:3, :]</code> and <code>av_transform[:3, :]</code> i.e. between the rotation/scaling \\(3\\times3\\) matrix part of the transforms.</li> <li>\\(D_{shift}^2 = \\sum_{j=0}^2(M_{3j} - M_{a_{3j}})^2\\) is the squared distance between <code>transform[3]</code> and <code>av_transform[3]</code> i.e. between the shift part of the transforms.</li> <li>\\(\\lambda\\) is <code>config['register']['regularize_constant']</code>. Default: 500</li> <li>\\(\\mu\\) is <code>config['register']['regularize_factor']</code>. Default: \\(5\\times10^4\\)</li> </ul> Implementing regularized least squares in python <p>Lets suppose we are doing normal least squares with \\(n_{neighb} = 4\\),</p> \\[ x = \\begin{pmatrix}     \\lambda_0 &amp; 0 &amp; 0 &amp; 0\\\\     0 &amp; \\lambda_0 &amp; 0 &amp; 0\\\\     0 &amp; 0 &amp; \\lambda_0 &amp; 0\\\\     0 &amp; 0 &amp; 0 &amp; \\lambda_1     \\end{pmatrix}, \\] \\[ y = xM_a   = \\begin{pmatrix}     \\lambda_0M_{a_{00}} &amp; 0 &amp; 0\\\\     0 &amp; \\lambda_0M_{a_{11}} &amp; 0\\\\     0 &amp; 0 &amp; \\lambda_0M_{a_{22}}\\\\     \\lambda_1M_{a_{30}} &amp; \\lambda_1M_{a_{31}} &amp; \\lambda_1M_{a_{32}}     \\end{pmatrix} \\] <p>Then the least squares solution for \\(M\\) will be the one which minimises the loss function:</p> \\[ L = \\sum(y - xM)^2 = \\sum x^2(M_a - M)^2 = \\sum_{i=0}^3x_{ii}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2 =  \\lambda_0^2 D_{scale}^2 + \\lambda_1^2 D_{shift}^2 \\] <p>This matches the additional term in the regularised least squares loss function if:</p> \\[ \\lambda_0 = \\sqrt{0.5\\lambda\\mu} \\] \\[ \\lambda_1 = \\sqrt{0.5\\lambda} \\] <p>Hence in python,  we append to the \\(y\\) array containing <code>spot_yxz_use</code> the form of y here so it now has shape \\((n_{neighb}+4) \\times 3\\). We also append to the \\(x\\) array containing <code>ref_spot_yxz_use</code> the form of x here so it now has shape \\((n_{neighb}+4) \\times 4\\).</p> <p>If we then just do normal least squares (<code>np.linalg.lstsq(x, y)</code>) on these modified \\(y\\) and \\(x\\) arrays, we get the regularized least squares solution for \\(M\\). </p> <p>The intuition for suitable values of \\(\\lambda\\) and \\(\\mu\\) comes from the following:</p> <ul> <li>The desired distance between the shift found and the target shift, \\(D_{shift}\\),  is the equal to the average distance between neighbouring spots (the average of \\(D_s\\)). </li> <li>If \\(\\lambda = n_{neighb}\\), the contribution of the two terms in the loss function will then be equal if \\(\\mu = D_{shift}^2/D_{scale}^2\\) (in this case, regularised term is \\(n_{neighb}D_{shift}^2\\) but \\(D_{shift}^2\\) is average of \\(D_s^2\\) so this is equivalent to \\(\\sum_{s=0}^{n_{neighb}-1} D_{s}^2\\)).</li> <li> <p>A typical value of \\(D_s\\) is 2 (must be below <code>neighb_dist_thresh</code>) and a typical target value of \\(D_{scale}\\) is 0.009 hence \\(\\mu = 2^2/0.009^2 \\approx 5\\times10^4\\). </p> <p>The larger \\(\\mu\\), the more the regularization will affect the scaling/rotation at the expense of the shift.</p> </li> <li> <p>Regularised term dominates the loss function if \\(\\lambda &gt; n_{neighb}\\) so that as  \\(\\lambda \\rightarrow \\infty\\), \\(M \\rightarrow M_a\\). Regularised term has little effect if \\(\\lambda &lt; n_{neighb}\\) such that \\(M\\) tends to the normal least squares solution as \\(\\lambda \\rightarrow 0\\).</p> <p>Hence, the value of \\(\\lambda\\) should be thought of as an \\(n_{neighb}\\) threshold. If there are more neighbours than \\(\\lambda\\) used to determine \\(M\\) then we trust that this is enough to get the correct transform. If there are less than \\(\\lambda\\) neighbours used, we don't think this is enough to trust the transform it would produce freely, so we restrict the value of \\(M\\) produced to be near the expected \\(M_a\\). </p> <p>Typically, 500 neighbours is quite a good value, <code>config['register']['regularize_constant']</code>  should always be larger than <code>config['register']['matches_thresh_max']</code>.</p> </li> </ul>"},{"location":"pipeline/register/#view_icp_reg","title":"<code>view_icp_reg</code>","text":"<p>The function <code>view_icp_reg</code> is also useful for investigating the effect of \\(\\lambda\\) and \\(\\mu\\).</p> <p>It is very similar to <code>view_icp</code> but <code>view_icp_reg(t, r, c, reg_constant=[lambda1, lambda2],  reg_factor=[mu1, mu2])</code> will have blue point clouds corresponding to:</p> <ul> <li>Reference: Reference (\\(r_{ref}\\)/\\(c_{ref}\\)) point cloud for tile \\(t\\) with no transform applied.</li> <li>\\(\\lambda=0\\): Reference point cloud transformed according to transform found with no regularization.</li> <li>\\(\\lambda=\\infty\\): Reference point cloud transformed according to target transform for regularization, \\(M_a\\). This is found from <code>av_scaling[c]</code> and <code>av_shifts[t, r]</code> unless it is provided.</li> <li>\\(\\lambda=\\)<code>lambda1</code>, \\(\\mu=\\)<code>mu1</code>: Reference point cloud transformed according to transform found with  regularized ICP using <code>lambda1</code> and <code>mu1</code> as the regularization parameters.</li> <li>\\(\\lambda=\\)<code>lambda2</code>, \\(\\mu=\\)<code>mu2</code>: Reference point cloud transformed according to transform found with  regularized ICP using <code>lambda2</code> and <code>mu2</code> as the regularization parameters.</li> </ul> <p>If <code>reg_constant</code> and <code>reg_factor</code> are not provided, <code>config['register']['regularize_constant']</code> and <code>config['register']['regularize_factor']</code> will be used.</p> <p>The example below shows the point clouds produced after running <code>view_icp_reg(0, 1, 0, reg_constant=[3e4], reg_factor=[1e6])</code>:</p> No transform\\(\\lambda=0\\)\\(\\lambda=\\infty\\)\\(\\lambda=30000, \\mu=1\\times10^6\\) <p></p> <p></p> <p></p> <p></p> <p>Here we see that all the transforms are visibly different but the \\(\\lambda=30000, \\mu=1\\times10^6\\) case is closer to \\(\\lambda=\\infty\\) than \\(\\lambda=0\\). This makes sense because as shown in the right sidebar  of the plots, the number of matches is about 5600 so \\(\\lambda = 30000 &gt; n_{neighb}\\) so  the final transform should be close to the target transform we are regularising towards.</p> <p>If <code>view_icp_reg</code> is run with <code>plot_residual=True</code>, it produces another plot which indicates how \\(D_{shift}\\) and \\(D_{scale}\\) vary with different values of <code>reg_constant</code> and <code>reg_factor</code>. </p> <p><code>view_icp_reg(nb, 0, 1, 0, reg_constant=np.logspace(0, 7, 8), reg_factor=[5e4]*8, plot_residual=True)</code>  produces the following additional plot:</p> <p></p> <p>Here, each colored marker refers to a different \\(\\lambda/\\mu\\) combination.  The \\(\\lambda=0\\) horizontal line indicates the values with no regularisation and  the \\(n_{matches}\\) line indicates the number of matches found with \\(\\lambda=\\infty\\). </p> <p>This plot nicely shows that for \\(\\lambda &lt; n_{matches}\\), the transform found is pretty close to  the normal least squares solution but for \\(\\lambda &gt; n_{matches}\\), it is closer to the  target transform we are regularising towards.</p> <p><code>view_icp_reg(nb, 0, 1, 0, reg_constant=[500] * 8, reg_factor=np.logspace(2, 9, 8), plot_residual=True)</code>  produces the following additional plot:</p> <p></p> <p>This shows that varying \\(\\mu\\) while keeping \\(\\lambda\\) constant does not affect \\(D_{shift}\\) much  but \\(D_{scale}\\) does decrease as \\(\\mu\\) increases.</p>"},{"location":"pipeline/register/#error-too-few-matches","title":"Error - too few matches","text":"<p>After the call reference spots step,  <code>check_transforms</code>  will be run.</p> <p>This will produce a warning for any tile, round, channel for which</p> <p><code>nb.register_debug.n_matches &lt; nb.register_debug.n_matches_thresh</code></p> <p>An error will be raised if any of the following is satisfied:</p> <ul> <li> <p>For any given channel, the number of transforms with  <code>n_matches &lt; n_matches_thresh</code> exceeds <code>error_fraction</code>.</p> <p>The faulty channels should then be removed from <code>use_channels</code>.</p> </li> <li> <p>For any given tile, the number of transforms with  <code>n_matches &lt; n_matches_thresh</code> exceeds <code>error_fraction</code>.</p> <p>The faulty tiles should then be removed from <code>use_tiles</code>.</p> </li> <li> <p>For any given round, the number of transforms with  <code>n_matches &lt; n_matches_thresh</code> exceeds <code>error_fraction</code>.</p> <p>The faulty rounds should then be removed from <code>use_rounds</code>.</p> </li> </ul> <p><code>error_fraction</code> is given by <code>config['register']['n_transforms_error_fraction</code>].</p>"},{"location":"pipeline/register/#debugging","title":"Debugging","text":"<p>There are a few functions using matplotlib which may help to debug this section of the pipeline.</p>"},{"location":"pipeline/register/#view_affine_shift_info","title":"<code>view_affine_shift_info</code>","text":"<p>The <code>view_affine_shift_info</code> function plots the shift part of the affine transform (<code>nb.register.transform[t, r, c, 3]</code>) to  all tiles of a given round and channel on the same plot (there are 3 plots for each round and channel).</p> <p>It also includes a plot of <code>nb.register_debug.n_matches</code> vs <code>nb.register_debug.error</code> for each round and channel. The error is the root-mean-square distance between neighbours used to compute the transform. Thus, the lower  this value, and the higher \\(n_{matches}\\), the better the transform.</p> Channel 0Channel 2 <p></p> <p></p> <p>The channel shown is changed by scrolling with the mouse and as a sanity check, we do not expect the top two plots to vary much when the channel changes (the round 0 shifts above is quite a good example).</p> <p>The numbers refer to the tile, and they are blue if  <code>nb.register_debug.n_matches[t, r, c] &gt; nb.register_debug.n_matches_thresh[t, r, c]</code>. Otherwise, they are red, as with tile 2 in round 1 and 4 of the channel 2 plots.</p> <p>As with <code>view_register_shift_info</code>, this function  can also be used to show <code>nb.register_debug.transform_outlier[t, r, c, 3]</code> by setting <code>outlier=True</code>, although  the bottom plot will always show <code>nb.register_debug.n_matches</code> vs <code>nb.register_debug.error</code>.</p>"},{"location":"pipeline/register/#scale_box_plots","title":"<code>scale_box_plots</code>","text":"<p>This function produces two or three plots, one for each dimension. In plot \\(i\\), there is a boxplot  for each round and channel of <code>nb.register.transform[:, r, c, i, i]</code>:</p> <p></p> <p>In this plot, we expect for a given channel, the scaling should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any  outlier tiles (white crosses, +) not far from the boxplot). Also, we expect the Scaling - Y and Scaling - X to be very similar. Scaling - Z will likely be different, but it won't be shown if the range of Scaling - Z is less than 0.00001, which usually the case.</p>"},{"location":"pipeline/register/#view_icp","title":"<code>view_icp</code>","text":"<p>This is similar to <code>view_stitch_overlap</code>. <code>view_icp(nb, t, r, c)</code> will always show the local coordinates of the point cloud for tile \\(t\\), round \\(r\\), channel \\(c\\) in red. This is <code>spot_yxz</code> in the psuedocode.</p> <p>There are then buttons to select which reference point cloud for tile \\(t\\), round \\(r_{ref}\\), channel  \\(c_{ref}\\) is plotted in blue:</p> <ul> <li>No transform: This is the reference point cloud with no transform applied (<code>ref_spot_yxz</code> in the psuedocode).</li> <li>Shift: This is the reference point cloud, shifted according to <code>nb.register_initial.shifts[t, r]</code> (<code>ref_spot_yxz_transform</code> computed in the first iteration in the psuedocode)</li> <li>Affine: This is the reference point cloud, transformed according to <code>nb.register.transform[t, r, c]</code> (<code>ref_spot_yxz_transform</code> computed in the last iteration in the psuedocode)</li> </ul> <p>An example is shown below:</p> No transformShiftAffine <p></p> <p></p> <p></p> <p>If regularized ICP was required for the chosen tile/round/channel, an additional button  will be present titled Regularized. The Affine button will then indicate  the reference point cloud, transformed according to <code>nb.register_debug.transform_outlier</code> (i.e. with no regularization). The Regularized button will indicate the reference point cloud, transformed according to  <code>nb.register.transform[t, r, c]</code> (i.e. final transform found with regularization).</p> Can I use <code>view_icp</code> before running the register stage of the pipeline? <p>The <code>view_icp</code> function can be used after the <code>find_spots</code> page has been added to the Notebook. </p> <p>In the case where the Notebook does not have the  <code>register_initial</code> page, the shift will be computed. In the case where the Notebook does not have the  <code>register</code> page, the affine transform will be  computed with no regularization.</p>"},{"location":"pipeline/register/#psuedocode","title":"Psuedocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline. For more detailed pseudocode about how the transform is found, see the ICP section.</p> <pre><code>r_ref = reference round\nc_ref = reference round\nspot_yxz[t, r, c] = yxz coordinates for spots detected on tile t,\n                    round r, channel c.\nfor t in use_tiles:      \n    Center reference point cloud:\n        spot_yxz[t, r_ref, c_ref] = spot_yxz[t, r_ref, c_ref] - tile_centre\n    Convert z coordinate into yx-pixels:\n        spot_yxz[t, r_ref, c_ref][:, 2] = spot_yxz[t, r_ref, c_ref][:, 2] * z_scale\n    for r in use_rounds:            \n        for c in use_channels:\n            Center point cloud:\n                spot_yxz[t, r, c] = spot_yxz[t, r, c] - tile_centre\n            Convert z coordinate into yx-pixels:\n                spot_yxz[t, r, c][:, 2] = spot_yxz[t, r, c][:, 2] * z_scale\n            Only keep spots whose nearest neighbour is far\n                spot_yxz[t, r, c] = spot_yxz[t, r, c][isolated]           \n            Find transform between spot_yxz[t, r_ref, c_ref] and \n                                   spot_yxz[t, r, c] using ICP.\n\nCompute av_scaling and av_shift.\n\ntransforms[t, r, c] is failed if one of the following is satisfied:\n    - n_matches &lt; n_matches_thresh.\n    - transforms[t, r, c][i, i] significantly different to av_scaling[c, i] for any i.\n    - transforms[t, r, c][3, i] significantly different to av_shift[t, r, i] for any i.\n\nFor failed transforms, recompute transform between \n    spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c]\n    using regularized ICP.           \n\nAdd transform to register NotebookPage.\nAdd debugging information to register_debug NotebookPage.      \nAdd register and register_debug NotebookPages to Notebook.  \n</code></pre>"},{"location":"pipeline/register_initial/","title":"Register Initial","text":"<p>The register initial step of the pipeline uses the point clouds added to the Notebook during the <code>find_spots</code> step to find the shift between the reference round and each imaging round for every tile. The \\(yxz\\) shift to tile \\(t\\), round \\(r\\) is saved as <code>nb.register_initial.shift[t, r]</code>. This shift is then used as a starting point for finding the affine transforms in the  register step of the pipeline  to all channels of tile \\(t\\), round \\(r\\).</p> <p>The <code>register_initial</code> NotebookPage is added to the  Notebook after this stage is finished.</p>"},{"location":"pipeline/register_initial/#shift","title":"Shift","text":"<p>The channel used for finding the shifts is specified by \\(c_{shift}\\) = <code>config['register_initial']['shift_channel']</code>. If it is left blank, it will be set to \\(c_{ref}\\) (<code>nb.basic_info.ref_channel</code>). This channel should be one with lots of spots and if an error  is hit, it may be worth re-running with a different value of this parameter. </p> <p>So, for tile \\(t\\), round \\(r\\), we find the shift between \\(r_{ref}\\)/\\(c_{ref}\\) and \\(r\\)/\\(c_{shift}\\).</p> <p>The function to compute these shifts is exactly the  same as the one used in the stitch section of the pipeline and the parameters in the  register initial section of the config file do the same thing as the  corresponding parameters in the stitch section. A few details are different though, as explained below.</p>"},{"location":"pipeline/register_initial/#initial-range","title":"Initial range","text":"<p>The difference to the stitch case is that <code>config['register_initial']['shift_min']</code> and  <code>config['register_initial']['shift_max']</code> are always used. We expect the shift between rounds to be quite  small hence the default values, which perform an exhaustive search centered on 0 in  each direction with a range of 200 in \\(y\\) and \\(x\\) and a range of 6 in \\(z\\).</p>"},{"location":"pipeline/register_initial/#updating-initial-range","title":"Updating initial range","text":"<p>We assume that the shifts to a given round will be approximately the same for all tiles. So, after we have found  at least 3 shifts to a round which have <code>score &gt; score_thresh</code>, we update our initial exhaustive search range to save time for future tiles. See the example  in the stitch section for how the update is performed.</p>"},{"location":"pipeline/register_initial/#amend-low-score-shifts","title":"Amend low score shifts","text":"<p>This is very similar to the stitch case,  but the names of the variables saved to the Notebook are slightly different:</p> <p>After the shifts to all rounds for all tiles have been found, the ones with <code>score &lt; score_thresh</code> are  amended.</p> <p>If for round \\(r\\), tile \\(t\\), the best shift found had a <code>score &lt; score_thresh</code>,  the shift and score are saved in the notebook as <code>nb.register_initial.shift_outlier[t, r]</code> and  <code>nb.register_initial.shift_score_outlier[t, r]</code> respectively.</p> <p>The shift is then re-computed using a new initial exhaustive search range  (saved as <code>nb.register_initial.final_shift_search</code>). This range is computed using the  <code>update_shifts</code> function to centre  it on all the shifts found to round \\(r\\) for which <code>score &gt; score_thresh</code>. For this re-computation, no widening is allowed either. The idea behind this,  is that it will force the shift to be within the range we expect based on the successful shifts.  I.e. a shift with a slightly lower <code>score</code> but with a shift more similar to the successful shifts is probably more reliable than a shift with  a slightly higher <code>score</code> but with a shift significantly different from the successful ones.</p> <p>The new shift and score will be saved in <code>nb.register_initial.shift[t, r]</code> and  <code>nb.register_initial.shift_score[t, r]</code> respectively.</p>"},{"location":"pipeline/register_initial/#error-too-many-bad-shifts","title":"Error - too many bad shifts","text":"<p>After the call reference spots step,  <code>check_shifts_register</code>  will be run.</p> <p>This will produce a warning for any shift found with <code>score &lt; score_thresh</code>.</p> <p>An error will be raised if the fraction of shifts with <code>score &lt; score_thresh</code> exceeds  <code>config['register_initial']['n_shifts_error_fraction']</code>.</p> <p>If this error does occur, it is probably worth looking at the <code>Viewer</code> and  debugging plots to see if the shifts found looks good enough to use as a starting point for the iterative closest point algorithm or if it should be re-run with different configuration file parameters (e.g. different <code>config['register_initial']['shift_channel']</code> corresponding to a channel with more spots, smaller <code>config['register_initial']['shift_step']</code> or larger  <code>config['register_initial']['shift_max_range']</code>). </p>"},{"location":"pipeline/register_initial/#debugging","title":"Debugging","text":"<p>There are a few functions using matplotlib which may help to debug this section of the pipeline.</p> <p>To view how the shift matches up the point clouds, an analogous function to  <code>view_stitch_overlap</code> is <code>view_icp</code>, which is explained  in the next step of the pipeline. The register stage of the pipeline does  not need to have been run to use this function though.</p>"},{"location":"pipeline/register_initial/#view_register_shift_info","title":"<code>view_register_shift_info</code>","text":"<p>The <code>view_register_shift_info</code> function plots the shifts to all tiles of a given round on the same plot (there are 3 plots for each round). This allows you to see if they are similar, as we expect or if there are some outliers.</p> <p>It also includes a plot of <code>score</code> vs <code>score_thresh</code> for each round:</p> <p></p> <p>In this case, all the shifts seem reasonable as the top two plots show quite a small range and the bottom plot shows <code>score &gt; score_thresh</code> for every shift (blue numbers are all above the green line). If a shift had <code>score &lt; score_thresh</code>, it would be shown in red in each of the three plots for that direction.</p> <p>The numbers refer to the tile.</p> Viewing outlier shifts <p>The shifts saved as <code>nb.register_initial.shift_outlier</code> can be viewed by calling <code>view_register_shift_info(nb, True)</code>.</p> <p>The example below shows the difference between the outlier shifts and the final shifts (<code>nb.register_initial.shift</code>) saved for a data set which did not work well.</p> <code>shift_outlier</code><code>shift</code> <p></p> <p></p> <p>Clearly from the top plot, the range of <code>shift</code> is much smaller than the range of <code>shift_outlier</code>, as we expect from the reduced range of the exhaustive search to find these.</p>"},{"location":"pipeline/register_initial/#view_register_search","title":"<code>view_register_search</code>","text":"<p>The <code>view_register_search</code> function is exactly the same as <code>view_stitch_search</code>.</p>"},{"location":"pipeline/register_initial/#pseudocode","title":"Pseudocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline. For more detailed pseudocode about how the best shift is found, see the shift section.</p> <pre><code>r_ref = reference round\nc_ref = reference round\nc_shift = config['register_initial']['shift_channel']\nspot_yxz[t, r, c] = yxz coordinates for spots detected on tile t,\n                    round r, channel c.\nfor r in use_rounds:            \n    for t in use_tiles:\n        Find best_shift between spot_yxz[t, r_ref, c_ref] and \n                                spot_yxz[t, r, c_shift].\n        If 3 or more shifts to round r with score &gt; score_thresh:\n            Update search range around these shifts.   \nAmend shifts with score &lt; score_thresh using new search range for each round.\n\nAdd shifts and debugging info to register_initial NotebookPage.\nAdd register_initial NotebookPage to Notebook.      \n</code></pre>"},{"location":"pipeline/stitch/","title":"Stitch","text":"<p>The stitch step of the pipeline uses the reference point clouds (all tiles of <code>ref_round</code>/<code>ref_channel</code>) added to the Notebook during the <code>find_spots</code> step to find the overlap between neighbouring tiles in the form of shifts. It then uses these shifts to get the origin of each tile in a global coordinate system. The tile origins are saved to the Notebook as <code>nb.stitch.tile_origins</code> and this is the only variable computed in  this section which is used later in the pipeline.</p> <p>The <code>stitch</code> NotebookPage is added to the Notebook after this stage is finished.</p>"},{"location":"pipeline/stitch/#shift","title":"Shift","text":"<p>We need to find the overlap between each pair of neighbouring tiles. To do this, for each tile, we ask whether there is a tile to the north of it, and if there is we  compute the shift between the two. We then ask if there is a tile to the east of it, and if there is we compute the shift between the two.</p> Example <p>For a \\(2\\times3\\) (\\(n_y \\times n_x\\)) grid of tiles, the indices are:</p> <p>| 2  | 1  | 0  |</p> <p>| 5  | 4  | 3  |</p> <p>We consider each tile in turn:</p> <ul> <li>Tile 0 has no tiles to the north or east so we go to tile 1.</li> <li>Tile 1 has a tile to the east (0) so we find the shift between tile 1 and tile 0.</li> <li>Tile 2 has a tile to the east (1) so we find the shift between tile 2 and tile 1.</li> <li>Tile 3 has a tile to the north (0) so we find the shift between tile 3 and tile 0.</li> <li>Tile 4 has a tile to the north (1) so we find the shift between tile 4 and tile 1. Tile 4 also has a tile to the east (3) so we find the shift between tile 4 and tile 3.</li> <li>Tile 5 has a tile to the north (2) so we find the shift between tile 5 and tile 2. Tile 5 also has a tile to the east (4) so we find the shift between tile 5 and tile 4.</li> </ul> <p>We will always be finding the offset of a tile relative to a tile with a smaller index.</p> <p>The tile indices for neighbours for which we find the overlap in the north/south direction are saved as <code>nb.stitch.south_pairs</code>. The shift between tile <code>nb.stitch.south_pairs[i, 0]</code> and tile <code>nb.stitch.south_pairs[i, 1]</code> is saved as <code>nb.stitch.south_shifts[i]</code>.</p>"},{"location":"pipeline/stitch/#initial-range","title":"Initial range","text":"<p>We compute the shift through an exhaustive search in a given range. The initial range used for a tile to the north  can be specified through <code>config['stitch']['shift_south_min']</code> and <code>config['stitch']['shift_south_max']</code>. The range used for a tile to the east can be specified through <code>config['stitch']['shift_west_min']</code> and <code>config['stitch']['shift_west_max']</code>.</p> Confusion between north/south and east/west <p>For finding the shift to a tile in the north, the parameters used in the config file and saved to the  NotebookPage have the <code>south</code> prefix. This is because if tile B is to the north of tile A, the shift applied to tile A to get the correct overlap is to the south (i.e. negative in the y direction).</p> <p>Equally, for finding the shift to a tile in the east, the parameters used in the config file and saved to the  NotebookPage have the <code>west</code> prefix. This is because if tile B is to the east of tile A, the shift applied to tile A to get the correct overlap is to the west (i.e. negative in the x direction).</p> <p>The range in the \\(i\\) direction will then be between <code>shift_min[i]</code> and <code>shift_max[i]</code> with a spacing of  <code>config['stitch']['shift_step'][i]</code>.</p> <p>If these are left blank, the range will be computed automatically using <code>config['stitch']['expected_overlap']</code> and <code>config['stitch']['auto_n_shifts']</code>.</p> Example automatic range calculation <p>For an experiment with </p> <ul> <li><code>nb.basic_info.tile_sz = 2048</code></li> <li><code>config['stitch']['expected_overlap'] = 0.1</code></li> <li><code>config['stitch']['auto_n_shifts'] = 20, 20, 1</code></li> <li><code>config['stitch']['shift_step'] = 5, 5, 3</code></li> </ul> <p>the range used for a tile to the north is computed as follows:</p> CodeOutput <pre><code>import numpy as np\nexpected_overlap = config['stitch']['expected_overlap']\nauto_n_shifts = config['stitch']['auto_n_shifts']\nshift_step = config['stitch']['shift_step']\n\nexpected_shift = np.array([-(1 - expected_overlap]) * [tile_sz, 0, 0]).astype(int)\nprint(f\"Expected shift = {expected_shift}\")\nrange_extent = auto_n_shifts * shift_step\nprint(f\"YXZ range extent: {range_extent}\")  \nrange_min = expected_shift - range_extent\nrange_max = expected_shift + range_extent\nprint(f\"YXZ range min: {range_min}\")\nprint(f\"YXZ range max: {range_max}\")\nshifts_y = np.arange(range_min[0], range_max[0] + shift_step[0]/2, shift_step[0])\nprint(f\"Y exhaustive search shifts:\\n{shifts_y}\")\nshifts_x = np.arange(range_min[1], range_max[1] + shift_step[1]/2, shift_step[1])\nprint(f\"X exhaustive search shifts:\\n{shifts_x}\")\nshifts_z = np.arange(range_min[2], range_max[2] + shift_step[2]/2, shift_step[2])\nprint(f\"Z exhaustive search shifts:\\n{shifts_z}\")\n</code></pre> <pre><code>Expected shift = [-1843     0     0]\nYXZ range extent: [100 100   3]\nYXZ range min: [-1943  -100    -3]\nYXZ range max: [-1743   100     3]\nY exhaustive search shifts:\n[-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888\n -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828\n -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768\n -1763 -1758 -1753 -1748 -1743]\nX exhaustive search shifts:\n[-100  -95  -90  -85  -80  -75  -70  -65  -60  -55  -50  -45  -40  -35\n  -30  -25  -20  -15  -10   -5    0    5   10   15   20   25   30   35\n   40   45   50   55   60   65   70   75   80   85   90   95  100]\nZ exhaustive search shifts:\n[-3  0  3]\n</code></pre> <p>For a tile to the east, the calculation is exactly the same except <code>expected shift = [0     -1843     0]</code>.</p> <p>The range used for north/south overlap is saved as <code>nb.stitch.south_start_shift_search</code>.</p>"},{"location":"pipeline/stitch/#obtaining-best-shift","title":"Obtaining best shift","text":"<p>Here is some pseudocode for how we  obtain the best shift between tile 5 and tile 4 from an exhaustive search. The comments (#) give the shape of the indicated array.</p> <pre><code>function find_neighbour_distances(yxz_0, yxz_1):\n    # yxz_0: [n_spots_0 x 3]\n    # yxz_1: [n_spots_1 x 3]\n    For i in range(n_spots_0):\n        find nearest spot in yxz_1 to yxz_0[i] to be the one at index j.\n        distances[i] = distance between yxz_0[i] and yxz_1[j].\n    return distances  # [n_spots_0]\n\nfunction get_score(distances, dist_thresh):\n    # distances: [n_spots]\n    This is a function that basically counts the number of values in \n    distances which are below dist_thresh. \n    I.e. the more close neighbours, the better the shift and thus the \n    score should be larger.\n    The function used in the pipeline returns the float given by: \n        score = sum(exp(-distances ** 2 / (2 * dist_thresh ** 2)))\n        If all values in distances where 0 (perfect), score = n_spots.\n        If all values in distances where infinity or much larger than \n        dist_thresh (bad), score = 0.\n\n\n# tile_5_yxz: [n_spots_t5 x 3]\n# tile_4_yxz: [n_spots_t4 x 3]\n# exhaustive_search: [n_shifts x 3]\n\nfor shift in exhaustive_search:\n    tile_5_yxz_shifted = tile_5_yxz + shift   # [n_spots_t5 x 3]\n    distances = find_neighbour_distances(tile_5_yxz_shifted, \n                                         tile_4_yxz)  # [n_spots_t5]\n    score = get_score(distances, dist_thresh)  # float\nbest_shift is the shift with the best score\n</code></pre>"},{"location":"pipeline/stitch/#score","title":"<code>score</code>","text":"<p>In the score function, the <code>dist_thresh</code> parameter  thus specifies the distance below which neighbours are a good match. It is specified through <code>config['stitch']['neighb_dist_thresh']</code>. </p> <p>The <code>score</code> computed with this function is approximately the number of neighbouring points between the two point clouds with a distance between them less than <code>config['stitch']['neighb_dist_thresh']</code>.</p>"},{"location":"pipeline/stitch/#3d","title":"3D","text":"<p>For speed, rather than considering an exhaustive search in three dimensions, we first ignore any shift in \\(z\\) and  just find the best \\(yx\\) shift.</p> <p>To do this, we split  each 3D point cloud into a number of 2D point clouds.  The number is determined by <code>config['stitch']['nz_collapse']</code> to be:</p> <pre><code>ceil(nb.basic_info.nz / config['stitch']['nz_collapse'])\n</code></pre> <p>We then consider the corresponding point clouds independently.</p> Pseudocode - \\(yx\\) shift <p>Lets consider finding the best yx shift between tile 5 and tile 4 with:</p> <ul> <li><code>nb.basic_info.nz = 50</code></li> <li><code>config['stitch']['nz_collapse'] = 30</code></li> <li><code>config['stitch']['neighb_dist_thresh'] = 2</code></li> </ul> <p>The pseudocode is:</p> <pre><code># tile_5_yxz: [n_spots_t5 x 3]\n# tile_4_yxz: [n_spots_t4 x 3]\n# exhaustive_search_yx: [n_shifts_yx x 2]\n\nn_2d_point_clouds = ceil(50 / 30) = 2 so we need 2 2D point clouds\nsplit tile_5_yxz into tile_5A_yx and tile_5B_yx:\n    - tile_5A_yx are the yx coordinates of every spot in \n      tile_5_yxz with z coordinate between 0 and 24 inclusive.\n      # [n_spots_t5A x 2]\n    - tile_5B_yx are the yx coordinates of every spot in \n      tile_5_yxz with z coordinate between 25 and 49 inclusive.\n      # [n_spots_t5B x 2]\nsplit tile_4_yxz into tile_4A_yx and tile_4B_yx:\n    - tile_4A_yx are the yx coordinates of every spot in \n      tile_4_yxz with z coordinate between 0 and 24 inclusive.\n      # [n_spots_t4A x 2]\n    - tile_4B_yx are the yx coordinates of every spot in \n      tile_4_yxz with z coordinate between 25 and 49 inclusive.\n      # [n_spots_t4B x 2]\n\nfor shift_yx in exhaustive_search_yx:\n    tile_5A_yx_shifted = tile_5A_yx + shift_yx   # [n_spots_t5A x 2]\n    distancesA = find_neighbour_distances(tile_5A_yx_shifted, \n                                          tile_4A_yx)  # [n_spots_t5A]\n    scoreA = get_score(distancesA, 2)  # float\n\n    tile_5B_yx_shifted = tile_5B_yx + shift_yx   # [n_spots_t5B x 2]\n    distances = find_neighbour_distances(tile_5B_yx_shifted, \n                                         tile_4B_yx)  # [n_spots_t5B]\n    scoreB = get_score(distancesB, 2)  # float\n    score = scoreA + scoreB\nbest_shift_yx is the shift_yx with the best score\n</code></pre> <p>Once the best \\(yx\\) shift is found (and after any necessary widening of the range), the shift in \\(z\\) is found by using the full 3D point clouds again but doing just an exhaustive search in \\(z\\). </p> <p>Before this is done, it is important that the \\(z\\) coordinate of the point clouds is in the same unit  as the \\(yx\\) coordinate so distances are computed correctly. The conversion from \\(z\\) pixel units to \\(yx\\) pixel units is achieved by multiplying  the \\(z\\) coordinate by <code>nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy</code>. The \\(z\\) shifts in the  exhaustive search must also be put into \\(yx\\) pixel units.</p> Pseudocode - \\(z\\) shift <p>If the previous example found the best \\(yx\\) shift to be <code>best_shift_yx</code>,  the pseudocode below is what follows  this to find the best \\(yxz\\) shift.</p> <pre><code># tile_5_yxz: [n_spots_t5 x 3]\n# tile_4_yxz: [n_spots_t4 x 3]\n# exhaustive_search_z: [n_shifts_z]\n# best_shift_yx: [2]\n\nshift_yxz = [0, 0, 0]\nThe yx shift is constant in this search, always set to the best \nyx shift we found in the 2D search.\nshift_yxz[0] = best_shift_yx[0]\nshift_yxz[1] = best_shift_yx[1]\nfor shift_z in exhaustive_search_z:\n    shift_yxz[2] = shift_z\n    tile_5_yxz_shifted = tile_5_yxz + shift_yxz   # [n_spots_t5 x 3]\n    distances = find_neighbour_distances(tile_5_yxz_shifted, \n                                         tile_4_yxz)  # [n_spots_t5]\n    score = get_score(distances, 2)  # float\nbest_shift is the shift with the best score\n</code></pre>"},{"location":"pipeline/stitch/#score-threshold","title":"Score Threshold","text":"<p>Once we have found the best shift in the exhaustive search and its score, we need to determine if the score is large enough for us to accept the shift or if we should widen the range. </p> <p>We accept the shift if the score is above a <code>score_thresh</code>. This can either be specified through  <code>config['stitch']['shift_score_thresh']</code> or if this is left empty, it is  computed for each shift. This computation is done after we have found  the best \\(yx\\) shift to be <code>best_shift_yx</code>, as explained by the following pseudocode (also shown with <code>view_stitch_search</code>):</p> <pre><code># exhaustive_search_yx: [n_shifts_yx x 2]\n# best_shift_yx: [2]\n\nshifts_yx_use = all shifts between min_dist and max_dist from \n                best_shift_yx in exhaustive_search_yx  # [n_shifts_use x 2]\nshift_yx_thresh = shift in shifts_yx_use with the max score.\nscore_thresh = the score corresponding to shift_yx_thresh \n               multiplied by thresh_multiplier.\n</code></pre> <p>Where various parameters are specified through the configuration file:</p> <ul> <li><code>min_dist</code>: <code>config['stitch']['shift_score_thresh_min_dist']</code></li> <li><code>max_dist</code>: <code>config['stitch']['shift_score_thresh_max_dist']</code></li> <li><code>thresh_multiplier</code>: <code>config['stitch']['shift_score_thresh_multiplier']</code></li> </ul>"},{"location":"pipeline/stitch/#view_stitch_search","title":"<code>view_stitch_search</code>","text":"<p>A good debugging tool to visualise how the best shift was computed is  <code>view_stitch_search</code>. </p> <p>In 2D, it shows the <code>score</code> for all \\(yx\\) shifts in the exhaustive search:</p> <p></p> <p>White in the colorbar refers to the value of <code>score_thresh</code>. The green x indicates  the shift that was found after the initial exhaustive search. The black x indicates  the final shift found after the refined search. </p> <p>This plot is also useful for understanding the <code>score_thresh</code> computation. The green + indicates the <code>shift_yx_thresh</code>,  the shift with the largest <code>score</code> between the two green circles.  The inner circle has a radius of <code>config['stitch']['shift_score_thresh_min_dist']</code> and  is centered on the green x. The outer circle has a radius of  <code>config['stitch']['shift_score_thresh_max_dist']</code> and is centered on the green x. </p> <p><code>score_thresh</code> is then set to <code>config['stitch']['shift_score_thresh_multiplier']</code>  multiplied by the score at the green +. In this case, this multiplier is 2 and thus the  <code>score</code> at the green + appears blue (<code>score</code> at the green + is approximately 150 and so <code>score_thresh</code> and the white in the image is about 300).</p> <p>We use this method of determining <code>score_thresh</code> because the most striking feature of the plot is the sharp gradient near the global maxima in <code>score</code>. Requiring  the <code>score</code> for an acceptable shift to be much larger than the max nearby <code>score</code> is just saying we require a large gradient.</p> <code>score_thresh</code> in 3D <p>For the 3D pipeline, the <code>score_thresh</code> is computed in exactly the same way  to determine if the initial \\(yx\\) shift found is acceptable or the \\(yx\\) range needs  widening:</p> <p></p> <p>Once an acceptable \\(yx\\) shift has been found, we set <code>score_thresh</code> for the 3D search  to the max score at the \\(yx\\) shift used to find <code>score_thresh</code> in 2D across all z planes searched, multiplied by <code>config['stitch']['shift_score_thresh_multiplier']</code>.</p> <p>I.e. the green + in the Z=0 plot is at the same \\(yx\\) coordinate as the green + in the 2D plot. The maximum score at this \\(yx\\) coordinate across z-shifts between -6 and 6 occurs at Z=0. Thus, we set the <code>score_thresh</code> to be equal to the score at this \\(yxz\\) shift multiplied by  <code>config['stitch']['shift_score_thresh_multiplier']</code>.</p> <p>The <code>score_thresh</code> is smaller when looking for the \\(yxz\\) shift because we expect in 3D, if neighbouring points are on slightly different z-planes but  very close in \\(yx\\) they would have a small contribution to the <code>score</code>.  This is because the \\(z\\) pixels are larger than the \\(yx\\) pixels.  In 2D, a pair such as this would give a large contribution to the <code>score</code> because only the \\(yx\\) distance would matter.</p> <p>See the example in the refined search section to see how a shift of 1 in  \\(z\\) can almost double the score.</p>"},{"location":"pipeline/stitch/#widening-range","title":"Widening range","text":"<p>If the best shift found has <code>score &lt; score_thresh</code>, then exhaustive search will continue  until either a shift with <code>score &gt; score_thresh</code> is found or the search range exceeds a <code>max_range</code>:</p> <pre><code>while best_score &lt; score_thresh:\n    extend exhaustive search\n    if range of exhaustive search &gt; max_range in all dimensions:\n        Return current best_shift and best_score.\n    else:\n        Find best_shift and corresponding best_score in new larger \n        exhaustive_search.\n</code></pre> <p>The <code>max_range</code> in the \\(y\\), \\(x\\), \\(z\\) direction is set to be <code>config['stitch']['shift_max_range']</code>.</p> <p>The exhaustive search is extended  using <code>config['stitch']['shift_widen']</code>. The possible shifts in dimension \\(i\\) are extended by <code>config['stitch']['shift_widen'][i]</code> values either side of the current min/max values of the  shifts while maintaining the same spacing.</p> Example exhaustive search extension <p>Lets consider a single dimension with initial an initial exhaustive search containing the following shifts: <pre><code>array([-10,  -5,   0,   5,  10])\n</code></pre></p> <p>With <code>shift_widen = 10</code> in this dimension, the updated exhaustive search would contain the following shifts: <pre><code>array([-60, -55, -50, -45, -40, -35, -30, -25, -20, -15, -10,  -5,   0,\n         5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60])\n</code></pre></p> <p>This new search has a range of 120 so if <code>max_range</code> in this dimension was 100,  the updated exhaustive search would not take place.</p> <p>An example in 2D where the widening worked successfully is shown below using <code>view_stitch_search</code>:</p> Pre-widenPost-widen <p></p> <p></p> <p>Here we see that before widening the search range, the maximum score is barely distinguishable from  the rest and thus falls below <code>score_thresh</code>. After widening though, we uncover a shift with a score far exceeding <code>score_thresh</code>.</p> 3D <p>In 3D, we do the \\(yx\\) search first and then widen according to  <code>config['stitch']['shift_widen'][:2]</code> and <code>config['stitch']['shift_max_range'][:2]</code> to find the best \\(yx\\) shift.</p> <p>Then we do the \\(z\\) search keeping the \\(yx\\) shift equal to the best \\(yx\\) shift and  widen according to <code>config['stitch']['shift_widen'][2]</code> and <code>config['stitch']['shift_max_range'][2]</code>. This gives us the best \\(yxz\\) shift.</p> <p>The example below shows a case where widening was required in \\(z\\) but not in \\(yx\\):</p> Pre-widenPost-widen <p></p> <p></p>"},{"location":"pipeline/stitch/#refined-search","title":"Refined search","text":"<p>After we have found the best \\(yxz\\) shift from the exhaustive search, we find the  final shift by looking in the neighbourhood of this shift with a  smaller spacing. Initially, we halve the spacing, then we reduce the spacing to 1.</p> Example <p>Lets consider a 3D example where an exhaustive search with \\(yxz\\) spacing <code>[5, 5, 3]</code> found the best \\(yxz\\) shift to be <code>[-1848, 20, 0]</code> with a <code>score</code> of 247.7.</p> <p>The refined search range in this neighbourhood with half the step size is set to: <pre><code>Y shifts: [-1858 -1855 -1852 -1849 -1846 -1843 -1840 -1837]\nX shifts: [10 13 16 19 22 25 28 31]\nZ shifts: [-6 -4 -2  0  2  4  6]\n</code></pre></p> <p>This search finds the best \\(yxz\\) shift to be <code>[-1846, 19, 0]</code> with a <code>score</code> of 298.3.</p> <p>The final search with a spacing of 1 is set to: <pre><code>Y shifts: [-1849 -1848 -1847 -1846 -1845 -1844 -1843]\nX shifts: [16 17 18 19 20 21 22]\nZ shifts: [-2 -1  0  1  2]\n</code></pre></p> <p>This search finds the best \\(yxz\\) shift to be <code>[-1846, 20, 1]</code> with a <code>score</code> of 423.8.</p> <p>This is why the <code>view_stitch_search</code> plot shows more detail near the black x and why the black x is in a  different location from the green x even when no widening is required.</p>"},{"location":"pipeline/stitch/#updating-initial-range","title":"Updating initial range","text":"<p>We assume that all tiles overlapping in the same direction should have approximately the same shift between them. So, after we have found at least 3 shifts in a given direction  which have <code>score &gt; score_thresh</code>, we update  our initial exhaustive search range to save time for future tiles.</p> Example <p>The code below shows how <code>update_shifts</code>  works to refine the initial search  range, given that the following \\(yxz\\) shifts have been found for north/south overlapping tiles (all with <code>score &gt; score_thresh</code>):</p> <ul> <li><code>[-1846, 20, 1]</code></li> <li><code>[-1853, 22, 0]</code></li> <li><code>[-1854, 20, 0]</code></li> </ul> CodeOutput <pre><code>import numpy as np\nfrom coppafish.stitch.shift import update_shifts\ny_shifts_found = [-1846, -1853, -1854]\nx_shifts_found = [20, 22, 0]\nz_shifts_found = [1, 0, 0]\n\nstep = [5, 5, 3]\ny_search = np.arange(-1943, -1743+step[0], step[0])\nx_search = np.arange(-100, 100+step[1], step[1])\nz_search = np.arange(-3, 3 + step[2], step[2])\n\n# y\nprint(f\"Initial y search:\\n{y_search}\")\ny_search_new = update_shifts(y_search, y_shifts_found)\nprint(f\"Updated y search:\\n{y_search_new}\")\n\n# x\nprint(f\"Initial x search:\\n{x_search}\")\nx_search_new = update_shifts(x_search, x_shifts_found)\nprint(f\"Updated x search:\\n{x_search_new}\")\n\n# z\nprint(f\"Initial z search:\\n{z_search}\")\nz_search_new = update_shifts(z_search, z_shifts_found)\nprint(f\"Updated z search:\\n{z_search_new}\")\n\n# Number of shifts to search\nprint(f\"Initial number of shifts in search: \"\n      f\"{y_search.size * x_search.size * z_search.size}\")\nprint(f\"Updated number of shifts in search: \"\n      f\"{y_search_new.size * x_search_new.size * z_search_new.size}\") \n</code></pre> <pre><code>Initial y search:\n[-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888\n -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828\n -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768\n -1763 -1758 -1753 -1748 -1743]\nUpdated y search:\n[-1861 -1856 -1851 -1846 -1841]\nInitial x search:\n[-100  -95  -90  -85  -80  -75  -70  -65  -60  -55  -50  -45  -40  -35\n  -30  -25  -20  -15  -10   -5    0    5   10   15   20   25   30   35\n   40   45   50   55   60   65   70   75   80   85   90   95  100]\nUpdated x search:\n[-1  4  9 14 19 24 29]\nInitial z search:\n[-3  0  3]\nUpdated z search:\n[-3  0  3]\nInitial number of shifts in search: 5043\nUpdated number of shifts in search: 105\n</code></pre>"},{"location":"pipeline/stitch/#amend-low-score-shifts","title":"Amend low score shifts","text":"<p>After all the shifts between neighbouring tiles have been found, the ones with <code>score &lt; score_thresh</code> are  amended.</p> <p>If for a particular pair of overlapping tiles in the north/south direction, the best shift found had a <code>score &lt; score_thresh</code>,  the shift and score are saved in the notebook in <code>nb.stitch.south_outlier_shifts</code> and  <code>nb.stitch.south_outlier_score</code> respectively.</p> <p>The shift is then re-computed using a new initial exhaustive search range  (saved as <code>nb.stitch.south_final_shift_search</code>). This range is computed using the  <code>update_shifts</code> function to centre  it on all the shifts found in the <code>south</code> direction for which <code>score &gt; score_thresh</code>. For this re-computation, no widening is allowed either. The idea behind this  is that it will force the shift to be within the range we expect based on the successful shifts.  I.e. a shift with a slightly lower <code>score</code> but with a shift more similar to the successful shifts is probably more reliable than a shift with  a slightly higher <code>score</code> but with a shift significantly different from the successful ones.</p> <p>The new shift and score will be saved in <code>nb.stitch.south_shifts</code> and  <code>nb.stitch.south_score</code> respectively.</p>"},{"location":"pipeline/stitch/#global-coordinates","title":"Global coordinates","text":"<p>After finding the overlap for the set of neighbouring tile pairs (\\(\\mathcal{R}\\)), we are left with a shift vector \\(\\pmb{\\Delta}_{T_1, T_2}\\) for every pair of neighbouring tiles \\(T_1\\) and \\(T_2\\), that specifies the \\(yxz\\) offsets of tile \\(T_2\\) relative to tile \\(T_1\\). </p> <p>We define a single global coordinate system by finding the coordinate origin \\(\\pmb{\\mathrm{X}}_T\\) (bottom left corner)  for each tile \\(T\\). Note however that this problem is overdetermined as there are more neighbor pairs than there  are tiles. We therefore compute the offsets by minimizing the loss function:</p> \\[ L = \\sum_{(T_1, T_2) \\in \\mathcal{R}} \\pmb{|} \\pmb{\\mathrm{X}}_{T_1} - \\pmb{\\mathrm{X}}_{T_2} - \\pmb{\\Delta}_{T_1, T_2} \\pmb{|}^2 \\] <p>Differentiating this loss function with respect to \\(\\pmb{\\mathrm{X}}_T\\)  yields a set of simultaneous linear equations, whose solution yields the origins of each  tile on the reference round/channel.</p> <p>This procedure is done with the <code>get_tile_origin</code> function, with  the tile origins saved to the Notebook as <code>nb.stitch.tile_origin</code>.</p>"},{"location":"pipeline/stitch/#error-too-many-bad-shifts","title":"Error - too many bad shifts","text":"<p>After the call reference spots step,  <code>check_shifts_stitch</code>  will be run.</p> <p>This will produce a warning for any shift found with <code>score &lt; score_thresh</code>.</p> <p>An error will be raised if the fraction of shifts with <code>score &lt; score_thresh</code> exceeds  <code>config['stitch']['n_shifts_error_fraction']</code>.</p> <p>If this error does occur, it is probably worth looking at the <code>Viewer</code>  and the debugging plots to see if the stitching looks good enough to continue with the rest of the pipeline or if it should be re-run with different configuration file parameters (e.g. smaller <code>config['stitch']['shift_step']</code> or larger <code>config['stitch']['shift_max_range']</code>).</p>"},{"location":"pipeline/stitch/#saving-stitched-images","title":"Saving stitched images","text":"<p>After  the <code>tile_origin</code> has been computed and the <code>stitch</code> NotebookPage has been added to the Notebook,  a stitched image of the <code>ref_round</code>/<code>ref_channel</code> will be saved  to the <code>output_dir</code> as a npz file with the file name <code>nb.file_names.big_anchor_image</code>. </p> <p>To save memory, the stitched reference image will be saved as int16 after rescaling to fill the range. We do this because the image is useful for plotting, but we do not care much about the actual pixel values.</p> DAPI <p>If <code>dapi_channel</code> is specified, a stitched image of the  <code>anchor_round</code>/<code>dapi_channel</code> will be saved  to the <code>output_dir</code> as a npz file with the file name <code>nb.file_names.big_dapi_image</code>.</p> <p>If DAPI tophat filtering was specified, the filtered images save to <code>tile_dir</code> will be loaded in and stitched together. Otherwise, the raw data will be  loading in from the <code>input_dir</code> and stitched together with no filtering.  I.e. <code>from_raw == True</code> in<code>save_stitched</code>.</p> <p>The stitched DAPI image will be saved as uint16 with no rescaling.</p> <p>Also, to save memory, all pixels with absolute value less than <code>config['stitch']['save_image_zero_thresh']</code> will have their pixel value set to \\(0\\) before saving.</p>"},{"location":"pipeline/stitch/#debugging","title":"Debugging","text":"<p>There are a few functions using matplotlib which may help to debug this section of the pipeline.</p>"},{"location":"pipeline/stitch/#view_stitch_shift_info","title":"<code>view_stitch_shift_info</code>","text":"<p>The <code>view_stitch_shift_info</code> function plots the shifts found for all neighbouring tiles in a given direction on the same plot (there are 3 plots for each direction). This allows you to see if they are similar, as we expect or if there are some outliers.</p> <p>It also includes a plot of <code>score</code> vs <code>score_thresh</code> for each pair of neighbouring tiles:</p> <p></p> <p>In this case, all the shifts seem reasonable as the top two plots show quite a small range, and the bottom plot shows <code>score &gt; score_thresh</code> for every shift (blue numbers are all above the green line). If a shift had <code>score &lt; score_thresh</code>, it would be shown in red in each of the three plots for that direction.</p> <p>The numbers refer to the tile with a neighbouring tile to either the north or east of it. This example is for a \\(4\\) (\\(n_y\\)) \\(\\times\\) \\(3\\) (\\(n_x\\)) grid of tiles, so the number \\(1\\) in the West column  of plots refers to the shift found between tile \\(1\\) and tile \\(0\\).  The number \\(3\\) in the South column of plots refers to the shift found between tile \\(3\\) and tile \\(0\\).</p> <p>This function can also be used to view <code>nb.stitch.outlier_shifts</code> by running  <code>view_stitch_shift_info(nb, True)</code>.</p>"},{"location":"pipeline/stitch/#view_stitch_overlap","title":"<code>view_stitch_overlap</code>","text":"<p>For an experiment with tile \\(0\\) to the north of tile \\(1\\),  <code>view_stitch_overlap(nb, 1, 'north')</code> will always show the global coordinates of the point cloud for tile \\(0\\) in red  (<code>global_yxz = local_yxz + nb.stitch.tile_origin[0]</code>).</p> <p>There are then buttons to select which point cloud for tile \\(1\\) is plotted in blue:</p> <ul> <li>No overlap: This is assuming there is \\(0\\%\\) overlap between the two tiles.</li> <li>\\(x\\%\\) overlap: \\(x\\) here will be <code>config['stitch']['expected_overlap']</code>. This is our starting guess, i.e. the expected overlap in \\(y\\) and a shift of 0 in \\(x\\) and \\(z\\).</li> <li>Shift: This is the best shift found, saved in <code>nb.stitch.south_shifts</code>.</li> <li>Final: This is the coordinates of tile \\(1\\) spots in the global coordinate system  (<code>local_yxz + nb.stitch.tile_origin[1]</code>).</li> </ul> <p>An example is shown below:</p> No overlap10% overlapShiftFinal <p></p> <p></p> <p></p> <p></p> <p>The z-plane is changed by scrolling with the mouse. You can change the value of z-thick in the bottom right. Spots detected on the current z-plane and this many z-planes either side of it will be shown.</p> <p>The white lines (only really visible in the Final plot) indicate neighbouring points with  a distance between them of less than or equal to <code>config['stitch']['neighb_dist_thresh']</code>. The number of matches listed on the right is then the number of these white lines (across all z-planes),  this will be similar to the <code>score</code>.</p>"},{"location":"pipeline/stitch/#view_stitch","title":"<code>view_stitch</code>","text":"<p>Another useful function is <code>view_stitch</code>.  This plots all the spots found in the <code>ref_round</code>/<code>ref_channel</code> in the global coordinate system specified by <code>nb.stitch.tile_origin</code>.</p> <p>The example below is for a \\(4\\times3\\) grid of tiles in 2D.</p> FullZoom <p></p> <p></p> <p>The blue spots are duplicate spots (detected on a tile which is not the tile whose centre they are closest to). For each duplicate spot, we expect there is a non-duplicate spot in red, detected on a different tile but with the same global coordinate. We can see this in the Zoom plot, showing the intersection between tile 1 and  tile 2 (indicated by a green box in the Full image).</p> <p>These duplicate spots will be removed in the get reference spots  step of the pipeline, so we don't double count the same spot.</p> <p>The white lines and number of matches are the same as for <code>view_stitch_overlap</code>. Also in 3D, you can scroll between z-planes with the mouse and specify z-thick in the same way.</p>"},{"location":"pipeline/stitch/#pseudocode","title":"Pseudocode","text":"<p>This is the pseudocode outlining the basics of this step of the pipeline. For more detailed pseudocode about how the best shift is found, see the shift section.</p> <pre><code>r_ref = reference round\nc_ref = reference round\nspot_yxz[t, r, c] = yxz coordinates for spots detected on tile t,\n                    round r, channel c.\n\nfor t in use_tiles:\n    if tile to north of t:\n        Find best_shift between spot_yxz[t, r_ref, c_ref] and \n                                spot_yxz[t_north, r_ref, c_ref].\n        If 3 or more north/south shifts with score &gt; score_thresh:\n            Update search range around these shifts.              \n    if tile to east of t:\n        Find best_shift between spot_yxz[t, r_ref, c_ref] and \n                                spot_yxz[t_east, r_ref, c_ref].\n        If 3 or more east/west shifts with score &gt; score_thresh:\n            Update search range around these shifts.       \n\nAmend shifts with score &lt; score_thresh using new search range for \neach direction.\nFind tile_origin specifying global coordinate system.\n\nAdd tile_origin and debugging info to stitch NotebookPage.\nAdd stitch NotebookPage to Notebook.         \nUse tile_origin to save stitched ref_channel (and DAPI) image to \noutput directory.\n</code></pre>"}]}